[
  {
    "id": "1812.11006",
    "title": "TOP-GAN: Label-Free Cancer Cell Classification Using Deep Learning with\n  a Small Training Set",
    "abstract": "  We propose a new deep learning approach for medical imaging that copes with\nthe problem of a small training set, the main bottleneck of deep learning, and\napply it for classification of healthy and cancer cells acquired by\nquantitative phase imaging. The proposed method, called transferring of\npre-trained generative adversarial network (TOP-GAN), is a hybridization\nbetween transfer learning and generative adversarial networks (GANs). Healthy\ncells and cancer cells of different metastatic potential have been imaged by\nlow-coherence off-axis holography. After the acquisition, the optical path\ndelay maps of the cells have been extracted and directly used as an input to\nthe deep networks. In order to cope with the small number of classified images,\nwe have used GANs to train a large number of unclassified images from another\ncell type (sperm cells). After this preliminary training, and after\ntransforming the last layer of the network with new ones, we have designed an\nautomatic classifier for the correct cell type (healthy/primary\ncancer/metastatic cancer) with 90-99% accuracy, although small training sets of\ndown to several images have been used. These results are better in comparison\nto other classic methods that aim at coping with the same problem of a small\ntraining set. We believe that our approach makes the combination of holographic\nmicroscopy and deep learning networks more accessible to the medical field by\nenabling a rapid, automatic and accurate classification in stain-free imaging\nflow cytometry. Furthermore, our approach is expected to be applicable to many\nother medical image classification tasks, suffering from a small training set.\n",
    "topics": "{'Transfer Learning': 0.9681368, 'Image Classification': 0.9023586}",
    "score": 0.9356662797
  },
  {
    "id": "1912.04643",
    "title": "WCE Polyp Detection with Triplet based Embeddings",
    "abstract": "  Wireless capsule endoscopy is a medical procedure used to visualize the\nentire gastrointestinal tract and to diagnose intestinal conditions, such as\npolyps or bleeding. Current analyses are performed by manually inspecting\nnearly each one of the frames of the video, a tedious and error-prone task.\nAutomatic image analysis methods can be used to reduce the time needed for\nphysicians to evaluate a capsule endoscopy video, however these methods are\nstill in a research phase. In this paper we focus on computer-aided polyp\ndetection in capsule endoscopy images. This is a challenging problem because of\nthe diversity of polyp appearance, the imbalanced dataset structure and the\nscarcity of data. We have developed a new polyp computer-aided decision system\nthat combines a deep convolutional neural network and metric learning. The key\npoint of the method is the use of the triplet loss function with the aim of\nimproving feature extraction from the images when having small dataset. The\ntriplet loss function allows to train robust detectors by forcing images from\nthe same category to be represented by similar embedding vectors while ensuring\nthat images from different categories are represented by dissimilar vectors.\nEmpirical results show a meaningful increase of AUC values compared to baseline\nmethods. A good performance is not the only requirement when considering the\nadoption of this technology to clinical practice. Trust and explainability of\ndecisions are as important as performance. With this purpose, we also provide a\nmethod to generate visual explanations of the outcome of our polyp detector.\nThese explanations can be used to build a physician's trust in the system and\nalso to convey information about the inner working of the method to the\ndesigner for debugging purposes.\n",
    "topics": "{'Metric Learning': 0.905992}",
    "score": 0.9263282896
  },
  {
    "id": "1812.00555",
    "title": "SUSAN: Segment Unannotated image Structure using Adversarial Network",
    "abstract": "  Segmentation of magnetic resonance (MR) images is a fundamental step in many\nmedical imaging-based applications. The recent implementation of deep\nconvolutional neural networks (CNNs) in image processing has been shown to have\nsignificant impacts on medical image segmentation. Network training of\nsegmentation CNNs typically requires images and paired annotation data\nrepresenting pixel-wise tissue labels referred to as masks. However, the\nsupervised training of highly efficient CNNs with deeper structure and more\nnetwork parameters requires a large number of training images and paired tissue\nmasks. Thus, there is great need to develop a generalized CNN-based\nsegmentation method which would be applicable for a wide variety of MR image\ndatasets with different tissue contrasts. The purpose of this study was to\ndevelop and evaluate a generalized CNN-based method for fully-automated\nsegmentation of different MR image datasets using a single set of annotated\ntraining data. A technique called cycle-consistent generative adversarial\nnetwork (CycleGAN) is applied as the core of the proposed method to perform\nimage-to-image translation between MR image datasets with different tissue\ncontrasts. A joint segmentation network is incorporated into the adversarial\nnetwork to obtain additional segmentation functionality. The proposed method\nwas evaluated for segmenting bone and cartilage on two clinical knee MR image\ndatasets acquired at our institution using only a single set of annotated data\nfrom a publicly available knee MR image dataset. The new technique may further\nimprove the applicability and efficiency of CNN-based segmentation of medical\nimages while eliminating the need for large amounts of annotated training data.\n",
    "topics": "{'Semantic Segmentation': 0.99724567, 'Medical Image Segmentation': 0.9299718, 'Image-to-Image Translation': 0.58637875}",
    "score": 0.9256668728
  },
  {
    "id": "1905.07754",
    "title": "Multimodal 3D Object Detection from Simulated Pretraining",
    "abstract": "  The need for simulated data in autonomous driving applications has become\nincreasingly important, both for validation of pretrained models and for\ntraining new models. In order for these models to generalize to real-world\napplications, it is critical that the underlying dataset contains a variety of\ndriving scenarios and that simulated sensor readings closely mimics real-world\nsensors. We present the Carla Automated Dataset Extraction Tool (CADET), a\nnovel tool for generating training data from the CARLA simulator to be used in\nautonomous driving research. The tool is able to export high-quality,\nsynchronized LIDAR and camera data with object annotations, and offers\nconfiguration to accurately reflect a real-life sensor array. Furthermore, we\nuse this tool to generate a dataset consisting of 10 000 samples and use this\ndataset in order to train the 3D object detection network AVOD-FPN, with\nfinetuning on the KITTI dataset in order to evaluate the potential for\neffective pretraining. We also present two novel LIDAR feature map\nconfigurations in Bird's Eye View for use with AVOD-FPN that can be easily\nmodified. These configurations are tested on the KITTI and CADET datasets in\norder to evaluate their performance as well as the usability of the simulated\ndataset for pretraining. Although insufficient to fully replace the use of real\nworld data, and generally not able to exceed the performance of systems fully\ntrained on real data, our results indicate that simulated data can considerably\nreduce the amount of training on real data required to achieve satisfactory\nlevels of accuracy.\n",
    "topics": "{'Autonomous Driving': 0.99999845, '3D Object Detection': 0.99996126, 'Object Detection': 0.9967288}",
    "score": 0.9219520298
  },
  {
    "id": "1611.03130",
    "title": "Computationally Efficient Target Classification in Multispectral Image\n  Data with Deep Neural Networks",
    "abstract": "  Detecting and classifying targets in video streams from surveillance cameras\nis a cumbersome, error-prone and expensive task. Often, the incurred costs are\nprohibitive for real-time monitoring. This leads to data being stored locally\nor transmitted to a central storage site for post-incident examination. The\nrequired communication links and archiving of the video data are still\nexpensive and this setup excludes preemptive actions to respond to imminent\nthreats. An effective way to overcome these limitations is to build a smart\ncamera that transmits alerts when relevant video sequences are detected. Deep\nneural networks (DNNs) have come to outperform humans in visual classifications\ntasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be\nextended to make use of higher-dimensional input data such as multispectral\ndata. We explore this opportunity in terms of achievable accuracy and required\ncomputational effort. To analyze the precision of DNNs for scene labeling in an\nurban surveillance scenario we have created a dataset with 8 classes obtained\nin a field experiment. We combine an RGB camera with a 25-channel VIS-NIR\nsnapshot sensor to assess the potential of multispectral image data for target\nclassification. We evaluate several new DNNs, showing that the spectral\ninformation fused together with the RGB frames can be used to improve the\naccuracy of the system or to achieve similar accuracy with a 3x smaller\ncomputation effort. We achieve a very high per-pixel accuracy of 99.1%. Even\nfor scarcely occurring, but particularly interesting classes, such as cars, 75%\nof the pixels are labeled correctly with errors occurring only around the\nborder of the objects. This high accuracy was obtained with a training set of\nonly 30 labeled images, paving the way for fast adaptation to various\napplication scenarios.\n",
    "topics": "{}",
    "score": 0.9216132893
  },
  {
    "id": "1901.05733",
    "title": "Multiple Sclerosis Lesion Synthesis in MRI using an encoder-decoder\n  U-NET",
    "abstract": "  In this paper, we propose generating synthetic multiple sclerosis (MS)\nlesions on MRI images with the final aim to improve the performance of\nsupervised machine learning algorithms, therefore avoiding the problem of the\nlack of available ground truth. We propose a two-input two-output fully\nconvolutional neural network model for MS lesion synthesis in MRI images. The\nlesion information is encoded as discrete binary intensity level masks passed\nto the model and stacked with the input images. The model is trained end-to-end\nwithout the need for manually annotating the lesions in the training set. We\nthen perform the generation of synthetic lesions on healthy images via\nregistration of patient images, which are subsequently used for data\naugmentation to increase the performance for supervised MS lesion detection\nalgorithms. Our pipeline is evaluated on MS patient data from an in-house\nclinical dataset and the public ISBI2015 challenge dataset. The evaluation is\nbased on measuring the similarities between the real and the synthetic images\nas well as in terms of lesion detection performance by segmenting both the\noriginal and synthetic images individually using a state-of-the-art\nsegmentation framework. We also demonstrate the usage of synthetic MS lesions\ngenerated on healthy images as data augmentation. We analyze a scenario of\nlimited training data (one-image training) to demonstrate the effect of the\ndata augmentation on both datasets. Our results significantly show the\neffectiveness of the usage of synthetic MS lesion images. For the ISBI2015\nchallenge, our one-image model trained using only a single image plus the\nsynthetic data augmentation strategy showed a performance similar to that of\nother CNN methods that were fully trained using the entire training set,\nyielding a comparable human expert rater performance\n",
    "topics": "{'Data Augmentation': 1.0}",
    "score": 0.9181389993
  },
  {
    "id": "2003.10690",
    "title": "Organ Segmentation From Full-size CT Images Using Memory-Efficient FCN",
    "abstract": "  In this work, we present a memory-efficient fully convolutional network (FCN)\nincorporated with several memory-optimized techniques to reduce the run-time\nGPU memory demand during training phase. In medical image segmentation tasks,\nsubvolume cropping has become a common preprocessing. Subvolumes (or small\npatch volumes) were cropped to reduce GPU memory demand. However, small patch\nvolumes capture less spatial context that leads to lower accuracy. As a pilot\nstudy, the purpose of this work is to propose a memory-efficient FCN which\nenables us to train the model on full size CT image directly without subvolume\ncropping, while maintaining the segmentation accuracy. We optimize our network\nfrom both architecture and implementation. With the development of computing\nhardware, such as graphics processing unit (GPU) and tensor processing unit\n(TPU), now deep learning applications is able to train networks with large\ndatasets within acceptable time. Among these applications, semantic\nsegmentation using fully convolutional network (FCN) also has gained a\nsignificant improvement against traditional image processing approaches in both\ncomputer vision and medical image processing fields. However, unlike general\ncolor images used in computer vision tasks, medical images have larger scales\nthan color images such as 3D computed tomography (CT) images, micro CT images,\nand histopathological images. For training these medical images, the large\ndemand of computing resource become a severe problem. In this paper, we present\na memory-efficient FCN to tackle the high GPU memory demand challenge in organ\nsegmentation problem from clinical CT images. The experimental results\ndemonstrated that our GPU memory demand is about 40% of baseline architecture,\nparameter amount is about 30% of the baseline.\n",
    "topics": "{'Computed Tomography (CT)': 0.99992657, 'Medical Image Segmentation': 0.99986374, 'Semantic Segmentation': 0.9888278}",
    "score": 0.9170244133
  },
  {
    "id": "1911.00686",
    "title": "Unmasking DeepFakes with simple Features",
    "abstract": "  Deep generative models have recently achieved impressive results for many\nreal-world applications, successfully generating high-resolution and diverse\nsamples from complex datasets. Due to this improvement, fake digital contents\nhave proliferated growing concern and spreading distrust in image content,\nleading to an urgent need for automated ways to detect these AI-generated fake\nimages.\n  Despite the fact that many face editing algorithms seem to produce realistic\nhuman faces, upon closer examination, they do exhibit artifacts in certain\ndomains which are often hidden to the naked eye. In this work, we present a\nsimple way to detect such fake face images - so-called DeepFakes. Our method is\nbased on a classical frequency domain analysis followed by basic classifier.\nCompared to previous systems, which need to be fed with large amounts of\nlabeled data, our approach showed very good results using only a few annotated\ntraining samples and even achieved good accuracies in fully unsupervised\nscenarios. For the evaluation on high resolution face images, we combined\nseveral public datasets of real and fake faces into a new benchmark: Faces-HQ.\nGiven such high-resolution images, our approach reaches a perfect\nclassification accuracy of 100% when it is trained on as little as 20 annotated\nsamples. In a second experiment, in the evaluation of the medium-resolution\nimages of the CelebA dataset, our method achieves 100% accuracy supervised and\n96% in an unsupervised setting. Finally, evaluating a low-resolution video\nsequences of the FaceForensics++ dataset, our method achieves 91% accuracy\ndetecting manipulated videos.\n  Source Code: https://github.com/cc-hpc-itwm/DeepFakeDetection\n",
    "topics": "{}",
    "score": 0.9165007198
  },
  {
    "id": "1709.00042",
    "title": "Multi-task Dictionary Learning based Convolutional Neural Network for\n  Computer aided Diagnosis with Longitudinal Images",
    "abstract": "  Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases\non longitudinal data has drawn great interest from computer vision researchers.\nThe current state-of-the-art models for many image classification tasks are\nbased on the Convolutional Neural Networks (CNN). However, a key challenge in\napplying CNN to biological problems is that the available labeled training\nsamples are very limited. Another issue for CNN to be applied in computer aided\ndiagnosis applications is that to achieve better diagnosis and prognosis\naccuracy, one usually has to deal with the longitudinal dataset, i.e., the\ndataset of images scanned at different time points. Here we argue that an\nenhanced CNN model with transfer learning for the joint analysis of tasks from\nmultiple time points or regions of interests may have a potential to improve\nthe accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN\nbased deep learning multi-task dictionary learning framework to address the\nabove challenges. Firstly, we pre-train CNN on the ImageNet dataset and\ntransfer the knowledge from the pre-trained model to the medical imaging\nprogression representation, generating the features for different tasks. Then,\nwe propose a novel unsupervised learning method, termed Multi-task Stochastic\nCoordinate Coding (MSCC), for learning different tasks by using shared and\nindividual dictionaries and generating the sparse features required to predict\nthe future cognitive clinical scores. We apply our new model in a publicly\navailable neuroimaging cohort to predict clinical measures with two different\nfeature sets and compare them with seven other state-of-the-art methods. The\nexperimental results show our proposed method achieved superior results.\n",
    "topics": "{'Dictionary Learning': 1.0, 'Image Classification': 0.96680343, 'Transfer Learning': 0.9362963}",
    "score": 0.9118518308
  },
  {
    "id": "2003.14395",
    "title": "COVID-ResNet: A Deep Learning Framework for Screening of COVID19 from\n  Radiographs",
    "abstract": "  In the last few months, the novel COVID19 pandemic has spread all over the\nworld. Due to its easy transmission, developing techniques to accurately and\neasily identify the presence of COVID19 and distinguish it from other forms of\nflu and pneumonia is crucial. Recent research has shown that the chest Xrays of\npatients suffering from COVID19 depicts certain abnormalities in the\nradiography. However, those approaches are closed source and not made available\nto the research community for re-producibility and gaining deeper insight. The\ngoal of this work is to build open source and open access datasets and present\nan accurate Convolutional Neural Network framework for differentiating COVID19\ncases from other pneumonia cases. Our work utilizes state of the art training\ntechniques including progressive resizing, cyclical learning rate finding and\ndiscriminative learning rates to training fast and accurate residual neural\nnetworks. Using these techniques, we showed the state of the art results on the\nopen-access COVID-19 dataset. This work presents a 3-step technique to\nfine-tune a pre-trained ResNet-50 architecture to improve model performance and\nreduce training time. We call it COVIDResNet. This is achieved through\nprogressively re-sizing of input images to 128x128x3, 224x224x3, and 229x229x3\npixels and fine-tuning the network at each stage. This approach along with the\nautomatic learning rate selection enabled us to achieve the state of the art\naccuracy of 96.23% (on all the classes) on the COVIDx dataset with only 41\nepochs. This work presented a computationally efficient and highly accurate\nmodel for multi-class classification of three different infection types from\nalong with Normal individuals. This model can help in the early screening of\nCOVID19 cases and help reduce the burden on healthcare systems.\n",
    "topics": "{'Multi-class Classification': 0.99540764}",
    "score": 0.9099740609
  },
  {
    "id": "1810.12185",
    "title": "Automatic CNN-based detection of cardiac MR motion artefacts using\n  k-space data augmentation and curriculum learning",
    "abstract": "  Good quality of medical images is a prerequisite for the success of\nsubsequent image analysis pipelines. Quality assessment of medical images is\ntherefore an essential activity and for large population studies such as the UK\nBiobank (UKBB), manual identification of artefacts such as those caused by\nunanticipated motion is tedious and time-consuming. Therefore, there is an\nurgent need for automatic image quality assessment techniques. In this paper,\nwe propose a method to automatically detect the presence of motion-related\nartefacts in cardiac magnetic resonance (CMR) cine images. We compare two deep\nlearning architectures to classify poor quality CMR images: 1) 3D\nspatio-temporal Convolutional Neural Networks (3D-CNN), 2) Long-term Recurrent\nConvolutional Network (LRCN). Though in real clinical setup motion artefacts\nare common, high-quality imaging of UKBB, which comprises cross-sectional\npopulation data of volunteers who do not necessarily have health problems\ncreates a highly imbalanced classification problem. Due to the high number of\ngood quality images compared to the relatively low number of images with motion\nartefacts, we propose a novel data augmentation scheme based on synthetic\nartefact creation in k-space. We also investigate a learning approach using a\npredetermined curriculum based on synthetic artefact severity. We evaluate our\npipeline on a subset of the UK Biobank data set consisting of 3510 CMR images.\nThe LRCN architecture outperformed the 3D-CNN architecture and was able to\ndetect 2D+time short axis images with motion artefacts in less than 1ms with\nhigh recall. We compare our approach to a range of state-of-the-art quality\nassessment methods. The novel data augmentation and curriculum learning\napproaches both improved classification performance achieving overall area\nunder the ROC curve of 0.89.\n",
    "topics": "{'Data Augmentation': 1.0, 'Curriculum Learning': 0.9999999, 'Image Quality Assessment': 0.9998981}",
    "score": 0.9081692135
  },
  {
    "id": "2010.06418",
    "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of\n  COVID-19 in Chest X-ray",
    "abstract": "  COVID-19 spread across the globe at an immense rate has left healthcare\nsystems incapacitated to diagnose and test patients at the needed rate. Studies\nhave shown promising results for detection of COVID-19 from viral bacterial\npneumonia in chest X-rays. Automation of COVID-19 testing using medical images\ncan speed up the testing process of patients where health care systems lack\nsufficient numbers of the reverse-transcription polymerase chain reaction\n(RT-PCR) tests. Supervised deep learning models such as convolutional neural\nnetworks (CNN) need enough labeled data for all classes to correctly learn the\ntask of detection. Gathering labeled data is a cumbersome task and requires\ntime and resources which could further strain health care systems and\nradiologists at the early stages of a pandemic such as COVID-19. In this study,\nwe propose a randomized generative adversarial network (RANDGAN) that detects\nimages of an unknown class (COVID-19) from known and labelled classes (Normal\nand Viral Pneumonia) without the need for labels and training data from the\nunknown class of images (COVID-19). We used the largest publicly available\nCOVID-19 chest X-ray dataset, COVIDx, which is comprised of Normal, Pneumonia,\nand COVID-19 images from multiple public databases. In this work, we use\ntransfer learning to segment the lungs in the COVIDx dataset. Next, we show why\nsegmentation of the region of interest (lungs) is vital to correctly learn the\ntask of classification, specifically in datasets that contain images from\ndifferent resources as it is the case for the COVIDx dataset. Finally, we show\nimproved results in detection of COVID-19 cases using our generative model\n(RANDGAN) compared to conventional generative adversarial networks (GANs) for\nanomaly detection in medical images, improving the area under the ROC curve\nfrom 0.71 to 0.77.\n",
    "topics": "{'COVID-19 Diagnosis': 0.99986565, 'Anomaly Detection': 0.9984863, 'Transfer Learning': 0.97490394}",
    "score": 0.9079632176
  },
  {
    "id": "1711.09726",
    "title": "Exploiting the potential of unlabeled endoscopic video data with\n  self-supervised learning",
    "abstract": "  Surgical data science is a new research field that aims to observe all\naspects of the patient treatment process in order to provide the right\nassistance at the right time. Due to the breakthrough successes of deep\nlearning-based solutions for automatic image annotation, the availability of\nreference annotations for algorithm training is becoming a major bottleneck in\nthe field. The purpose of this paper was to investigate the concept of\nself-supervised learning to address this issue.\n  Our approach is guided by the hypothesis that unlabeled video data can be\nused to learn a representation of the target domain that boosts the performance\nof state-of-the-art machine learning algorithms when used for pre-training.\nCore of the method is an auxiliary task based on raw endoscopic video data of\nthe target domain that is used to initialize the convolutional neural network\n(CNN) for the target task. In this paper, we propose the re-colorization of\nmedical images with a generative adversarial network (GAN)-based architecture\nas auxiliary task. A variant of the method involves a second pre-training step\nbased on labeled data for the target task from a related domain. We validate\nboth variants using medical instrument segmentation as target task.\n  The proposed approach can be used to radically reduce the manual annotation\neffort involved in training CNNs. Compared to the baseline approach of\ngenerating annotated data from scratch, our method decreases exploratively the\nnumber of labeled images by up to 75% without sacrificing performance. Our\nmethod also outperforms alternative methods for CNN pre-training, such as\npre-training on publicly available non-medical or medical data using the target\ntask (in this instance: segmentation).\n  As it makes efficient use of available (non-)public and (un-)labeled data,\nthe approach has the potential to become a valuable tool for CNN\n(pre-)training.\n",
    "topics": "{'Self-Supervised Learning': 1.0, 'Instance Segmentation': 0.99370414, 'Semantic Segmentation': 0.41754717}",
    "score": 0.9078771041
  },
  {
    "id": "1801.04334",
    "title": "TieNet: Text-Image Embedding Network for Common Thorax Disease\n  Classification and Reporting in Chest X-rays",
    "abstract": "  Chest X-rays are one of the most common radiological examinations in daily\nclinical routines. Reporting thorax diseases using chest X-rays is often an\nentry-level task for radiologist trainees. Yet, reading a chest X-ray image\nremains a challenging job for learning-oriented machine intelligence, due to\n(1) shortage of large-scale machine-learnable medical image datasets, and (2)\nlack of techniques that can mimic the high-level reasoning of human\nradiologists that requires years of knowledge accumulation and professional\ntraining. In this paper, we show the clinical free-text radiological reports\ncan be utilized as a priori knowledge for tackling these two key problems. We\npropose a novel Text-Image Embedding network (TieNet) for extracting the\ndistinctive image and text representations. Multi-level attention models are\nintegrated into an end-to-end trainable CNN-RNN architecture for highlighting\nthe meaningful text words and image regions. We first apply TieNet to classify\nthe chest X-rays by using both image features and text embeddings extracted\nfrom associated reports. The proposed auto-annotation framework achieves high\naccuracy (over 0.9 on average in AUCs) in assigning disease labels for our\nhand-label evaluation dataset. Furthermore, we transform the TieNet into a\nchest X-ray reporting system. It simulates the reporting process and can output\ndisease classification and a preliminary report together. The classification\nresults are significantly improved (6% increase on average in AUCs) compared to\nthe state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).\n",
    "topics": "{}",
    "score": 0.9064790113
  },
  {
    "id": "1609.02770",
    "title": "Image and Video Mining through Online Learning",
    "abstract": "  Within the field of image and video recognition, the traditional approach is\na dataset split into fixed training and test partitions. However, the labelling\nof the training set is time-consuming, especially as datasets grow in size and\ncomplexity. Furthermore, this approach is not applicable to the home user, who\nwants to intuitively group their media without tirelessly labelling the\ncontent. Our interactive approach is able to iteratively cluster classes of\nimages and video. Our approach is based around the concept of an image\nsignature which, unlike a standard bag of words model, can express\nco-occurrence statistics as well as symbol frequency. We efficiently compute\nmetric distances between signatures despite their inherent high dimensionality\nand provide discriminative feature selection, to allow common and distinctive\nelements to be identified from a small set of user labelled examples. These\nelements are then accentuated in the image signature to increase similarity\nbetween examples and pull correct classes together. By repeating this process\nin an online learning framework, the accuracy of similarity increases\ndramatically despite labelling only a few training examples. To demonstrate\nthat the approach is agnostic to media type and features used, we evaluate on\nthree image datasets (15 scene, Caltech101 and FG-NET), a mixed text and image\ndataset (ImageTag), a dataset used in active learning (Iris) and on three\naction recognition datasets (UCF11, KTH and Hollywood2). On the UCF11 video\ndataset, the accuracy is 86.7% despite using only 90 labelled examples from a\ndataset of over 1200 videos, instead of the standard 1122 training videos. The\napproach is both scalable and efficient, with a single iteration over the full\nUCF11 dataset of around 1200 videos taking approximately 1 minute on a standard\ndesktop machine.\n",
    "topics": "{'Video Recognition': 0.9997886, 'Action Recognition': 0.9993812, 'Temporal Action Localization': 0.9968893, 'Feature Selection': 0.9426801, 'Active Learning': 0.88654554}",
    "score": 0.9064100233
  },
  {
    "id": "2004.12333",
    "title": "DeepSeg: Deep Neural Network Framework for Automatic Brain Tumor\n  Segmentation using Magnetic Resonance FLAIR Images",
    "abstract": "  Purpose: Gliomas are the most common and aggressive type of brain tumors due\nto their infiltrative nature and rapid progression. The process of\ndistinguishing tumor boundaries from healthy cells is still a challenging task\nin the clinical routine. Fluid-Attenuated Inversion Recovery (FLAIR) MRI\nmodality can provide the physician with information about tumor infiltration.\nTherefore, this paper proposes a new generic deep learning architecture; namely\nDeepSeg for fully automated detection and segmentation of the brain lesion\nusing FLAIR MRI data.\n  Methods: The developed DeepSeg is a modular decoupling framework. It consists\nof two connected core parts based on an encoding and decoding relationship. The\nencoder part is a convolutional neural network (CNN) responsible for spatial\ninformation extraction. The resulting semantic map is inserted into the decoder\npart to get the full resolution probability map. Based on modified U-Net\narchitecture, different CNN models such as Residual Neural Network (ResNet),\nDense Convolutional Network (DenseNet), and NASNet have been utilized in this\nstudy.\n  Results: The proposed deep learning architectures have been successfully\ntested and evaluated on-line based on MRI datasets of Brain Tumor Segmentation\n(BraTS 2019) challenge, including s336 cases as training data and 125 cases for\nvalidation data. The dice and Hausdorff distance scores of obtained\nsegmentation results are about 0.81 to 0.84 and 9.8 to 19.7 correspondingly.\n  Conclusion: This study showed successful feasibility and comparative\nperformance of applying different deep learning models in a new DeepSeg\nframework for automated brain tumor segmentation in FLAIR MR images. The\nproposed DeepSeg is open-source and freely available at\nhttps://github.com/razeineldin/DeepSeg/.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'Brain Tumor Segmentation': 1.0}",
    "score": 0.9056247266
  },
  {
    "id": "2009.12007",
    "title": "G-SimCLR : Self-Supervised Contrastive Learning with Guided Projection\n  via Pseudo Labelling",
    "abstract": "  In the realms of computer vision, it is evident that deep neural networks\nperform better in a supervised setting with a large amount of labeled data. The\nrepresentations learned with supervision are not only of high quality but also\nhelps the model in enhancing its accuracy. However, the collection and\nannotation of a large dataset are costly and time-consuming. To avoid the same,\nthere has been a lot of research going on in the field of unsupervised visual\nrepresentation learning especially in a self-supervised setting. Amongst the\nrecent advancements in self-supervised methods for visual recognition, in\nSimCLR Chen et al. shows that good quality representations can indeed be\nlearned without explicit supervision. In SimCLR, the authors maximize the\nsimilarity of augmentations of the same image and minimize the similarity of\naugmentations of different images. A linear classifier trained with the\nrepresentations learned using this approach yields 76.5% top-1 accuracy on the\nImageNet ILSVRC-2012 dataset. In this work, we propose that, with the\nnormalized temperature-scaled cross-entropy (NT-Xent) loss function (as used in\nSimCLR), it is beneficial to not have images of the same category in the same\nbatch. In an unsupervised setting, the information of images pertaining to the\nsame category is missing. We use the latent space representation of a denoising\nautoencoder trained on the unlabeled dataset and cluster them with k-means to\nobtain pseudo labels. With this apriori information we batch images, where no\ntwo images from the same category are to be found. We report comparable\nperformance enhancements on the CIFAR10 dataset and a subset of the ImageNet\ndataset. We refer to our method as G-SimCLR.\n",
    "topics": "{'Contrastive Learning': 0.99998105, 'Denoising': 0.998042, 'Representation Learning': 0.98899955}",
    "score": 0.9046844145
  },
  {
    "id": "1809.08168",
    "title": "Exclusive Independent Probability Estimation using Deep 3D Fully\n  Convolutional DenseNets: Application to IsoIntense Infant Brain MRI\n  Segmentation",
    "abstract": "  The most recent fast and accurate image segmentation methods are built upon\nfully convolutional deep neural networks. In this paper, we propose new deep\nlearning strategies for DenseNets to improve segmenting images with subtle\ndifferences in intensity values and features. We aim to segment brain tissue on\ninfant brain MRI at about 6 months of age where white matter and gray matter of\nthe developing brain show similar T1 and T2 relaxation times, thus appear to\nhave similar intensity values on both T1- and T2-weighted MRI scans. Brain\ntissue segmentation at this age is, therefore, very challenging. To this end,\nwe propose an exclusive multi-label training strategy to segment the mutually\nexclusive brain tissues with similarity loss functions that automatically\nbalance the training based on class prevalence. Using our proposed training\nstrategy based on similarity loss functions and patch prediction fusion we\ndecrease the number of parameters in the network, reduce the complexity of the\ntraining process focusing the attention on less number of tasks, while\nmitigating the effects of data imbalance between labels and inaccuracies near\npatch borders. By taking advantage of these strategies we were able to perform\nfast image segmentation (90 seconds per 3D volume), using a network with less\nparameters than many state-of-the-art networks, overcoming issues such as\n3Dvs2D training and large vs small patch size selection, while achieving the\ntop performance in segmenting brain tissue among all methods tested in first\nand second round submissions of the isointense infant brain MRI segmentation\n(iSeg) challenge according to the official challenge test results. Our proposed\nstrategy improves the training process through balanced training and by\nreducing its complexity while providing a trained model that works for any size\ninput image and is fast and more accurate than many state-of-the-art methods.\n",
    "topics": "{'Semantic Segmentation': 0.9935603, 'Brain Segmentation': 0.9595954, 'Test results': 0.67044187}",
    "score": 0.9040262168
  },
  {
    "id": "1805.08692",
    "title": "Assessing a mobile-based deep learning model for plant disease\n  surveillance",
    "abstract": "  Convolutional neural network models (CNNs) have made major advances in\ncomputer vision tasks in the last five years. Given the challenge in collecting\nreal world datasets, most studies report performance metrics based on available\nresearch datasets. In scenarios where CNNs are to be deployed on images or\nvideos from mobile devices, models are presented with new challenges due to\nlighting, angle, and camera specifications, which are not accounted for in\nresearch datasets. It is essential for assessment to also be conducted on real\nworld datasets if such models are to be reliably integrated with products and\nservices in society. Plant disease datasets can be used to test CNNs in real\ntime and gain insight into real world performance. We train a CNN object\ndetection model to identify foliar symptoms of diseases (or lack thereof) in\ncassava (Manihot esculenta Crantz). We then deploy the model on a mobile app\nand test its performance on mobile images and video of 720 diseased leaflets in\nan agricultural field in Tanzania. Within each disease category we test two\nlevels of severity of symptoms - mild and pronounced, to assess the model\nperformance for early detection of symptoms. In both severities we see a\ndecrease in the F-1 score for real world images and video. The F-1 score\ndropped by 32% for pronounced symptoms in real world images (the closest data\nto the training data) due to a drop in model recall. If the potential of\nsmartphone CNNs are to be realized our data suggest it is crucial to consider\ntuning precision and recall performance in order to achieve the desired\nperformance in real world settings. In addition, the varied performance related\nto different input data (image or video) is an important consideration for the\ndesign of CNNs in real world applications.\n",
    "topics": "{}",
    "score": 0.9023365629
  },
  {
    "id": "2004.12537",
    "title": "Towards Efficient COVID-19 CT Annotation: A Benchmark for Lung and\n  Infection Segmentation",
    "abstract": "  Accurate segmentation of lung and infection in COVID-19 CT scans plays an\nimportant role in the quantitative management of patients. Most of the existing\nstudies are based on large and private annotated datasets that are impractical\nto obtain from a single institution, especially when radiologists are busy\nfighting the coronavirus disease. Furthermore, it is hard to compare current\nCOVID-19 CT segmentation methods as they are developed on different datasets,\ntrained in different settings, and evaluated with different metrics. In this\npaper, we created a COVID-19 3D CT dataset with 20 cases that contains 1800+\nannotated slices and made it publicly available. To promote the development of\nannotation-efficient deep learning methods, we built three benchmarks for lung\nand infection segmentation that contain current main research interests, e.g.,\nfew-shot learning, domain generalization, and knowledge transfer. For a fair\ncomparison among different segmentation methods, we also provide unified\ntraining, validation and testing dataset splits, and evaluation metrics and\ncorresponding code. In addition, we provided more than 40 pre-trained baseline\nmodels for the benchmarks, which not only serve as out-of-the-box segmentation\ntools but also save computational time for researchers who are interested in\nCOVID-19 lung and infection segmentation. To the best of our knowledge, this\nwork presents the largest public annotated COVID-19 CT volume dataset, the\nfirst segmentation benchmark, and the most pre-trained models up to now. We\nhope these resources\n(\\url{https://gitee.com/junma11/COVID-19-CT-Seg-Benchmark}) could advance the\ndevelopment of deep learning methods for COVID-19 CT segmentation with limited\ndata.\n",
    "topics": "{'Domain Generalization': 0.99996626, 'Few-Shot Learning': 0.9980951, 'Transfer Learning': 0.9155405}",
    "score": 0.901505001
  },
  {
    "id": "2004.05835",
    "title": "COVID-19 identification in chest X-ray images on flat and hierarchical\n  classification scenarios",
    "abstract": "  The COVID-19 can cause severe pneumonia and is estimated to have a high\nimpact on the healthcare system. The standard image diagnosis tests for\npneumonia are chest X-ray (CXR) and computed tomography (CT) scan. CXR are\nuseful in because it is cheaper, faster and more widespread than CT. This study\naims to identify pneumonia caused by COVID-19 from other types and also healthy\nlungs using only CXR images. In order to achieve the objectives, we have\nproposed a classification schema considering the multi-class and hierarchical\nperspectives, since pneumonia can be structured as a hierarchy. Given the\nnatural data imbalance in this domain, we also proposed the use of resampling\nalgorithms in order to re-balance the classes distribution. Our classification\nschema extract features using some well-known texture descriptors and also\nusing a pre-trained CNN model. We also explored early and late fusion\ntechniques in order to leverage the strength of multiple texture descriptors\nand base classifiers at once. To evaluate the approach, we composed a database,\nnamed RYDLS-20, containing CXR images of pneumonia caused by different\npathogens as well as CXR images of healthy lungs. The classes distribution\nfollows a real-world scenario in which some pathogens are more common than\nothers. The proposed approach achieved a macro-avg F1-Score of 0.65 using a\nmulti-class approach and a F1-Score of 0.89 for the COVID-19 identification in\nthe hierarchical classification scenario. As far as we know, we achieved the\nbest nominal rate obtained for COVID-19 identification in an unbalanced\nenvironment with more than three classes. We must also highlight the novel\nproposed hierarchical classification approach for this task, which considers\nthe types of pneumonia caused by the different pathogens and lead us to the\nbest COVID-19 recognition rate obtained here.\n",
    "topics": "{'COVID-19 Diagnosis': 0.91169226}",
    "score": 0.9007324181
  },
  {
    "id": "1805.00334",
    "title": "Deep learning approach to Fourier ptychographic microscopy",
    "abstract": "  Convolutional neural networks (CNNs) have gained tremendous success in\nsolving complex inverse problems. The aim of this work is to develop a novel\nCNN framework to reconstruct video sequence of dynamic live cells captured\nusing a computational microscopy technique, Fourier ptychographic microscopy\n(FPM). The unique feature of the FPM is its capability to reconstruct images\nwith both wide field-of-view (FOV) and high resolution, i.e. a large\nspace-bandwidth-product (SBP), by taking a series of low resolution intensity\nimages. For live cell imaging, a single FPM frame contains thousands of cell\nsamples with different morphological features. Our idea is to fully exploit the\nstatistical information provided by this large spatial ensemble so as to make\npredictions in a sequential measurement, without using any additional temporal\ndataset. Specifically, we show that it is possible to reconstruct high-SBP\ndynamic cell videos by a CNN trained only on the first FPM dataset captured at\nthe beginning of a time-series experiment. Our CNN approach reconstructs a\n12800X10800 pixels phase image using only ~25 seconds, a 50X speedup compared\nto the model-based FPM algorithm. In addition, the CNN further reduces the\nrequired number of images in each time frame by ~6X. Overall, this\nsignificantly improves the imaging throughput by reducing both the acquisition\nand computational times. The proposed CNN is based on the conditional\ngenerative adversarial network (cGAN) framework. Additionally, we also exploit\ntransfer learning so that our pre-trained CNN can be further optimized to image\nother cell types. Our technique demonstrates a promising deep learning approach\nto continuously monitor large live-cell populations over an extended time and\ngather useful spatial and temporal information with sub-cellular resolution.\n",
    "topics": "{'Time Series': 0.8801005, 'Transfer Learning': 0.8711013}",
    "score": 0.8998810296
  },
  {
    "id": "1912.13258",
    "title": "Automated Testing for Deep Learning Systems with Differential Behavior\n  Criteria",
    "abstract": "  In this work, we conducted a study on building an automated testing system\nfor deep learning systems based on differential behavior criteria. The\nautomated testing goals were achieved by jointly optimizing two objective\nfunctions: maximizing differential behaviors from models under testing and\nmaximizing neuron coverage. By observing differential behaviors from three\npre-trained models during each testing iteration, the input image that\ntriggered erroneous feedback was registered as a corner-case. The generated\ncorner-cases can be used to examine the robustness of DNNs and consequently\nimprove model accuracy. A project called DeepXplore was also used as a baseline\nmodel. After we fully implemented and optimized the baseline system, we\nexplored its application as an augmenting training dataset with newly generated\ncorner cases. With the GTRSB dataset, by retraining the model based on\nautomated generated corner cases, the accuracy of three generic models\nincreased by 259.2%, 53.6%, and 58.3%, respectively. Further, to extend the\ncapability of automated testing, we explored other approaches based on\ndifferential behavior criteria to generate photo-realistic images for deep\nlearning systems. One approach was to apply various transformations to the seed\nimages for the deep learning framework. The other approach was to utilize the\nGenerative Adversarial Networks (GAN) technique, which was implemented on MNIST\nand Driving datasets. The style transferring capability has been observed very\neffective in adding additional visual effects, replacing image elements, and\nstyle-shifting (virtual image to real images). The GAN-based testing sample\ngeneration system was shown to be the next frontier for automated testing for\ndeep learning systems.\n",
    "topics": "{}",
    "score": 0.8994882825
  },
  {
    "id": "2006.01228",
    "title": "An embedded system for the automated generation of labeled plant images\n  to enable machine learning applications in agriculture",
    "abstract": "  A lack of sufficient training data, both in terms of variety and quantity, is\noften the bottleneck in the development of machine learning (ML) applications\nin any domain. For agricultural applications, ML-based models designed to\nperform tasks such as autonomous plant classification will typically be coupled\nto just one or perhaps a few plant species. As a consequence, each\ncrop-specific task is very likely to require its own specialized training data,\nand the question of how to serve this need for data now often overshadows the\nmore routine exercise of actually training such models. To tackle this problem,\nwe have developed an embedded robotic system to automatically generate and\nlabel large datasets of plant images for ML applications in agriculture. The\nsystem can image plants from virtually any angle, thereby ensuring a wide\nvariety of data; and with an imaging rate of up to one image per second, it can\nproduce lableled datasets on the scale of thousands to tens of thousands of\nimages per day. As such, this system offers an important alternative to time-\nand cost-intensive methods of manual generation and labeling. Furthermore, the\nuse of a uniform background made of blue keying fabric enables additional image\nprocessing techniques such as background replacement and plant segmentation. It\nalso helps in the training process, essentially forcing the model to focus on\nthe plant features and eliminating random correlations. To demonstrate the\ncapabilities of our system, we generated a dataset of over 34,000 labeled\nimages, with which we trained an ML-model to distinguish grasses from\nnon-grasses in test data from a variety of sources. We now plan to generate\nmuch larger datasets of Canadian crop plants and weeds that will be made\npublicly available in the hope of further enabling ML applications in the\nagriculture sector.\n",
    "topics": "{}",
    "score": 0.8994710771
  },
  {
    "id": "1802.02213",
    "title": "A Multiresolution Convolutional Neural Network with Partial Label\n  Training for Annotating Reflectance Confocal Microscopy Images of Skin",
    "abstract": "  We describe a new multiresolution \"nested encoder-decoder\" convolutional\nnetwork architecture and use it to annotate morphological patterns in\nreflectance confocal microscopy (RCM) images of human skin for aiding cancer\ndiagnosis. Skin cancers are the most common types of cancers, melanoma being\nthe deadliest among them. RCM is an effective, non-invasive pre-screening tool\nfor skin cancer diagnosis, with the required cellular resolution. However,\nimages are complex, low-contrast, and highly variable, so that clinicians\nrequire months to years of expert-level training to be able to make accurate\nassessments. In this paper, we address classifying 4 key clinically important\nstructural/textural patterns in RCM images. The occurrence and morphology of\nthese patterns are used by clinicians for diagnosis of melanomas. The large\nsize of RCM images, the large variance of pattern size, the large-scale range\nover which patterns appear, the class imbalance in collected images, and the\nlack of fully-labeled images all make this a challenging problem to address,\neven with automated machine learning tools. We designed a novel nested U-net\narchitecture to cope with these challenges, and a selective loss function to\nhandle partial labeling. Trained and tested on 56 melanoma-suspicious,\npartially labeled, 12k x 12k pixel images, our network automatically annotated\ndiagnostic patterns with high sensitivity and specificity, providing consistent\nlabels for unlabeled sections of the test images. Providing such annotation\nwill aid clinicians in achieving diagnostic accuracy, and perhaps more\nimportant, dramatically facilitate clinical training, thus enabling much more\nrapid adoption of RCM into widespread clinical use process. In addition, our\nadaptation of U-net architecture provides an intrinsically multiresolution deep\nnetwork that may be useful in other challenging biomedical image analysis\napplications.\n",
    "topics": "{}",
    "score": 0.8994307258
  },
  {
    "id": "1803.05431",
    "title": "An application of cascaded 3D fully convolutional networks for medical\n  image segmentation",
    "abstract": "  Recent advances in 3D fully convolutional networks (FCN) have made it\nfeasible to produce dense voxel-wise predictions of volumetric images. In this\nwork, we show that a multi-class 3D FCN trained on manually labeled CT scans of\nseveral anatomical structures (ranging from the large organs to thin vessels)\ncan achieve competitive segmentation results, while avoiding the need for\nhandcrafting features or training class-specific models.\n  To this end, we propose a two-stage, coarse-to-fine approach that will first\nuse a 3D FCN to roughly define a candidate region, which will then be used as\ninput to a second 3D FCN. This reduces the number of voxels the second FCN has\nto classify to ~10% and allows it to focus on more detailed segmentation of the\norgans and vessels.\n  We utilize training and validation sets consisting of 331 clinical CT images\nand test our models on a completely unseen data collection acquired at a\ndifferent hospital that includes 150 CT scans, targeting three anatomical\norgans (liver, spleen, and pancreas). In challenging organs such as the\npancreas, our cascaded approach improves the mean Dice score from 68.5 to\n82.2%, achieving the highest reported average score on this dataset. We compare\nwith a 2D FCN method on a separate dataset of 240 CT scans with 18 classes and\nachieve a significantly higher performance in small organs and vessels.\nFurthermore, we explore fine-tuning our models to different datasets.\n  Our experiments illustrate the promise and robustness of current 3D FCN based\nsemantic segmentation of medical images, achieving state-of-the-art results.\nOur code and trained models are available for download:\nhttps://github.com/holgerroth/3Dunet_abdomen_cascade.\n",
    "topics": "{'Medical Image Segmentation': 0.9999851, 'Semantic Segmentation': 0.9916437}",
    "score": 0.8992216502
  },
  {
    "id": "1908.08520",
    "title": "Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and\n  Noisy Data Refinement",
    "abstract": "  Generic Image recognition is a fundamental and fairly important visual\nproblem in computer vision. One of the major challenges of this task lies in\nthe fact that single image usually has multiple objects inside while the labels\nare still one-hot, another one is noisy and sometimes missing labels when\nannotated by humans. In this paper, we focus on tackling these challenges\naccompanying with two different image recognition problems: multi-model\nensemble and noisy data recognition with a unified framework. As is well-known,\nusually the best performing deep neural models are ensembles of multiple\nbase-level networks, as it can mitigate the variation or noise containing in\nthe dataset. Unfortunately, the space required to store these many networks,\nand the time required to execute them at runtime, prohibit their use in\napplications where test sets are large (e.g., ImageNet). In this paper, we\npresent a method for compressing large, complex trained ensembles into a single\nnetwork, where the knowledge from a variety of trained deep neural networks\n(DNNs) is distilled and transferred to a single DNN. In order to distill\ndiverse knowledge from different trained (teacher) models, we propose to use\nadversarial-based learning strategy where we define a block-wise training loss\nto guide and optimize the predefined student network to recover the knowledge\nin teacher models, and to promote the discriminator network to distinguish\nteacher vs. student features simultaneously. Extensive experiments on\nCIFAR-10/100, SVHN, ImageNet and iMaterialist Challenge Dataset demonstrate the\neffectiveness of our MEAL method. On ImageNet, our ResNet-50 based MEAL\nachieves top-1/5 21.79%/5.99% val error, which outperforms the original model\nby 2.06%/1.14%. On iMaterialist Challenge Dataset, our MEAL obtains a\nremarkable improvement of top-3 1.15% (official evaluation metric) on a strong\nbaseline model of ResNet-101.\n",
    "topics": "{}",
    "score": 0.8983234695
  },
  {
    "id": "1801.05173",
    "title": "Fully Convolutional Multi-scale Residual DenseNets for Cardiac\n  Segmentation and Automated Cardiac Diagnosis using Ensemble of Classifiers",
    "abstract": "  Deep fully convolutional neural network (FCN) based architectures have shown\ngreat potential in medical image segmentation. However, such architectures\nusually have millions of parameters and inadequate number of training samples\nleading to over-fitting and poor generalization. In this paper, we present a\nnovel highly parameter and memory efficient FCN based architecture for medical\nimage analysis. We propose a novel up-sampling path which incorporates long\nskip and short-cut connections to overcome the feature map explosion in FCN\nlike architectures. In order to processes the input images at multiple scales\nand view points simultaneously, we propose to incorporate Inception module's\nparallel structures. We also propose a novel dual loss function whose weighting\nscheme allows to combine advantages of cross-entropy and dice loss. We have\nvalidated our proposed network architecture on two publicly available datasets,\nnamely: (i) Automated Cardiac Disease Diagnosis Challenge (ACDC-2017), (ii)\nLeft Ventricular Segmentation Challenge (LV-2011). Our approach in ACDC-2017\nchallenge stands second place for segmentation and first place in automated\ncardiac disease diagnosis tasks with an accuracy of 100%. In the LV-2011\nchallenge our approach attained 0.74 Jaccard index, which is so far the highest\npublished result in fully automated algorithms. From the segmentation we\nextracted clinically relevant cardiac parameters and hand-crafted features\nwhich reflected the clinical diagnostic analysis to train an ensemble system\nfor cardiac disease classification. Our approach combined both cardiac\nsegmentation and disease diagnosis into a fully automated framework which is\ncomputational efficient and hence has the potential to be incorporated in\ncomputer-aided diagnosis (CAD) tools for clinical application.\n",
    "topics": "{'Medical Image Segmentation': 0.9600172, 'Semantic Segmentation': 0.9483558}",
    "score": 0.8975513648
  },
  {
    "id": "2001.03637",
    "title": "Can Giraffes Become Birds? An Evaluation of Image-to-image Translation\n  for Data Generation",
    "abstract": "  There is an increasing interest in image-to-image translation with\napplications ranging from generating maps from satellite images to creating\nentire clothes' images from only contours. In the present work, we investigate\nimage-to-image translation using Generative Adversarial Networks (GANs) for\ngenerating new data, taking as a case study the morphing of giraffes images\ninto bird images. Morphing a giraffe into a bird is a challenging task, as they\nhave different scales, textures, and morphology. An unsupervised cross-domain\ntranslator entitled InstaGAN was trained on giraffes and birds, along with\ntheir respective masks, to learn translation between both domains. A dataset of\nsynthetic bird images was generated using translation from originally giraffe\nimages while preserving the original spatial arrangement and background. It is\nimportant to stress that the generated birds do not exist, being only the\nresult of a latent representation learned by InstaGAN. Two subsets of common\nliterature datasets were used for training the GAN and generating the\ntranslated images: COCO and Caltech-UCSD Birds 200-2011. To evaluate the\nrealness and quality of the generated images and masks, qualitative and\nquantitative analyses were made. For the quantitative analysis, a pre-trained\nMask R-CNN was used for the detection and segmentation of birds on Pascal VOC,\nCaltech-UCSD Birds 200-2011, and our new dataset entitled FakeSet. The\ngenerated dataset achieved detection and segmentation results close to the real\ndatasets, suggesting that the generated images are realistic enough to be\ndetected and segmented by a state-of-the-art deep neural network.\n",
    "topics": "{'Image-to-Image Translation': 1.0}",
    "score": 0.8972254404
  },
  {
    "id": "1909.00489",
    "title": "An Efficient Convolutional Neural Network for Coronary Heart Disease\n  Prediction",
    "abstract": "  This study proposes an efficient neural network with convolutional layers to\nclassify significantly class-imbalanced clinical data. The data are curated\nfrom the National Health and Nutritional Examination Survey (NHANES) with the\ngoal of predicting the occurrence of Coronary Heart Disease (CHD). While the\nmajority of the existing machine learning models that have been used on this\nclass of data are vulnerable to class imbalance even after the adjustment of\nclass-specific weights, our simple two-layer CNN exhibits resilience to the\nimbalance with fair harmony in class-specific performance. In order to obtain\nsignificant improvement in classification accuracy under supervised learning\nsettings, it is a common practice to train a neural network architecture with a\nmassive data and thereafter, test the resulting network on a comparatively\nsmaller amount of data. However, given a highly imbalanced dataset, it is often\nchallenging to achieve a high class 1 (true CHD prediction rate) accuracy as\nthe testing data size increases. We adopt a two-step approach: first, we employ\nleast absolute shrinkage and selection operator (LASSO) based feature weight\nassessment followed by majority-voting based identification of important\nfeatures. Next, the important features are homogenized by using a fully\nconnected layer, a crucial step before passing the output of the layer to\nsuccessive convolutional stages. We also propose a training routine per epoch,\nakin to a simulated annealing process, to boost the classification accuracy.\nDespite a 35:1 (Non-CHD:CHD) ratio in the NHANES dataset, the investigation\nconfirms that our proposed CNN architecture has the classification power of 77%\nto correctly classify the presence of CHD and 81.8% the absence of CHD cases on\na testing data, which is 85.70% of the total dataset. ( (<1920\ncharacters)Please check the paper for full abstract)\n",
    "topics": "{}",
    "score": 0.8971523751
  },
  {
    "id": "2001.01265",
    "title": "FDFtNet: Facing Off Fake Images using Fake Detection Fine-tuning Network",
    "abstract": "  Creating fake images and videos such as \"Deepfake\" has become much easier\nthese days due to the advancement in Generative Adversarial Networks (GANs).\nMoreover, recent research such as the few-shot learning can create highly\nrealistic personalized fake images with only a few images. Therefore, the\nthreat of Deepfake to be used for a variety of malicious intents such as\npropagating fake images and videos becomes prevalent. And detecting these\nmachine-generated fake images has been quite challenging than ever. In this\nwork, we propose a light-weight robust fine-tuning neural network-based\nclassifier architecture called Fake Detection Fine-tuning Network (FDFtNet),\nwhich is capable of detecting many of the new fake face image generation\nmodels, and can be easily combined with existing image classification networks\nand finetuned on a few datasets. In contrast to many existing methods, our\napproach aims to reuse popular pre-trained models with only a few images for\nfine-tuning to effectively detect fake images. The core of our approach is to\nintroduce an image-based self-attention module called Fine-Tune Transformer\nthat uses only the attention module and the down-sampling layer. This module is\nadded to the pre-trained model and fine-tuned on a few data to search for new\nsets of feature space to detect fake images. We experiment with our FDFtNet on\nthe GANsbased dataset (Progressive Growing GAN) and Deepfake-based dataset\n(Deepfake and Face2Face) with a small input image resolution of 64x64 that\ncomplicates detection. Our FDFtNet achieves an overall accuracy of 90.29% in\ndetecting fake images generated from the GANs-based dataset, outperforming the\nstate-of-the-art.\n",
    "topics": "{'Face Swapping': 0.9678252, 'Image Generation': 0.9157245, 'Few-Shot Learning': 0.3166474}",
    "score": 0.8966899155
  },
  {
    "id": "2009.10608",
    "title": "Dual Encoder Fusion U-Net (DEFU-Net) for Cross-manufacturer Chest X-ray\n  Segmentation",
    "abstract": "  A number of methods based on the deep learning have been applied to medical\nimage segmentation and have achieved state-of-the-art performance. Due to the\nimportance of chest x-ray data in studying COVID-19, there is a demand for\nstate-of-the-art models capable of precisely segmenting soft tissue on the\nchest x-rays before obtaining mask annotations about this sort of dataset. The\ndataset for exploring best pre-trained model is from Montgomery and Shenzhen\nhospital which had opened in 2014. The most famous technique is U-Net which has\nbeen used to many medical datasets including the Chest X-ray. However, most\nvariant U-Nets mainly focus on extraction of contextual information and skip\nconnection. There is still a large space for improving extraction of spatial\nfeatures. In this paper, we propose a dual encoder fusion U-Net framework for\nChest X-rays based on Inception Convolutional Neural Network with dilation,\nDensely Connected Recurrent Convolutional Neural Network, which is named\nDEFU-Net. The densely connected recurrent path extends the network deeper for\nfacilitating context feature extraction. In order to increase the width of\nnetwork and enrich representation of features, the inception blocks with\ndilation have been used. The inception blocks can capture globally and locally\nspatial information by various receptive fields. At the same time, the two\npaths are fused by summing features, thus preserving context and the spatial\ninformation for decoding part. This multi-learning-scale model is benefiting in\nChest X-ray dataset from two different manufacturers (Montgomery and Shenzhen\nhospital). The DEFU-Net achieves the better performance than basic U-Net,\nresidual U-Net, BCDU-Net, modified R2U-Net and modified attention R2U-Net. This\nmodel has proved the feasibility for mixed dataset. The open source code for\nthis proposed framework will be public soon.\n",
    "topics": "{'Medical Image Segmentation': 0.9975049, 'Semantic Segmentation': 0.8739772}",
    "score": 0.8963063686
  },
  {
    "id": "1907.12347",
    "title": "FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation",
    "abstract": "  Over the past few years, we have witnessed the success of deep learning in\nimage recognition thanks to the availability of large-scale human-annotated\ndatasets such as PASCAL VOC, ImageNet, and COCO. Although these datasets have\ncovered a wide range of object categories, there are still a significant number\nof objects that are not included. Can we perform the same task without a lot of\nhuman annotations? In this paper, we are interested in few-shot object\nsegmentation where the number of annotated training examples are limited to 5\nonly. To evaluate and validate the performance of our approach, we have built a\nfew-shot segmentation dataset, FSS-1000, which consists of 1000 object classes\nwith pixelwise annotation of ground-truth segmentation. Unique in FSS-1000, our\ndataset contains significant number of objects that have never been seen or\nannotated in previous datasets, such as tiny daily objects, merchandise,\ncartoon characters, logos, etc. We build our baseline model using standard\nbackbone networks such as VGG-16, ResNet-101, and Inception. To our surprise,\nwe found that training our model from scratch using FSS-1000 achieves\ncomparable and even better results than training with weights pre-trained by\nImageNet which is more than 100 times larger than FSS-1000. Both our approach\nand dataset are simple, effective, and easily extensible to learn segmentation\nof new object classes given very few annotated training examples. Dataset is\navailable at https://github.com/HKUSTCV/FSS-1000.\n",
    "topics": "{'Semantic Segmentation': 0.9661582}",
    "score": 0.8958634366
  },
  {
    "id": "1703.06370",
    "title": "Weakly-supervised DCNN for RGB-D Object Recognition in Real-World\n  Applications Which Lack Large-scale Annotated Training Data",
    "abstract": "  This paper addresses the problem of RGBD object recognition in real-world\napplications, where large amounts of annotated training data are typically\nunavailable. To overcome this problem, we propose a novel, weakly-supervised\nlearning architecture (DCNN-GPC) which combines parametric models (a pair of\nDeep Convolutional Neural Networks (DCNN) for RGB and D modalities) with\nnon-parametric models (Gaussian Process Classification). Our system is\ninitially trained using a small amount of labeled data, and then automatically\nprop- agates labels to large-scale unlabeled data. We first run 3D- based\nobjectness detection on RGBD videos to acquire many unlabeled object proposals,\nand then employ DCNN-GPC to label them. As a result, our multi-modal DCNN can\nbe trained end-to-end using only a small amount of human annotation. Finally,\nour 3D-based objectness detection and multi-modal DCNN are integrated into a\nreal-time detection and recognition pipeline. In our approach, bounding-box\nannotations are not required and boundary-aware detection is achieved. We also\npropose a novel way to pretrain a DCNN for the depth modality, by training on\nvirtual depth images projected from CAD models. We pretrain our multi-modal\nDCNN on public 3D datasets, achieving performance comparable to\nstate-of-the-art methods on Washington RGBS Dataset. We then finetune the\nnetwork by further training on a small amount of annotated data from our novel\ndataset of industrial objects (nuclear waste simulants). Our weakly supervised\napproach has demonstrated to be highly effective in solving a novel RGBD object\nrecognition application which lacks of human annotations.\n",
    "topics": "{'Object Recognition': 1.0}",
    "score": 0.8955917335
  },
  {
    "id": "1904.00592",
    "title": "ResUNet-a: a deep learning framework for semantic segmentation of\n  remotely sensed data",
    "abstract": "  Scene understanding of high resolution aerial images is of great importance\nfor the task of automated monitoring in various remote sensing applications.\nDue to the large within-class and small between-class variance in pixel values\nof objects of interest, this remains a challenging task. In recent years, deep\nconvolutional neural networks have started being used in remote sensing\napplications and demonstrate state of the art performance for pixel level\nclassification of objects. \\textcolor{black}{Here we propose a reliable\nframework for performant results for the task of semantic segmentation of\nmonotemporal very high resolution aerial images. Our framework consists of a\nnovel deep learning architecture, ResUNet-a, and a novel loss function based on\nthe Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination\nwith residual connections, atrous convolutions, pyramid scene parsing pooling\nand multi-tasking inference. ResUNet-a infers sequentially the boundary of the\nobjects, the distance transform of the segmentation mask, the segmentation mask\nand a colored reconstruction of the input. Each of the tasks is conditioned on\nthe inference of the previous ones, thus establishing a conditioned\nrelationship between the various tasks, as this is described through the\narchitecture's computation graph. We analyse the performance of several\nflavours of the Generalized Dice loss for semantic segmentation, and we\nintroduce a novel variant loss function for semantic segmentation of objects\nthat has excellent convergence properties and behaves well even under the\npresence of highly imbalanced classes.} The performance of our modeling\nframework is evaluated on the ISPRS 2D Potsdam dataset. Results show\nstate-of-the-art performance with an average F1 score of 92.9\\% over all\nclasses for our best model.\n",
    "topics": "{'Scene Understanding': 1.0, 'Semantic Segmentation': 0.99994385, 'Scene Parsing': 0.9989831}",
    "score": 0.8954975286
  },
  {
    "id": "1907.12282",
    "title": "Regularizing Proxies with Multi-Adversarial Training for Unsupervised\n  Domain-Adaptive Semantic Segmentation",
    "abstract": "  Training a semantic segmentation model requires a large amount of pixel-level\nannotation, hampering its application at scale. With computer graphics, we can\ngenerate almost unlimited training data with precise annotation. However,a deep\nmodel trained with synthetic data usually cannot directly generalize well to\nrealistic images due to domain shift. It has been observed that highly\nconfident labels for the unlabeled real images may be predicted relying on the\nlabeled synthetic data. To tackle the unsupervised domain adaptation problem,\nwe explore the possibilities to generate high-quality labels as proxy labels to\nsupervise the training on target data. Specifically, we propose a novel\nproxy-based method using multi-adversarial training. We first train the model\nusing synthetic data (source domain). Multiple discriminators are used to align\nthe features be-tween the source and target domain (real images) at different\nlevels. Then we focus on obtaining and selecting high-quality proxy labels by\nincorporating both the confidence of the class predictor and that from the\nadversarial discriminators. Our discriminators not only work as a regularizer\nto encourage feature alignment but also provide an alternative confidence\nmeasure for generating proxy labels. Relying on the generated high-quality\nproxies, our model can be trained in a \"supervised manner\" on the target\ndo-main. On two major tasks, GTA5->Cityscapes and SYNTHIA->Cityscapes, our\nmethod achieves state-of-the-art results, outperforming the previous by a large\nmargin.\n",
    "topics": "{'Unsupervised Domain Adaptation': 0.99243414, 'Semantic Segmentation': 0.9907541, 'Domain Adaptation': 0.94222474}",
    "score": 0.8954617886
  },
  {
    "id": "1610.07086",
    "title": "Deep image mining for diabetic retinopathy screening",
    "abstract": "  Deep learning is quickly becoming the leading methodology for medical image\nanalysis. Given a large medical archive, where each image is associated with a\ndiagnosis, efficient pathology detectors or classifiers can be trained with\nvirtually no expert knowledge about the target pathologies. However, deep\nlearning algorithms, including the popular ConvNets, are black boxes: little is\nknown about the local patterns analyzed by ConvNets to make a decision at the\nimage level. A solution is proposed in this paper to create heatmaps showing\nwhich pixels in images play a role in the image-level predictions. In other\nwords, a ConvNet trained for image-level classification can be used to detect\nlesions as well. A generalization of the backpropagation method is proposed in\norder to train ConvNets that produce high-quality heatmaps. The proposed\nsolution is applied to diabetic retinopathy (DR) screening in a dataset of\nalmost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy\ncompetition and a private dataset of almost 110,000 photographs (e-ophtha). For\nthe task of detecting referable DR, very good detection performance was\nachieved: $A_z = 0.954$ in Kaggle's dataset and $A_z = 0.949$ in e-ophtha.\nPerformance was also evaluated at the image level and at the lesion level in\nthe DiaretDB1 dataset, where four types of lesions are manually segmented:\nmicroaneurysms, hemorrhages, exudates and cotton-wool spots. The proposed\ndetector outperforms recent algorithms trained to detect those lesions\nspecifically, as well as competing heatmap generation algorithms for ConvNets.\nThis detector is part of the Messidor system for mobile eye pathology\nscreening. Because it does not rely on expert knowledge or manual segmentation\nfor detecting relevant patterns, the proposed solution is a promising image\nmining tool, which has the potential to discover new biomarkers in images.\n",
    "topics": "{}",
    "score": 0.8953567976
  },
  {
    "id": "1812.02793",
    "title": "Generation of Synthetic Electronic Medical Record Text",
    "abstract": "  Machine learning (ML) and Natural Language Processing (NLP) have achieved\nremarkable success in many fields and have brought new opportunities and high\nexpectation in the analyses of medical data. The most common type of medical\ndata is the massive free-text electronic medical records (EMR). It is widely\nregarded that mining such massive data can bring up important information for\nimproving medical practices as well as for possible new discoveries on complex\ndiseases. However, the free EMR texts are lacking consistent standards, rich of\nprivate information, and limited in availability. Also, as they are accumulated\nfrom everyday practices, it is often hard to have a balanced number of samples\nfor the types of diseases under study. These problems hinder the development of\nML and NLP methods for EMR data analysis. To tackle these problems, we\ndeveloped a model to generate synthetic text of EMRs called Medical Text\nGenerative Adversarial Network or mtGAN. It is based on the GAN framework and\nis trained by the REINFORCE algorithm. It takes disease features as inputs and\ngenerates synthetic texts as EMRs for the corresponding diseases. We evaluate\nthe model from micro-level, macro-level and application-level on a Chinese EMR\ntext dataset. The results show that the method has a good capacity to fit real\ndata and can generate realistic and diverse EMR samples. This provides a novel\nway to avoid potential leakage of patient privacy while still supply sufficient\nwell-controlled cohort data for developing downstream ML and NLP methods. It\ncan also be used as a data augmentation method to assist studies based on real\nEMR data.\n",
    "topics": "{'Data Augmentation': 0.99957937}",
    "score": 0.8950141681
  },
  {
    "id": "1903.09240",
    "title": "Deep Radiomics for Brain Tumor Detection and Classification from\n  Multi-Sequence MRI",
    "abstract": "  Glioma constitutes 80% of malignant primary brain tumors and is usually\nclassified as HGG and LGG. The LGG tumors are less aggressive, with slower\ngrowth rate as compared to HGG, and are responsive to therapy. Tumor biopsy\nbeing challenging for brain tumor patients, noninvasive imaging techniques like\nMagnetic Resonance Imaging (MRI) have been extensively employed in diagnosing\nbrain tumors. Therefore automated systems for the detection and prediction of\nthe grade of tumors based on MRI data becomes necessary for assisting doctors\nin the framework of augmented intelligence. In this paper, we thoroughly\ninvestigate the power of Deep ConvNets for classification of brain tumors using\nmulti-sequence MR images. We propose novel ConvNet models, which are trained\nfrom scratch, on MRI patches, slices, and multi-planar volumetric slices. The\nsuitability of transfer learning for the task is next studied by applying two\nexisting ConvNets models (VGGNet and ResNet) trained on ImageNet dataset,\nthrough fine-tuning of the last few layers. LOPO testing, and testing on the\nholdout dataset are used to evaluate the performance of the ConvNets. Results\ndemonstrate that the proposed ConvNets achieve better accuracy in all cases\nwhere the model is trained on the multi-planar volumetric dataset. Unlike\nconventional models, it obtains a testing accuracy of 95% for the low/high\ngrade glioma classification problem. A score of 97% is generated for\nclassification of LGG with/without 1p/19q codeletion, without any additional\neffort towards extraction and selection of features. We study the properties of\nself-learned kernels/ filters in different layers, through visualization of the\nintermediate layer outputs. We also compare the results with that of\nstate-of-the-art methods, demonstrating a maximum improvement of 7% on the\ngrading performance of ConvNets and 9% on the prediction of 1p/19q codeletion\nstatus.\n",
    "topics": "{'Transfer Learning': 0.9162163, 'Tumor Segmentation': 0.45762786, 'Brain Tumor Segmentation': 0.42569596}",
    "score": 0.8946833239
  },
  {
    "id": "1803.08840",
    "title": "Effective deep learning training for single-image super-resolution in\n  endomicroscopy exploiting video-registration-based reconstruction",
    "abstract": "  Purpose: Probe-based Confocal Laser Endomicroscopy (pCLE) is a recent imaging\nmodality that allows performing in vivo optical biopsies. The design of pCLE\nhardware, and its reliance on an optical fibre bundle, fundamentally limits the\nimage quality with a few tens of thousands fibres, each acting as the\nequivalent of a single-pixel detector, assembled into a single fibre bundle.\nVideo-registration techniques can be used to estimate high-resolution (HR)\nimages by exploiting the temporal information contained in a sequence of\nlow-resolution (LR) images. However, the alignment of LR frames, required for\nthe fusion, is computationally demanding and prone to artefacts. Methods: In\nthis work, we propose a novel synthetic data generation approach to train\nexemplar-based Deep Neural Networks (DNNs). HR pCLE images with enhanced\nquality are recovered by the models trained on pairs of estimated HR images\n(generated by the video-registration algorithm) and realistic synthetic LR\nimages. Performance of three different state-of-the-art DNNs techniques were\nanalysed on a Smart Atlas database of 8806 images from 238 pCLE video\nsequences. The results were validated through an extensive Image Quality\nAssessment (IQA) that takes into account different quality scores, including a\nMean Opinion Score (MOS). Results: Results indicate that the proposed solution\nproduces an effective improvement in the quality of the obtained reconstructed\nimage. Conclusion: The proposed training strategy and associated DNNs allows us\nto perform convincing super-resolution of pCLE images.\n",
    "topics": "{'Synthetic Data Generation': 0.9999664, 'Image Quality Assessment': 0.9999602, 'Super-Resolution': 0.9994473, 'Image Super-Resolution': 0.9992805, 'Super Resolution': 0.9981406}",
    "score": 0.8946107068
  },
  {
    "id": "2003.00637",
    "title": "A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view\n  Stereo Reconstruction from An Open Aerial Dataset",
    "abstract": "  A great deal of research has demonstrated recently that multi-view stereo\n(MVS) matching can be solved with deep learning methods. However, these efforts\nwere focused on close-range objects and only a very few of the deep\nlearning-based methods were specifically designed for large-scale 3D urban\nreconstruction due to the lack of multi-view aerial image benchmarks. In this\npaper, we present a synthetic aerial dataset, called the WHU dataset, we\ncreated for MVS tasks, which, to our knowledge, is the first large-scale\nmulti-view aerial dataset. It was generated from a highly accurate 3D digital\nsurface model produced from thousands of real aerial images with precise camera\nparameters. We also introduce in this paper a novel network, called RED-Net,\nfor wide-range depth inference, which we developed from a recurrent\nencoder-decoder structure to regularize cost maps across depths and a 2D fully\nconvolutional network as framework. RED-Net's low memory requirements and high\nperformance make it suitable for large-scale and highly accurate 3D Earth\nsurface reconstruction. Our experiments confirmed that not only did our method\nexceed the current state-of-the-art MVS methods by more than 50% mean absolute\nerror (MAE) with less memory and computational cost, but its efficiency as\nwell. It outperformed one of the best commercial software programs based on\nconventional methods, improving their efficiency 16 times over. Moreover, we\nproved that our RED-Net model pre-trained on the synthetic WHU dataset can be\nefficiently transferred to very different multi-view aerial image datasets\nwithout any fine-tuning. Dataset are available at http://gpcv.whu.edu.cn/data.\n",
    "topics": "{'3D Reconstruction': 0.3038464}",
    "score": 0.8941015359
  },
  {
    "id": "1911.02098",
    "title": "A Scalable Multilabel Classification to Deploy Deep Learning\n  Architectures For Edge Devices",
    "abstract": "  Convolution Neural Networks (CNN) have performed well in many applications\nsuch as object detection, pattern recognition, video surveillance and so on.\nCNN carryout feature extraction on labelled data to perform classification.\nMulti-label classification assigns more than one label to a particular data\nsample in a data set. In multi-label classification, properties of a data point\nthat are considered to be mutually exclusive are classified. However, existing\nmulti-label classification requires some form of data pre-processing that\ninvolves image training data cropping or image tiling. The computation and\nmemory requirement of these multi-label CNN models makes their deployment on\nedge devices challenging. In this paper, we propose a methodology that solves\nthis problem by extending the capability of existing multi-label classification\nand provide models with lower latency that requires smaller memory size when\ndeployed on edge devices. We make use of a single CNN model designed with\nmultiple loss layers and multiple accuracy layers. This methodology is tested\non state-of-the-art deep learning algorithms such as AlexNet, GoogleNet and\nSqueezeNet using the Stanford Cars Dataset and deployed on Raspberry Pi3. From\nthe results the proposed methodology achieves comparable accuracy with 1.8x\nless MACC operation, 0.97x reduction in latency and 0.5x, 0.84x and 0.97x\nreduction in size for the generated AlexNet, GoogleNet and SqueezeNet CNN\nmodels respectively when compared to conventional ways of achieving multi-label\nclassification like hard-coding multi-label instances into single labels. The\nmethodology also yields CNN models that achieve 50\\% less MACC operations, 50%\nreduction in latency and size of generated versions of AlexNet, GoogleNet and\nSqueezeNet respectively when compared to conventional ways using 2 different\nsingle-labelled models to achieve multi-label classification.\n",
    "topics": "{'Multi-Label Classification': 1.0, 'Object Detection': 0.3133998}",
    "score": 0.893862234
  },
  {
    "id": "1904.04691",
    "title": "Fast Enhanced CT Metal Artifact Reduction using Data Domain Deep\n  Learning",
    "abstract": "  Filtered back projection (FBP) is the most widely used method for image\nreconstruction in X-ray computed tomography (CT) scanners. The presence of\nhyper-dense materials in a scene, such as metals, can strongly attenuate\nX-rays, producing severe streaking artifacts in the reconstruction. These metal\nartifacts can greatly limit subsequent object delineation and information\nextraction from the images, restricting their diagnostic value. This problem is\nparticularly acute in the security domain, where there is great heterogeneity\nin the objects that can appear in a scene, highly accurate decisions must be\nmade quickly. The standard practical approaches to reducing metal artifacts in\nCT imagery are either simplistic non-adaptive interpolation-based projection\ndata completion methods or direct image post-processing methods. These standard\napproaches have had limited success. Motivated primarily by security\napplications, we present a new deep-learning-based metal artifact reduction\n(MAR) approach that tackles the problem in the projection data domain. We treat\nthe projection data corresponding to metal objects as missing data and train an\nadversarial deep network to complete the missing data in the projection domain.\nThe subsequent complete projection data is then used with FBP to reconstruct\nimage intended to be free of artifacts. This new approach results in an\nend-to-end MAR algorithm that is computationally efficient so practical and\nfits well into existing CT workflows allowing easy adoption in existing\nscanners. Training deep networks can be challenging, and another contribution\nof our work is to demonstrate that training data generated using an accurate\nX-ray simulation can be used to successfully train the deep network when\ncombined with transfer learning using limited real data sets. We demonstrate\nthe effectiveness and potential of our algorithm on simulated and real\nexamples.\n",
    "topics": "{'Image Reconstruction': 0.99912447, 'Computed Tomography (CT)': 0.82968336, 'Transfer Learning': 0.43133423}",
    "score": 0.8937729956
  },
  {
    "id": "2005.08551",
    "title": "Omni-supervised Facial Expression Recognition: A Simple Baseline",
    "abstract": "  In this paper, we target on advancing the performance in facial expression\nrecognition (FER) by exploiting omni-supervised learning. The current state of\nthe art FER approaches usually aim to recognize facial expressions in a\ncontrolled environment by training models with a limited number of samples. To\nenhance the robustness of the learned models for various scenarios, we propose\nto perform omni-supervised learning by exploiting the labeled samples together\nwith a large number of unlabeled data. Particularly, we first employ\nMS-Celeb-1M as the facial-pool where around 5,822K unlabeled facial images are\nincluded. Then, a primitive model learned on a small number of labeled samples\nis adopted to select samples with high confidence from the facial-pool by\nconducting feature-based similarity comparison. We find the new dataset\nconstructed in such an omni-supervised manner can significantly improve the\ngeneralization ability of the learned FER model and boost the performance\nconsequently. However, as more training samples are used, more computation\nresources and training time are required, which is usually not affordable in\nmany circumstances. To relieve the requirement of computational resources, we\nfurther adopt a dataset distillation strategy to distill the target\ntask-related knowledge from the new mined samples and compressed them into a\nvery small set of images. This distilled dataset is capable of boosting the\nperformance of FER with few additional computational cost introduced. We\nperform extensive experiments on five popular benchmarks and a newly\nconstructed dataset, where consistent gains can be achieved under various\nsettings using the proposed framework. We hope this work will serve as a solid\nbaseline and help ease future research in FER.\n",
    "topics": "{'Facial Expression Recognition': 1.0}",
    "score": 0.8936391481
  },
  {
    "id": "1807.10711",
    "title": "Region of Interest Detection in Dermoscopic Images for Natural\n  Data-augmentation",
    "abstract": "  With the rapid growth of medical imaging research, there is a great interest\nin the automated detection of skin lesions with computer algorithms. The\nstate-of-the-art datasets for skin lesions are often accompanied with very\nlimited amount of ground truth labeling as it is laborious and expensive. The\nRegion Of Interest (ROI) detection is vital to locate the lesion accurately and\nmust be robust to subtle features of different skin lesion types. In this work,\nwe propose the use of two object localization meta-architectures for end-to-end\nROI skin lesion detection in dermoscopic images. We trained the\nFaster-RCNN-InceptionV2 and SSD-InceptionV2 on the ISBI-2017 training dataset\nand evaluated their performance on the ISBI-2017 testing set, PH2 and HAM10000\ndatasets. Since there was no earlier work in ROI detection for skin lesion with\nCNNs, we compared the performance of skin localization methods with the\nstate-of-the-art segmentation method. The localization methods proved superior\nto the segmentation method in ROI detection on skin lesion datasets. In\naddition, based on the detected ROI, an automated natural data-augmentation\nmethod is proposed and used as pre-processing in the lesion diagnosis and\nsegmentation task. To further demonstrate the potential of our work, we\ndeveloped a real-time smart-phone application for automated skin lesions\ndetection.\n",
    "topics": "{'Object Localization': 0.9999999, 'Lesion Classification': 0.9900013, 'Data Augmentation': 0.4232922}",
    "score": 0.8928067842
  },
  {
    "id": "1807.05713",
    "title": "Land-Cover Classification with High-Resolution Remote Sensing Images\n  Using Transferable Deep Models",
    "abstract": "  In recent years, large amount of high spatial-resolution remote sensing\n(HRRS) images are available for land-cover mapping. However, due to the complex\ninformation brought by the increased spatial resolution and the data\ndisturbances caused by different conditions of image acquisition, it is often\ndifficult to find an efficient method for achieving accurate land-cover\nclassification with high-resolution and heterogeneous remote sensing images. In\nthis paper, we propose a scheme to apply deep model obtained from labeled\nland-cover dataset to classify unlabeled HRRS images. The main idea is to rely\non deep neural networks for presenting the contextual information contained in\ndifferent types of land-covers and propose a pseudo-labeling and sample\nselection scheme for improving the transferability of deep models. More\nprecisely, a deep Convolutional Neural Networks is first pre-trained with a\nwell-annotated land-cover dataset, referred to as the source data. Then, given\na target image with no labels, the pre-trained CNN model is utilized to\nclassify the image in a patch-wise manner. The patches with high confidence are\nassigned with pseudo-labels and employed as the queries to retrieve related\nsamples from the source data. The pseudo-labels confirmed with the retrieved\nresults are regarded as supervised information for fine-tuning the pre-trained\ndeep model. To obtain a pixel-wise land-cover classification with the target\nimage, we rely on the fine-tuned CNN and develop a hybrid classification by\ncombining patch-wise classification and hierarchical segmentation. In addition,\nwe create a large-scale land-cover dataset containing 150 Gaofen-2 satellite\nimages for CNN pre-training. Experiments on multi-source HRRS images show\nencouraging results and demonstrate the applicability of the proposed scheme to\nland-cover classification.\n",
    "topics": "{}",
    "score": 0.89236221
  },
  {
    "id": "1809.07436",
    "title": "Deep Generative Classifiers for Thoracic Disease Diagnosis with Chest\n  X-ray Images",
    "abstract": "  Thoracic diseases are very serious health problems that plague a large number\nof people. Chest X-ray is currently one of the most popular methods to diagnose\nthoracic diseases, playing an important role in the healthcare workflow.\nHowever, reading the chest X-ray images and giving an accurate diagnosis remain\nchallenging tasks for expert radiologists. With the success of deep learning in\ncomputer vision, a growing number of deep neural network architectures were\napplied to chest X-ray image classification. However, most of the previous deep\nneural network classifiers were based on deterministic architectures which are\nusually very noise-sensitive and are likely to aggravate the overfitting issue.\nIn this paper, to make a deep architecture more robust to noise and to reduce\noverfitting, we propose using deep generative classifiers to automatically\ndiagnose thorax diseases from the chest X-ray images. Unlike the traditional\ndeterministic classifier, a deep generative classifier has a distribution\nmiddle layer in the deep neural network. A sampling layer then draws a random\nsample from the distribution layer and input it to the following layer for\nclassification. The classifier is generative because the class label is\ngenerated from samples of a related distribution. Through training the model\nwith a certain amount of randomness, the deep generative classifiers are\nexpected to be robust to noise and can reduce overfitting and then achieve good\nperformances. We implemented our deep generative classifiers based on a number\nof well-known deterministic neural network architectures, and tested our models\non the chest X-ray14 dataset. The results demonstrated the superiority of deep\ngenerative classifiers compared with the corresponding deep deterministic\nclassifiers.\n",
    "topics": "{'Image Classification': 0.93606883}",
    "score": 0.8920062072
  },
  {
    "id": "2003.04210",
    "title": "Semantic Object Prediction and Spatial Sound Super-Resolution with\n  Binaural Sounds",
    "abstract": "  Humans can robustly recognize and localize objects by integrating visual and\nauditory cues. While machines are able to do the same now with images, less\nwork has been done with sounds. This work develops an approach for dense\nsemantic labelling of sound-making objects, purely based on binaural sounds. We\npropose a novel sensor setup and record a new audio-visual dataset of street\nscenes with eight professional binaural microphones and a 360 degree camera.\nThe co-existence of visual and audio cues is leveraged for supervision\ntransfer. In particular, we employ a cross-modal distillation framework that\nconsists of a vision `teacher' method and a sound `student' method -- the\nstudent method is trained to generate the same results as the teacher method.\nThis way, the auditory system can be trained without using human annotations.\nWe also propose two auxiliary tasks namely, a) a novel task on Spatial Sound\nSuper-resolution to increase the spatial resolution of sounds, and b) dense\ndepth prediction of the scene. We then formulate the three tasks into one\nend-to-end trainable multi-tasking network aiming to boost the overall\nperformance. Experimental results on the dataset show that 1) our method\nachieves promising results for semantic prediction and the two auxiliary tasks;\nand 2) the three tasks are mutually beneficial -- training them together\nachieves the best performance and 3) the number and orientations of microphones\nare both important. The data and code will be released to facilitate the\nresearch in this new direction.\n",
    "topics": "{'Super Resolution': 0.99919766, 'Super-Resolution': 0.9984762, 'Depth Estimation': 0.90861374}",
    "score": 0.891754297
  },
  {
    "id": "1912.04670",
    "title": "DR-GAN: Conditional Generative Adversarial Network for Fine-Grained\n  Lesion Synthesis on Diabetic Retinopathy Images",
    "abstract": "  Diabetic retinopathy (DR) is a complication of diabetes that severely affects\neyes. It can be graded into five levels of severity according to international\nprotocol. However, optimizing a grading model to have strong generalizability\nrequires a large amount of balanced training data, which is difficult to\ncollect particularly for the high severity levels. Typical data augmentation\nmethods, including random flipping and rotation, cannot generate data with high\ndiversity. In this paper, we propose a diabetic retinopathy generative\nadversarial network (DR-GAN) to synthesize high-resolution fundus images which\ncan be manipulated with arbitrary grading and lesion information. Thus,\nlarge-scale generated data can be used for more meaningful augmentation to\ntrain a DR grading and lesion segmentation model. The proposed retina generator\nis conditioned on the structural and lesion masks, as well as adaptive grading\nvectors sampled from the latent grading space, which can be adopted to control\nthe synthesized grading severity. Moreover, a multi-scale spatial and channel\nattention module is devised to improve the generation ability to synthesize\ndetails. Multi-scale discriminators are designed to operate from large to small\nreceptive fields, and joint adversarial losses are adopted to optimize the\nwhole network in an end-to-end manner. With extensive experiments evaluated on\nthe EyePACS dataset connected to Kaggle, as well as the FGADR dataset, we\nvalidate the effectiveness of our method, which can both synthesize highly\nrealistic (1280 x 1280) controllable fundus images and contribute to the DR\ngrading task.\n",
    "topics": "{'Lesion Segmentation': 0.99988484, 'Data Augmentation': 0.9995808}",
    "score": 0.891712701
  },
  {
    "id": "1509.03371",
    "title": "Efficient Convolutional Neural Networks for Pixelwise Classification on\n  Heterogeneous Hardware Systems",
    "abstract": "  This work presents and analyzes three convolutional neural network (CNN)\nmodels for efficient pixelwise classification of images. When using\nconvolutional neural networks to classify single pixels in patches of a whole\nimage, a lot of redundant computations are carried out when using sliding\nwindow networks. This set of new architectures solve this issue by either\nremoving redundant computations or using fully convolutional architectures that\ninherently predict many pixels at once.\n  The implementations of the three models are accessible through a new utility\non top of the Caffe library. The utility provides support for a wide range of\nimage input and output formats, pre-processing parameters and methods to\nequalize the label histogram during training. The Caffe library has been\nextended by new layers and a new backend for availability on a wider range of\nhardware such as CPUs and GPUs through OpenCL.\n  On AMD GPUs, speedups of $54\\times$ (SK-Net), $437\\times$ (U-Net) and\n$320\\times$ (USK-Net) have been observed, taking the SK equivalent SW (sliding\nwindow) network as the baseline. The label throughput is up to one megapixel\nper second.\n  The analyzed neural networks have distinctive characteristics that apply\nduring training or processing, and not every data set is suitable to every\narchitecture. The quality of the predictions is assessed on two neural tissue\ndata sets, of which one is the ISBI 2012 challenge data set. Two different loss\nfunctions, Malis loss and Softmax loss, were used during training.\n  The whole pipeline, consisting of models, interface and modified Caffe\nlibrary, is available as Open Source software under the working title Project\nGreentea.\n",
    "topics": "{}",
    "score": 0.8916021447
  },
  {
    "id": "1912.02851",
    "title": "Cross-Resolution Learning for Face Recognition",
    "abstract": "  Convolutional Neural Networks have reached extremely high performances on the\nFace Recognition task. Largely used datasets, such as VGGFace2, focus on\ngender, pose and age variations trying to balance them to achieve better\nresults. However, the fact that images have different resolutions is not\nusually discussed and resize to 256 pixels before cropping is used. While\nspecific datasets for very low resolution faces have been proposed, less\nattention has been payed on the task of cross-resolution matching. Such\nscenarios are of particular interest for forensic and surveillance systems in\nwhich it usually happens that a low-resolution probe has to be matched with\nhigher-resolution galleries. While it is always possible to either increase the\nresolution of the probe image or to reduce the size of the gallery images, to\nthe best of our knowledge an extensive experimentation of cross-resolution\nmatching was missing in the recent deep learning based literature. In the\ncontext of low- and cross-resolution Face Recognition, the contributions of our\nwork are: i) we proposed a training method to fine-tune a state-of-the-art\nmodel in order to make it able to extract resolution-robust deep features; ii)\nwe tested our models on the benchmark datasets IJB-B/C considering images at\nboth full and low resolutions in order to show the effectiveness of the\nproposed training algorithm. To the best of our knowledge, this is the first\nwork testing extensively the performance of a FR model in a cross-resolution\nscenario; iii) we tested our models on the low resolution and low quality\ndatasets QMUL-SurvFace and TinyFace and showed their superior performances,\neven though we did not train our model on low-resolution faces only and our\nmain focus was cross-resolution; iv) we showed that our approach can be more\neffective with respect to preprocessing faces with super resolution techniques.\n",
    "topics": "{'Face Recognition': 1.0, 'Super-Resolution': 0.79974604, 'Super Resolution': 0.7835261}",
    "score": 0.8910940523
  },
  {
    "id": "1903.00348",
    "title": "Transformation Consistent Self-ensembling Model for Semi-supervised\n  Medical Image Segmentation",
    "abstract": "  Deep convolutional neural networks have achieved remarkable progress on a\nvariety of medical image computing tasks. A common problem when applying\nsupervised deep learning methods to medical images is the lack of labeled data,\nwhich is very expensive and time-consuming to be collected. In this paper, we\npresent a novel semi-supervised method for medical image segmentation, where\nthe network is optimized by the weighted combination of a common supervised\nloss for labeled inputs only and a regularization loss for both labeled and\nunlabeled data. To utilize the unlabeled data, our method encourages the\nconsistent predictions of the network-in-training for the same input under\ndifferent regularizations. Aiming for the semi-supervised segmentation problem,\nwe enhance the effect of regularization for pixel-level predictions by\nintroducing a transformation, including rotation and flipping, consistent\nscheme in our self-ensembling model. With the aim of semi-supervised\nsegmentation tasks, we introduce a transformation consistent strategy in our\nself-ensembling model to enhance the regularization effect for pixel-level\npredictions. We have extensively validated the proposed semi-supervised method\non three typical yet challenging medical image segmentation tasks: (i) skin\nlesion segmentation from dermoscopy images on International Skin Imaging\nCollaboration (ISIC) 2017 dataset, (ii) optic disc segmentation from fundus\nimages on Retinal Fundus Glaucoma Challenge (REFUGE) dataset, and (iii) liver\nsegmentation from volumetric CT scans on Liver Tumor Segmentation Challenge\n(LiTS) dataset. Compared to the state-of-the-arts, our proposed method shows\nsuperior segmentation performance on challenging 2D/3D medical images,\ndemonstrating the effectiveness of our semi-supervised method for medical image\nsegmentation.\n",
    "topics": "{'Medical Image Segmentation': 1.0, 'Semantic Segmentation': 0.99999917, 'Tumor Segmentation': 0.9999882, 'Lesion Segmentation': 0.99121}",
    "score": 0.8909695761
  },
  {
    "id": "2010.00243",
    "title": "MLRSNet: A Multi-label High Spatial Resolution Remote Sensing Dataset\n  for Semantic Scene Understanding",
    "abstract": "  To better understand scene images in the field of remote sensing, multi-label\nannotation of scene images is necessary. Moreover, to enhance the performance\nof deep learning models for dealing with semantic scene understanding tasks, it\nis vital to train them on large-scale annotated data. However, most existing\ndatasets are annotated by a single label, which cannot describe the complex\nremote sensing images well because scene images might have multiple land cover\nclasses. Few multi-label high spatial resolution remote sensing datasets have\nbeen developed to train deep learning models for multi-label based tasks, such\nas scene classification and image retrieval. To address this issue, in this\npaper, we construct a multi-label high spatial resolution remote sensing\ndataset named MLRSNet for semantic scene understanding with deep learning from\nthe overhead perspective. It is composed of high-resolution optical satellite\nor aerial images. MLRSNet contains a total of 109,161 samples within 46 scene\ncategories, and each image has at least one of 60 predefined labels. We have\ndesigned visual recognition tasks, including multi-label based image\nclassification and image retrieval, in which a wide variety of deep learning\napproaches are evaluated with MLRSNet. The experimental results demonstrate\nthat MLRSNet is a significant benchmark for future research, and it complements\nthe current widely used datasets such as ImageNet, which fills gaps in\nmulti-label image research. Furthermore, we will continue to expand the\nMLRSNet. MLRSNet and all related materials have been made publicly available at\nhttps://data.mendeley.com/datasets/7j9bv9vwsx/2 and\nhttps://github.com/cugbrs/MLRSNet.git.\n",
    "topics": "{'Scene Understanding': 1.0, 'Scene Classification': 0.99959475, 'Image Retrieval': 0.97409886, 'Image Classification': 0.7820363, 'Semantic Segmentation': 0.45051345}",
    "score": 0.8907673435
  },
  {
    "id": "1802.08960",
    "title": "Bonnet: An Open-Source Training and Deployment Framework for Semantic\n  Segmentation in Robotics using CNNs",
    "abstract": "  The ability to interpret a scene is an important capability for a robot that\nis supposed to interact with its environment. The knowledge of what is in front\nof the robot is, for example, relevant for navigation, manipulation, or\nplanning. Semantic segmentation labels each pixel of an image with a class\nlabel and thus provides a detailed semantic annotation of the surroundings to\nthe robot. Convolutional neural networks (CNNs) are popular methods for\naddressing this type of problem. The available software for training and the\nintegration of CNNs for real robots, however, is quite fragmented and often\ndifficult to use for non-experts, despite the availability of several\nhigh-quality open-source frameworks for neural network implementation and\ntraining. In this paper, we propose a tool called Bonnet, which addresses this\nfragmentation problem by building a higher abstraction that is specific for the\nsemantic segmentation task. It provides a modular approach to simplify the\ntraining of a semantic segmentation CNN independently of the used dataset and\nthe intended task. Furthermore, we also address the deployment on a real\nrobotic platform. Thus, we do not propose a new CNN approach in this paper.\nInstead, we provide a stable and easy-to-use tool to make this technology more\napproachable in the context of autonomous systems. In this sense, we aim at\nclosing a gap between computer vision research and its use in robotics\nresearch. We provide an open-source codebase for training and deployment. The\ntraining interface is implemented in Python using TensorFlow and the deployment\ninterface provides a C++ library that can be easily integrated in an existing\nrobotics codebase, a ROS node, and two standalone applications for label\nprediction in images and videos.\n",
    "topics": "{'Semantic Segmentation': 0.9999862}",
    "score": 0.8906833991
  },
  {
    "id": "1711.05942",
    "title": "Learning from Millions of 3D Scans for Large-scale 3D Face Recognition",
    "abstract": "  Deep networks trained on millions of facial images are believed to be closely\napproaching human-level performance in face recognition. However, open world\nface recognition still remains a challenge. Although, 3D face recognition has\nan inherent edge over its 2D counterpart, it has not benefited from the recent\ndevelopments in deep learning due to the unavailability of large training as\nwell as large test datasets. Recognition accuracies have already saturated on\nexisting 3D face datasets due to their small gallery sizes. Unlike 2D\nphotographs, 3D facial scans cannot be sourced from the web causing a\nbottleneck in the development of deep 3D face recognition networks and\ndatasets. In this backdrop, we propose a method for generating a large corpus\nof labeled 3D face identities and their multiple instances for training and a\nprotocol for merging the most challenging existing 3D datasets for testing. We\nalso propose the first deep CNN model designed specifically for 3D face\nrecognition and trained on 3.1 Million 3D facial scans of 100K identities. Our\ntest dataset comprises 1,853 identities with a single 3D scan in the gallery\nand another 31K scans as probes, which is several orders of magnitude larger\nthan existing ones. Without fine tuning on this dataset, our network already\noutperforms state of the art face recognition by over 10%. We fine tune our\nnetwork on the gallery set to perform end-to-end large scale 3D face\nrecognition which further improves accuracy. Finally, we show the efficacy of\nour method for the open world face recognition problem.\n",
    "topics": "{'Face Recognition': 1.0}",
    "score": 0.8905041412
  },
  {
    "id": "2007.03083",
    "title": "Scalable, Proposal-free Instance Segmentation Network for 3D Pixel\n  Clustering and Particle Trajectory Reconstruction in Liquid Argon Time\n  Projection Chambers",
    "abstract": "  Liquid Argon Time Projection Chambers (LArTPCs) are high resolution particle\nimaging detectors, employed by accelerator-based neutrino oscillation\nexperiments for high precision physics measurements. While images of particle\ntrajectories are intuitive to analyze for physicists, the development of a high\nquality, automated data reconstruction chain remains challenging. One of the\nmost critical reconstruction steps is particle clustering: the task of grouping\n3D image pixels into different particle instances that share the same particle\ntype. In this paper, we propose the first scalable deep learning algorithm for\nparticle clustering in LArTPC data using sparse convolutional neural networks\n(SCNN). Building on previous works on SCNNs and proposal free instance\nsegmentation, we build an end-to-end trainable instance segmentation network\nthat learns an embedding of the image pixels to perform point cloud clustering\nin a transformed space. We benchmark the performance of our algorithm on\nPILArNet, a public 3D particle imaging dataset, with respect to common\nclustering evaluation metrics. 3D pixels were successfully clustered into\nindividual particle trajectories with 90% of them having an adjusted Rand index\nscore greater than 92% with a mean pixel clustering efficiency and purity above\n96%. This work contributes to the development of an end-to-end optimizable full\ndata reconstruction chain for LArTPCs, in particular pixel-based 3D imaging\ndetectors including the near detector of the Deep Underground Neutrino\nExperiment. Our algorithm is made available in the open access repository, and\nwe share our Singularity software container, which can be used to reproduce our\nwork on the dataset.\n",
    "topics": "{'Instance Segmentation': 1.0, 'Semantic Segmentation': 0.9956945}",
    "score": 0.8904625712
  },
  {
    "id": "2004.10507",
    "title": "Deep Learning for Screening COVID-19 using Chest X-Ray Images",
    "abstract": "  With the ever increasing demand for screening millions of prospective \"novel\ncoronavirus\" or COVID-19 cases, and due to the emergence of high false\nnegatives in the commonly used PCR tests, the necessity for probing an\nalternative simple screening mechanism of COVID-19 using radiological images\n(like chest X-Rays) assumes importance. In this scenario, machine learning (ML)\nand deep learning (DL) offer fast, automated, effective strategies to detect\nabnormalities and extract key features of the altered lung parenchyma, which\nmay be related to specific signatures of the COVID-19 virus. However, the\navailable COVID-19 datasets are inadequate to train deep neural networks.\nTherefore, we propose a new concept called domain extension transfer learning\n(DETL). We employ DETL, with pre-trained deep convolutional neural network, on\na related large chest X-Ray dataset that is tuned for classifying between four\nclasses \\textit{viz.} $normal$, $pneumonia$, $other\\_disease$, and $Covid-19$.\nA 5-fold cross validation is performed to estimate the feasibility of using\nchest X-Rays to diagnose COVID-19. The initial results show promise, with the\npossibility of replication on bigger and more diverse data sets. The overall\naccuracy was measured as $90.13\\% \\pm 0.14$. In order to get an idea about the\nCOVID-19 detection transparency, we employed the concept of Gradient Class\nActivation Map (Grad-CAM) for detecting the regions where the model paid more\nattention during the classification. This was found to strongly correlate with\nclinical findings, as validated by experts.\n",
    "topics": "{'Transfer Learning': 0.9747873}",
    "score": 0.8903587233
  },
  {
    "id": "2010.01663",
    "title": "KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image\n  and Volumetric Segmentation",
    "abstract": "  Most methods for medical image segmentation use U-Net or its variants as they\nhave been successful in most of the applications. After a detailed analysis of\nthese \"traditional\" encoder-decoder based approaches, we observed that they\nperform poorly in detecting smaller structures and are unable to segment\nboundary regions precisely. This issue can be attributed to the increase in\nreceptive field size as we go deeper into the encoder. The extra focus on\nlearning high level features causes the U-Net based approaches to learn less\ninformation about low-level features which are crucial for detecting small\nstructures. To overcome this issue, we propose using an overcomplete\nconvolutional architecture where we project our input image into a higher\ndimension such that we constrain the receptive field from increasing in the\ndeep layers of the network. We design a new architecture for image\nsegmentation- KiU-Net which has two branches: (1) an overcomplete convolutional\nnetwork Kite-Net which learns to capture fine details and accurate edges of the\ninput, and (2) U-Net which learns high level features. Furthermore, we also\npropose KiU-Net 3D which is a 3D convolutional architecture for volumetric\nsegmentation. We perform a detailed study of KiU-Net by performing experiments\non five different datasets covering various image modalities like ultrasound\n(US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic\nand fundus images. The proposed method achieves a better performance as\ncompared to all the recent methods with an additional benefit of fewer\nparameters and faster convergence. Additionally, we also demonstrate that the\nextensions of KiU-Net based on residual blocks and dense blocks result in\nfurther performance improvements. The implementation of KiU-Net can be found\nhere: https://github.com/jeya-maria-jose/KiU-Net-pytorch\n",
    "topics": "{'Medical Image Segmentation': 0.9998933, 'Computed Tomography (CT)': 0.9998667, 'Brain Tumor Segmentation': 0.9998165, 'Semantic Segmentation': 0.9957759}",
    "score": 0.8900796704
  },
  {
    "id": "1903.02871",
    "title": "Exploit fully automatic low-level segmented PET data for training\n  high-level deep learning algorithms for the corresponding CT data",
    "abstract": "  We present an approach for fully automatic urinary bladder segmentation in CT\nimages with artificial neural networks in this study. Automatic medical image\nanalysis has become an invaluable tool in the different treatment stages of\ndiseases. Especially medical image segmentation plays a vital role, since\nsegmentation is often the initial step in an image analysis pipeline. Since\ndeep neural networks have made a large impact on the field of image processing\nin the past years, we use two different deep learning architectures to segment\nthe urinary bladder. Both of these architectures are based on pre-trained\nclassification networks that are adapted to perform semantic segmentation.\nSince deep neural networks require a large amount of training data,\nspecifically images and corresponding ground truth labels, we furthermore\npropose a method to generate such a suitable training data set from Positron\nEmission Tomography/Computed Tomography image data. This is done by applying\nthresholding to the Positron Emission Tomography data for obtaining a ground\ntruth and by utilizing data augmentation to enlarge the dataset. In this study,\nwe discuss the influence of data augmentation on the segmentation results, and\ncompare and evaluate the proposed architectures in terms of qualitative and\nquantitative segmentation performance. The results presented in this study\nallow concluding that deep neural networks can be considered a promising\napproach to segment the urinary bladder in CT images.\n",
    "topics": "{'Data Augmentation': 0.9998814, 'Medical Image Segmentation': 0.999033, 'Semantic Segmentation': 0.9979777}",
    "score": 0.8898850165
  },
  {
    "id": "1611.08107",
    "title": "Automatically Building Face Datasets of New Domains from Weakly Labeled\n  Data with Pretrained Models",
    "abstract": "  Training data are critical in face recognition systems. However, labeling a\nlarge scale face data for a particular domain is very tedious. In this paper,\nwe propose a method to automatically and incrementally construct datasets from\nmassive weakly labeled data of the target domain which are readily available on\nthe Internet under the help of a pretrained face model. More specifically,\ngiven a large scale weakly labeled dataset in which each face image is\nassociated with a label, i.e. the name of an identity, we create a graph for\neach identity with edges linking matched faces verified by the existing model\nunder a tight threshold. Then we use the maximal subgraph as the cleaned data\nfor that identity. With the cleaned dataset, we update the existing face model\nand use the new model to filter the original dataset to get a larger cleaned\ndataset. We collect a large weakly labeled dataset containing 530,560 Asian\nface images of 7,962 identities from the Internet, which will be published for\nthe study of face recognition. By running the filtering process, we obtain a\ncleaned datasets (99.7+% purity) of size 223,767 (recall 70.9%). On our testing\ndataset of Asian faces, the model trained by the cleaned dataset achieves\nrecognition rate 93.1%, which obviously outperforms the model trained by the\npublic dataset CASIA whose recognition rate is 85.9%.\n",
    "topics": "{'Face Recognition': 0.9999895, 'Face Model': 0.9996897}",
    "score": 0.889801149
  },
  {
    "id": "1903.08536",
    "title": "Segmentation-Based Deep-Learning Approach for Surface-Defect Detection",
    "abstract": "  Automated surface-anomaly detection using machine learning has become an\ninteresting and promising area of research, with a very high and direct impact\non the application domain of visual inspection. Deep-learning methods have\nbecome the most suitable approaches for this task. They allow the inspection\nsystem to learn to detect the surface anomaly by simply showing it a number of\nexemplar images. This paper presents a segmentation-based deep-learning\narchitecture that is designed for the detection and segmentation of surface\nanomalies and is demonstrated on a specific domain of surface-crack detection.\nThe design of the architecture enables the model to be trained using a small\nnumber of samples, which is an important requirement for practical\napplications. The proposed model is compared with the related deep-learning\nmethods, including the state-of-the-art commercial software, showing that the\nproposed approach outperforms the related methods on the specific domain of\nsurface-crack detection. The large number of experiments also shed light on the\nrequired precision of the annotation, the number of required training samples\nand on the required computational cost. Experiments are performed on a newly\ncreated dataset based on a real-world quality control case and demonstrates\nthat the proposed approach is able to learn on a small number of defected\nsurfaces, using only approximately 25-30 defective training samples, instead of\nhundreds or thousands, which is usually the case in deep-learning applications.\nThis makes the deep-learning method practical for use in industry where the\nnumber of available defective samples is limited. The dataset is also made\npublicly available to encourage the development and evaluation of new methods\nfor surface-defect detection.\n",
    "topics": "{}",
    "score": 0.8894866157
  },
  {
    "id": "1810.09658",
    "title": "Face Recognition from Sequential Sparse 3D Data via Deep Registration",
    "abstract": "  Previous works have shown that face recognition with high accurate 3D data is\nmore reliable and insensitive to pose and illumination variations. Recently,\nlow-cost and portable 3D acquisition techniques like ToF(Time of Flight) and\nDoE based structured light systems enable us to access 3D data easily, e.g.,\nvia a mobile phone. However, such devices only provide sparse(limited speckles\nin structured light system) and noisy 3D data which can not support face\nrecognition directly. In this paper, we aim at achieving high-performance face\nrecognition for devices equipped with such modules which is very meaningful in\npractice as such devices will be very popular. We propose a framework to\nperform face recognition by fusing a sequence of low-quality 3D data. As 3D\ndata are sparse and noisy which can not be well handled by conventional methods\nlike the ICP algorithm, we design a PointNet-like Deep Registration\nNetwork(DRNet) which works with ordered 3D point coordinates while preserving\nthe ability of mining local structures via convolution. Meanwhile we develop a\nnovel loss function to optimize our DRNet based on the quaternion expression\nwhich obviously outperforms other widely used functions. For face recognition,\nwe design a deep convolutional network which takes the fused 3D depth-map as\ninput based on AMSoftmax model. Experiments show that our DRNet can achieve\nrotation error 0.95{\\deg} and translation error 0.28mm for registration. The\nface recognition on fused data also achieves rank-1 accuracy 99.2% , FAR-0.001\n97.5% on Bosphorus dataset which is comparable with state-of-the-art\nhigh-quality data based recognition performance.\n",
    "topics": "{'Face Recognition': 1.0}",
    "score": 0.889393312
  },
  {
    "id": "1806.07754",
    "title": "Spatio-Temporal Channel Correlation Networks for Action Classification",
    "abstract": "  The work in this paper is driven by the question if spatio-temporal\ncorrelations are enough for 3D convolutional neural networks (CNN)? Most of the\ntraditional 3D networks use local spatio-temporal features. We introduce a new\nblock that models correlations between channels of a 3D CNN with respect to\ntemporal and spatial features. This new block can be added as a residual unit\nto different parts of 3D CNNs. We name our novel block 'Spatio-Temporal Channel\nCorrelation' (STC). By embedding this block to the current state-of-the-art\narchitectures such as ResNext and ResNet, we improved the performance by 2-3\\%\non Kinetics dataset. Our experiments show that adding STC blocks to current\nstate-of-the-art architectures outperforms the state-of-the-art methods on the\nHMDB51, UCF101 and Kinetics datasets. The other issue in training 3D CNNs is\nabout training them from scratch with a huge labeled dataset to get a\nreasonable performance. So the knowledge learned in 2D CNNs is completely\nignored. Another contribution in this work is a simple and effective technique\nto transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D\nCNN for a stable weight initialization. This allows us to significantly reduce\nthe number of training samples for 3D CNNs. Thus, by fine-tuning this network,\nwe beat the performance of generic and recent methods in 3D CNNs, which were\ntrained on large video datasets, e.g. Sports-1M, and fine-tuned on the target\ndatasets, e.g. HMDB51/UCF101.\n",
    "topics": "{'Action Classification ': 1.0, 'Action Classification': 0.9999999}",
    "score": 0.8886998672
  },
  {
    "id": "1712.07116",
    "title": "Detection and classification of masses in mammographic images in a\n  multi-kernel approach",
    "abstract": "  According to the World Health Organization, breast cancer is the main cause\nof cancer death among adult women in the world. Although breast cancer occurs\nindiscriminately in countries with several degrees of social and economic\ndevelopment, among developing and underdevelopment countries mortality rates\nare still high, due to low availability of early detection technologies. From\nthe clinical point of view, mammography is still the most effective diagnostic\ntechnology, given the wide diffusion of the use and interpretation of these\nimages. Herein this work we propose a method to detect and classify\nmammographic lesions using the regions of interest of images. Our proposal\nconsists in decomposing each image using multi-resolution wavelets. Zernike\nmoments are extracted from each wavelet component. Using this approach we can\ncombine both texture and shape features, which can be applied both to the\ndetection and classification of mammary lesions. We used 355 images of fatty\nbreast tissue of IRMA database, with 233 normal instances (no lesion), 72\nbenign, and 83 malignant cases. Classification was performed by using SVM and\nELM networks with modified kernels, in order to optimize accuracy rates,\nreaching 94.11%. Considering both accuracy rates and training times, we defined\nthe ration between average percentage accuracy and average training time in a\nreverse order. Our proposal was 50 times higher than the ratio obtained using\nthe best method of the state-of-the-art. As our proposed model can combine high\naccuracy rate with low learning time, whenever a new data is received, our work\nwill be able to save a lot of time, hours, in learning process in relation to\nthe best method of the state-of-the-art.\n",
    "topics": "{}",
    "score": 0.8885601141
  },
  {
    "id": "2005.03654",
    "title": "Deep Learning on Point Clouds for False Positive Reduction at Nodule\n  Detection in Chest CT Scans",
    "abstract": "  This paper focuses on a novel approach for false-positive reduction (FPR) of\nnodule candidates in Computer-aided detection (CADe) systems following the\nsuspicious lesions detection stage. Contrary to typical decisions in medical\nimage analysis, the proposed approach considers input data not as a 2D or 3D\nimage, but rather as a point cloud, and uses deep learning models for point\nclouds. We discovered that point cloud models require less memory and are\nfaster both in training and inference compared to traditional CNN 3D, they\nachieve better performance and do not impose restrictions on the size of the\ninput image, i.e. no restrictions on the size of the nodule candidate. We\npropose an algorithm for transforming 3D CT scan data to point cloud. In some\ncases, the volume of the nodule candidate can be much smaller than the\nsurrounding context, for example, in the case of subpleural localization of the\nnodule. Therefore, we developed an algorithm for sampling points from a point\ncloud constructed from a 3D image of the candidate region. The algorithm is\nable to guarantee the capture of both context and candidate information as part\nof the point cloud of the nodule candidate. We designed and set up an\nexperiment in creating a dataset from an open LIDC-IDRI database for a feature\nof the FPR task, and is herein described in detail. Data augmentation was\napplied both to avoid overfitting and as an upsampling method. Experiments were\nconducted with PointNet, PointNet++, and DGCNN. We show that the proposed\napproach outperforms baseline CNN 3D models and resulted in 85.98 FROC versus\n77.26 FROC for baseline models. We compare our algorithm with published SOTA\nand demonstrate that even without significant modifications it works at the\nappropriate performance level on LUNA2016 and shows SOTA on LIDC-IDRI.\n",
    "topics": "{'Data Augmentation': 0.9940602}",
    "score": 0.8885113592
  },
  {
    "id": "2010.07830",
    "title": "Semi-Supervised Semantic Segmentation in Earth Observation: The\n  MiniFrance Suite, Dataset Analysis and Multi-task Network Study",
    "abstract": "  The development of semi-supervised learning techniques is essential to\nenhance the generalization capacities of machine learning algorithms. Indeed,\nraw image data are abundant while labels are scarce, therefore it is crucial to\nleverage unlabeled inputs to build better models. The availability of large\ndatabases have been key for the development of learning algorithms with high\nlevel performance.\n  Despite the major role of machine learning in Earth Observation to derive\nproducts such as land cover maps, datasets in the field are still limited,\neither because of modest surface coverage, lack of variety of scenes or\nrestricted classes to identify. We introduce a novel large-scale dataset for\nsemi-supervised semantic segmentation in Earth Observation, the MiniFrance\nsuite. MiniFrance has several unprecedented properties: it is large-scale,\ncontaining over 2000 very high resolution aerial images, accounting for more\nthan 200 billions samples (pixels); it is varied, covering 16 conurbations in\nFrance, with various climates, different landscapes, and urban as well as\ncountryside scenes; and it is challenging, considering land use classes with\nhigh-level semantics. Nevertheless, the most distinctive quality of MiniFrance\nis being the only dataset in the field especially designed for semi-supervised\nlearning: it contains labeled and unlabeled images in its training partition,\nwhich reproduces a life-like scenario. Along with this dataset, we present\ntools for data representativeness analysis in terms of appearance similarity\nand a thorough study of MiniFrance data, demonstrating that it is suitable for\nlearning and generalizes well in a semi-supervised setting. Finally, we present\nsemi-supervised deep architectures based on multi-task learning and the first\nexperiments on MiniFrance.\n",
    "topics": "{'Multi-Task Learning': 0.9981021, 'Semantic Segmentation': 0.99565256}",
    "score": 0.8882957344
  },
  {
    "id": "1801.05449",
    "title": "ConvSRC: SmartPhone based Periocular Recognition using Deep\n  Convolutional Neural Network and Sparsity Augmented Collaborative\n  Representation",
    "abstract": "  Smartphone based periocular recognition has gained significant attention from\nbiometric research community because of the limitations of biometric modalities\nlike face, iris etc. Most of the existing methods for periocular recognition\nemploy hand-crafted features. Recently, learning based image representation\ntechniques like deep Convolutional Neural Network (CNN) have shown outstanding\nperformance in many visual recognition tasks. CNN needs a huge volume of data\nfor its learning, but for periocular recognition only limited amount of data is\navailable. The solution is to use CNN pre-trained on the dataset from the\nrelated domain, in this case the challenge is to extract efficiently the\ndiscriminative features. Using a pertained CNN model (VGG-Net), we propose a\nsimple, efficient and compact image representation technique that takes into\naccount the wealth of information and sparsity existing in the activations of\nthe convolutional layers and employs principle component analysis. For\nrecognition, we use an efficient and robust Sparse Augmented Collaborative\nRepresentation based Classification (SA-CRC) technique. For thorough evaluation\nof ConvSRC (the proposed system), experiments were carried out on the VISOB\nchallenging database which was presented for periocular recognition competition\nin ICIP2016. The obtained results show the superiority of ConvSRC over the\nstate-of-the-art methods; it obtains a GMR of more than 99% at FMR = 10-3 and\noutperforms the first winner of ICIP2016 challenge by 10%.\n",
    "topics": "{}",
    "score": 0.8882077211
  },
  {
    "id": "2008.07090",
    "title": "Spherical coordinates transformation pre-processing in Deep Convolution\n  Neural Networks for brain tumor segmentation in MRI",
    "abstract": "  Magnetic Resonance Imaging (MRI) is used in everyday clinical practice to\nassess brain tumors. Several automatic or semi-automatic segmentation\nalgorithms have been introduced to segment brain tumors and achieve an\nexpert-like accuracy. Deep Convolutional Neural Networks (DCNN) have recently\nshown very promising results, however, DCNN models are still far from achieving\nclinically meaningful results mainly because of the lack of generalization of\nthe models. DCNN models need large annotated datasets to achieve good\nperformance. Models are often optimized on the domain dataset on which they\nhave been trained, and then fail the task when the same model is applied to\ndifferent datasets from different institutions. One of the reasons is due to\nthe lack of data standardization to adjust for different models and MR\nmachines. In this work, a 3D Spherical coordinates transform during the\npre-processing phase has been hypothesized to improve DCNN models' accuracy and\nto allow more generalizable results even when the model is trained on small and\nheterogeneous datasets and translated into different domains. Indeed, the\nspherical coordinate system avoids several standardization issues since it\nworks independently of resolution and imaging settings. Both Cartesian and\nspherical volumes were evaluated in two DCNN models with the same network\nstructure using the BraTS 2019 dataset. The model trained on spherical\ntransform pre-processed inputs resulted in superior performance over the\nCartesian-input trained model on predicting gliomas' segmentation on tumor core\nand enhancing tumor classes (increase of 0.011 and 0.014 respectively on the\nvalidation dataset), achieving a further improvement in accuracy by merging the\ntwo models together. Furthermore, the spherical transform is not\nresolution-dependent and achieve same results on different input resolution.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'Brain Tumor Segmentation': 0.99999917}",
    "score": 0.8877084773
  },
  {
    "id": "2006.08696",
    "title": "Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images\n  through Generative Latent Search",
    "abstract": "  Segmentation of the pixels corresponding to human skin is an essential first\nstep in multiple applications ranging from surveillance to heart-rate\nestimation from remote-photoplethysmography. However, the existing literature\nconsiders the problem only in the visible-range of the EM-spectrum which limits\ntheir utility in low or no light settings where the criticality of the\napplication is higher. To alleviate this problem, we consider the problem of\nskin segmentation from the Near-infrared images. However, Deep learning based\nstate-of-the-art segmentation techniques demands large amounts of labelled data\nthat is unavailable for the current problem. Therefore we cast the skin\nsegmentation problem as that of target-independent Unsupervised Domain\nAdaptation (UDA) where we use the data from the Red-channel of the\nvisible-range to develop skin segmentation algorithm on NIR images. We propose\na method for target-independent segmentation where the 'nearest-clone' of a\ntarget image in the source domain is searched and used as a proxy in the\nsegmentation network trained only on the source domain. We prove the existence\nof 'nearest-clone' and propose a method to find it through an optimization\nalgorithm over the latent space of a Deep generative model based on variational\ninference. We demonstrate the efficacy of the proposed method for NIR skin\nsegmentation over the state-of-the-art UDA segmentation methods on the two\nnewly created skin segmentation datasets in NIR domain despite not having\naccess to the target NIR data. Additionally, we report state-of-the-art results\nfor adaption from Synthia to Cityscapes which is a popular setting in\nUnsupervised Domain Adaptation for semantic segmentation. The code and datasets\nare available at https://github.com/ambekarsameer96/GLSS.\n",
    "topics": "{'Unsupervised Domain Adaptation': 1.0, 'Domain Adaptation': 0.9973127, 'Semantic Segmentation': 0.9962549, 'Image-to-Image Translation': 0.76675415}",
    "score": 0.8876211665
  },
  {
    "id": "1409.4127",
    "title": "Transfer Learning for Video Recognition with Scarce Training Data for\n  Deep Convolutional Neural Network",
    "abstract": "  Unconstrained video recognition and Deep Convolution Network (DCN) are two\nactive topics in computer vision recently. In this work, we apply DCNs as\nframe-based recognizers for video recognition. Our preliminary studies,\nhowever, show that video corpora with complete ground truth are usually not\nlarge and diverse enough to learn a robust model. The networks trained directly\non the video data set suffer from significant overfitting and have poor\nrecognition rate on the test set. The same lack-of-training-sample problem\nlimits the usage of deep models on a wide range of computer vision problems\nwhere obtaining training data are difficult. To overcome the problem, we\nperform transfer learning from images to videos to utilize the knowledge in the\nweakly labeled image corpus for video recognition. The image corpus help to\nlearn important visual patterns for natural images, while these patterns are\nignored by models trained only on the video corpus. Therefore, the resultant\nnetworks have better generalizability and better recognition rate. We show that\nby means of transfer learning from image to video, we can learn a frame-based\nrecognizer with only 4k videos. Because the image corpus is weakly labeled, the\nentire learning process requires only 4k annotated instances, which is far less\nthan the million scale image data sets required by previous works. The same\napproach may be applied to other visual recognition tasks where only scarce\ntraining data is available, and it improves the applicability of DCNs in\nvarious computer vision problems. Our experiments also reveal the correlation\nbetween meta-parameters and the performance of DCNs, given the properties of\nthe target problem and data. These results lead to a heuristic for\nmeta-parameter selection for future researches, which does not rely on the time\nconsuming meta-parameter search.\n",
    "topics": "{'Transfer Learning': 0.99998176, 'Temporal Action Localization': 0.5601665, 'Video Generation': 0.39502835}",
    "score": 0.8868171216
  },
  {
    "id": "2009.12053",
    "title": "DPN: Detail-Preserving Network with High Resolution Representation for\n  Efficient Segmentation of Retinal Vessels",
    "abstract": "  Retinal vessels are important biomarkers for many ophthalmological and\ncardiovascular diseases. It is of great significance to develop an accurate and\nfast vessel segmentation model for computer-aided diagnosis. Existing methods,\nsuch as U-Net follows the encoder-decoder pipeline, where detailed information\nis lost in the encoder in order to achieve a large field of view. Although\ndetailed information could be recovered in the decoder via multi-scale fusion,\nit still contains noise. In this paper, we propose a deep segmentation model,\ncalled detail-preserving network (DPN) for efficient vessel segmentation. To\npreserve detailed spatial information and learn structural information at the\nsame time, we designed the detail-preserving block (DP-Block). Further, we\nstacked eight DP-Blocks together to form the DPN. More importantly, there are\nno down-sampling operations among these blocks. As a result, the DPN could\nmaintain a high resolution during the processing, which is helpful to locate\nthe boundaries of thin vessels. To illustrate the effectiveness of our method,\nwe conducted experiments over three public datasets. Experimental results show,\ncompared to state-of-the-art methods, our method shows competitive/better\nperformance in terms of segmentation accuracy, segmentation speed,\nextensibility and the number of parameters. Specifically, 1) the AUC of our\nmethod ranks first/second/third on the STARE/CHASE_DB1/DRIVE datasets,\nrespectively. 2) Only one forward pass is required of our method to generate a\nvessel segmentation map, and the segmentation speed of our method is over\n20-160x faster than other methods on the DRIVE dataset. 3) We conducted\ncross-training experiments to demonstrate the extensibility of our method, and\nresults revealed that our method shows superior performance. 4) The number of\nparameters of our method is only around 96k, less then all comparison methods.\n",
    "topics": "{'Retinal Vessel Segmentation': 0.99984777}",
    "score": 0.8868111724
  },
  {
    "id": "1902.01314",
    "title": "'Squeeze & Excite' Guided Few-Shot Segmentation of Volumetric Images",
    "abstract": "  Deep neural networks enable highly accurate image segmentation, but require\nlarge amounts of manually annotated data for supervised training. Few-shot\nlearning aims to address this shortcoming by learning a new class from a few\nannotated support examples. We introduce, a novel few-shot framework, for the\nsegmentation of volumetric medical images with only a few annotated slices.\nCompared to other related works in computer vision, the major challenges are\nthe absence of pre-trained networks and the volumetric nature of medical scans.\nWe address these challenges by proposing a new architecture for few-shot\nsegmentation that incorporates 'squeeze & excite' blocks. Our two-armed\narchitecture consists of a conditioner arm, which processes the annotated\nsupport input and generates a task-specific representation. This representation\nis passed on to the segmenter arm that uses this information to segment the new\nquery image. To facilitate efficient interaction between the conditioner and\nthe segmenter arm, we propose to use 'channel squeeze & spatial excitation'\nblocks - a light-weight computational module - that enables heavy interaction\nbetween both the arms with negligible increase in model complexity. This\ncontribution allows us to perform image segmentation without relying on a\npre-trained model, which generally is unavailable for medical scans.\nFurthermore, we propose an efficient strategy for volumetric segmentation by\noptimally pairing a few slices of the support volume to all the slices of the\nquery volume. We perform experiments for organ segmentation on whole-body\ncontrast-enhanced CT scans from the Visceral Dataset. Our proposed model\noutperforms multiple baselines and existing approaches with respect to the\nsegmentation accuracy by a significant margin. The source code is available at\nhttps://github.com/abhi4ssj/few-shot-segmentation.\n",
    "topics": "{'Semantic Segmentation': 0.99984396, 'Few-Shot Learning': 0.9576263}",
    "score": 0.8865476376
  },
  {
    "id": "1701.06439",
    "title": "Segmentation-free Vehicle License Plate Recognition using ConvNet-RNN",
    "abstract": "  While vehicle license plate recognition (VLPR) is usually done with a sliding\nwindow approach, it can have limited performance on datasets with characters\nthat are of variable width. This can be solved by hand-crafting algorithms to\nprescale the characters. While this approach can work fairly well, the\nrecognizer is only aware of the pixels within each detector window, and fails\nto account for other contextual information that might be present in other\nparts of the image. A sliding window approach also requires training data in\nthe form of presegmented characters, which can be more difficult to obtain. In\nthis paper, we propose a unified ConvNet-RNN model to recognize real-world\ncaptured license plate photographs. By using a Convolutional Neural Network\n(ConvNet) to perform feature extraction and using a Recurrent Neural Network\n(RNN) for sequencing, we address the problem of sliding window approaches being\nunable to access the context of the entire image by feeding the entire image as\ninput to the ConvNet. This has the added benefit of being able to perform\nend-to-end training of the entire model on labelled, full license plate images.\nExperimental results comparing the ConvNet-RNN architecture to a sliding\nwindow-based approach shows that the ConvNet-RNN architecture performs\nsignificantly better.\n",
    "topics": "{}",
    "score": 0.8858771731
  },
  {
    "id": "1602.03409",
    "title": "Deep Convolutional Neural Networks for Computer-Aided Detection: CNN\n  Architectures, Dataset Characteristics and Transfer Learning",
    "abstract": "  Remarkable progress has been made in image recognition, primarily due to the\navailability of large-scale annotated datasets and the revival of deep CNN.\nCNNs enable learning data-driven, highly representative, layered hierarchical\nimage features from sufficient training data. However, obtaining datasets as\ncomprehensively annotated as ImageNet in the medical imaging domain remains a\nchallenge. There are currently three major techniques that successfully employ\nCNNs to medical image classification: training the CNN from scratch, using\noff-the-shelf pre-trained CNN features, and conducting unsupervised CNN\npre-training with supervised fine-tuning. Another effective method is transfer\nlearning, i.e., fine-tuning CNN models pre-trained from natural image dataset\nto medical image tasks. In this paper, we exploit three important, but\npreviously understudied factors of employing deep convolutional neural networks\nto computer-aided detection problems. We first explore and evaluate different\nCNN architectures. The studied models contain 5 thousand to 160 million\nparameters, and vary in numbers of layers. We then evaluate the influence of\ndataset scale and spatial image context on performance. Finally, we examine\nwhen and why transfer learning from pre-trained ImageNet (via fine-tuning) can\nbe useful. We study two specific computer-aided detection (CADe) problems,\nnamely thoraco-abdominal lymph node (LN) detection and interstitial lung\ndisease (ILD) classification. We achieve the state-of-the-art performance on\nthe mediastinal LN detection, with 85% sensitivity at 3 false positive per\npatient, and report the first five-fold cross-validation classification results\non predicting axial CT slices with ILD categories. Our extensive empirical\nevaluation, CNN model analysis and valuable insights can be extended to the\ndesign of high performance CAD systems for other medical imaging tasks.\n",
    "topics": "{'Transfer Learning': 0.99995494, 'Image Classification': 0.78430223}",
    "score": 0.885320002
  },
  {
    "id": "1907.12296",
    "title": "A Two Stage GAN for High Resolution Retinal Image Generation and\n  Segmentation",
    "abstract": "  In recent years, the use of deep learning is becoming increasingly popular in\ncomputer vision. However, the effective training of deep architectures usually\nrelies on huge sets of annotated data. This is critical in the medical field\nwhere it is difficult and expensive to obtain annotated images. In this paper,\nwe use Generative Adversarial Networks (GANs) for synthesizing high quality\nretinal images, along with the corresponding semantic label-maps, to be used\ninstead of real images during the training process. Differently from other\nprevious proposals, we suggest a two step approach: first, a progressively\ngrowing GAN is trained to generate the semantic label-maps, which describe the\nblood vessel structure (i.e. vasculature); second, an image-to-image\ntranslation approach is used to obtain realistic retinal images from the\ngenerated vasculature. By using only a handful of training samples, our\napproach generates realistic high resolution images, that can be effectively\nused to enlarge small available datasets. Comparable results have been obtained\nemploying the generated images in place of real data during training. The\npractical viability of the proposed approach has been demonstrated by applying\nit on two well established benchmark sets for retinal vessel segmentation, both\ncontaining a very small number of training samples. Our method obtained better\nperformances with respect to state-of-the-art techniques.\n",
    "topics": "{'Retinal Vessel Segmentation': 0.99999833, 'Image Generation': 0.98998684, 'Image-to-Image Translation': 0.98849773}",
    "score": 0.8851366327
  },
  {
    "id": "1806.10342",
    "title": "3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor\n  Segmentation",
    "abstract": "  Segmentation of colorectal cancerous regions from 3D Magnetic Resonance (MR)\nimages is a crucial procedure for radiotherapy which conventionally requires\naccurate delineation of tumour boundaries at an expense of labor, time and\nreproducibility. While deep learning based methods serve good baselines in 3D\nimage segmentation tasks, small applicable patch size limits effective\nreceptive field and degrades segmentation performance. In addition, Regions of\ninterest (RoIs) localization from large whole volume 3D images serves as a\npreceding operation that brings about multiple benefits in terms of speed,\ntarget completeness, reduction of false positives. Distinct from sliding window\nor non-joint localization-segmentation based models, we propose a novel\nmultitask framework referred to as 3D RoI-aware U-Net (3D RU-Net), for RoI\nlocalization and in-region segmentation where the two tasks share one backbone\nencoder network. With the region proposals from the encoder, we crop\nmulti-level RoI in-region features from the encoder to form a GPU\nmemory-efficient decoder for detailpreserving segmentation and therefore\nenlarged applicable volume size and effective receptive field. To effectively\ntrain the model, we designed a Dice formulated loss function for the\nglobal-to-local multi-task learning procedure. Based on the efficiency gains,\nwe went on to ensemble models with different receptive fields to achieve even\nhigher performance costing minor extra computational expensiveness. Extensive\nexperiments were conducted on 64 cancerous cases with a four-fold\ncross-validation, and the results showed significant superiority in terms of\naccuracy and efficiency over conventional frameworks. In conclusion, the\nproposed method has a huge potential for extension to other 3D object\nsegmentation tasks from medical images due to its inherent generalizability.\nThe code for the proposed method is publicly available.\n",
    "topics": "{'Tumor Segmentation': 0.9999999, 'Multi-Task Learning': 0.9998155, 'Semantic Segmentation': 0.99975365}",
    "score": 0.8850480505
  },
  {
    "id": "2007.13408",
    "title": "XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on\n  Anatomically Variable XCAT Phantoms",
    "abstract": "  Generative adversarial networks (GANs) have provided promising data\nenrichment solutions by synthesizing high-fidelity images. However, generating\nlarge sets of labeled images with new anatomical variations remains unexplored.\nWe propose a novel method for synthesizing cardiac magnetic resonance (CMR)\nimages on a population of virtual subjects with a large anatomical variation,\nintroduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human\nphantom. We investigate two conditional image synthesis approaches grounded on\na semantically-consistent mask-guided image generation technique: 4-class and\n8-class XCAT-GANs. The 4-class technique relies on only the annotations of the\nheart; while the 8-class technique employs a predicted multi-tissue label map\nof the heart-surrounding organs and provides better guidance for our\nconditional image synthesis. For both techniques, we train our conditional\nXCAT-GAN with real images paired with corresponding labels and subsequently at\nthe inference time, we substitute the labels with the XCAT derived ones.\nTherefore, the trained network accurately transfers the tissue-specific\ntextures to the new label maps. By creating 33 virtual subjects of synthetic\nCMR images at the end-diastolic and end-systolic phases, we evaluate the\nusefulness of such data in the downstream cardiac cavity segmentation task\nunder different augmentation strategies. Results demonstrate that even with\nonly 20% of real images (40 volumes) seen during training, segmentation\nperformance is retained with the addition of synthetic CMR images. Moreover,\nthe improvement in utilizing synthetic images for augmenting the real data is\nevident through the reduction of Hausdorff distance up to 28% and an increase\nin the Dice score up to 5%, indicating a higher similarity to the ground truth\nin all dimensions.\n",
    "topics": "{'Image Generation': 1.0}",
    "score": 0.8848935011
  },
  {
    "id": "1906.07078",
    "title": "Exemplar Guided Face Image Super-Resolution without Facial Landmarks",
    "abstract": "  Nowadays, due to the ubiquitous visual media there are vast amounts of\nalready available high-resolution (HR) face images. Therefore, for\nsuper-resolving a given very low-resolution (LR) face image of a person it is\nvery likely to find another HR face image of the same person which can be used\nto guide the process. In this paper, we propose a convolutional neural network\n(CNN)-based solution, namely GWAInet, which applies super-resolution (SR) by a\nfactor 8x on face images guided by another unconstrained HR face image of the\nsame person with possible differences in age, expression, pose or size. GWAInet\nis trained in an adversarial generative manner to produce the desired high\nquality perceptual image results. The utilization of the HR guiding image is\nrealized via the use of a warper subnetwork that aligns its contents to the\ninput image and the use of a feature fusion chain for the extracted features\nfrom the warped guiding image and the input image. In training, the identity\nloss further helps in preserving the identity related features by minimizing\nthe distance between the embedding vectors of SR and HR ground truth images.\nContrary to the current state-of-the-art in face super-resolution, our method\ndoes not require facial landmark points for its training, which helps its\nrobustness and allows it to produce fine details also for the surrounding face\nregion in a uniform manner. Our method GWAInet produces photo-realistic images\nin upscaling factor 8x and outperforms state-of-the-art in quantitative terms\nand perceptual quality.\n",
    "topics": "{'Super Resolution': 0.9999845, 'Image Super-Resolution': 0.99998415, 'Super-Resolution': 0.9999461}",
    "score": 0.8835224526
  },
  {
    "id": "2009.04227",
    "title": "Anonymization of labeled TOF-MRA images for brain vessel segmentation\n  using generative adversarial networks",
    "abstract": "  Anonymization and data sharing are crucial for privacy protection and\nacquisition of large datasets for medical image analysis. This is a big\nchallenge, especially for neuroimaging. Here, the brain's unique structure\nallows for re-identification and thus requires non-conventional anonymization.\nGenerative adversarial networks (GANs) have the potential to provide anonymous\nimages while preserving predictive properties. Analyzing brain vessel\nsegmentation as a use case, we trained 3 GANs on time-of-flight (TOF) magnetic\nresonance angiography (MRA) patches for image-label generation: 1) Deep\nconvolutional GAN, 2) Wasserstein-GAN with gradient penalty (WGAN-GP) and 3)\nWGAN-GP with spectral normalization (WGAN-GP-SN). The generated image-labels\nfrom each GAN were used to train a U-net for segmentation and tested on real\ndata. Moreover, we applied our synthetic patches using transfer learning on a\nsecond dataset. For an increasing number of up to 15 patients we evaluated the\nmodel performance on real data with and without pre-training. The performance\nfor all models was assessed by the Dice Similarity Coefficient (DSC) and the\n95th percentile of the Hausdorff Distance (95HD). Comparing the 3 GANs, the\nU-net trained on synthetic data generated by the WGAN-GP-SN showed the highest\nperformance to predict vessels (DSC/95HD 0.82/28.97) benchmarked by the U-net\ntrained on real data (0.89/26.61). The transfer learning approach showed\nsuperior performance for the same GAN compared to no pre-training, especially\nfor one patient only (0.91/25.68 vs. 0.85/27.36). In this work, synthetic\nimage-label pairs retained generalizable information and showed good\nperformance for vessel segmentation. Besides, we showed that synthetic patches\ncan be used in a transfer learning approach with independent data. This paves\nthe way to overcome the challenges of scarce data and anonymization in medical\nimaging.\n",
    "topics": "{'Transfer Learning': 0.99439394}",
    "score": 0.8833090328
  },
  {
    "id": "1801.04264",
    "title": "Real-world Anomaly Detection in Surveillance Videos",
    "abstract": "  Surveillance videos are able to capture a variety of realistic anomalies. In\nthis paper, we propose to learn anomalies by exploiting both normal and\nanomalous videos. To avoid annotating the anomalous segments or clips in\ntraining videos, which is very time consuming, we propose to learn anomaly\nthrough the deep multiple instance ranking framework by leveraging weakly\nlabeled training videos, i.e. the training labels (anomalous or normal) are at\nvideo-level instead of clip-level. In our approach, we consider normal and\nanomalous videos as bags and video segments as instances in multiple instance\nlearning (MIL), and automatically learn a deep anomaly ranking model that\npredicts high anomaly scores for anomalous video segments. Furthermore, we\nintroduce sparsity and temporal smoothness constraints in the ranking loss\nfunction to better localize anomaly during training. We also introduce a new\nlarge-scale first of its kind dataset of 128 hours of videos. It consists of\n1900 long and untrimmed real-world surveillance videos, with 13 realistic\nanomalies such as fighting, road accident, burglary, robbery, etc. as well as\nnormal activities. This dataset can be used for two tasks. First, general\nanomaly detection considering all anomalies in one group and all normal\nactivities in another group. Second, for recognizing each of 13 anomalous\nactivities. Our experimental results show that our MIL method for anomaly\ndetection achieves significant improvement on anomaly detection performance as\ncompared to the state-of-the-art approaches. We provide the results of several\nrecent deep learning baselines on anomalous activity recognition. The low\nrecognition performance of these baselines reveals that our dataset is very\nchallenging and opens more opportunities for future work. The dataset is\navailable at: https://webpages.uncc.edu/cchen62/dataset.html\n",
    "topics": "{'Anomaly Detection': 1.0, 'Multiple Instance Learning': 0.99995947, 'Activity Recognition': 0.7837535}",
    "score": 0.8832110908
  },
  {
    "id": "1907.10267",
    "title": "Discriminative Consistent Domain Generation for Semi-supervised Learning",
    "abstract": "  Deep learning based task systems normally rely on a large amount of manually\nlabeled training data, which is expensive to obtain and subject to operator\nvariations. Moreover, it does not always hold that the manually labeled data\nand the unlabeled data are sitting in the same distribution. In this paper, we\nalleviate these problems by proposing a discriminative consistent domain\ngeneration (DCDG) approach to achieve a semi-supervised learning. The\ndiscriminative consistent domain is achieved by a double-sided domain\nadaptation. The double-sided domain adaptation aims to make a fusion of the\nfeature spaces of labeled data and unlabeled data. In this way, we can fit the\ndifferences of various distributions between labeled data and unlabeled data.\nIn order to keep the discriminativeness of generated consistent domain for the\ntask learning, we apply an indirect learning for the double-sided domain\nadaptation. Based on the generated discriminative consistent domain, we can use\nthe unlabeled data to learn the task model along with the labeled data via a\nconsistent image generation. We demonstrate the performance of our proposed\nDCDG on the late gadolinium enhancement cardiac MRI (LGE-CMRI) images acquired\nfrom patients with atrial fibrillation in two clinical centers for the\nsegmentation of the left atrium anatomy (LA) and proximal pulmonary veins\n(PVs). The experiments show that our semi-supervised approach achieves\ncompelling segmentation results, which can prove the robustness of DCDG for the\nsemi-supervised learning using the unlabeled data along with labeled data\nacquired from a single center or multicenter studies.\n",
    "topics": "{'Domain Adaptation': 0.9765425}",
    "score": 0.8830760017
  },
  {
    "id": "1911.09026",
    "title": "Weak Supervision for Generating Pixel-Level Annotations in Scene Text\n  Segmentation",
    "abstract": "  Providing pixel-level supervisions for scene text segmentation is inherently\ndifficult and costly, so that only few small datasets are available for this\ntask. To face the scarcity of training data, previous approaches based on\nConvolutional Neural Networks (CNNs) rely on the use of a synthetic dataset for\npre-training. However, synthetic data cannot reproduce the complexity and\nvariability of natural images. In this work, we propose to use a weakly\nsupervised learning approach to reduce the domain-shift between synthetic and\nreal data. Leveraging the bounding-box supervision of the COCO-Text and the MLT\ndatasets, we generate weak pixel-level supervisions of real images. In\nparticular, the COCO-Text-Segmentation (COCO_TS) and the MLT-Segmentation\n(MLT_S) datasets are created and released. These two datasets are used to train\na CNN, the Segmentation Multiscale Attention Network (SMANet), which is\nspecifically designed to face some peculiarities of the scene text segmentation\ntask. The SMANet is trained end-to-end on the proposed datasets, and the\nexperiments show that COCO_TS and MLT_S are a valid alternative to synthetic\nimages, allowing to use only a fraction of the training samples and improving\nsignificantly the performances.\n",
    "topics": "{'Scene Text': 1.0}",
    "score": 0.8829293921
  },
  {
    "id": "2004.08052",
    "title": "A New Modified Deep Convolutional Neural Network for Detecting COVID-19\n  from X-ray Images",
    "abstract": "  COVID-19 has become a serious health problem all around the world. It is\nconfirmed that this virus has taken over 126,607 lives until today. Since the\nbeginning of its spreading, many Artificial Intelligence researchers developed\nsystems and methods for predicting the virus's behavior or detecting the\ninfection. One of the possible ways of determining the patient infection to\nCOVID-19 is through analyzing the chest X-ray images. As there are a large\nnumber of patients in hospitals, it would be time-consuming and difficult to\nexamine lots of X-ray images, so it can be very useful to develop an AI network\nthat does this job automatically. In this paper, we have trained several deep\nconvolutional networks with the introduced training techniques for classifying\nX-ray images into three classes: normal, pneumonia, and COVID-19, based on two\nopen-source datasets. Unfortunately, most of the previous works on this subject\nhave not shared their dataset, and we had to deal with few data on covid19\ncases. Our data contains 180 X-ray images that belong to persons infected to\nCOVID-19, so we tried to apply methods to achieve the best possible results. In\nthis research, we introduce some training techniques that help the network\nlearn better when we have few cases of COVID-19, and also we propose a neural\nnetwork that is a concatenation of Xception and ResNet50V2 networks. This\nnetwork achieved the best accuracy by utilizing multiple features extracted by\ntwo robust networks. In this paper, despite some other researches, we have\ntested our network on 11302 images to report the actual accuracy our network\ncan achieve in real circumstances. The average accuracy of the proposed network\nfor detecting COVID-19 cases is 99.56%, and the overall average accuracy for\nall classes is 91.4%.\n",
    "topics": "{'COVID-19 Diagnosis': 0.99633443}",
    "score": 0.8829010234
  },
  {
    "id": "2005.03948",
    "title": "Layer-wise training convolutional neural networks with smaller filters\n  for human activity recognition using wearable sensors",
    "abstract": "  Recently, convolutional neural networks (CNNs) have set latest\nstate-of-the-art on various human activity recognition (HAR) datasets. However,\ndeep CNNs often require more computing resources, which limits their\napplications in embedded HAR. Although many successful methods have been\nproposed to reduce memory and FLOPs of CNNs, they often involve special network\narchitectures designed for visual tasks, which are not suitable for deep HAR\ntasks with time series sensor signals, due to remarkable discrepancy.\nTherefore, it is necessary to develop lightweight deep models to perform HAR.\nAs filter is the basic unit in constructing CNNs, it deserves further research\nwhether re-designing smaller filters is applicable for deep HAR. In the paper,\ninspired by the idea, we proposed a lightweight CNN using Lego filters for HAR.\nA set of lower-dimensional filters is used as Lego bricks to be stacked for\nconventional filters, which does not rely on any special network structure. The\nlocal loss function is used to train model. To our knowledge, this is the first\npaper that proposes lightweight CNN for HAR in ubiquitous and wearable\ncomputing arena. The experiment results on five public HAR datasets, UCI-HAR\ndataset, OPPORTUNITY dataset, UNIMIB-SHAR dataset, PAMAP2 dataset, and WISDM\ndataset collected from either smartphones or multiple sensor nodes, indicate\nthat our novel Lego CNN with local loss can greatly reduce memory and\ncomputation cost over CNN, while achieving higher accuracy. That is to say, the\nproposed model is smaller, faster and more accurate. Finally, we evaluate the\nactual performance on an Android smartphone.\n",
    "topics": "{'Activity Recognition': 1.0, 'Time Series': 0.9025187}",
    "score": 0.8824134855
  },
  {
    "id": "2008.05566",
    "title": "An Efficient Confidence Measure-Based Evaluation Metric for Breast\n  Cancer Screening Using Bayesian Neural Networks",
    "abstract": "  Screening mammograms is the gold standard for detecting breast cancer early.\nWhile a good amount of work has been performed on mammography image\nclassification, especially with deep neural networks, there has not been much\nexploration into the confidence or uncertainty measurement of the\nclassification. In this paper, we propose a confidence measure-based evaluation\nmetric for breast cancer screening. We propose a modular network architecture,\nwhere a traditional neural network is used as a feature extractor with transfer\nlearning, followed by a simple Bayesian neural network. Utilizing a two-stage\napproach helps reducing the computational complexity, making the proposed\nframework attractive for wider deployment. We show that by providing the\nmedical practitioners with a tool to tune two hyperparameters of the Bayesian\nneural network, namely, fraction of sampled number of networks and minimum\nprobability, the framework can be adapted as needed by the domain expert.\nFinally, we argue that instead of just a single number such as accuracy, a\ntuple (accuracy, coverage, sampled number of networks, and minimum probability)\ncan be utilized as an evaluation metric of our framework. We provide\nexperimental results on the CBIS-DDSM dataset, where we show the trends in\naccuracy-coverage tradeoff while tuning the two hyperparameters. We also show\nthat our confidence tuning results in increased accuracy with a reduced set of\nimages with high confidence when compared to the baseline transfer learning. To\nmake the proposed framework readily deployable, we provide (anonymized) source\ncode with reproducible results at https://git.io/JvRqE.\n",
    "topics": "{'Transfer Learning': 0.9969503, 'Image Classification': 0.9339692}",
    "score": 0.8823809855
  },
  {
    "id": "1702.04528",
    "title": "A deep learning model integrating FCNNs and CRFs for brain tumor\n  segmentation",
    "abstract": "  Accurate and reliable brain tumor segmentation is a critical component in\ncancer diagnosis, treatment planning, and treatment outcome evaluation. Build\nupon successful deep learning techniques, a novel brain tumor segmentation\nmethod is developed by integrating fully convolutional neural networks (FCNNs)\nand Conditional Random Fields (CRFs) in a unified framework to obtain\nsegmentation results with appearance and spatial consistency. We train a deep\nlearning based segmentation model using 2D image patches and image slices in\nfollowing steps: 1) training FCNNs using image patches; 2) training CRFs as\nRecurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs\nfixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices.\nParticularly, we train 3 segmentation models using 2D image patches and slices\nobtained in axial, coronal and sagittal views respectively, and combine them to\nsegment brain tumors using a voting based fusion strategy. Our method could\nsegment brain images slice-by-slice, much faster than those based on image\npatches. We have evaluated our method based on imaging data provided by the\nMultimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015\nand BRATS 2016. The experimental results have demonstrated that our method\ncould build a segmentation model with Flair, T1c, and T2 scans and achieve\ncompetitive performance as those built with Flair, T1, T1c, and T2 scans.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'Brain Tumor Segmentation': 1.0, 'Semantic Segmentation': 0.87847775}",
    "score": 0.8823249362
  },
  {
    "id": "1912.08519",
    "title": "Real-Time Object Detection and Localization in Compressive Sensed Video\n  on Embedded Hardware",
    "abstract": "  Every day around the world, interminable terabytes of data are being captured\nfor surveillance purposes. A typical 1-2MP CCTV camera generates around 7-12GB\nof data per day. Frame-by-frame processing of such enormous amount of data\nrequires hefty computational resources. In recent years, compressive sensing\napproaches have shown impressive results in signal processing by reducing the\nsampling bandwidth. Different sampling mechanisms were developed to incorporate\ncompressive sensing in image and video acquisition. Pixel-wise coded exposure\nis one among the promising sensing paradigms for capturing videos in the\ncompressed domain, which was also realized into an all-CMOS sensor\n\\cite{Xiong2017}. Though cameras that perform compressive sensing save a lot of\nbandwidth at the time of sampling and minimize the memory required to store\nvideos, we cannot do much in terms of processing until the videos are\nreconstructed to the original frames. But, the reconstruction of\ncompressive-sensed (CS) videos still takes a lot of time and is also\ncomputationally expensive. In this work, we show that object detection and\nlocalization can be possible directly on the CS frames (easily upto 20x\ncompression). To our knowledge, this is the first time that the problem of\nobject detection and localization on CS frames has been attempted. Hence, we\nalso created a dataset for training in the CS domain. We were able to achieve a\ngood accuracy of 46.27\\% mAP(Mean Average Precision) with the proposed model\nwith an inference time of 23ms directly on the compressed frames(approx. 20\noriginal domain frames), this facilitated for real-time inference which was\nverified on NVIDIA TX2 embedded board. Our framework will significantly reduce\nthe communication bandwidth, and thus reduction in power as the video\ncompression will be done at the image sensor processing core.\n",
    "topics": "{'Compressive Sensing': 1.0, 'Video Compression': 0.99990547, 'Real-Time Object Detection': 0.999866, 'Object Detection': 0.998276}",
    "score": 0.8818527994
  },
  {
    "id": "1810.07003",
    "title": "Dense Multi-path U-Net for Ischemic Stroke Lesion Segmentation in\n  Multiple Image Modalities",
    "abstract": "  Delineating infarcted tissue in ischemic stroke lesions is crucial to\ndetermine the extend of damage and optimal treatment for this life-threatening\ncondition. However, this problem remains challenging due to high variability of\nischemic strokes' location and shape. Recently, fully-convolutional neural\nnetworks (CNN), in particular those based on U-Net, have led to improved\nperformances for this task. In this work, we propose a novel architecture that\nimproves standard U-Net based methods in three important ways. First, instead\nof combining the available image modalities at the input, each of them is\nprocessed in a different path to better exploit their unique information.\nMoreover, the network is densely-connected (i.e., each layer is connected to\nall following layers), both within each path and across different paths,\nsimilar to HyperDenseNet. This gives our model the freedom to learn the scale\nat which modalities should be processed and combined. Finally, inspired by the\nInception architecture, we improve standard U-Net modules by extending\ninception modules with two convolutional blocks with dilated convolutions of\ndifferent scale. This helps handling the variability in lesion sizes. We split\nthe 93 stroke datasets into training and validation sets containing 83 and 9\nexamples respectively. Our network was trained on a NVidia TITAN XP GPU with 16\nGBs RAM, using ADAM as optimizer and a learning rate of 1$\\times$10$^{-5}$\nduring 200 epochs. Training took around 5 hours and segmentation of a whole\nvolume took between 0.2 and 2 seconds, as average. The performance on the test\nset obtained by our method is compared to several baselines, to demonstrate the\neffectiveness of our architecture, and to a state-of-art architecture that\nemploys factorized dilated convolutions, i.e., ERFNet.\n",
    "topics": "{'Lesion Segmentation': 0.999853}",
    "score": 0.8816708175
  },
  {
    "id": "1707.06879",
    "title": "Learning Aerial Image Segmentation from Online Maps",
    "abstract": "  This study deals with semantic segmentation of high-resolution (aerial)\nimages where a semantic class label is assigned to each pixel via supervised\nclassification as a basis for automatic map generation. Recently, deep\nconvolutional neural networks (CNNs) have shown impressive performance and have\nquickly become the de-facto standard for semantic segmentation, with the added\nbenefit that task-specific feature design is no longer necessary. However, a\nmajor downside of deep learning methods is that they are extremely data-hungry,\nthus aggravating the perennial bottleneck of supervised classification, to\nobtain enough annotated training data. On the other hand, it has been observed\nthat they are rather robust against noise in the training labels. This opens up\nthe intriguing possibility to avoid annotating huge amounts of training data,\nand instead train the classifier from existing legacy data or crowd-sourced\nmaps which can exhibit high levels of noise. The question addressed in this\npaper is: can training with large-scale, publicly available labels replace a\nsubstantial part of the manual labeling effort and still achieve sufficient\nperformance? Such data will inevitably contain a significant portion of errors,\nbut in return virtually unlimited quantities of it are available in larger\nparts of the world. We adapt a state-of-the-art CNN architecture for semantic\nsegmentation of buildings and roads in aerial images, and compare its\nperformance when using different training data sets, ranging from manually\nlabeled, pixel-accurate ground truth of the same city to automatic training\ndata derived from OpenStreetMap data from distant locations. We report our\nresults that indicate that satisfying performance can be obtained with\nsignificantly less manual annotation effort, by exploiting noisy large-scale\ntraining data.\n",
    "topics": "{'Semantic Segmentation': 0.99999285}",
    "score": 0.8813564408
  },
  {
    "id": "1711.05512",
    "title": "Spectral-spatial classification of hyperspectral images: three tricks\n  and a new supervised learning setting",
    "abstract": "  Spectral-spatial classification of hyperspectral images has been the subject\nof many studies in recent years. In the presence of only very few labeled\npixels, this task becomes challenging. In this paper we address the following\ntwo research questions: 1) Can a simple neural network with just a single\nhidden layer achieve state of the art performance in the presence of few\nlabeled pixels? 2) How is the performance of hyperspectral image classification\nmethods affected when using disjoint train and test sets? We give a positive\nanswer to the first question by using three tricks within a very basic shallow\nConvolutional Neural Network (CNN) architecture: a tailored loss function, and\nsmooth- and label-based data augmentation. The tailored loss function enforces\nthat neighborhood wavelengths have similar contributions to the features\ngenerated during training. A new label-based technique here proposed favors\nselection of pixels in smaller classes, which is beneficial in the presence of\nvery few labeled pixels and skewed class distributions. To address the second\nquestion, we introduce a new sampling procedure to generate disjoint train and\ntest set. Then the train set is used to obtain the CNN model, which is then\napplied to pixels in the test set to estimate their labels. We assess the\nefficacy of the simple neural network method on five publicly available\nhyperspectral images. On these images our method significantly outperforms\nconsidered baselines. Notably, with just 1% of labeled pixels per class, on\nthese datasets our method achieves an accuracy that goes from 86.42%\n(challenging dataset) to 99.52% (easy dataset). Furthermore we show that the\nsimple neural network method improves over other baselines in the new\nchallenging supervised setting. Our analysis substantiates the highly\nbeneficial effect of using the entire image (so train and test data) for\nconstructing a model.\n",
    "topics": "{'Hyperspectral Image Classification': 0.99977416, 'Data Augmentation': 0.9948086, 'Image Classification': 0.8971961}",
    "score": 0.8809905937
  },
  {
    "id": "1903.00683",
    "title": "Fully Convolutional One-Shot Object Segmentation for Industrial Robotics",
    "abstract": "  The ability to identify and localize new objects robustly and effectively is\nvital for robotic grasping and manipulation in warehouses or smart factories.\nDeep convolutional neural networks (DCNNs) have achieved the state-of-the-art\nperformance on established image datasets for object detection and\nsegmentation. However, applying DCNNs in dynamic industrial scenarios, e.g.,\nwarehouses and autonomous production, remains a challenging problem. DCNNs\nquickly become ineffective when tasked with detecting objects that they have\nnot been trained on. Given that re-training using the latest data is time\nconsuming, DCNNs cannot meet the requirement of the Factory of the Future (FoF)\nregarding rapid development and production cycles. To address this problem, we\npropose a novel one-shot object segmentation framework, using a fully\nconvolutional Siamese network architecture, to detect previously unknown\nobjects based on a single prototype image. We turn to multi-task learning to\nreduce training time and improve classification accuracy. Furthermore, we\nintroduce a novel approach to automatically cluster the learnt feature space\nrepresentation in a weakly supervised manner. We test the proposed framework on\nthe RoboCup@Work dataset, simulating requirements for the FoF. Results show\nthat the trained network on average identifies 73% of previously unseen objects\ncorrectly from a single example image. Correctly identified objects are\nestimated to have a 87.53% successful pick-up rate. Finally, multi-task\nlearning lowers the convergence time by up to 33%, and increases accuracy by\n2.99%.\n",
    "topics": "{'Multi-Task Learning': 0.99999845, 'Object Detection': 0.905912}",
    "score": 0.8803414539
  },
  {
    "id": "1906.02191",
    "title": "Uncertainty-based graph convolutional networks for organ segmentation\n  refinement",
    "abstract": "  Organ segmentation in CT volumes is an important pre-processing step in many\ncomputer assisted intervention and diagnosis methods. In recent years,\nconvolutional neural networks have dominated the state of the art in this task.\nHowever, since this problem presents a challenging environment due to high\nvariability in the organ's shape and similarity between tissues, the generation\nof false negative and false positive regions in the output segmentation is a\ncommon issue. Recent works have shown that the uncertainty analysis of the\nmodel can provide us with useful information about potential errors in the\nsegmentation. In this context, we proposed a segmentation refinement method\nbased on uncertainty analysis and graph convolutional networks. We employ the\nuncertainty levels of the convolutional network in a particular input volume to\nformulate a semi-supervised graph learning problem that is solved by training a\ngraph convolutional network. To test our method we refine the initial output of\na 2D U-Net. We validate our framework with the NIH pancreas dataset and the\nspleen dataset of the medical segmentation decathlon. We show that our method\noutperforms the state-of-the art CRF refinement method by improving the dice\nscore by 1% for the pancreas and 2% for spleen, with respect to the original\nU-Net's prediction. Finally, we discuss the results and current limitations of\nthe model for future work in this research direction. For reproducibility\npurposes, we make our code publicly available.\n",
    "topics": "{'Graph Learning': 0.999998}",
    "score": 0.8802792474
  },
  {
    "id": "2004.04931",
    "title": "CoroNet: A deep neural network for detection and diagnosis of COVID-19\n  from chest x-ray images",
    "abstract": "  Background and Objective\n  The novel Coronavirus also called COVID-19 originated in Wuhan, China in\nDecember 2019 and has now spread across the world. It has so far infected\naround 1.8 million people and claimed approximately 114,698 lives overall. As\nthe number of cases are rapidly increasing, most of the countries are facing\nshortage of testing kits and resources. The limited quantity of testing kits\nand increasing number of daily cases encouraged us to come up with a Deep\nLearning model that can aid radiologists and clinicians in detecting COVID-19\ncases using chest X-rays.\n  Methods\n  In this study, we propose CoroNet, a Deep Convolutional Neural Network model\nto automatically detect COVID-19 infection from chest X-ray images. The\nproposed model is based on Xception architecture pre-trained on ImageNet\ndataset and trained end-to-end on a dataset prepared by collecting COVID-19 and\nother chest pneumonia X-ray images from two different publically available\ndatabases.\n  Results and Conclusion\n  CoroNet has been trained and tested on the prepared dataset and the\nexperimental results show that our proposed model achieved an overall accuracy\nof 89.6%, and more importantly the precision and recall rate for COVID-19 cases\nare 93% and 98.2% for 4-class cases (COVID vs Pneumonia bacterial vs pneumonia\nviral vs normal). For 3-class classification (COVID vs Pneumonia vs normal),\nthe proposed model produced a classification accuracy of 95%. The preliminary\nresults of this study look promising which can be further improved as more\ntraining data becomes available. Overall, the proposed model substantially\nadvances the current radiology based methodology and during COVID-19 pandemic,\nit can be very helpful tool for clinical practitioners and radiologists to aid\nthem in diagnosis, quantification and follow-up of COVID-19 cases.\n",
    "topics": "{'COVID-19 Diagnosis': 0.9989446}",
    "score": 0.879859276
  },
  {
    "id": "1712.00075",
    "title": "Multi-Channel CNN-based Object Detection for Enhanced Situation\n  Awareness",
    "abstract": "  Object Detection is critical for automatic military operations. However, the\nperformance of current object detection algorithms is deficient in terms of the\nrequirements in military scenarios. This is mainly because the object presence\nis hard to detect due to the indistinguishable appearance and dramatic changes\nof object's size which is determined by the distance to the detection sensors.\nRecent advances in deep learning have achieved promising results in many\nchallenging tasks. The state-of-the-art in object detection is represented by\nconvolutional neural networks (CNNs), such as the fast R-CNN algorithm. These\nCNN-based methods improve the detection performance significantly on several\npublic generic object detection datasets. However, their performance on\ndetecting small objects or undistinguishable objects in visible spectrum images\nis still insufficient. In this study, we propose a novel detection algorithm\nfor military objects by fusing multi-channel CNNs. We combine spatial, temporal\nand thermal information by generating a three-channel image, and they will be\nfused as CNN feature maps in an unsupervised manner. The backbone of our object\ndetection framework is from the fast R-CNN algorithm, and we utilize\ncross-domain transfer learning technique to fine-tune the CNN model on\ngenerated multi-channel images. In the experiments, we validated the proposed\nmethod with the images from SENSIAC (Military Sensing Information Analysis\nCentre) database and compared it with the state-of-the-art. The experimental\nresults demonstrated the effectiveness of the proposed method on both accuracy\nand computational efficiency.\n",
    "topics": "{'Object Detection': 1.0, 'Transfer Learning': 0.9861692}",
    "score": 0.8795036551
  },
  {
    "id": "2008.07030",
    "title": "Training CNN Classifiers for Semantic Segmentation using Partially\n  Annotated Images: with Application on Human Thigh and Calf MRI",
    "abstract": "  Objective: Medical image datasets with pixel-level labels tend to have a\nlimited number of organ or tissue label classes annotated, even when the images\nhave wide anatomical coverage. With supervised learning, multiple classifiers\nare usually needed given these partially annotated datasets. In this work, we\npropose a set of strategies to train one single classifier in segmenting all\nlabel classes that are heterogeneously annotated across multiple datasets\nwithout moving into semi-supervised learning. Methods: Masks were first created\nfrom each label image through a process we termed presence masking. Three\npresence masking modes were evaluated, differing mainly in weightage assigned\nto the annotated and unannotated classes. These masks were then applied to the\nloss function during training to remove the influence of unannotated classes.\nResults: Evaluation against publicly available CT datasets shows that presence\nmasking is a viable method for training class-generic classifiers. Our\nclass-generic classifier can perform as well as multiple class-specific\nclassifiers combined, while the training duration is similar to that required\nfor one class-specific classifier. Furthermore, the class-generic classifier\ncan outperform the class-specific classifiers when trained on smaller datasets.\nFinally, consistent results are observed from evaluations against human thigh\nand calf MRI datasets collected in-house. Conclusion: The evaluation outcomes\nshow that presence masking is capable of significantly improving both training\nand inference efficiency across imaging modalities and anatomical regions.\nImproved performance may even be observed on small datasets. Significance:\nPresence masking strategies can reduce the computational resources and costs\ninvolved in manual medical image annotations. All codes are publicly available\nat https://github.com/wong-ck/DeepSegment.\n",
    "topics": "{'Semantic Segmentation': 0.9912198}",
    "score": 0.8793806512
  },
  {
    "id": "1910.02021",
    "title": "Dynamic data fusion using multi-input models for malware classification",
    "abstract": "  Criminals use malware to disrupt cyber-systems. The number of these\nmalware-vulnerable systems is increasing quickly as common systems, such as\nvehicles, routers, and lightbulbs, become increasingly interconnected\ncyber-systems. To address the scale of this problem, analysts divide malware\ninto classes and develop, for each class, a specialized defense. In this\nproject we classified malware with machine learning. In particular, we used a\nsupervised multi-class long short term memory (LSTM) model. We trained the\nalgorithm with thousands of malware files annotated with class labels (the\ntraining set), and the algorithm learned patterns indicative of each class. We\nused disassembled malware files (provided by Microsoft) and separated the\nconstituent data into parsed instructions, which look like human-readable\nmachine code text, and raw bytes, which are hexadecimal values. We are\ninterested in which format, text or hex, is more valuable as input for\nclassification. To solve this, we investigated four cases: a text-only model, a\nhexadecimal-only model, a multi-input model using both text and hexadecimal\ninputs, and a model based on combining the individual results. We performed\nthis investigation using the machine learning Python package Keras, which\nallows easily configurable deep learning architectures and training. We hoped\nto understand the trade-offs between the different formats. Due to the class\nimbalance in the data, we used multiple methods to compare the formats, using\ntest accuracies, balanced accuracies (taking into account weights of classes),\nand an accuracy derived from tables of confusion. We found that the multi-input\nmodel, which allows learning on both input types simultaneously, resulted in\nthe best performance. Our finding expedites malware classification research by\nproviding researchers a suitable deep learning architecture to train a tailored\nversion to their malware.\n",
    "topics": "{'Malware Classification': 1.0}",
    "score": 0.8793756432
  },
  {
    "id": "2007.16032",
    "title": "Pixel-wise Crowd Understanding via Synthetic Data",
    "abstract": "  Crowd analysis via computer vision techniques is an important topic in the\nfield of video surveillance, which has wide-spread applications including crowd\nmonitoring, public safety, space design and so on. Pixel-wise crowd\nunderstanding is the most fundamental task in crowd analysis because of its\nfiner results for video sequences or still images than other analysis tasks.\nUnfortunately, pixel-level understanding needs a large amount of labeled\ntraining data. Annotating them is an expensive work, which causes that current\ncrowd datasets are small. As a result, most algorithms suffer from over-fitting\nto varying degrees. In this paper, take crowd counting and segmentation as\nexamples from the pixel-wise crowd understanding, we attempt to remedy these\nproblems from two aspects, namely data and methodology. Firstly, we develop a\nfree data collector and labeler to generate synthetic and labeled crowd scenes\nin a computer game, Grand Theft Auto V. Then we use it to construct a\nlarge-scale, diverse synthetic crowd dataset, which is named as \"GCC Dataset\".\nSecondly, we propose two simple methods to improve the performance of crowd\nunderstanding via exploiting the synthetic data. To be specific, 1) supervised\ncrowd understanding: pre-train a crowd analysis model on the synthetic data,\nthen fine-tune it using the real data and labels, which makes the model perform\nbetter on the real world; 2) crowd understanding via domain adaptation:\ntranslate the synthetic data to photo-realistic images, then train the model on\ntranslated data and labels. As a result, the trained model works well in real\ncrowd scenes.\n",
    "topics": "{'Crowd Counting': 0.99994016}",
    "score": 0.8793658099
  },
  {
    "id": "1404.4888",
    "title": "Supervised detection of anomalous light-curves in massive astronomical\n  catalogs",
    "abstract": "  The development of synoptic sky surveys has led to a massive amount of data\nfor which resources needed for analysis are beyond human capabilities. To\nprocess this information and to extract all possible knowledge, machine\nlearning techniques become necessary. Here we present a new method to\nautomatically discover unknown variable objects in large astronomical catalogs.\nWith the aim of taking full advantage of all the information we have about\nknown objects, our method is based on a supervised algorithm. In particular, we\ntrain a random forest classifier using known variability classes of objects and\nobtain votes for each of the objects in the training set. We then model this\nvoting distribution with a Bayesian network and obtain the joint voting\ndistribution among the training objects. Consequently, an unknown object is\nconsidered as an outlier insofar it has a low joint probability. Our method is\nsuitable for exploring massive datasets given that the training process is\nperformed offline. We tested our algorithm on 20 millions light-curves from the\nMACHO catalog and generated a list of anomalous candidates. We divided the\ncandidates into two main classes of outliers: artifacts and intrinsic outliers.\nArtifacts were principally due to air mass variation, seasonal variation, bad\ncalibration or instrumental errors and were consequently removed from our\noutlier list and added to the training set. After retraining, we selected about\n4000 objects, which we passed to a post analysis stage by perfoming a\ncross-match with all publicly available catalogs. Within these candidates we\nidentified certain known but rare objects such as eclipsing Cepheids, blue\nvariables, cataclysmic variables and X-ray sources. For some outliers there\nwere no additional information. Among them we identified three unknown\nvariability types and few individual outliers that will be followed up for a\ndeeper analysis.\n",
    "topics": "{}",
    "score": 0.879042402
  },
  {
    "id": "1909.00240",
    "title": "Integrating Data and Image Domain Deep Learning for Limited Angle\n  Tomography using Consensus Equilibrium",
    "abstract": "  Computed Tomography (CT) is a non-invasive imaging modality with applications\nranging from healthcare to security. It reconstructs cross-sectional images of\nan object using a collection of projection data collected at different angles.\nConventional methods, such as FBP, require that the projection data be\nuniformly acquired over the complete angular range. In some applications, it is\nnot possible to acquire such data. Security is one such domain where\nnon-rotational scanning configurations are being developed which violate the\ncomplete data assumption. Conventional methods produce images from such data\nthat are filled with artifacts. The recent success of deep learning (DL)\nmethods has inspired researchers to post-process these artifact laden images\nusing deep neural networks (DNNs). This approach has seen limited success on\nreal CT problems. Another approach has been to pre-process the incomplete data\nusing DNNs aiming to avoid the creation of artifacts altogether. Due to\nimperfections in the learning process, this approach can still leave\nperceptible residual artifacts. In this work, we aim to combine the power of\ndeep learning in both the data and image domains through a two-step process\nbased on the consensus equilibrium (CE) framework. Specifically, we use\nconditional generative adversarial networks (cGANs) in both the data and the\nimage domain for enhanced performance and efficient computation and combine\nthem through a consensus process. We demonstrate the effectiveness of our\napproach on a real security CT dataset for a challenging 90 degree\nlimited-angle problem. The same framework can be applied to other limited data\nproblems arising in applications such as electron microscopy, non-destructive\nevaluation, and medical imaging.\n",
    "topics": "{'Electron Microscopy': 0.99999356, 'Computed Tomography (CT)': 0.99995124}",
    "score": 0.878294334
  },
  {
    "id": "2005.02463",
    "title": "Spatio-Temporal Event Segmentation and Localization for Wildlife\n  Extended Videos",
    "abstract": "  Using offline training schemes, researchers have tackled the event\nsegmentation problem by providing full or weak-supervision through manually\nannotated labels or self-supervised epoch-based training. Most works consider\nvideos that are at most 10's of minutes long. We present a self-supervised\nperceptual prediction framework capable of temporal event segmentation by\nbuilding stable representations of objects over time and demonstrate it on long\nvideos, spanning several days. The approach is deceptively simple but quite\neffective. We rely on predictions of high-level features computed by a standard\ndeep learning backbone. For prediction, we use an LSTM, augmented with an\nattention mechanism, trained in a self-supervised manner using the prediction\nerror. The self-learned attention maps effectively localize and track the\nevent-related objects in each frame. The proposed approach does not require\nlabels. It requires only a single pass through the video, with no separate\ntraining set. Given the lack of datasets of very long videos, we demonstrate\nour method on video from 10 days (254 hours) of continuous wildlife monitoring\ndata that we had collected with required permissions. We find that the approach\nis robust to various environmental conditions such as day/night conditions,\nrain, sharp shadows, and windy conditions. For the task of temporally locating\nevents, we had an 80% recall rate at 20% false-positive rate for frame-level\nsegmentation. At the activity level, we had an 80% activity recall rate for one\nfalse activity detection every 50 minutes. We will make the dataset, which is\nthe first of its kind, and the code available to the research community.\n",
    "topics": "{'Activity Detection': 0.9999715, 'Continual Learning': 0.99527967, 'Action Detection': 0.9928236}",
    "score": 0.8782703617
  },
  {
    "id": "2001.06968",
    "title": "FD-GAN: Generative Adversarial Networks with Fusion-discriminator for\n  Single Image Dehazing",
    "abstract": "  Recently, convolutional neural networks (CNNs) have achieved great\nimprovements in single image dehazing and attained much attention in research.\nMost existing learning-based dehazing methods are not fully end-to-end, which\nstill follow the traditional dehazing procedure: first estimate the medium\ntransmission and the atmospheric light, then recover the haze-free image based\non the atmospheric scattering model. However, in practice, due to lack of\npriors and constraints, it is hard to precisely estimate these intermediate\nparameters. Inaccurate estimation further degrades the performance of dehazing,\nresulting in artifacts, color distortion and insufficient haze removal. To\naddress this, we propose a fully end-to-end Generative Adversarial Networks\nwith Fusion-discriminator (FD-GAN) for image dehazing. With the proposed\nFusion-discriminator which takes frequency information as additional priors,\nour model can generator more natural and realistic dehazed images with less\ncolor distortion and fewer artifacts. Moreover, we synthesize a large-scale\ntraining dataset including various indoor and outdoor hazy images to boost the\nperformance and we reveal that for learning-based dehazing methods, the\nperformance is strictly influenced by the training data. Experiments have shown\nthat our method reaches state-of-the-art performance on both public synthetic\ndatasets and real-world images with more visually pleasing dehazed results.\n",
    "topics": "{'Image Dehazing': 0.9999999, 'Single Image Dehazing': 0.99999964}",
    "score": 0.8782636199
  },
  {
    "id": "1806.10787",
    "title": "How To Extract Fashion Trends From Social Media? A Robust Object\n  Detector With Support For Unsupervised Learning",
    "abstract": "  With the proliferation of social media, fashion inspired from celebrities,\nreputed designers as well as fashion influencers has shortened the cycle of\nfashion design and manufacturing. However, with the explosion of fashion\nrelated content and large number of user generated fashion photos, it is an\narduous task for fashion designers to wade through social media photos and\ncreate a digest of trending fashion. This necessitates deep parsing of fashion\nphotos on social media to localize and classify multiple fashion items from a\ngiven fashion photo. While object detection competitions such as MSCOCO have\nthousands of samples for each of the object categories, it is quite difficult\nto get large labeled datasets for fast fashion items. Moreover,\nstate-of-the-art object detectors do not have any functionality to ingest large\namount of unlabeled data available on social media in order to fine tune object\ndetectors with labeled datasets. In this work, we show application of a generic\nobject detector, that can be pretrained in an unsupervised manner, on 24\ncategories from recently released Open Images V4 dataset. We first train the\nbase architecture of the object detector using unsupervisd learning on 60K\nunlabeled photos from 24 categories gathered from social media, and then\nsubsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.\nOn 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K\nphotos while performing 11% to 17% better as compared to the state-of-the-art\nobject detectors. We show that this improvement is due to our choice of\narchitecture that lets us do unsupervised learning and that performs\nsignificantly better in identifying small objects.\n",
    "topics": "{'Object Detection': 0.9159984}",
    "score": 0.8781940156
  },
  {
    "id": "1803.09340",
    "title": "DeepVesselNet: Vessel Segmentation, Centerline Prediction, and\n  Bifurcation Detection in 3-D Angiographic Volumes",
    "abstract": "  We present DeepVesselNet, an architecture tailored to the challenges faced\nwhen extracting vessel networks or trees and corresponding features in 3-D\nangiographic volumes using deep learning. We discuss the problems of low\nexecution speed and high memory requirements associated with full 3-D\nconvolutional networks, high-class imbalance arising from the low percentage of\nvessel voxels, and unavailability of accurately annotated training data - and\noffer solutions as the building blocks of DeepVesselNet.\n  First, we formulate 2-D orthogonal cross-hair filters which make use of 3-D\ncontext information at a reduced computational burden. Second, we introduce a\nclass balancing cross-entropy loss function with false positive rate correction\nto handle the high-class imbalance and high false positive rate problems\nassociated with existing loss functions. Finally, we generate synthetic dataset\nusing a computational angiogenesis model capable of generating vascular trees\nunder physiological constraints on local network structure and topology and use\nthese data for transfer learning.\n  DeepVesselNet is optimized for segmenting and analyzing vessels, and we test\nthe performance on a range of angiographic volumes including clinical MRA data\nof the human brain, as well as X-ray tomographic microscopy scans of the rat\nbrain. Our experiments show that, by replacing 3-D filters with cross-hair\nfilters in our network, we achieve over 23% improvement in speed, lower memory\nfootprint, lower network complexity which prevents overfitting and comparable\naccuracy (with a Cox-Wilcoxon paired sample significance test p-value of 0.07\nwhen compared to full 3-D filters). Our class balancing metric is crucial for\ntraining the network and transfer learning with synthetic data is an efficient,\nrobust, and very generalizable approach leading to a network that excels in a\nvariety of angiography segmentation tasks.\n",
    "topics": "{'Transfer Learning': 0.7738785}",
    "score": 0.8779421822
  },
  {
    "id": "1609.08675",
    "title": "YouTube-8M: A Large-Scale Video Classification Benchmark",
    "abstract": "  Many recent advancements in Computer Vision are attributed to large datasets.\nOpen-source software packages for Machine Learning and inexpensive commodity\nhardware have reduced the barrier of entry for exploring novel approaches at\nscale. It is possible to train models over millions of examples within a few\ndays. Although large-scale datasets exist for image understanding, such as\nImageNet, there are no comparable size video classification datasets.\n  In this paper, we introduce YouTube-8M, the largest multi-label video\nclassification dataset, composed of ~8 million videos (500K hours of video),\nannotated with a vocabulary of 4800 visual entities. To get the videos and\ntheir labels, we used a YouTube video annotation system, which labels videos\nwith their main topics. While the labels are machine-generated, they have\nhigh-precision and are derived from a variety of human-based signals including\nmetadata and query click signals. We filtered the video labels (Knowledge Graph\nentities) using both automated and manual curation strategies, including asking\nhuman raters if the labels are visually recognizable. Then, we decoded each\nvideo at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to\nextract the hidden representation immediately prior to the classification\nlayer. Finally, we compressed the frame features and make both the features and\nvideo-level labels available for download.\n  We trained various (modest) classification models on the dataset, evaluated\nthem using popular evaluation metrics, and report them as baselines. Despite\nthe size of the dataset, some of our models train to convergence in less than a\nday on a single machine using TensorFlow. We plan to release code for training\na TensorFlow model and for computing metrics.\n",
    "topics": "{'Video Classification': 1.0, 'Action Recognition': 0.9965204}",
    "score": 0.877828336
  },
  {
    "id": "2002.04608",
    "title": "Constructing a Highlight Classifier with an Attention Based LSTM Neural\n  Network",
    "abstract": "  Data is being produced in larger quantities than ever before in human\nhistory. It's only natural to expect a rise in demand for technology that aids\nhumans in sifting through and analyzing this inexhaustible supply of\ninformation. This need exists in the market research industry, where large\namounts of consumer research data is collected through video recordings. At\npresent, the standard method for analyzing video data is human labor. Market\nresearchers manually review the vast majority of consumer research video in\norder to identify relevant portions - highlights. The industry state of the art\nturnaround ratio is 2.2 - for every hour of video content 2.2 hours of manpower\nare required. In this study we present a novel approach for NLP-based highlight\nidentification and extraction based on a supervised learning model that aides\nmarket researchers in sifting through their data. Our approach hinges on a\nmanually curated user-generated highlight clips constructed from long and\nshort-form video data. The problem is best suited for an NLP approach due to\nthe availability of video transcription. We evaluate multiple classes of\nmodels, from gradient boosting to recurrent neural networks, comparing their\nperformance in extraction and identification of highlights. The best performing\nmodels are then evaluated using four sampling methods designed to analyze\ndocuments much larger than the maximum input length of the classifiers. We\nreport very high performances for the standalone classifiers, ROC AUC scores in\nthe range 0.93-0.94, but observe a significant drop in effectiveness when\nevaluated on large documents. Based on our results we suggest combinations of\nmodels/sampling algorithms for various use cases.\n",
    "topics": "{}",
    "score": 0.8777850275
  },
  {
    "id": "1710.05982",
    "title": "Pushing the envelope in deep visual recognition for mobile platforms",
    "abstract": "  Image classification is the task of assigning to an input image a label from\na fixed set of categories. One of its most important applicative fields is that\nof robotics, in particular the needing of a robot to be aware of what's around\nand the consequent exploitation of that information as a benefit for its tasks.\nIn this work we consider the problem of a robot that enters a new environment\nand wants to understand visual data coming from its camera, so to extract\nknowledge from them. As main novelty we want to overcome the needing of a\nphysical robot, as it could be expensive and unhandy, so to hopefully enhance,\nspeed up and ease the research in this field. That's why we propose to develop\nan application for a mobile platform that wraps several deep visual recognition\ntasks. First we deal with a simple Image classification, testing a model\nobtained from an AlexNet trained on the ILSVRC 2012 dataset. Several photo\nsettings are considered to better understand which factors affect most the\nquality of classification. For the same purpose we are interested to integrate\nthe classification task with an extra module dealing with segmentation of the\nobject inside the image. In particular we propose a technique for extracting\nthe object shape and moving out all the background, so to focus the\nclassification only on the region occupied by the object. Another significant\ntask that is included is that of object discovery. Its purpose is to simulate\nthe situation in which the robot needs a certain object to complete one of its\nactivities. It starts searching for what it needs by looking around and trying\nto understand the location of the object by scanning the surrounding\nenvironment. Finally we provide a tool for dealing with the creation of\ncustomized task-specific databases, meant to better suit to one's needing in a\nparticular vision task.\n",
    "topics": "{'Object Discovery': 0.99981517, 'Image Classification': 0.9967044}",
    "score": 0.8776389322
  },
  {
    "id": "1801.09927",
    "title": "Diagnose like a Radiologist: Attention Guided Convolutional Neural\n  Network for Thorax Disease Classification",
    "abstract": "  This paper considers the task of thorax disease classification on chest X-ray\nimages. Existing methods generally use the global image as input for network\nlearning. Such a strategy is limited in two aspects. 1) A thorax disease\nusually happens in (small) localized areas which are disease specific. Training\nCNNs using global image may be affected by the (excessive) irrelevant noisy\nareas. 2) Due to the poor alignment of some CXR images, the existence of\nirregular borders hinders the network performance. In this paper, we address\nthe above problems by proposing a three-branch attention guided convolution\nneural network (AG-CNN). AG-CNN 1) learns from disease-specific regions to\navoid noise and improve alignment, 2) also integrates a global branch to\ncompensate the lost discriminative cues by local branch. Specifically, we first\nlearn a global CNN branch using global images. Then, guided by the attention\nheat map generated from the global branch, we inference a mask to crop a\ndiscriminative region from the global image. The local region is used for\ntraining a local CNN branch. Lastly, we concatenate the last pooling layers of\nboth the global and local branches for fine-tuning the fusion branch. The\nComprehensive experiment is conducted on the ChestX-ray14 dataset. We first\nreport a strong global baseline producing an average AUC of 0.841 with\nResNet-50 as backbone. After combining the local cues with the global\ninformation, AG-CNN improves the average AUC to 0.868. While DenseNet-121 is\nused, the average AUC achieves 0.871, which is a new state of the art in the\ncommunity.\n",
    "topics": "{}",
    "score": 0.8775502405
  },
  {
    "id": "2003.06080",
    "title": "Coronary Artery Segmentation from Intravascular Optical Coherence\n  Tomography Using Deep Capsules",
    "abstract": "  The segmentation and analysis of coronary arteries from intravascular optical\ncoherence tomography (IVOCT) is an important aspect of diagnosing and managing\ncoronary artery disease. Automated, robust, and timely geometry extraction from\nIVOCT, using image processing, would be beneficial to clinicians as modern\nbiomechanical analysis relies on these geometries. Current image processing\nmethods are hindered by the time needed to generate these expert-labelled\ndatasets and the potential for bias during the analysis. Here we present a new\ndeep learning method based on capsules to automatically produce lumen\nsegmentations, built using a large IVOCT dataset of 12,011 images with\nground-truth segmentations. With clinical application in mind, our model aims\nto have a small memory footprint and be fast at inference time without\nsacrificing segmentation quality. Our dataset contains images with both blood\nand light artefacts (22.8%), as well as metallic (23.1%) and bioresorbable\nstents (2.5%). We split the dataset into a training (70%), validation (20%) and\ntest (10%) set and rigorously investigate design variations with respect to\nupsampling regimes and input selection. We also show that our model outperforms\na UNet-ResNet-18 on a test set, with a better soft Dice score, pixel\nsensitivity and specificity, while only taking up 19% of the disk space and\nbeing 39% faster during CPU inference. Finally, we show that our fully trained\nand optimized model achieves a mean soft Dice score of 97.31% (median of\n98.22%) on a test set.\n",
    "topics": "{}",
    "score": 0.8771672615
  },
  {
    "id": "1703.02083",
    "title": "Auto-context Convolutional Neural Network (Auto-Net) for Brain\n  Extraction in Magnetic Resonance Imaging",
    "abstract": "  Brain extraction or whole brain segmentation is an important first step in\nmany of the neuroimage analysis pipelines. The accuracy and robustness of brain\nextraction, therefore, is crucial for the accuracy of the entire brain analysis\nprocess. With the aim of designing a learning-based, geometry-independent and\nregistration-free brain extraction tool in this study, we present a technique\nbased on an auto-context convolutional neural network (CNN), in which intrinsic\nlocal and global image features are learned through 2D patches of different\nwindow sizes. In this architecture three parallel 2D convolutional pathways for\nthree different directions (axial, coronal, and sagittal) implicitly learn 3D\nimage information without the need for computationally expensive 3D\nconvolutions. Posterior probability maps generated by the network are used\niteratively as context information along with the original image patches to\nlearn the local shape and connectedness of the brain, to extract it from\nnon-brain tissue.\n  The brain extraction results we have obtained from our algorithm are superior\nto the recently reported results in the literature on two publicly available\nbenchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap\ncoefficients of 97.42% and 95.40%, respectively. Furthermore, we evaluated the\nperformance of our algorithm in the challenging problem of extracting\narbitrarily-oriented fetal brains in reconstructed fetal brain magnetic\nresonance imaging (MRI) datasets. In this application our algorithm performed\nmuch better than the other methods (Dice coefficient: 95.98%), where the other\nmethods performed poorly due to the non-standard orientation and geometry of\nthe fetal brain in MRI. Our CNN-based method can provide accurate,\ngeometry-independent brain extraction in challenging applications.\n",
    "topics": "{'Brain Segmentation': 0.99998975}",
    "score": 0.8771051879
  },
  {
    "id": "1712.02170",
    "title": "Detecting Curve Text in the Wild: New Dataset and New Solution",
    "abstract": "  Scene text detection has been made great progress in recent years. The\ndetection manners are evolving from axis-aligned rectangle to rotated rectangle\nand further to quadrangle. However, current datasets contain very little curve\ntext, which can be widely observed in scene images such as signboard, product\nname and so on. To raise the concerns of reading curve text in the wild, in\nthis paper, we construct a curve text dataset named CTW1500, which includes\nover 10k text annotations in 1,500 images (1000 for training and 500 for\ntesting). Based on this dataset, we pioneering propose a polygon based curve\ntext detector (CTD) which can directly detect curve text without empirical\ncombination. Moreover, by seamlessly integrating the recurrent transverse and\nlongitudinal offset connection (TLOC), the proposed method can be end-to-end\ntrainable to learn the inherent connection among the position offsets. This\nallows the CTD to explore context information instead of predicting points\nindependently, resulting in more smooth and accurate detection. We also propose\ntwo simple but effective post-processing methods named non-polygon suppress\n(NPS) and polygonal non-maximum suppression (PNMS) to further improve the\ndetection accuracy. Furthermore, the proposed approach in this paper is\ndesigned in an universal manner, which can also be trained with rectangular or\nquadrilateral bounding boxes without extra efforts. Experimental results on\nCTW-1500 demonstrate our method with only a light backbone can outperform\nstate-of-the-art methods with a large margin. By evaluating only in the curve\nor non-curve subset, the CTD + TLOC can still achieve the best results. Code is\navailable at https://github.com/Yuliang-Liu/Curve-Text-Detector.\n",
    "topics": "{'Scene Text Detection': 0.99988997, 'Scene Text': 0.9997298}",
    "score": 0.8765615808
  },
  {
    "id": "1909.07808",
    "title": "Chinese Street View Text: Large-scale Chinese Text Reading with\n  Partially Supervised Learning",
    "abstract": "  Most existing text reading benchmarks make it difficult to evaluate the\nperformance of more advanced deep learning models in large vocabularies due to\nthe limited amount of training data. To address this issue, we introduce a new\nlarge-scale text reading benchmark dataset named Chinese Street View Text\n(C-SVT) with 430,000 street view images, which is at least 14 times as large as\nthe existing Chinese text reading benchmarks. To recognize Chinese text in the\nwild while keeping large-scale datasets labeling cost-effective, we propose to\nannotate one part of the CSVT dataset (30,000 images) in locations and text\nlabels as full annotations and add 400,000 more images, where only the\ncorresponding text-of-interest in the regions is given as weak annotations. To\nexploit the rich information from the weakly annotated data, we design a text\nreading network in a partially supervised learning framework, which enables to\nlocalize and recognize text, learn from fully and weakly annotated data\nsimultaneously. To localize the best matched text proposals from weakly labeled\nimages, we propose an online proposal matching module incorporated in the whole\nmodel, spotting the keyword regions by sharing parameters for end-to-end\ntraining. Compared with fully supervised training algorithms, this model can\nimprove the end-to-end recognition performance remarkably by 4.03% in F-score\nat the same labeling cost. The proposed model can also achieve state-of-the-art\nresults on the ICDAR 2017-RCTW dataset, which demonstrates the effectiveness of\nthe proposed partially supervised learning framework.\n",
    "topics": "{'Part-Of-Speech Tagging': 0.7240957}",
    "score": 0.8763916724
  },
  {
    "id": "2001.06144",
    "title": "Learning to Augment Expressions for Few-shot Fine-grained Facial\n  Expression Recognition",
    "abstract": "  Affective computing and cognitive theory are widely used in modern\nhuman-computer interaction scenarios. Human faces, as the most prominent and\neasily accessible features, have attracted great attention from researchers.\nSince humans have rich emotions and developed musculature, there exist a lot of\nfine-grained expressions in real-world applications. However, it is extremely\ntime-consuming to collect and annotate a large number of facial images, of\nwhich may even require psychologists to correctly categorize them. To the best\nof our knowledge, the existing expression datasets are only limited to several\nbasic facial expressions, which are not sufficient to support our ambitions in\ndeveloping successful human-computer interaction systems. To this end, a novel\nFine-grained Facial Expression Database - F2ED is contributed in this paper,\nand it includes more than 200k images with 54 facial expressions from 119\npersons. Considering the phenomenon of uneven data distribution and lack of\nsamples is common in real-world scenarios, we further evaluate several tasks of\nfew-shot expression learning by virtue of our F2ED, which are to recognize the\nfacial expressions given only few training instances. These tasks mimic human\nperformance to learn robust and general representation from few examples. To\naddress such few-shot tasks, we propose a unified task-driven framework -\nCompositional Generative Adversarial Network (Comp-GAN) learning to synthesize\nfacial images and thus augmenting the instances of few-shot expression classes.\nExtensive experiments are conducted on F2ED and existing facial expression\ndatasets, i.e., JAFFE and FER2013, to validate the efficacy of our F2ED in\npre-training facial expression recognition network and the effectiveness of our\nproposed approach Comp-GAN to improve the performance of few-shot recognition\ntasks.\n",
    "topics": "{'Facial Expression Recognition': 1.0}",
    "score": 0.8762240454
  },
  {
    "id": "1904.07470",
    "title": "Super Resolution Convolutional Neural Network Models for Enhancing\n  Resolution of Rock Micro-CT Images",
    "abstract": "  Single Image Super Resolution (SISR) techniques based on Super Resolution\nConvolutional Neural Networks (SRCNN) are applied to micro-computed tomography\n({\\mu}CT) images of sandstone and carbonate rocks. Digital rock imaging is\nlimited by the capability of the scanning device resulting in trade-offs\nbetween resolution and field of view, and super resolution methods tested in\nthis study aim to compensate for these limits. SRCNN models SR-Resnet, Enhanced\nDeep SR (EDSR), and Wide-Activation Deep SR (WDSR) are used on the Digital Rock\nSuper Resolution 1 (DRSRD1) Dataset of 4x downsampled images, comprising of\n2000 high resolution (800x800) raw micro-CT images of Bentheimer sandstone and\nEstaillades carbonate. The trained models are applied to the validation and\ntest data within the dataset and show a 3-5 dB rise in image quality compared\nto bicubic interpolation, with all tested models performing within a 0.1 dB\nrange. Difference maps indicate that edge sharpness is completely recovered in\nimages within the scope of the trained model, with only high frequency noise\nrelated detail loss. We find that aside from generation of high-resolution\nimages, a beneficial side effect of super resolution methods applied to\nsynthetically downgraded images is the removal of image noise while recovering\nedgewise sharpness which is beneficial for the segmentation process. The model\nis also tested against real low-resolution images of Bentheimer rock with image\naugmentation to account for natural noise and blur. The SRCNN method is shown\nto act as a preconditioner for image segmentation under these circumstances\nwhich naturally leads to further future development and training of models that\nsegment an image directly. Image restoration by SRCNN on the rock images is of\nsignificantly higher quality than traditional methods and suggests SRCNN\nmethods are a viable processing step in a digital rock workflow.\n",
    "topics": "{'Super-Resolution': 1.0, 'Super Resolution': 1.0, 'Image Super-Resolution': 1.0, 'Image Restoration': 0.99852884, 'Image Augmentation': 0.9980089, 'Semantic Segmentation': 0.8082081}",
    "score": 0.8760263825
  },
  {
    "id": "1509.03150",
    "title": "STC: A Simple to Complex Framework for Weakly-supervised Semantic\n  Segmentation",
    "abstract": "  Recently, significant improvement has been made on semantic object\nsegmentation due to the development of deep convolutional neural networks\n(DCNNs). Training such a DCNN usually relies on a large number of images with\npixel-level segmentation masks, and annotating these images is very costly in\nterms of both finance and human effort. In this paper, we propose a simple to\ncomplex (STC) framework in which only image-level annotations are utilized to\nlearn DCNNs for semantic segmentation. Specifically, we first train an initial\nsegmentation network called Initial-DCNN with the saliency maps of simple\nimages (i.e., those with a single category of major object(s) and clean\nbackground). These saliency maps can be automatically obtained by existing\nbottom-up salient object detection techniques, where no supervision information\nis needed. Then, a better network called Enhanced-DCNN is learned with\nsupervision from the predicted segmentation masks of simple images based on the\nInitial-DCNN as well as the image-level annotations. Finally, more pixel-level\nsegmentation masks of complex images (two or more categories of objects with\ncluttered background), which are inferred by using Enhanced-DCNN and\nimage-level annotations, are utilized as the supervision information to learn\nthe Powerful-DCNN for semantic segmentation. Our method utilizes $40$K simple\nimages from Flickr.com and 10K complex images from PASCAL VOC for step-wisely\nboosting the segmentation network. Extensive experimental results on PASCAL VOC\n2012 segmentation benchmark well demonstrate the superiority of the proposed\nSTC framework compared with other state-of-the-arts.\n",
    "topics": "{'Semantic Segmentation': 0.99999857, 'Weakly-Supervised Semantic Segmentation': 0.99779284}",
    "score": 0.875982285
  },
  {
    "id": "1906.01160",
    "title": "Transfer Learning with intelligent training data selection for\n  prediction of Alzheimer's Disease",
    "abstract": "  Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI\nthrough machine learning has been a subject of intense research in recent\nyears. Recent success of deep learning in computer vision has progressed such\nresearch further. However, common limitations with such algorithms are reliance\non a large number of training images, and requirement of careful optimization\nof the architecture of deep networks. In this paper, we attempt solving these\nissues with transfer learning, where the state-of-the-art VGG architecture is\ninitialized with pre-trained weights from large benchmark datasets consisting\nof natural images. The network is then fine-tuned with layer-wise tuning, where\nonly a pre-defined group of layers are trained on MRI images. To shrink the\ntraining data size, we employ image entropy to select the most informative\nslices. Through experimentation on the ADNI dataset, we show that with training\nsize of 10 to 20 times smaller than the other contemporary methods, we reach\nstate-of-the-art performance in AD vs. NC, AD vs. MCI, and MCI vs. NC\nclassification problems, with a 4% and a 7% increase in accuracy over the\nstate-of-the-art for AD vs. MCI and MCI vs. NC, respectively. We also provide\ndetailed analysis of the effect of the intelligent training data selection\nmethod, changing the training size, and changing the number of layers to be\nfine-tuned. Finally, we provide Class Activation Maps (CAM) that demonstrate\nhow the proposed model focuses on discriminative image regions that are\nneuropathologically relevant, and can help the healthcare practitioner in\ninterpreting the model's decision making process.\n",
    "topics": "{'Transfer Learning': 0.99770325, 'Decision Making': 0.9814636}",
    "score": 0.8758703262
  },
  {
    "id": "1705.03737",
    "title": "Efficient and Scalable View Generation from a Single Image using Fully\n  Convolutional Networks",
    "abstract": "  Single-image-based view generation (SIVG) is important for producing 3D\nstereoscopic content. Here, handling different spatial resolutions as input and\noptimizing both reconstruction accuracy and processing speed is desirable.\nLatest approaches are based on convolutional neural network (CNN), and they\ngenerate promising results. However, their use of fully connected layers as\nwell as pre-trained VGG forces a compromise between reconstruction accuracy and\nprocessing speed. In addition, this approach is limited to the use of a\nspecific spatial resolution. To remedy these problems, we propose exploiting\nfully convolutional networks (FCN) for SIVG. We present two FCN architectures\nfor SIVG. The first one is based on combination of an FCN and a view-rendering\nnetwork called DeepView$_{ren}$. The second one consists of decoupled networks\nfor luminance and chrominance signals, denoted by DeepView$_{dec}$. To train\nour solutions we present a large dataset of 2M stereoscopic images. Results\nshow that both of our architectures improve accuracy and speed over the state\nof the art. DeepView$_{ren}$ generates competitive accuracy to the state of the\nart, however, with the fastest processing speed of all. That is x5 times faster\nspeed and x24 times lower memory consumption compared to the state of the art.\nDeepView$_{dec}$ has much higher accuracy, but with x2.5 times faster speed and\nx12 times lower memory consumption. We evaluated our approach with both\nobjective and subjective studies.\n",
    "topics": "{}",
    "score": 0.8757147169
  },
  {
    "id": "1910.06038",
    "title": "Sketch-Specific Data Augmentation for Freehand Sketch Recognition",
    "abstract": "  Sketch recognition remains a significant challenge due to the limited\ntraining data and the substantial intra-class variance of freehand sketches for\nthe same object. Conventional methods for this task often rely on the\navailability of the temporal order of sketch strokes, additional cues acquired\nfrom different modalities and supervised augmentation of sketch datasets with\nreal images, which also limit the applicability and feasibility of these\nmethods in real scenarios.\n  In this paper, we propose a novel sketch-specific data augmentation (SSDA)\nmethod that leverages the quantity and quality of the sketches automatically.\nFrom the aspect of quantity, we introduce a Bezier pivot based deformation\n(BPD) strategy to enrich the training data. Towards quality improvement, we\npresent a mean stroke reconstruction (MSR) approach to generate a set of novel\ntypes of sketches with smaller intra-class variances. Both of these solutions\nare unrestricted from any multi-source data and temporal cues of sketches.\nFurthermore, we show that some recent deep convolutional neural network models\nthat are trained on generic classes of real images can be better choices than\nmost of the elaborate architectures that are designed explicitly for sketch\nrecognition. As SSDA can be integrated with any convolutional neural networks,\nit has a distinct advantage over the existing methods. Our extensive\nexperimental evaluations demonstrate that the proposed method achieves\nstate-of-the-art results (84.27%) on the TU-Berlin dataset, outperforming the\nhuman performance by a remarkable 11.17% increase. We also present a new\nbenchmark named Sketchy-R to facilitate future research in sketch recognition.\nFinally, more experiments show the practical value of our approach to the task\nof sketch-based image retrieval.\n",
    "topics": "{'Sketch-Based Image Retrieval': 0.99999857, 'Data Augmentation': 0.9999957, 'Image Retrieval': 0.9409769}",
    "score": 0.8751155158
  },
  {
    "id": "1405.5732",
    "title": "Self-tuned Visual Subclass Learning with Shared Samples An Incremental\n  Approach",
    "abstract": "  Computer vision tasks are traditionally defined and evaluated using semantic\ncategories. However, it is known to the field that semantic classes do not\nnecessarily correspond to a unique visual class (e.g. inside and outside of a\ncar). Furthermore, many of the feasible learning techniques at hand cannot\nmodel a visual class which appears consistent to the human eye. These problems\nhave motivated the use of 1) Unsupervised or supervised clustering as a\npreprocessing step to identify the visual subclasses to be used in a\nmixture-of-experts learning regime. 2) Felzenszwalb et al. part model and other\nworks model mixture assignment with latent variables which is optimized during\nlearning 3) Highly non-linear classifiers which are inherently capable of\nmodelling multi-modal input space but are inefficient at the test time. In this\nwork, we promote an incremental view over the recognition of semantic classes\nwith varied appearances. We propose an optimization technique which\nincrementally finds maximal visual subclasses in a regularized risk\nminimization framework. Our proposed approach unifies the clustering and\nclassification steps in a single algorithm. The importance of this approach is\nits compliance with the classification via the fact that it does not need to\nknow about the number of clusters, the representation and similarity measures\nused in pre-processing clustering methods a priori. Following this approach we\nshow both qualitatively and quantitatively significant results. We show that\nthe visual subclasses demonstrate a long tail distribution. Finally, we show\nthat state of the art object detection methods (e.g. DPM) are unable to use the\ntails of this distribution comprising 50\\% of the training samples. In fact we\nshow that DPM performance slightly increases on average by the removal of this\nhalf of the data.\n",
    "topics": "{}",
    "score": 0.8750055137
  },
  {
    "id": "1806.03969",
    "title": "End to End Brain Fiber Orientation Estimation using Deep Learning",
    "abstract": "  In this work, we explore the various Brain Neuron tracking techniques, which\nis one of the most significant applications of Diffusion Tensor Imaging.\nTractography provides us with a non-invasive method to analyze underlying\ntissue micro-structure. Understanding the structure and organization of the\ntissues facilitates us with a diagnosis method to identify any aberrations and\nprovide acute information on the occurrences of brain ischemia or stroke, the\nmutation of neurological diseases such as Alzheimer, multiple sclerosis and so\non. Time if of essence and accurate localization of the aberrations can help\nsave or change a diseased life. Following up with the limitations introduced by\nthe current Tractography techniques such as computational complexity,\nreconstruction errors during tensor estimation and standardization, we aim to\nelucidate these limitations through our research findings. We introduce an end\nto end Deep Learning framework which can accurately estimate the most probable\nlikelihood orientation at each voxel along a neuronal pathway. We use\nProbabilistic Tractography as our baseline model to obtain the training data\nand which also serve as a Tractography Gold Standard for our evaluations.\nThrough experiments we show that our Deep Network can do a significant\nimprovement over current Tractography implementations by reducing the run-time\ncomplexity to a significant new level. Our architecture also allows for\nvariable sized input DWI signals eliminating the need to worry about memory\nissues as seen with the traditional techniques. The advantage of this\narchitecture is that it is perfectly desirable to be processed on a cloud setup\nand utilize the existing multi GPU frameworks to perform whole brain\nTractography in minutes rather than hours. We evaluate our network with Gold\nStandard and benchmark its performance across several parameters.\n",
    "topics": "{}",
    "score": 0.8746072671
  },
  {
    "id": "1911.07937",
    "title": "Inverse Graphics: Unsupervised Learning of 3D Shapes from Single Images",
    "abstract": "  Using generative models for Inverse Graphics is an active area of research.\nHowever, most works focus on developing models for supervised and\nsemi-supervised methods. In this paper, we study the problem of unsupervised\nlearning of 3D geometry from single images. Our approach is to use a generative\nmodel that produces 2-D images as projections of a latent 3D voxel grid, which\nwe train either as a variational auto-encoder or using adversarial methods. Our\ncontributions are as follows: First, we show how to recover 3D shape and pose\nfrom general datasets such as MNIST, and MNIST Fashion in good quality. Second,\nwe compare the shapes learned using adversarial and variational methods.\nAdversarial approach gives denser 3D shapes. Third, we explore the idea of\nmodelling the pose of an object as uniform distribution to recover 3D shape\nfrom a single image. Our experiment with the CelebA dataset\n\\cite{liu2015faceattributes} proves that we can recover complete 3D shape from\na single image when the object is symmetric along one, or more axis whilst\nresults obtained using ModelNet40 \\cite{wu20153d} show the potential\nside-effects, in which the model learns 3D shapes such that it can render the\nsame image from any viewpoint. Forth, we present a general end-to-end approach\nto learning 3D shapes from single images in a completely unsupervised fashion\nby modelling the factors of variation such as azimuth as independent latent\nvariables. Our method makes no assumptions about the dataset, and can work with\nsynthetic as well as real images (i.e. unsupervised in true sense). We present\nour results, by training the model using the $\\mu$-VAE objective\n\\cite{ucar2019bridging} and a dataset combining all images from MNIST, MNIST\nFashion, CelebA and six categories of ModelNet40. The model is able to learn 3D\nshapes and the pose in qood quality and leverages information learned across\nall datasets.\n",
    "topics": "{'3D Shape Reconstruction': 0.5538543}",
    "score": 0.8745453895
  },
  {
    "id": "1907.09375",
    "title": "DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D\n  Lung Models from Single-View Projections by Deep Deformation Network",
    "abstract": "  This paper introduces a deep neural network based method, i.e., DeepOrganNet,\nto generate and visualize high-fidelity 3D / 4D organ geometric models from\nsingle-view medical image in real time. Traditional 3D / 4D medical image\nreconstruction requires near hundreds of projections, which cost insufferable\ncomputational time and deliver undesirable high imaging / radiation dose to\nhuman subjects. Moreover, it always needs further notorious processes to\nextract the accurate 3D organ models subsequently. To our knowledge, there is\nno method directly and explicitly reconstructing multiple 3D organ meshes from\na single 2D medical grayscale image on the fly. Given single-view 2D medical\nimages, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end\nDeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung\nmodels with a variety of geometric shapes by learning the smooth deformation\nfields from multiple templates based on a trivariate tensor-product deformation\ntechnique, leveraging an informative latent descriptor extracted from input 2D\nimages. The proposed method can guarantee to generate high-quality and\nhigh-fidelity manifold meshes for 3D / 4D lung models. The major contributions\nof this work are to accurately reconstruct the 3D organ shapes from 2D\nsingle-view projection, significantly improve the procedure time to allow\non-the-fly visualization, and dramatically reduce the imaging dose for human\nsubjects. Experimental results are evaluated and compared with the traditional\nreconstruction method and the state-of-the-art in deep learning, by using\nextensive 3D and 4D examples from synthetic phantom and real patient datasets.\nThe proposed method only needs several milliseconds to generate organ meshes\nwith 10K vertices, which has a great potential to be used in real-time image\nguided radiation therapy (IGRT).\n",
    "topics": "{}",
    "score": 0.874544257
  },
  {
    "id": "1806.10840",
    "title": "Training Discriminative Models to Evaluate Generative Ones",
    "abstract": "  Generative models are known to be difficult to assess. Recent works,\nespecially on generative adversarial networks (GANs), produce good visual\nsamples of varied categories of images. However, the validation of their\nquality is still difficult to define and there is no existing agreement on the\nbest evaluation process. This paper aims at making a step toward an objective\nevaluation process for generative models. It presents a new method to assess a\ntrained generative model by evaluating the test accuracy of a classifier\ntrained with generated data. The test set is composed of real images.\nTherefore, The classifier accuracy is used as a proxy to evaluate if the\ngenerative model fit the true data distribution. By comparing results with\ndifferent generated datasets we are able to classify and compare generative\nmodels. The motivation of this approach is also to evaluate if generative\nmodels can help discriminative neural networks to learn, i.e., measure if\ntraining on generated data is able to make a model successful at testing on\nreal settings. Our experiments compare different generators from the\nVariational Auto-Encoders (VAE) and Generative Adversarial Network (GAN)\nframeworks on MNIST and fashion MNIST datasets. Our results show that none of\nthe generative models is able to replace completely true data to train a\ndiscriminative model. But they also show that the initial GAN and WGAN are the\nbest choices to generate on MNIST database (Modified National Institute of\nStandards and Technology database) and fashion MNIST database.\n",
    "topics": "{}",
    "score": 0.8743639397
  },
  {
    "id": "1903.11821",
    "title": "SRDGAN: learning the noise prior for Super Resolution with Dual\n  Generative Adversarial Networks",
    "abstract": "  Single Image Super Resolution (SISR) is the task of producing a high\nresolution (HR) image from a given low-resolution (LR) image. It is a well\nresearched problem with extensive commercial applications such as digital\ncamera, video compression, medical imaging and so on. Most super resolution\nworks focus on the features learning architecture, which can recover the\ntexture details as close as possible. However, these works suffer from the\nfollowing challenges: (1) The low-resolution (LR) training images are\nartificially synthesized using HR images with bicubic downsampling, which have\nmuch richer-information than real demosaic-upscaled mobile images. The mismatch\nbetween training and inference mobile data heavily blocks the improvement of\npractical super resolution algorithms. (2) These methods cannot effectively\nhandle the blind distortions during super resolution in practical applications.\nIn this work, an end-to-end novel framework, including high-to-low network and\nlow-to-high network, is proposed to solve the above problems with dual\nGenerative Adversarial Networks (GAN). First, the above mismatch problems are\nwell explored with the high-to-low network, where clear high-resolution image\nand the corresponding realistic low-resolution image pairs can be generated.\nMoreover, a large-scale General Mobile Super Resolution Dataset, GMSR, is\nproposed, which can be utilized for training or as a fair comparison benchmark\nfor super resolution methods. Second, an effective low-to-high network (super\nresolution network) is proposed in the framework. Benefiting from the GMSR\ndataset and novel training strategies, the super resolution model can\neffectively handle detail recovery and denoising at the same time.\n",
    "topics": "{'Super-Resolution': 1.0, 'Super Resolution': 1.0, 'Image Super-Resolution': 1.0}",
    "score": 0.8739574498
  },
  {
    "id": "1809.05522",
    "title": "Deep Compressive Autoencoder for Action Potential Compression in\n  Large-Scale Neural Recording",
    "abstract": "  Understanding the coordinated activity underlying brain computations requires\nlarge-scale, simultaneous recordings from distributed neuronal structures at a\ncellular-level resolution. One major hurdle to design high-bandwidth,\nhigh-precision, large-scale neural interfaces lies in the formidable data\nstreams that are generated by the recorder chip and need to be online\ntransferred to a remote computer. The data rates can require hundreds to\nthousands of I/O pads on the recorder chip and power consumption on the order\nof Watts for data streaming alone. We developed a deep learning-based\ncompression model to reduce the data rate of multichannel action potentials.\nThe proposed model is built upon a deep compressive autoencoder (CAE) with\ndiscrete latent embeddings. The encoder is equipped with residual\ntransformations to extract representative features from spikes, which are\nmapped into the latent embedding space and updated via vector quantization\n(VQ). The decoder network reconstructs spike waveforms from the quantized\nlatent embeddings. Experimental results show that the proposed model\nconsistently outperforms conventional methods by achieving much higher\ncompression ratios (20-500x) and better or comparable reconstruction\naccuracies. Testing results also indicate that CAE is robust against a diverse\nrange of imperfections, such as waveform variation and spike misalignment, and\nhas minor influence on spike sorting accuracy. Furthermore, we have estimated\nthe hardware cost and real-time performance of CAE and shown that it could\nsupport thousands of recording channels simultaneously without excessive\npower/heat dissipation. The proposed model can reduce the required data\ntransmission bandwidth in large-scale recording experiments and maintain good\nsignal qualities. The code of this work has been made available at\nhttps://github.com/tong-wu-umn/spike-compression-autoencoder\n",
    "topics": "{'Text Generation': 0.31145334}",
    "score": 0.8737648175
  },
  {
    "id": "1909.06763",
    "title": "Deep Learning for Low-Field to High-Field MR: Image Quality Transfer\n  with Probabilistic Decimation Simulator",
    "abstract": "  MR images scanned at low magnetic field ($<1$T) have lower resolution in the\nslice direction and lower contrast, due to a relatively small signal-to-noise\nratio (SNR) than those from high field (typically 1.5T and 3T). We adapt the\nrecent idea of Image Quality Transfer (IQT) to enhance very low-field\nstructural images aiming to estimate the resolution, spatial coverage, and\ncontrast of high-field images. Analogous to many learning-based image\nenhancement techniques, IQT generates training data from high-field scans alone\nby simulating low-field images through a pre-defined decimation model. However,\nthe ground truth decimation model is not well-known in practice, and lack of\nits specification can bias the trained model, aggravating performance on the\nreal low-field scans. In this paper we propose a probabilistic decimation\nsimulator to improve robustness of model training. It is used to generate and\naugment various low-field images whose parameters are random variables and\nsampled from an empirical distribution related to tissue-specific SNR on a\n0.36T scanner. The probabilistic decimation simulator is model-agnostic, that\nis, it can be used with any super-resolution networks. Furthermore we propose a\nvariant of U-Net architecture to improve its learning performance. We show\npromising qualitative results from clinical low-field images confirming the\nstrong efficacy of IQT in an important new application area: epilepsy diagnosis\nin sub-Saharan Africa where only low-field scanners are normally available.\n",
    "topics": "{'Image Enhancement': 1.0, 'Super-Resolution': 0.97821665, 'Super Resolution': 0.96222275}",
    "score": 0.8736129358
  },
  {
    "id": "1805.02798",
    "title": "Combo Loss: Handling Input and Output Imbalance in Multi-Organ\n  Segmentation",
    "abstract": "  Simultaneous segmentation of multiple organs from different medical imaging\nmodalities is a crucial task as it can be utilized for computer-aided\ndiagnosis, computer-assisted surgery, and therapy planning. Thanks to the\nrecent advances in deep learning, several deep neural networks for medical\nimage segmentation have been introduced successfully for this purpose. In this\npaper, we focus on learning a deep multi-organ segmentation network that labels\nvoxels. In particular, we examine the critical choice of a loss function in\norder to handle the notorious imbalance problem that plagues both the input and\noutput of a learning model. The input imbalance refers to the class-imbalance\nin the input training samples (i.e., small foreground objects embedded in an\nabundance of background voxels, as well as organs of varying sizes). The output\nimbalance refers to the imbalance between the false positives and false\nnegatives of the inference model. In order to tackle both types of imbalance\nduring training and inference, we introduce a new curriculum learning based\nloss function. Specifically, we leverage Dice similarity coefficient to deter\nmodel parameters from being held at bad local minima and at the same time\ngradually learn better model parameters by penalizing for false\npositives/negatives using a cross entropy term. We evaluated the proposed loss\nfunction on three datasets: whole body positron emission tomography (PET) scans\nwith 5 target organs, magnetic resonance imaging (MRI) prostate scans, and\nultrasound echocardigraphy images with a single target organ i.e., left\nventricular. We show that a simple network architecture with the proposed\nintegrative loss function can outperform state-of-the-art methods and results\nof the competing methods can be improved when our proposed loss is used.\n",
    "topics": "{'Medical Image Segmentation': 0.99996066, 'Semantic Segmentation': 0.9936266, 'Curriculum Learning': 0.9895431}",
    "score": 0.8733944996
  },
  {
    "id": "1805.10485",
    "title": "Vehicle Instance Segmentation from Aerial Image and Video Using a\n  Multi-Task Learning Residual Fully Convolutional Network",
    "abstract": "  Object detection and semantic segmentation are two main themes in object\nretrieval from high-resolution remote sensing images, which have recently\nachieved remarkable performance by surfing the wave of deep learning and, more\nnotably, convolutional neural networks (CNNs). In this paper, we are interested\nin a novel, more challenging problem of vehicle instance segmentation, which\nentails identifying, at a pixel-level, where the vehicles appear as well as\nassociating each pixel with a physical instance of a vehicle. In contrast,\nvehicle detection and semantic segmentation each only concern one of the two.\nWe propose to tackle this problem with a semantic boundary-aware multi-task\nlearning network. More specifically, we utilize the philosophy of residual\nlearning (ResNet) to construct a fully convolutional network that is capable of\nharnessing multi-level contextual feature representations learned from\ndifferent residual blocks. We theoretically analyze and discuss why residual\nnetworks can produce better probability maps for pixel-wise segmentation tasks.\nThen, based on this network architecture, we propose a unified multi-task\nlearning network that can simultaneously learn two complementary tasks, namely,\nsegmenting vehicle regions and detecting semantic boundaries. The latter\nsubproblem is helpful for differentiating closely spaced vehicles, which are\nusually not correctly separated into instances. Currently, datasets with\npixel-wise annotation for vehicle extraction are ISPRS dataset and IEEE GRSS\nDFC2015 dataset over Zeebrugge, which specializes in semantic segmentation.\nTherefore, we built a new, more challenging dataset for vehicle instance\nsegmentation, called the Busy Parking Lot UAV Video dataset, and we make our\ndataset available at http://www.sipeo.bgu.tum.de/download so that it can be\nused to benchmark future vehicle instance segmentation algorithms.\n",
    "topics": "{'Semantic Segmentation': 1.0, 'Instance Segmentation': 1.0, 'Multi-Task Learning': 0.9999999, 'Object Detection': 0.77803636}",
    "score": 0.8729896771
  },
  {
    "id": "1911.08388",
    "title": "Multi-Resolution 3D CNN for MRI Brain Tumor Segmentation and Survival\n  Prediction",
    "abstract": "  In this study, an automated three dimensional (3D) deep segmentation approach\nfor detecting gliomas in 3D pre-operative MRI scans is proposed. Then, a\nclassi-fication algorithm based on random forests, for survival prediction is\npresented. The objective is to segment the glioma area and produce segmentation\nlabels for its different sub-regions, i.e. necrotic and the non-enhancing tumor\ncore, the peri-tumoral edema, and enhancing tumor. The proposed deep\narchitecture for the segmentation task encompasses two parallel streamlines\nwith two different reso-lutions. One deep convolutional neural network is to\nlearn local features of the input data while the other one is set to have a\nglobal observation on whole image. Deemed to be complementary, the outputs of\neach stream are then merged to pro-vide an ensemble complete learning of the\ninput image. The proposed network takes the whole image as input instead of\npatch-based approaches in order to con-sider the semantic features throughout\nthe whole volume. The algorithm is trained on BraTS 2019 which included 335\ntraining cases, and validated on 127 unseen cases from the validation dataset\nusing a blind testing approach. The proposed method was also evaluated on the\nBraTS 2019 challenge test dataset of 166 cases. The results show that the\nproposed methods provide promising segmentations as well as survival\nprediction. The mean Dice overlap measures of automatic brain tumor\nsegmentation for validation set were 0.84, 0.74 and 0.71 for the whole tu-mor,\ncore and enhancing tumor, respectively. The corresponding results for the\nchallenge test dataset were 0.82, 0.72, and 0.70, respectively. The overall\naccura-cy of the proposed model for the survival prediction task is %52 for the\nvalida-tion and %49 for the test dataset.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'Brain Tumor Segmentation': 1.0}",
    "score": 0.8729415418
  },
  {
    "id": "1901.03146",
    "title": "Cosine-similarity penalty to discriminate sound classes in\n  weakly-supervised sound event detection",
    "abstract": "  The design of new methods and models when only weakly-labeled data are\navailable is of paramount importance in order to reduce the costs of manual\nannotation and the considerable human effort associated with it. In this work,\nwe address Sound Event Detection in the case where a weakly annotated dataset\nis available for training. The weak annotations provide tags of audio events\nbut do not provide temporal boundaries. The objective is twofold: 1) audio\ntagging, i.e. multi-label classification at recording level, 2) sound event\ndetection, i.e. localization of the event boundaries within the recordings.\nThis work focuses mainly on the second objective. We explore an approach\ninspired by Multiple Instance Learning, in which we train a convolutional\nrecurrent neural network to give predictions at frame-level, using a custom\nloss function based on the weak labels and the statistics of the frame-based\npredictions. Since some sound classes cannot be distinguished with this\napproach, we improve the method by penalizing similarity between the\npredictions of the positive classes during training. On the test set used in\nthe DCASE 2018 challenge, consisting of 288 recordings and 10 sound classes,\nthe addition of a penalty resulted in a localization F-score of 34.75%, and\nbrought 10% relative improvement compared to not using the penalty. Our best\nmodel achieved a 26.20% F-score on the DCASE-2018 official Eval subset close to\nthe 10-system ensemble approach that ranked second in the challenge with a\n29.9% F-score.\n",
    "topics": "{'Multiple Instance Learning': 0.999982, 'Multi-Label Classification': 0.9978561}",
    "score": 0.8728745764
  },
  {
    "id": "1912.02743",
    "title": "Toward Filament Segmentation Using Deep Neural Networks",
    "abstract": "  We use a well-known deep neural network framework, called Mask R-CNN, for\nidentification of solar filaments in full-disk H-alpha images from Big Bear\nSolar Observatory (BBSO). The image data, collected from BBSO's archive, are\nintegrated with the spatiotemporal metadata of filaments retrieved from the\nHeliophysics Events Knowledgebase (HEK) system. This integrated data is then\ntreated as the ground-truth in the training process of the model. The available\nspatial metadata are the output of a currently running filament-detection\nmodule developed and maintained by the Feature Finding Team; an international\nconsortium selected by NASA. Despite the known challenges in the identification\nand characterization of filaments by the existing module, which in turn are\ninherited into any other module that intends to learn from such outputs, Mask\nR-CNN shows promising results. Trained and validated on two years worth of BBSO\ndata, this model is then tested on the three following years. Our case-by-case\nand overall analyses show that Mask R-CNN can clearly compete with the existing\nmodule and in some cases even perform better. Several cases of false positives\nand false negatives, that are correctly segmented by this model are also shown.\nThe overall advantages of using the proposed model are two-fold: First, deep\nneural networks' performance generally improves as more annotated data, or\nbetter annotations are provided. Second, such a model can be scaled up to\ndetect other solar events, as well as a single multi-purpose module. The\nresults presented in this study introduce a proof of concept in benefits of\nemploying deep neural networks for detection of solar events, and in\nparticular, filaments.\n",
    "topics": "{}",
    "score": 0.8728318549
  },
  {
    "id": "2006.02801",
    "title": "Height estimation from single aerial images using a deep ordinal\n  regression network",
    "abstract": "  Understanding the 3D geometric structure of the Earth's surface has been an\nactive research topic in photogrammetry and remote sensing community for\ndecades, serving as an essential building block for various applications such\nas 3D digital city modeling, change detection, and city management. Previous\nresearches have extensively studied the problem of height estimation from\naerial images based on stereo or multi-view image matching. These methods\nrequire two or more images from different perspectives to reconstruct 3D\ncoordinates with camera information provided. In this paper, we deal with the\nambiguous and unsolved problem of height estimation from a single aerial image.\nDriven by the great success of deep learning, especially deep convolution\nneural networks (CNNs), some researches have proposed to estimate height\ninformation from a single aerial image by training a deep CNN model with\nlarge-scale annotated datasets. These methods treat height estimation as a\nregression problem and directly use an encoder-decoder network to regress the\nheight values. In this paper, we proposed to divide height values into\nspacing-increasing intervals and transform the regression problem into an\nordinal regression problem, using an ordinal loss for network training. To\nenable multi-scale feature extraction, we further incorporate an Atrous Spatial\nPyramid Pooling (ASPP) module to extract features from multiple dilated\nconvolution layers. After that, a post-processing technique is designed to\ntransform the predicted height map of each patch into a seamless height map.\nFinally, we conduct extensive experiments on ISPRS Vaihingen and Potsdam\ndatasets. Experimental results demonstrate significantly better performance of\nour method compared to the state-of-the-art methods.\n",
    "topics": "{}",
    "score": 0.8723571294
  },
  {
    "id": "1911.07205",
    "title": "REFIT: a Unified Watermark Removal Framework for Deep Learning Systems\n  with Limited Data",
    "abstract": "  Deep neural networks (DNNs) have achieved tremendous success in various\nfields; however, training these models from scratch could be computationally\nexpensive and requires a lot of training data. Recent work has explored\ndifferent watermarking techniques to protect the pre-trained deep neural\nnetworks from potential copyright infringements; however, they could be\nvulnerable to adversaries who aim at removing the watermarks. In this work, we\npropose REFIT, a unified watermark removal framework based on fine-tuning,\nwhich does not rely on the knowledge of the watermarks and even the\nwatermarking schemes. Firstly, we demonstrate that by properly designing the\nlearning rate schedule for fine-tuning, such approaches could be effective\ninstead. Furthermore, we conduct a comprehensive study of a realistic attack\nscenario where the adversary has limited training data. To effectively remove\nthe watermarks without compromising the model functionality under this weak\nthreat model, we propose to incorporate two techniques: (1) an adaption of the\nelastic weight consolidation (EWC) algorithm, which is originally proposed for\nmitigating the catastrophic forgetting phenomenon; and (2) unlabeled data\naugmentation (AU), where we leverage auxiliary unlabeled data from other\nsources. Our extensive evaluation shows the effectiveness of REFIT against\ndiverse watermark embedding schemes. In particular, both EWC and AU\nsignificantly decrease the amount of labeled training data needed for effective\nwatermark removal, and the unlabeled data samples used for AU do not\nnecessarily need to be drawn from the same distribution as the benign data for\nmodel evaluation. The experimental results demonstrate that our fine-tuning\nbased watermark removal attacks could pose real threats to the copyright of\npre-trained models, and thus highlight the importance of further investigation\nof the watermarking problem.\n",
    "topics": "{'Data Augmentation': 0.99822456}",
    "score": 0.8723479089
  },
  {
    "id": "1804.04241",
    "title": "Capsules for Object Segmentation",
    "abstract": "  Convolutional neural networks (CNNs) have shown remarkable results over the\nlast several years for a wide range of computer vision tasks. A new\narchitecture recently introduced by Sabour et al., referred to as a capsule\nnetworks with dynamic routing, has shown great initial results for digit\nrecognition and small image classification. The success of capsule networks\nlies in their ability to preserve more information about the input by replacing\nmax-pooling layers with convolutional strides and dynamic routing, allowing for\npreservation of part-whole relationships in the data. This preservation of the\ninput is demonstrated by reconstructing the input from the output capsule\nvectors. Our work expands the use of capsule networks to the task of object\nsegmentation for the first time in the literature. We extend the idea of\nconvolutional capsules with locally-connected routing and propose the concept\nof deconvolutional capsules. Further, we extend the masked reconstruction to\nreconstruct the positive input class. The proposed\nconvolutional-deconvolutional capsule network, called SegCaps, shows strong\nresults for the task of object segmentation with substantial decrease in\nparameter space. As an example application, we applied the proposed SegCaps to\nsegment pathological lungs from low dose CT scans and compared its accuracy and\nefficiency with other U-Net-based architectures. SegCaps is able to handle\nlarge image sizes (512 x 512) as opposed to baseline capsules (typically less\nthan 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net\narchitecture by 95.4% while still providing a better segmentation accuracy.\n",
    "topics": "{'Semantic Segmentation': 0.99999285, 'Image Classification': 0.89470077}",
    "score": 0.8723089259
  },
  {
    "id": "1811.00445",
    "title": "CariGAN: Caricature Generation through Weakly Paired Adversarial\n  Learning",
    "abstract": "  Caricature generation is an interesting yet challenging task. The primary\ngoal is to generate plausible caricatures with reasonable exaggerations given\nface images. Conventional caricature generation approaches mainly use low-level\ngeometric transformations such as image warping to generate exaggerated images,\nwhich lack richness and diversity in terms of content and style. The recent\nprogress in generative adversarial networks (GANs) makes it possible to learn\nan image-to-image transformation from data, so that richer contents and styles\ncan be generated. However, directly applying the GAN-based models to this task\nleads to unsatisfactory results because there is a large variance in the\ncaricature distribution. Moreover, some models require strictly paired training\ndata which largely limits their usage scenarios. In this paper, we propose\nCariGAN overcome these problems. Instead of training on paired data, CariGAN\nlearns transformations only from weakly paired images. Specifically, to enforce\nreasonable exaggeration and facial deformation, facial landmarks are adopted as\nan additional condition to constrain the generated image. Furthermore, an\nattention mechanism is introduced to encourage our model to focus on the key\nfacial parts so that more vivid details in these regions can be generated.\nFinally, a Diversity Loss is proposed to encourage the model to produce diverse\nresults to help alleviate the `mode collapse' problem of the conventional\nGAN-based models. Extensive experiments on a new large-scale `WebCaricature'\ndataset show that the proposed CariGAN can generate more plausible caricatures\nwith larger diversity compared with the state-of-the-art models.\n",
    "topics": "{}",
    "score": 0.8722884916
  },
  {
    "id": "1912.06649",
    "title": "A Novel Automation-Assisted Cervical Cancer Reading Method Based on\n  Convolutional Neural Network",
    "abstract": "  While most previous automation-assisted reading methods can improve\nefficiency, their performance often relies on the success of accurate cell\nsegmentation and hand-craft feature extraction. This paper presents an\nefficient and totally segmentation-free method for automated cervical cell\nscreening that utilizes modern object detector to directly detect cervical\ncells or clumps, without the design of specific hand-crafted feature.\nSpecifically, we use the state-of-the-art CNN-based object detection methods,\nYOLOv3, as our baseline model. In order to improve the classification\nperformance of hard examples which are four highly similar categories, we\ncascade an additional task-specific classifier. We also investigate the\npresence of unreliable annotations and cope with them by smoothing the\ndistribution of noisy labels. We comprehensively evaluate our methods on test\nset which is consisted of 1,014 annotated cervical cell images with size of\n4000*3000 and complex cellular situation corresponding to 10 categories. Our\nmodel achieves 97.5% sensitivity (Sens) and 67.8% specificity (Spec) on\ncervical cell image-level screening. Moreover, we obtain a mean Average\nPrecision (mAP) of 63.4% on cervical cell-level diagnosis, and improve the\nAverage Precision (AP) of hard examples which are valuable but difficult to\ndistinguish. Our automation-assisted cervical cell reading method not only\nachieves cervical cell image-level classification but also provides more\ndetailed location and category information of abnormal cells. The results\nindicate feasible performance of our method, together with the efficiency and\nrobustness, providing a new idea for future development of computer-assisted\nreading system in clinical cervical screening.\n",
    "topics": "{'Cell Segmentation': 0.9999945, 'Object Detection': 0.97642547}",
    "score": 0.8720785028
  },
  {
    "id": "1910.03751",
    "title": "An MDL-Based Classifier for Transactional Datasets with Application in\n  Malware Detection",
    "abstract": "  We design a classifier for transactional datasets with application in malware\ndetection. We build the classifier based on the minimum description length\n(MDL) principle. This involves selecting a model that best compresses the\ntraining dataset for each class considering the MDL criterion. To select a\nmodel for a dataset, we first use clustering followed by closed frequent\npattern mining to extract a subset of closed frequent patterns (CFPs). We show\nthat this method acts as a pattern summarization method to avoid pattern\nexplosion; this is done by giving priority to longer CFPs, and without\nrequiring to extract all CFPs. We then use the MDL criterion to further\nsummarize extracted patterns, and construct a code table of patterns. This code\ntable is considered as the selected model for the compression of the dataset.\nWe evaluate our classifier for the problem of static malware detection in\nportable executable (PE) files. We consider API calls of PE files as their\ndistinguishing features. The presence-absence of API calls forms a\ntransactional dataset. Using our proposed method, we construct two code tables,\none for the benign training dataset, and one for the malware training dataset.\nOur dataset consists of 19696 benign, and 19696 malware samples, each a binary\nsequence of size 22761. We compare our classifier with deep neural networks\nproviding us with the state-of-the-art performance. The comparison shows that\nour classifier performs very close to deep neural networks. We also discuss\nthat our classifier is an interpretable classifier. This provides the\nmotivation to use this type of classifiers where some degree of explanation is\nrequired as to why a sample is classified under one class rather than the other\nclass.\n",
    "topics": "{'Malware Detection': 1.0, 'Model Compression': 0.9987205}",
    "score": 0.87206579
  },
  {
    "id": "2005.07983",
    "title": "Multi-level Feature Fusion-based CNN for Local Climate Zone\n  Classification from Sentinel-2 Images: Benchmark Results on the So2Sat LCZ42\n  Dataset",
    "abstract": "  As a unique classification scheme for urban forms and functions, the local\nclimate zone (LCZ) system provides essential general information for any\nstudies related to urban environments, especially on a large scale. Remote\nsensing data-based classification approaches are the key to large-scale mapping\nand monitoring of LCZs. The potential of deep learning-based approaches is not\nyet fully explored, even though advanced convolutional neural networks (CNNs)\ncontinue to push the frontiers for various computer vision tasks. One reason is\nthat published studies are based on different datasets, usually at a regional\nscale, which makes it impossible to fairly and consistently compare the\npotential of different CNNs for real-world scenarios. This study is based on\nthe big So2Sat LCZ42 benchmark dataset dedicated to LCZ classification. Using\nthis dataset, we studied a range of CNNs of varying sizes. In addition, we\nproposed a CNN to classify LCZs from Sentinel-2 images, Sen2LCZ-Net. Using this\nbase network, we propose fusing multi-level features using the extended\nSen2LCZ-Net-MF. With this proposed simple network architecture and the highly\ncompetitive benchmark dataset, we obtain results that are better than those\nobtained by the state-of-the-art CNNs, while requiring less computation with\nfewer layers and parameters. Large-scale LCZ classification examples of\ncompletely unseen areas are presented, demonstrating the potential of our\nproposed Sen2LCZ-Net-MF as well as the So2Sat LCZ42 dataset. We also\nintensively investigated the influence of network depth and width and the\neffectiveness of the design choices made for Sen2LCZ-Net-MF. Our work will\nprovide important baselines for future CNN-based algorithm developments for\nboth LCZ classification and other urban land cover land use classification.\n",
    "topics": "{'Semantic Segmentation': 0.34199077}",
    "score": 0.8720288164
  },
  {
    "id": "1910.11509",
    "title": "Deep 1D-Convnet for accurate Parkinson disease detection and severity\n  prediction from gait",
    "abstract": "  Diagnosing Parkinson's disease is a complex task that requires the evaluation\nof several motor and non-motor symptoms. During diagnosis, gait abnormalities\nare among the important symptoms that physicians should consider. However, gait\nevaluation is challenging and relies on the expertise and subjectivity of\nclinicians. In this context, the use of an intelligent gait analysis algorithm\nmay assist physicians in order to facilitate the diagnosis process. This paper\nproposes a novel intelligent Parkinson detection system based on deep learning\ntechniques to analyze gait information. We used 1D convolutional neural network\n(1D-Convnet) to build a Deep Neural Network (DNN) classifier. The proposed\nmodel processes 18 1D-signals coming from foot sensors measuring the vertical\nground reaction force (VGRF). The first part of the network consists of 18\nparallel 1D-Convnet corresponding to system inputs. The second part is a fully\nconnected network that connects the concatenated outputs of the 1D-Convnets to\nobtain a final classification. We tested our algorithm in Parkinson's detection\nand in the prediction of the severity of the disease with the Unified\nParkinson's Disease Rating Scale (UPDRS). Our experiments demonstrate the high\nefficiency of the proposed method in the detection of Parkinson disease based\non gait data. The proposed algorithm achieved an accuracy of 98.7 %. To our\nknowledge, this is the state-of-the-start performance in Parkinson's gait\nrecognition. Furthermore, we achieved an accuracy of 85.3 % in Parkinson's\nseverity prediction. To the best of our knowledge, this is the first algorithm\nto perform a severity prediction based on the UPDRS. Our results show that the\nmodel is able to learn intrinsic characteristics from gait data and to\ngeneralize to unseen subjects, which could be helpful in a clinical diagnosis.\n",
    "topics": "{'Gait Recognition': 0.99993074}",
    "score": 0.8716470438
  },
  {
    "id": "2002.06610",
    "title": "REST: Performance Improvement of a Black Box Model via RL-based Spatial\n  Transformation",
    "abstract": "  In recent years, deep neural networks (DNN) have become a highly active area\nof research, and shown remarkable achievements on a variety of computer vision\ntasks. DNNs, however, are known to often make overconfident yet incorrect\npredictions on out-of-distribution samples, which can be a major obstacle to\nreal-world deployments because the training dataset is always limited compared\nto diverse real-world samples. Thus, it is fundamental to provide guarantees of\nrobustness to the distribution shift between training and test time when we\nconstruct DNN models in practice. Moreover, in many cases, the deep learning\nmodels are deployed as black boxes and the performance has been already\noptimized for a training dataset, thus changing the black box itself can lead\nto performance degradation. We here study the robustness to the geometric\ntransformations in a specific condition where the black-box image classifier is\ngiven. We propose an additional learner, \\emph{REinforcement Spatial Transform\nlearner (REST)}, that transforms the warped input data into samples regarded as\nin-distribution by the black-box models. Our work aims to improve the\nrobustness by adding a REST module in front of any black boxes and training\nonly the REST module without retraining the original black box model in an\nend-to-end manner, i.e. we try to convert the real-world data into training\ndistribution which the performance of the black-box model is best suited for.\nWe use a confidence score that is obtained from the black-box model to\ndetermine whether the transformed input is drawn from in-distribution. We\nempirically show that our method has an advantage in generalization to\ngeometric transformations and sample efficiency.\n",
    "topics": "{}",
    "score": 0.8715541213
  },
  {
    "id": "1909.09349",
    "title": "Deep 3D-Zoom Net: Unsupervised Learning of Photo-Realistic 3D-Zoom",
    "abstract": "  The 3D-zoom operation is the positive translation of the camera in the\nZ-axis, perpendicular to the image plane. In contrast, the optical zoom changes\nthe focal length and the digital zoom is used to enlarge a certain region of an\nimage to the original image size. In this paper, we are the first to formulate\nan unsupervised 3D-zoom learning problem where images with an arbitrary zoom\nfactor can be generated from a given single image. An unsupervised framework is\nconvenient, as it is a challenging task to obtain a 3D-zoom dataset of natural\nscenes due to the need for special equipment to ensure camera movement is\nrestricted to the Z-axis. In addition, the objects in the scenes should not\nmove when being captured, which hinders the construction of a large dataset of\noutdoor scenes. We present a novel unsupervised framework to learn how to\ngenerate arbitrarily 3D-zoomed versions of a single image, not requiring a\n3D-zoom ground truth, called the Deep 3D-Zoom Net. The Deep 3D-Zoom Net\nincorporates the following features: (i) transfer learning from a pre-trained\ndisparity estimation network via a back re-projection reconstruction loss; (ii)\na fully convolutional network architecture that models depth-image-based\nrendering (DIBR), taking into account high-frequency details without the need\nfor estimating the intermediate disparity; and (iii) incorporating a\ndiscriminator network that acts as a no-reference penalty for unnaturally\nrendered areas. Even though there is no baseline to fairly compare our results,\nour method outperforms previous novel view synthesis research in terms of\nrealistic appearance on large camera baselines. We performed extensive\nexperiments to verify the effectiveness of our method on the KITTI and\nCityscapes datasets.\n",
    "topics": "{'Novel View Synthesis': 0.99997294, 'Disparity Estimation': 0.9999708, 'Transfer Learning': 0.8527178}",
    "score": 0.8715350603
  },
  {
    "id": "1908.03735",
    "title": "Automatic acute ischemic stroke lesion segmentation using\n  semi-supervised learning",
    "abstract": "  Ischemic stroke is a common disease in the elderly population, which can\ncause long-term disability and even death. However, the time window for\ntreatment of ischemic stroke in its acute stage is very short. To fast localize\nand quantitively evaluate the acute ischemic stroke (AIS) lesions, many\ndeep-learning-based lesion segmentation methods have been proposed in the\nliterature, where a deep convolutional neural network (CNN) was trained on\nhundreds of fully labeled subjects with accurate annotations of AIS lesions.\nDespite that high segmentation accuracy can be achieved, the accurate labels\nshould be annotated by experienced clinicians, and it is therefore very\ntime-consuming to obtain a large number of fully labeled subjects. In this\npaper, we propose a semi-supervised method to automatically segment AIS lesions\nin diffusion weighted images and apparent diffusion coefficient maps. By using\na large number of weakly labeled subjects and a small number of fully labeled\nsubjects, our proposed method is able to accurately detect and segment the AIS\nlesions. In particular, our proposed method consists of three parts: 1) a\ndouble-path classification net (DPC-Net) trained in a weakly-supervised way is\nused to detect the suspicious regions of AIS lesions; 2) a pixel-level K-Means\nclustering algorithm is used to identify the hyperintensive regions on the\nDWIs; and 3) a region-growing algorithm combines the outputs of the DPC-Net and\nthe K-Means to obtain the final precise lesion segmentation. In our experiment,\nwe use 460 weakly labeled subjects and 15 fully labeled subjects to train and\nfine-tune the proposed method. By evaluating on a clinical dataset with 150\nfully labeled subjects, our proposed method achieves a mean dice coefficient of\n0.642, and a lesion-wise F1 score of 0.822.\n",
    "topics": "{'Lesion Segmentation': 1.0}",
    "score": 0.8713434659
  },
  {
    "id": "2006.02627",
    "title": "Robust Automatic Whole Brain Extraction on Magnetic Resonance Imaging of\n  Brain Tumor Patients using Dense-Vnet",
    "abstract": "  Whole brain extraction, also known as skull stripping, is a process in\nneuroimaging in which non-brain tissue such as skull, eyeballs, skin, etc. are\nremoved from neuroimages. Skull striping is a preliminary step in presurgical\nplanning, cortical reconstruction, and automatic tumor segmentation. Despite a\nplethora of skull stripping approaches in the literature, few are sufficiently\naccurate for processing pathology-presenting MRIs, especially MRIs with brain\ntumors. In this work we propose a deep learning approach for skull striping\ncommon MRI sequences in oncology such as T1-weighted with gadolinium contrast\n(T1Gd) and T2-weighted fluid attenuated inversion recovery (FLAIR) in patients\nwith brain tumors. We automatically created gray matter, white matter, and CSF\nprobability masks using SPM12 software and merged the masks into one for a\nfinal whole-brain mask for model training. Dice agreement, sensitivity, and\nspecificity of the model (referred herein as DeepBrain) was tested against\nmanual brain masks. To assess data efficiency, we retrained our models using\nprogressively fewer training data examples and calculated average dice scores\non the test set for the models trained in each round. Further, we tested our\nmodel against MRI of healthy brains from the LBP40A dataset. Overall, DeepBrain\nyielded an average dice score of 94.5%, sensitivity of 96.4%, and specificity\nof 98.5% on brain tumor data. For healthy brains, model performance improved to\na dice score of 96.2%, sensitivity of 96.6% and specificity of 99.2%. The data\nefficiency experiment showed that, for this specific task, comparable levels of\naccuracy could have been achieved with as few as 50 training samples. In\nconclusion, this study demonstrated that a deep learning model trained on\nminimally processed automatically-generated labels can generate more accurate\nbrain masks on MRI of brain tumor patients within seconds.\n",
    "topics": "{'Tumor Segmentation': 0.9994112}",
    "score": 0.8711657763
  },
  {
    "id": "1712.00661",
    "title": "Mix-and-Match Tuning for Self-Supervised Semantic Segmentation",
    "abstract": "  Deep convolutional networks for semantic image segmentation typically require\nlarge-scale labeled data, e.g. ImageNet and MS COCO, for network pre-training.\nTo reduce annotation efforts, self-supervised semantic segmentation is recently\nproposed to pre-train a network without any human-provided labels. The key of\nthis new form of learning is to design a proxy task (e.g. image colorization),\nfrom which a discriminative loss can be formulated on unlabeled data. Many\nproxy tasks, however, lack the critical supervision signals that could induce\ndiscriminative representation for the target image segmentation task. Thus\nself-supervision's performance is still far from that of supervised\npre-training. In this study, we overcome this limitation by incorporating a\n\"mix-and-match\" (M&M) tuning stage in the self-supervision pipeline. The\nproposed approach is readily pluggable to many self-supervision methods and\ndoes not use more annotated samples than the original process. Yet, it is\ncapable of boosting the performance of target image segmentation task to\nsurpass fully-supervised pre-trained counterpart. The improvement is made\npossible by better harnessing the limited pixel-wise annotations in the target\ndataset. Specifically, we first introduce the \"mix\" stage, which sparsely\nsamples and mixes patches from the target set to reflect rich and diverse local\npatch statistics of target images. A \"match\" stage then forms a class-wise\nconnected graph, which can be used to derive a strong triplet-based\ndiscriminative loss for fine-tuning the network. Our paradigm follows the\nstandard practice in existing self-supervised studies and no extra data or\nlabel is required. With the proposed M&M approach, for the first time, a\nself-supervision method can achieve comparable or even better performance\ncompared to its ImageNet pre-trained counterpart on both PASCAL VOC2012 dataset\nand CityScapes dataset.\n",
    "topics": "{'Semantic Segmentation': 1.0, 'Colorization': 0.9576208}",
    "score": 0.8708532829
  },
  {
    "id": "1904.08632",
    "title": "Learning a No-Reference Quality Assessment Model of Enhanced Images With\n  Big Data",
    "abstract": "  In this paper we investigate into the problem of image quality assessment\n(IQA) and enhancement via machine learning. This issue has long attracted a\nwide range of attention in computational intelligence and image processing\ncommunities, since, for many practical applications, e.g. object detection and\nrecognition, raw images are usually needed to be appropriately enhanced to\nraise the visual quality (e.g. visibility and contrast). In fact, proper\nenhancement can noticeably improve the quality of input images, even better\nthan originally captured images which are generally thought to be of the best\nquality. In this work, we present two most important contributions. The first\ncontribution is to develop a new no-reference (NR) IQA model. Given an image,\nour quality measure first extracts 17 features through analysis of contrast,\nsharpness, brightness and more, and then yields a measre of visual quality\nusing a regression module, which is learned with big-data training samples that\nare much bigger than the size of relevant image datasets. Results of\nexperiments on nine datasets validate the superiority and efficiency of our\nblind metric compared with typical state-of-the-art full-, reduced- and\nno-reference IQA methods. The second contribution is that a robust image\nenhancement framework is established based on quality optimization. For an\ninput image, by the guidance of the proposed NR-IQA measure, we conduct\nhistogram modification to successively rectify image brightness and contrast to\na proper level. Thorough tests demonstrate that our framework can well enhance\nnatural images, low-contrast images, low-light images and dehazed images. The\nsource code will be released at\nhttps://sites.google.com/site/guke198701/publications.\n",
    "topics": "{'Image Enhancement': 0.99999976, 'Image Quality Assessment': 0.99986255, 'Object Detection': 0.9740302}",
    "score": 0.8708443563
  },
  {
    "id": "1803.03317",
    "title": "Analysis of Hand Segmentation in the Wild",
    "abstract": "  A large number of works in egocentric vision have concentrated on action and\nobject recognition. Detection and segmentation of hands in first-person videos,\nhowever, has less been explored. For many applications in this domain, it is\nnecessary to accurately segment not only hands of the camera wearer but also\nthe hands of others with whom he is interacting. Here, we take an in-depth look\nat the hand segmentation problem. In the quest for robust hand segmentation\nmethods, we evaluated the performance of the state of the art semantic\nsegmentation methods, off the shelf and fine-tuned, on existing datasets. We\nfine-tune RefineNet, a leading semantic segmentation method, for hand\nsegmentation and find that it does much better than the best contenders.\nExisting hand segmentation datasets are collected in the laboratory settings.\nTo overcome this limitation, we contribute by collecting two new datasets: a)\nEgoYouTubeHands including egocentric videos containing hands in the wild, and\nb) HandOverFace to analyze the performance of our models in presence of similar\nappearance occlusions. We further explore whether conditional random fields can\nhelp refine generated hand segmentations. To demonstrate the benefit of\naccurate hand maps, we train a CNN for hand-based activity recognition and\nachieve higher accuracy when a CNN was trained using hand maps produced by the\nfine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for\nfine-grained action recognition and show that an accuracy of 58.6% can be\nachieved by just looking at a single hand pose which is much better than the\nchance level (12.5%).\n",
    "topics": "{'Activity Recognition': 0.98954505, 'Action Recognition': 0.8454393, 'Semantic Segmentation': 0.68371123, 'Hand Pose Estimation': 0.6284042}",
    "score": 0.8704130934
  },
  {
    "id": "1904.02365",
    "title": "Template-Based Automatic Search of Compact Semantic Segmentation\n  Architectures",
    "abstract": "  Automatic search of neural architectures for various vision and natural\nlanguage tasks is becoming a prominent tool as it allows to discover\nhigh-performing structures on any dataset of interest. Nevertheless, on more\ndifficult domains, such as dense per-pixel classification, current automatic\napproaches are limited in their scope - due to their strong reliance on\nexisting image classifiers they tend to search only for a handful of additional\nlayers with discovered architectures still containing a large number of\nparameters. In contrast, in this work we propose a novel solution able to find\nlight-weight and accurate segmentation architectures starting from only few\nblocks of a pre-trained classification network. To this end, we progressively\nbuild up a methodology that relies on templates of sets of operations, predicts\nwhich template and how many times should be applied at each step, while also\ngenerating the connectivity structure and downsampling factors. All these\ndecisions are being made by a recurrent neural network that is rewarded based\non the score of the emitted architecture on the holdout set and trained using\nreinforcement learning. One discovered architecture achieves 63.2% mean IoU on\nCamVid and 67.8% on CityScapes having only 270K parameters. Pre-trained models\nand the search code are available at\nhttps://github.com/DrSleep/nas-segm-pytorch.\n",
    "topics": "{'Real-Time Semantic Segmentation': 0.9999485, 'Semantic Segmentation': 0.99613047}",
    "score": 0.8701317211
  },
  {
    "id": "1611.09007",
    "title": "Hyperspectral CNN Classification with Limited Training Samples",
    "abstract": "  Hyperspectral imaging sensors are becoming increasingly popular in robotics\napplications such as agriculture and mining, and allow per-pixel thematic\nclassification of materials in a scene based on their unique spectral\nsignatures. Recently, convolutional neural networks have shown remarkable\nperformance for classification tasks, but require substantial amounts of\nlabelled training data. This data must sufficiently cover the variability\nexpected to be encountered in the environment. For hyperspectral data, one of\nthe main variations encountered outdoors is due to incident illumination, which\ncan change in spectral shape and intensity depending on the scene geometry. For\nexample, regions occluded from the sun have a lower intensity and their\nincident irradiance skewed towards shorter wavelengths.\n  In this work, a data augmentation strategy based on relighting is used during\ntraining of a hyperspectral convolutional neural network. It allows training to\noccur in the outdoor environment given only a small labelled region, which does\nnot need to sufficiently represent the geometric variability of the entire\nscene. This is important for applications where obtaining large amounts of\ntraining data is labourious, hazardous or difficult, such as labelling pixels\nwithin shadows. Radiometric normalisation approaches for pre-processing the\nhyperspectral data are analysed and it is shown that methods based on the raw\npixel data are sufficient to be used as input for the classifier. This removes\nthe need for external hardware such as calibration boards, which can restrict\nthe application of hyperspectral sensors in robotics applications. Experiments\nto evaluate the classification system are carried out on two datasets captured\nfrom a field-based platform.\n",
    "topics": "{'Data Augmentation': 0.9965088}",
    "score": 0.8700213189
  },
  {
    "id": "2005.04014",
    "title": "Convolutional Sparse Support Estimator Based Covid-19 Recognition from\n  X-ray Images",
    "abstract": "  Coronavirus disease (Covid-19) has been the main agenda of the whole world\nsince it came in sight in December 2019. It has already caused thousands of\ncausalities and infected several millions worldwide. Any technological tool\nthat can be provided to healthcare practitioners to save time, effort, and\npossibly lives has crucial importance. The main tools practitioners currently\nuse to diagnose Covid-19 are Reverse Transcription-Polymerase Chain reaction\n(RT-PCR) and Computed Tomography (CT), which require significant time,\nresources and acknowledged experts. X-ray imaging is a common and easily\naccessible tool that has great potential for Covid-19 diagnosis. In this study,\nwe propose a novel approach for Covid-19 recognition from chest X-ray images.\nDespite the importance of the problem, recent studies in this domain produced\nnot so satisfactory results due to the limited datasets available for training.\nRecall that Deep Learning techniques can generally provide state-of-the-art\nperformance in many classification tasks when trained properly over large\ndatasets, such data scarcity can be a crucial obstacle when using them for\nCovid-19 detection. Alternative approaches such as representation-based\nclassification (collaborative or sparse representation) might provide\nsatisfactory performance with limited size datasets, but they generally fall\nshort in performance or speed compared to Machine Learning methods. To address\nthis deficiency, Convolution Support Estimation Network (CSEN) has recently\nbeen proposed as a bridge between model-based and Deep Learning approaches by\nproviding a non-iterative real-time mapping from query sample to ideally sparse\nrepresentation coefficient' support, which is critical information for class\ndecision in representation based techniques.\n",
    "topics": "{'COVID-19 Diagnosis': 0.98311687, 'Computed Tomography (CT)': 0.96189994}",
    "score": 0.8699722856
  },
  {
    "id": "2005.00220",
    "title": "Automatic Catalog of RRLyrae from $\\sim$ 14 million VVV Light Curves:\n  How far can we go with traditional machine-learning?",
    "abstract": "  The creation of a 3D map of the bulge using RRLyrae (RRL) is one of the main\ngoals of the VVV(X) surveys. The overwhelming number of sources under analysis\nrequest the use of automatic procedures. In this context, previous works\nintroduced the use of Machine Learning (ML) methods for the variable star\nclassification. Our goal is the development and analysis of an automatic\nprocedure, based on ML, for the identification of RRLs in the VVV Survey. This\nprocedure will be use to generate reliable catalogs integrated over several\ntiles in the survey. After the reconstruction of light-curves, we extract a set\nof period and intensity-based features. We use for the first time a new subset\nof pseudo color features. We discuss all the appropriate steps needed to define\nour automatic pipeline: selection of quality measures; sampling procedures;\nclassifier setup and model selection. As final result, we construct an ensemble\nclassifier with an average Recall of 0.48 and average Precision of 0.86 over 15\ntiles. We also make available our processed datasets and a catalog of candidate\nRRLs. Perhaps most interestingly, from a classification perspective based on\nphotometric broad-band data, is that our results indicate that Color is an\ninformative feature type of the RRL that should be considered for automatic\nclassification methods via ML. We also argue that Recall and Precision in both\ntables and curves are high quality metrics for this highly imbalanced problem.\nFurthermore, we show for our VVV data-set that to have good estimates it is\nimportant to use the original distribution more than reduced samples with an\nartificial balance. Finally, we show that the use of ensemble classifiers helps\nresolve the crucial model selection step, and that most errors in the\nidentification of RRLs are related to low quality observations of some sources\nor to the difficulty to resolve the RRL-C type given the date.\n",
    "topics": "{'Model Selection': 1.0}",
    "score": 0.8698473222
  },
  {
    "id": "1911.03966",
    "title": "SeismoGen: Seismic Waveform Synthesis Using Generative Adversarial\n  Networks",
    "abstract": "  Detecting earthquake events from seismic time series has proved itself a\nchallenging task. Manual detection can be expensive and tedious due to the\nintensive labor and large scale data set. In recent years, automatic detection\nmethods based on machine learning have been developed to improve accuracy and\nefficiency. However, the accuracy of those methods relies on a sufficient\namount of high-quality training data, which itself can be expensive to obtain\ndue to the requirement of domain knowledge and subject matter expertise. This\npaper is to resolve this dilemma by answering two questions: (1) provided with\na limited number of reliable labels, can we use them to generate more synthetic\nlabels; (2) Can we use those synthetic labels to improve the detectability?\nAmong all the existing generative models, the generative adversarial network\n(GAN) shows its supreme capability in generating high-quality synthetic samples\nin multiple domains. We designed our model based on GAN. In particular, we\nstudied several different network structures. By comparing the generated\nresults, our GAN-based generative model yields the highest quality. We further\ncombine the dataset with synthetic samples generated by our generative model\nand show that the detectability of our earthquake classification model is\nsignificantly improved than the one trained without augmenting the training\nset.\n",
    "topics": "{'Time Series': 0.8382651}",
    "score": 0.8697850863
  },
  {
    "id": "1804.00858",
    "title": "Prediction and Localization of Student Engagement in the Wild",
    "abstract": "  In this paper, we introduce a new dataset for student engagement detection\nand localization. Digital revolution has transformed the traditional teaching\nprocedure and a result analysis of the student engagement in an e-learning\nenvironment would facilitate effective task accomplishment and learning. Well\nknown social cues of engagement/disengagement can be inferred from facial\nexpressions, body movements and gaze pattern. In this paper, student's response\nto various stimuli videos are recorded and important cues are extracted to\nestimate variations in engagement level. In this paper, we study the\nassociation of a subject's behavioral cues with his/her engagement level, as\nannotated by labelers. We then localize engaging/non-engaging parts in the\nstimuli videos using a deep multiple instance learning based framework, which\ncan give useful insight into designing Massive Open Online Courses (MOOCs)\nvideo material. Recognizing the lack of any publicly available dataset in the\ndomain of user engagement, a new `in the wild' dataset is created to study the\nsubject engagement problem. The dataset contains 195 videos captured from 78\nsubjects which is about 16.5 hours of recording. We present detailed baseline\nresults using different classifiers ranging from traditional machine learning\nto deep learning based approaches. The subject independent analysis is\nperformed so that it can be generalized to new users. The problem of engagement\nprediction is modeled as a weakly supervised learning problem. The dataset is\nmanually annotated by different labelers for four levels of engagement\nindependently and the correlation studies between annotated and predicted\nlabels of videos by different classifiers is reported. This dataset creation is\nan effort to facilitate research in various e-learning environments such as\nintelligent tutoring systems, MOOCs, and others.\n",
    "topics": "{'Multiple Instance Learning': 0.99995923}",
    "score": 0.869628043
  },
  {
    "id": "1910.01877",
    "title": "A Topological Loss Function for Deep-Learning based Image Segmentation\n  using Persistent Homology",
    "abstract": "  We introduce a method for training neural networks to perform image or volume\nsegmentation in which prior knowledge about the topology of the segmented\nobject can be explicitly provided and then incorporated into the training\nprocess. By using the differentiable properties of persistent homology, a\nconcept used in topological data analysis, we can specify the desired topology\nof segmented objects in terms of their Betti numbers and then drive the\nproposed segmentations to contain the specified topological features.\nImportantly this process does not require any ground-truth labels, just prior\nknowledge of the topology of the structure being segmented. We demonstrate our\napproach in three experiments. Firstly we create a synthetic task in which\nhandwritten MNIST digits are de-noised, and show that using this kind of\ntopological prior knowledge in the training of the network significantly\nimproves the quality of the de-noised digits. Secondly we perform an experiment\nin which the task is segmenting the myocardium of the left ventricle from\ncardiac magnetic resonance images. We show that the incorporation of the prior\nknowledge of the topology of this anatomy improves the resulting segmentations\nin terms of both the topological accuracy and the Dice coefficient. Thirdly, we\nextend the method to 3D volumes and demonstrate its performance on the task of\nsegmenting the placenta from ultrasound data, again showing that incorporating\ntopological priors improves performance on this challenging task. We find that\nembedding explicit prior knowledge in neural network segmentation tasks is most\nbeneficial when the segmentation task is especially challenging and that it can\nbe used in either a semi-supervised or post-processing context to extract a\nuseful training gradient from images without pixelwise labels.\n",
    "topics": "{'Semantic Segmentation': 0.9949425, 'Topological Data Analysis': 0.99234605}",
    "score": 0.8695894957
  },
  {
    "id": "1910.13757",
    "title": "A CNN-based methodology for breast cancer diagnosis using thermal images",
    "abstract": "  Micro Abstract: A recent study from GLOBOCAN disclosed that during 2018 two\nmillion women worldwide had been diagnosed from breast cancer. This study\npresents a computer-aided diagnosis system based on convolutional neural\nnetworks as an alternative diagnosis methodology for breast cancer diagnosis\nwith thermal images. Experimental results showed that lower false-positives and\nfalse-negatives classification rates are obtained when data pre-processing and\ndata augmentation techniques are implemented in these thermal images.\nBackground: There are many types of breast cancer screening techniques such as,\nmammography, magnetic resonance imaging, ultrasound and blood sample tests,\nwhich require either, expensive devices or personal qualified. Currently, some\ncountries still lack access to these main screening techniques due to economic,\nsocial or cultural issues. The objective of this study is to demonstrate that\ncomputer-aided diagnosis(CAD) systems based on convolutional neural networks\n(CNN) are faster, reliable and robust than other techniques. Methods: We\nperformed a study of the influence of data pre-processing, data augmentation\nand database size versus a proposed set of CNN models. Furthermore, we\ndeveloped a CNN hyper-parameters fine-tuning optimization algorithm using a\ntree parzen estimator. Results: Among the 57 patients database, our CNN models\nobtained a higher accuracy (92\\%) and F1-score (92\\%) that outperforms several\nstate-of-the-art architectures such as ResNet50, SeResNet50 and Inception.\nAlso, we demonstrated that a CNN model that implements data-augmentation\ntechniques reach identical performance metrics in comparison with a CNN that\nuses a database up to 50\\% bigger. Conclusion: This study highlights the\nbenefits of data augmentation and CNNs in thermal breast images. Also, it\nmeasures the influence of the database size in the performance of CNNs.\n",
    "topics": "{'Data Augmentation': 0.9999995}",
    "score": 0.8695112287
  },
  {
    "id": "2005.14284",
    "title": "Two-stage framework for optic disc localization and glaucoma\n  classification in retinal fundus images using deep learning",
    "abstract": "  With the advancement of powerful image processing and machine learning\ntechniques, CAD has become ever more prevalent in all fields of medicine\nincluding ophthalmology. Since optic disc is the most important part of retinal\nfundus image for glaucoma detection, this paper proposes a two-stage framework\nthat first detects and localizes optic disc and then classifies it into healthy\nor glaucomatous. The first stage is based on RCNN and is responsible for\nlocalizing and extracting optic disc from a retinal fundus image while the\nsecond stage uses Deep CNN to classify the extracted disc into healthy or\nglaucomatous. In addition to the proposed solution, we also developed a\nrule-based semi-automatic ground truth generation method that provides\nnecessary annotations for training RCNN based model for automated disc\nlocalization. The proposed method is evaluated on seven publicly available\ndatasets for disc localization and on ORIGA dataset, which is the largest\npublicly available dataset for glaucoma classification. The results of\nautomatic localization mark new state-of-the-art on six datasets with accuracy\nreaching 100% on four of them. For glaucoma classification we achieved AUC\nequal to 0.874 which is 2.7% relative improvement over the state-of-the-art\nresults previously obtained for classification on ORIGA. Once trained on\ncarefully annotated data, Deep Learning based methods for optic disc detection\nand localization are not only robust, accurate and fully automated but also\neliminates the need for dataset-dependent heuristic algorithms. Our empirical\nevaluation of glaucoma classification on ORIGA reveals that reporting only AUC,\nfor datasets with class imbalance and without pre-defined train and test\nsplits, does not portray true picture of the classifier's performance and calls\nfor additional performance metrics to substantiate the results.\n",
    "topics": "{}",
    "score": 0.8694095837
  },
  {
    "id": "1701.07717",
    "title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification\n  Baseline in vitro",
    "abstract": "  The main contribution of this paper is a simple semi-supervised pipeline that\nonly uses the original training set without collecting extra data. It is\nchallenging in 1) how to obtain more training data only from the training set\nand 2) how to use the newly generated data. In this work, the generative\nadversarial network (GAN) is used to generate unlabeled samples. We propose the\nlabel smoothing regularization for outliers (LSRO). This method assigns a\nuniform label distribution to the unlabeled images, which regularizes the\nsupervised model and improves the baseline. We verify the proposed method on a\npractical problem: person re-identification (re-ID). This task aims to retrieve\na query person from other cameras. We adopt the deep convolutional generative\nadversarial network (DCGAN) for sample generation, and a baseline convolutional\nneural network (CNN) for representation learning. Experiments show that adding\nthe GAN-generated data effectively improves the discriminative ability of\nlearned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and\nDukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1\nprecision over the baseline CNN, respectively. We additionally apply the\nproposed method to fine-grained bird recognition and achieve a +0.6%\nimprovement over a strong baseline. The code is available at\nhttps://github.com/layumi/Person-reID_GAN.\n",
    "topics": "{'Person Re-Identification': 1.0, 'Representation Learning': 0.9799224}",
    "score": 0.8693974525
  },
  {
    "id": "1806.10850",
    "title": "DeepSDCS: Dissecting cancer proliferation heterogeneity in Ki67 digital\n  whole slide images",
    "abstract": "  Ki67 is an important biomarker for breast cancer. Classification of positive\nand negative Ki67 cells in histology slides is a common approach to determine\ncancer proliferation status. However, there is a lack of generalizable and\naccurate methods to automate Ki67 scoring in large-scale patient cohorts. In\nthis work, we have employed a novel deep learning technique based on\nhypercolumn descriptors for cell classification in Ki67 images. Specifically,\nwe developed the Simultaneous Detection and Cell Segmentation (DeepSDCS)\nnetwork to perform cell segmentation and detection. VGG16 network was used for\nthe training and fine tuning to training data. We extracted the hypercolumn\ndescriptors of each cell to form the vector of activation from specific layers\nto capture features at different granularity. Features from these layers that\ncorrespond to the same pixel were propagated using a stochastic gradient\ndescent optimizer to yield the detection of the nuclei and the final cell\nsegmentations. Subsequently, seeds generated from cell segmentation were\npropagated to a spatially constrained convolutional neural network for the\nclassification of the cells into stromal, lymphocyte, Ki67-positive cancer\ncell, and Ki67-negative cancer cell. We validated its accuracy in the context\nof a large-scale clinical trial of oestrogen-receptor-positive breast cancer.\nWe achieved 99.06% and 89.59% accuracy on two separate test sets of Ki67\nstained breast cancer dataset comprising biopsy and whole-slide images.\n",
    "topics": "{'Cell Segmentation': 0.99999595, 'whole slide images': 0.99993014}",
    "score": 0.8693523724
  },
  {
    "id": "2009.07652",
    "title": "Contrastive Cross-site Learning with Redesigned Net for COVID-19 CT\n  Classification",
    "abstract": "  The pandemic of coronavirus disease 2019 (COVID-19) has lead to a global\npublic health crisis spreading hundreds of countries. With the continuous\ngrowth of new infections, developing automated tools for COVID-19\nidentification with CT image is highly desired to assist the clinical diagnosis\nand reduce the tedious workload of image interpretation. To enlarge the\ndatasets for developing machine learning methods, it is essentially helpful to\naggregate the cases from different medical systems for learning robust and\ngeneralizable models. This paper proposes a novel joint learning framework to\nperform accurate COVID-19 identification by effectively learning with\nheterogeneous datasets with distribution discrepancy. We build a powerful\nbackbone by redesigning the recently proposed COVID-Net in aspects of network\narchitecture and learning strategy to improve the prediction accuracy and\nlearning efficiency. On top of our improved backbone, we further explicitly\ntackle the cross-site domain shift by conducting separate feature normalization\nin latent space. Moreover, we propose to use a contrastive training objective\nto enhance the domain invariance of semantic embeddings for boosting the\nclassification performance on each dataset. We develop and evaluate our method\nwith two public large-scale COVID-19 diagnosis datasets made up of CT images.\nExtensive experiments show that our approach consistently improves the\nperformances on both datasets, outperforming the original COVID-Net trained on\neach dataset by 12.16% and 14.23% in AUC respectively, also exceeding existing\nstate-of-the-art multi-site learning methods.\n",
    "topics": "{'COVID-19 Diagnosis': 0.99998343}",
    "score": 0.8687308972
  },
  {
    "id": "1906.06480",
    "title": "RECAL: Reuse of Established CNN classifer Apropos unsupervised Learning\n  paradigm",
    "abstract": "  Recently, clustering with deep network framework has attracted attention of\nseveral researchers in the computer vision community. Deep framework gains\nextensive attention due to its efficiency and scalability towards large-scale\nand high-dimensional data. In this paper, we transform supervised CNN\nclassifier architecture into an unsupervised clustering model, called RECAL,\nwhich jointly learns discriminative embedding subspace and cluster labels.\nRECAL is made up of feature extraction layers which are convolutional, followed\nby unsupervised classifier layers which is fully connected. A multinomial\nlogistic regression function (softmax) stacked on top of classifier layers. We\ntrain this network using stochastic gradient descent (SGD) optimizer. However,\nthe successful implementation of our model is revolved around the design of\nloss function. Our loss function uses the heuristics that true partitioning\nentails lower entropy given that the class distribution is not heavily skewed.\nThis is a trade-off between the situations of \"skewed distribution\" and\n\"low-entropy\". To handle this, we have proposed classification entropy and\nclass entropy which are the two components of our loss function. In this\napproach, size of the mini-batch should be kept high. Experimental results\nindicate the consistent and competitive behavior of our model for clustering\nwell-known digit, multi-viewed object and face datasets. Morever, we use this\nmodel to generate unsupervised patch segmentation for multi-spectral LISS-IV\nimages. We observe that it is able to distinguish built-up area, wet land,\nvegetation and waterbody from the underlying scene.\n",
    "topics": "{}",
    "score": 0.8684990213
  },
  {
    "id": "1806.11368",
    "title": "Detecting Mammals in UAV Images: Best Practices to address a\n  substantially Imbalanced Dataset with Deep Learning",
    "abstract": "  Knowledge over the number of animals in large wildlife reserves is a vital\nnecessity for park rangers in their efforts to protect endangered species.\nManual animal censuses are dangerous and expensive, hence Unmanned Aerial\nVehicles (UAVs) with consumer level digital cameras are becoming a popular\nalternative tool to estimate livestock. Several works have been proposed that\nsemi-automatically process UAV images to detect animals, of which some employ\nConvolutional Neural Networks (CNNs), a recent family of deep learning\nalgorithms that proved very effective in object detection in large datasets\nfrom computer vision. However, the majority of works related to wildlife\nfocuses only on small datasets (typically subsets of UAV campaigns), which\nmight be detrimental when presented with the sheer scale of real study areas\nfor large mammal census. Methods may yield thousands of false alarms in such\ncases. In this paper, we study how to scale CNNs to large wildlife census tasks\nand present a number of recommendations to train a CNN on a large UAV dataset.\nWe further introduce novel evaluation protocols that are tailored to censuses\nand model suitability for subsequent human verification of detections. Using\nour recommendations, we are able to train a CNN reducing the number of false\npositives by an order of magnitude compared to previous state-of-the-art.\nSetting the requirements at 90% recall, our CNN allows to reduce the amount of\ndata required for manual verification by three times, thus making it possible\nfor rangers to screen all the data acquired efficiently and to detect almost\nall animals in the reserve automatically.\n",
    "topics": "{'Object Detection': 0.9754488}",
    "score": 0.8684876285
  },
  {
    "id": "1607.03681",
    "title": "Unsupervised Feature Learning Based on Deep Models for Environmental\n  Audio Tagging",
    "abstract": "  Environmental audio tagging aims to predict only the presence or absence of\ncertain acoustic events in the interested acoustic scene. In this paper we make\ncontributions to audio tagging in two parts, respectively, acoustic modeling\nand feature learning. We propose to use a shrinking deep neural network (DNN)\nframework incorporating unsupervised feature learning to handle the multi-label\nclassification task. For the acoustic modeling, a large set of contextual\nframes of the chunk are fed into the DNN to perform a multi-label\nclassification for the expected tags, considering that only chunk (or\nutterance) level rather than frame-level labels are available. Dropout and\nbackground noise aware training are also adopted to improve the generalization\ncapability of the DNNs. For the unsupervised feature learning, we propose to\nuse a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to\ngenerate new data-driven features from the Mel-Filter Banks (MFBs) features.\nThe new features, which are smoothed against background noise and more compact\nwith contextual information, can further improve the performance of the DNN\nbaseline. Compared with the standard Gaussian Mixture Model (GMM) baseline of\nthe DCASE 2016 audio tagging challenge, our proposed method obtains a\nsignificant equal error rate (EER) reduction from 0.21 to 0.13 on the\ndevelopment set. The proposed aDAE system can get a relative 6.7% EER reduction\ncompared with the strong DNN baseline on the development set. Finally, the\nresults also show that our approach obtains the state-of-the-art performance\nwith 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while\nEER of the first prize of this challenge is 0.17.\n",
    "topics": "{'Multi-Label Classification': 1.0}",
    "score": 0.8684764915
  },
  {
    "id": "2004.01184",
    "title": "Detection of Coronavirus (COVID-19) Associated Pneumonia based on\n  Generative Adversarial Networks and a Fine-Tuned Deep Transfer Learning Model\n  using Chest X-ray Dataset",
    "abstract": "  The COVID-19 coronavirus is one of the devastating viruses according to the\nworld health organization. This novel virus leads to pneumonia, which is an\ninfection that inflames the lungs' air sacs of a human. One of the methods to\ndetect those inflames is by using x-rays for the chest. In this paper, a\npneumonia chest x-ray detection based on generative adversarial networks (GAN)\nwith a fine-tuned deep transfer learning for a limited dataset will be\npresented. The use of GAN positively affects the proposed model robustness and\nmade it immune to the overfitting problem and helps in generating more images\nfrom the dataset. The dataset used in this research consists of 5863 X-ray\nimages with two categories: Normal and Pneumonia. This research uses only 10%\nof the dataset for training data and generates 90% of images using GAN to prove\nthe efficiency of the proposed model. Through the paper, AlexNet, GoogLeNet,\nSqueeznet, and Resnet18 are selected as deep transfer learning models to detect\nthe pneumonia from chest x-rays. Those models are selected based on their small\nnumber of layers on their architectures, which will reflect in reducing the\ncomplexity of the models and the consumed memory and time. Using a combination\nof GAN and deep transfer models proved it is efficiency according to testing\naccuracy measurement. The research concludes that the Resnet18 is the most\nappropriate deep transfer model according to testing accuracy measurement and\nachieved 99% with the other performance metrics such as precision, recall, and\nF1 score while using GAN as an image augmenter. Finally, a comparison result\nwas carried out at the end of the research with related work which used the\nsame dataset except that this research used only 10% of original dataset. The\npresented work achieved a superior result than the related work in terms of\ntesting accuracy.\n",
    "topics": "{'Transfer Learning': 0.99999547}",
    "score": 0.8680623683
  },
  {
    "id": "1803.03852",
    "title": "A Deep Learning Approach for Pose Estimation from Volumetric OCT Data",
    "abstract": "  Tracking the pose of instruments is a central problem in image-guided\nsurgery. For microscopic scenarios, optical coherence tomography (OCT) is\nincreasingly used as an imaging modality. OCT is suitable for accurate pose\nestimation due to its micrometer range resolution and volumetric field of view.\nHowever, OCT image processing is challenging due to speckle noise and\nreflection artifacts in addition to the images' 3D nature. We address pose\nestimation from OCT volume data with a new deep learning-based tracking\nframework. For this purpose, we design a new 3D convolutional neural network\n(CNN) architecture to directly predict the 6D pose of a small marker geometry\nfrom OCT volumes. We use a hexapod robot to automatically acquire labeled data\npoints which we use to train 3D CNN architectures for multi-output regression.\nWe use this setup to provide an in-depth analysis on deep learning-based pose\nestimation from volumes. Specifically, we demonstrate that exploiting volume\ninformation for pose estimation yields higher accuracy than relying on 2D\nrepresentations with depth information. Supporting this observation, we provide\nquantitative and qualitative results that 3D CNNs effectively exploit the depth\nstructure of marker objects. Regarding the deep learning aspect, we present\nefficient design principles for 3D CNNs, making use of insights from the 2D\ndeep learning community. In particular, we present Inception3D as a new\narchitecture which performs best for our application. We show that our deep\nlearning approach reaches errors at our ground-truth label's resolution. We\nachieve a mean average error of $\\SI{14.89 \\pm 9.3}{\\micro\\metre}$ and\n$\\SI{0.096 \\pm 0.072}{\\degree}$ for position and orientation learning,\nrespectively.\n",
    "topics": "{'Pose Estimation': 1.0}",
    "score": 0.8678028504
  },
  {
    "id": "1705.09850",
    "title": "Abnormality Detection and Localization in Chest X-Rays using Deep\n  Convolutional Neural Networks",
    "abstract": "  Chest X-Rays (CXRs) are widely used for diagnosing abnormalities in the heart\nand lung area. Automatically detecting these abnormalities with high accuracy\ncould greatly enhance real world diagnosis processes. Lack of standard publicly\navailable dataset and benchmark studies, however, makes it difficult to compare\nvarious detection methods. In order to overcome these difficulties, we have\nused a publicly available Indiana CXR, JSRT and Shenzhen dataset and studied\nthe performance of known deep convolutional network (DCN) architectures on\ndifferent abnormalities. We find that the same DCN architecture doesn't perform\nwell across all abnormalities. Shallow features or earlier layers consistently\nprovide higher detection accuracy compared to deep features. We have also found\nensemble models to improve classification significantly compared to single\nmodel. Combining these insight, we report the highest accuracy on chest X-Ray\nabnormality detection on these datasets. We find that for cardiomegaly\ndetection, the deep learning method improves the accuracy by a staggering 17\npercentage point compared to rule based methods. We applied the techniques to\nthe problem of tuberculosis detection on a different dataset and achieved the\nhighest accuracy. Our localization experiments using these trained classifiers\nshow that for spatially spread out abnormalities like cardiomegaly and\npulmonary edema, the network can localize the abnormalities successfully most\nof the time. One remarkable result of the cardiomegaly localization is that the\nheart and its surrounding region is most responsible for cardiomegaly\ndetection, in contrast to the rule based models where the ratio of heart and\nlung area is used as the measure. We believe that through deep learning based\nclassification and localization, we will discover many more interesting\nfeatures in medical image diagnosis that are not considered traditionally.\n",
    "topics": "{'Anomaly Detection': 0.99978596}",
    "score": 0.8673164341
  },
  {
    "id": "1912.02254",
    "title": "Deep Model Compression via Deep Reinforcement Learning",
    "abstract": "  Besides accuracy, the storage of convolutional neural networks (CNN) models\nis another important factor considering limited hardware resources in practical\napplications. For example, autonomous driving requires the design of accurate\nyet fast CNN for low latency in object detection and classification. To fulfill\nthe need, we aim at obtaining CNN models with both high testing accuracy and\nsmall size/storage to address resource constraints in many embedded systems. In\nparticular, this paper focuses on proposing a generic reinforcement learning\nbased model compression approach in a two-stage compression pipeline: pruning\nand quantization. The first stage of compression, i.e., pruning, is achieved\nvia exploiting deep reinforcement learning (DRL) to co-learn the accuracy of\nCNN models updated after layer-wise channel pruning on a testing dataset and\nthe FLOPs, number of floating point operations in each layer, updated after\nkernel-wise variational pruning using information dropout. Layer-wise channel\npruning is to remove unimportant kernels from the input channel dimension while\nkernel-wise variational pruning is to remove unimportant kernels from the\n2D-kernel dimensions, namely, height and width. The second stage, i.e.,\nquantization, is achieved via a similar DRL approach but focuses on obtaining\nthe optimal weight bits for individual layers. We further conduct experimental\nresults on CIFAR-10 and ImageNet datasets. For the CIFAR-10 dataset, the\nproposed method can reduce the size of VGGNet by 9x from 20.04MB to 2.2MB with\n0.2% accuracy increase. For the ImageNet dataset, the proposed method can\nreduce the size of VGG-16 by 33x from 138MB to 4.14MB with no accuracy loss.\n",
    "topics": "{'Model Compression': 1.0, 'Autonomous Driving': 0.99597, 'Object Detection': 0.9669932, 'Quantization': 0.95144486}",
    "score": 0.8673122723
  },
  {
    "id": "1905.03218",
    "title": "MetaPred: Meta-Learning for Clinical Risk Prediction with Limited\n  Patient Electronic Health Records",
    "abstract": "  In recent years, increasingly augmentation of health data, such as patient\nElectronic Health Records (EHR), are becoming readily available. This provides\nan unprecedented opportunity for knowledge discovery and data mining algorithms\nto dig insights from them, which can, later on, be helpful to the improvement\nof the quality of care delivery. Predictive modeling of clinical risk,\nincluding in-hospital mortality, hospital readmission, chronic disease onset,\ncondition exacerbation, etc., from patient EHR, is one of the health data\nanalytic problems that attract most of the interests. The reason is not only\nbecause the problem is important in clinical settings, but also there are\nchallenges working with EHR such as sparsity, irregularity, temporality, etc.\nDifferent from applications in other domains such as computer vision and\nnatural language processing, the labeled data samples in medicine (patients)\nare relatively limited, which creates lots of troubles for effective predictive\nmodel learning, especially for complicated models such as deep learning. In\nthis paper, we propose MetaPred, a meta-learning for clinical risk prediction\nfrom longitudinal patient EHRs. In particular, in order to predict the target\nrisk where there are limited data samples, we train a meta-learner from a set\nof related risk prediction tasks which learns how a good predictor is learned.\nThe meta-learned can then be directly used in target risk prediction, and the\nlimited available samples can be used for further fine-tuning the model\nperformance. The effectiveness of MetaPred is tested on a real patient EHR\nrepository from Oregon Health & Science University. We are able to demonstrate\nthat with CNN and RNN as base predictors, MetaPred can achieve much better\nperformance for predicting target risk with low resources comparing with the\npredictor trained on the limited samples available for this risk.\n",
    "topics": "{'Meta-Learning': 0.9994362}",
    "score": 0.8672716788
  },
  {
    "id": "1912.05270",
    "title": "MineGAN: effective knowledge transfer from GANs to target domains with\n  few images",
    "abstract": "  One of the attractive characteristics of deep neural networks is their\nability to transfer knowledge obtained in one domain to other related domains.\nAs a result, high-quality networks can be trained in domains with relatively\nlittle training data. This property has been extensively studied for\ndiscriminative networks but has received significantly less attention for\ngenerative models. Given the often enormous effort required to train GANs, both\ncomputationally as well as in the dataset collection, the re-use of pretrained\nGANs is a desirable objective. We propose a novel knowledge transfer method for\ngenerative models based on mining the knowledge that is most beneficial to a\nspecific target domain, either from a single or multiple pretrained GANs. This\nis done using a miner network that identifies which part of the generative\ndistribution of each pretrained GAN outputs samples closest to the target\ndomain. Mining effectively steers GAN sampling towards suitable regions of the\nlatent space, which facilitates the posterior finetuning and avoids pathologies\nof other methods such as mode collapse and lack of flexibility. We perform\nexperiments on several complex datasets using various GAN architectures\n(BigGAN, Progressive GAN) and show that the proposed method, called MineGAN,\neffectively transfers knowledge to domains with few target images,\noutperforming existing methods. In addition, MineGAN can successfully transfer\nknowledge from multiple pretrained GANs. Our code is available at:\nhttps://github.com/yaxingwang/MineGAN.\n",
    "topics": "{'Transfer Learning': 0.99943393}",
    "score": 0.8670998937
  },
  {
    "id": "1811.11127",
    "title": "Unprocessing Images for Learned Raw Denoising",
    "abstract": "  Machine learning techniques work best when the data used for training\nresembles the data used for evaluation. This holds true for learned\nsingle-image denoising algorithms, which are applied to real raw camera sensor\nreadings but, due to practical constraints, are often trained on synthetic\nimage data. Though it is understood that generalizing from synthetic to real\ndata requires careful consideration of the noise properties of image sensors,\nthe other aspects of a camera's image processing pipeline (gain, color\ncorrection, tone mapping, etc) are often overlooked, despite their significant\neffect on how raw measurements are transformed into finished images. To address\nthis, we present a technique to \"unprocess\" images by inverting each step of an\nimage processing pipeline, thereby allowing us to synthesize realistic raw\nsensor measurements from commonly available internet photos. We additionally\nmodel the relevant components of an image processing pipeline when evaluating\nour loss function, which allows training to be aware of all relevant\nphotometric processing that will occur after denoising. By processing and\nunprocessing model outputs and training data in this way, we are able to train\na simple convolutional neural network that has 14%-38% lower error rates and is\n9x-18x faster than the previous state of the art on the Darmstadt Noise\nDataset, and generalizes to sensors outside of that dataset as well.\n",
    "topics": "{'Denoising': 0.9999999, 'Image Denoising': 0.5819815}",
    "score": 0.8669990261
  },
  {
    "id": "1808.04754",
    "title": "Treepedia 2.0: Applying Deep Learning for Large-scale Quantification of\n  Urban Tree Cover",
    "abstract": "  Recent advances in deep learning have made it possible to quantify urban\nmetrics at fine resolution, and over large extents using street-level images.\nHere, we focus on measuring urban tree cover using Google Street View (GSV)\nimages. First, we provide a small-scale labelled validation dataset and propose\nstandard metrics to compare the performance of automated estimations of street\ntree cover using GSV. We apply state-of-the-art deep learning models, and\ncompare their performance to a previously established benchmark of an\nunsupervised method. Our training procedure for deep learning models is novel;\nwe utilize the abundance of openly available and similarly labelled\nstreet-level image datasets to pre-train our model. We then perform additional\ntraining on a small training dataset consisting of GSV images. We find that\ndeep learning models significantly outperform the unsupervised benchmark\nmethod. Our semantic segmentation model increased mean intersection-over-union\n(IoU) from 44.10% to 60.42% relative to the unsupervised method and our\nend-to-end model decreased Mean Absolute Error from 10.04% to 4.67%. We also\nemploy a recently developed method called gradient-weighted class activation\nmap (Grad-CAM) to interpret the features learned by the end-to-end model. This\ntechnique confirms that the end-to-end model has accurately learned to identify\ntree cover area as key features for predicting percentage tree cover. Our paper\nprovides an example of applying advanced deep learning techniques on a\nlarge-scale, geo-tagged and image-based dataset to efficiently estimate\nimportant urban metrics. The results demonstrate that deep learning models are\nhighly accurate, can be interpretable, and can also be efficient in terms of\ndata-labelling effort and computational resources.\n",
    "topics": "{}",
    "score": 0.8669054727
  },
  {
    "id": "1905.03556",
    "title": "Cycle-IR: Deep Cyclic Image Retargeting",
    "abstract": "  Supervised deep learning techniques have achieved great success in various\nfields due to getting rid of the limitation of handcrafted representations.\nHowever, most previous image retargeting algorithms still employ fixed design\nprinciples such as using gradient map or handcrafted features to compute\nsaliency map, which inevitably restricts its generality. Deep learning\ntechniques may help to address this issue, but the challenging problem is that\nwe need to build a large-scale image retargeting dataset for the training of\ndeep retargeting models. However, building such a dataset requires huge human\nefforts.\n  In this paper, we propose a novel deep cyclic image retargeting approach,\ncalled Cycle-IR, to firstly implement image retargeting with a single deep\nmodel, without relying on any explicit user annotations. Our idea is built on\nthe reverse mapping from the retargeted images to the given input images. If\nthe retargeted image has serious distortion or excessive loss of important\nvisual information, the reverse mapping is unlikely to restore the input image\nwell. We constrain this forward-reverse consistency by introducing a cyclic\nperception coherence loss. In addition, we propose a simple yet effective image\nretargeting network (IRNet) to implement the image retargeting process. Our\nIRNet contains a spatial and channel attention layer, which is able to\ndiscriminate visually important regions of input images effectively, especially\nin cluttered images. Given arbitrary sizes of input images and desired aspect\nratios, our Cycle-IR can produce visually pleasing target images directly.\nExtensive experiments on the standard RetargetMe dataset show the superiority\nof our Cycle-IR. In addition, our Cycle-IR outperforms the Multiop method and\nobtains the best result in the user study. Code is available at\nhttps://github.com/mintanwei/Cycle-IR.\n",
    "topics": "{}",
    "score": 0.866517563
  },
  {
    "id": "1907.12902",
    "title": "Data augmentation with Symbolic-to-Real Image Translation GANs for\n  Traffic Sign Recognition",
    "abstract": "  Traffic sign recognition is an important component of many advanced driving\nassistance systems, and it is required for full autonomous driving.\nComputational performance is usually the bottleneck in using large scale neural\nnetworks for this purpose. SqueezeNet is a good candidate for efficient image\nclassification of traffic signs, but in our experiments it does not reach high\naccuracy, and we believe this is due to lack of data, requiring data\naugmentation. Generative adversarial networks can learn the high dimensional\ndistribution of empirical data, allowing the generation of new data points. In\nthis paper we apply pix2pix GANs architecture to generate new traffic sign\nimages and evaluate the use of these images in data augmentation. We were\nmotivated to use pix2pix to translate symbolic sign images to real ones due to\nthe mode collapse in Conditional GANs. Through our experiments we found that\ndata augmentation using GAN can increase classification accuracy for circular\ntraffic signs from 92.1% to 94.0%, and for triangular traffic signs from 93.8%\nto 95.3%, producing an overall improvement of 2%. However some traditional\naugmentation techniques can outperform GAN data augmentation, for example\ncontrast variation in circular traffic signs (95.5%) and displacement on\ntriangular traffic signs (96.7 %). Our negative results shows that while GANs\ncan be naively used for data augmentation, they are not always the best choice,\ndepending on the problem and variability in the data.\n",
    "topics": "{'Data Augmentation': 1.0, 'Autonomous Driving': 0.98139393, 'Image Classification': 0.9207577}",
    "score": 0.8664422725
  },
  {
    "id": "2006.04603",
    "title": "End-to-end learning for semiquantitative rating of COVID-19 severity on\n  Chest X-rays",
    "abstract": "  In this work we design an end-to-end deep learning architecture for\npredicting, on Chest X-rays images (CXR), a multi-regional score conveying the\ndegree of lung compromise in COVID-19 patients. Such semi-quantitative scoring\nsystem, namely \\texttt{Brixia~score}, is applied in serial monitoring of such\npatients, showing significant prognostic value, in one of the hospitals that\nexperienced one of the highest pandemic peaks in Italy. To solve such a\nchallenging visual task, we adopt a weakly supervised learning strategy\nstructured to handle different tasks (segmentation, spatial alignment, and\nscore estimation) trained with a \"from part to whole\" procedure involving\ndifferent datasets. In particular, we exploit a clinical dataset of almost\n5,000 CXR annotated images collected in the same hospital. Our BS-Net\ndemonstrates self-attentive behavior and a high degree of accuracy in all\nprocessing stages. Through inter-rater agreement tests and a gold standard\ncomparison, we show that our solution outperforms single human annotators in\nrating accuracy and consistency, thus supporting the possibility of using this\ntool in contexts of computer-assisted monitoring. Highly resolved (super-pixel\nlevel) explainability maps are also generated, with an original technique, to\nvisually help the understanding of the network activity on the lung areas. We\nalso consider other scores proposed in literature and provide a comparison with\na recently proposed non-specific approach. We eventually test the performance\nrobustness of our model on a variegated public COVID-19 dataset, for which we\nalso provide \\texttt{Brixia~score} annotations, observing good direct\ngeneralization and fine-tuning capabilities that highlight the portability of\nBS-Net in other clinical settings. The CXR dataset along with the source code\nand the trained model are publicly released for research purposes.\n",
    "topics": "{}",
    "score": 0.8662728328
  },
  {
    "id": "1904.06585",
    "title": "Recovery of Superquadrics from Range Images using Deep Learning: A\n  Preliminary Study",
    "abstract": "  It has been a longstanding goal in computer vision to describe the 3D\nphysical space in terms of parameterized volumetric models that would allow\nautonomous machines to understand and interact with their surroundings. Such\nmodels are typically motivated by human visual perception and aim to represents\nall elements of the physical word ranging from individual objects to complex\nscenes using a small set of parameters. One of the de facto stadards to\napproach this problem are superquadrics - volumetric models that define various\n3D shape primitives and can be fitted to actual 3D data (either in the form of\npoint clouds or range images). However, existing solutions to superquadric\nrecovery involve costly iterative fitting procedures, which limit the\napplicability of such techniques in practice. To alleviate this problem, we\nexplore in this paper the possibility to recover superquadrics from range\nimages without time consuming iterative parameter estimation techniques by\nusing contemporary deep-learning models, more specifically, convolutional\nneural networks (CNNs). We pose the superquadric recovery problem as a\nregression task and develop a CNN regressor that is able to estimate the\nparameters of a superquadric model from a given range image. We train the\nregressor on a large set of synthetic range images, each containing a single\n(unrotated) superquadric shape and evaluate the learned model in comparaitve\nexperiments with the current state-of-the-art. Additionally, we also present a\nqualitative analysis involving a dataset of real-world objects. The results of\nour experiments show that the proposed regressor not only outperforms the\nexisting state-of-the-art, but also ensures a 270x faster execution time.\n",
    "topics": "{}",
    "score": 0.8660840926
  },
  {
    "id": "2005.11003",
    "title": "SODA: Detecting Covid-19 in Chest X-rays with Semi-supervised Open Set\n  Domain Adaptation",
    "abstract": "  Due to the shortage of COVID-19 viral testing kits and the long waiting time,\nradiology imaging is used to complement the screening process and triage\npatients into different risk levels. Deep learning based methods have taken an\nactive role in automatically detecting COVID-19 disease in chest x-ray images,\nas witnessed in many recent works in early 2020. Most of these works first\ntrain a Convolutional Neural Network (CNN) on an existing large-scale chest\nx-ray image dataset and then fine-tune it with a COVID-19 dataset at a much\nsmaller scale. However, direct transfer across datasets from different domains\nmay lead to poor performance for CNN due to two issues, the large domain shift\npresent in the biomedical imaging datasets and the extremely small scale of the\nCOVID-19 chest x-ray dataset. In an attempt to address these two important\nissues, we formulate the problem of COVID-19 chest x-ray image classification\nin a semi-supervised open set domain adaptation setting and propose a novel\ndomain adaptation method, Semi-supervised Open set Domain Adversarial network\n(SODA). SODA is able to align the data distributions across different domains\nin a general domain space and also in a common subspace of source and target\ndata. In our experiments, SODA achieves a leading classification performance\ncompared with recent state-of-the-art models in separating COVID-19 with common\npneumonia. We also present initial results showing that SODA can produce better\npathology localizations in the chest x-rays.\n",
    "topics": "{'Domain Adaptation': 0.99950814, 'COVID-19 Diagnosis': 0.8078945}",
    "score": 0.8660507817
  },
  {
    "id": "1902.08897",
    "title": "TBNet:Pulmonary Tuberculosis Diagnosing System using Deep Neural\n  Networks",
    "abstract": "  Tuberculosis is a deadly infectious disease prevalent around the world. Due\nto the lack of proper technology in place, the early detection of this disease\nis unattainable. Also, the available methods to detect Tuberculosis is not\nup-to a commendable standards due to their dependency on unnecessary features,\nthis make such technology obsolete for a reliable health-care technology. In\nthis paper, I propose a deep-learning based system which diagnoses tuberculosis\nbased on the important features in Chest X-rays along with original chest\nX-rays. Employing our system will accelerate the process of tuberculosis\ndiagnosis by overcoming the need to perform the time-consuming sputum-based\ntesting method (Diagnostic Microbiology). In contrast to the previous methods\n\\cite{kant2018towards, melendez2016automated}, our work utilizes the\nstate-of-the-art ResNet \\cite{he2016deep} with proper data augmentation using\ntraditional robust features like Haar \\cite{viola2005detecting,viola2001rapid}\nand LBP \\cite{ojala1994performance,ojala1996comparative}. I observed that such\na procedure enhances the rate of tuberculosis detection to a highly\nsatisfactory level. Our work uses the publicly available pulmonary chest X-ray\ndataset to train our network \\cite{jaeger2014two}. Nevertheless, the publicly\navailable dataset is very small and is inadequate to achieve the best accuracy.\nTo overcome this issue I have devised an intuitive feature based data\naugmentation pipeline. Our approach shall help the deep neural network\n\\cite{lecun2015deep,he2016deep,krizhevsky2012imagenet} to focus its training on\ntuberculosis affected regions making it more robust and accurate, when compared\nto other conventional methods that use procedures like mirroring and rotation.\nBy using our simple yet powerful techniques, I observed a 10\\% boost in\nperformance accuracy.\n",
    "topics": "{'Data Augmentation': 0.99999976}",
    "score": 0.8658722819
  },
  {
    "id": "1903.11114",
    "title": "SuSi: Supervised Self-Organizing Maps for Regression and Classification\n  in Python",
    "abstract": "  In many research fields, the sizes of the existing datasets vary widely.\nHence, there is a need for machine learning techniques which are well-suited\nfor these different datasets. One possible technique is the self-organizing map\n(SOM), a type of artificial neural network which is, so far, weakly represented\nin the field of machine learning. The SOM's unique characteristic is the\nneighborhood relationship of the output neurons. This relationship improves the\nability of generalization on small datasets. SOMs are mostly applied in\nunsupervised learning and few studies focus on using SOMs as supervised\nlearning approach. Furthermore, no appropriate SOM package is available with\nrespect to machine learning standards and in the widely used programming\nlanguage Python. In this paper, we introduce the freely available Supervised\nSelf-organizing maps (SuSi) Python package which performs supervised regression\nand classification. The implementation of SuSi is described with respect to the\nunderlying mathematics. Then, we present first evaluations of the SOM for\nregression and classification datasets from two different domains of geospatial\nimage analysis. Despite the early stage of its development, the SuSi framework\nperforms well and is characterized by only small performance differences\nbetween the training and the test datasets. A comparison of the SuSi framework\nwith existing Python and R packages demonstrates the importance of the SuSi\nframework. In future work, the SuSi framework will be extended, optimized and\nupgraded e.g. with tools to better understand and visualize the input data as\nwell as the handling of missing and incomplete data.\n",
    "topics": "{}",
    "score": 0.8656715257
  },
  {
    "id": "2008.00362",
    "title": "Animating Through Warping: an Efficient Method for High-Quality Facial\n  Expression Animation",
    "abstract": "  Advances in deep neural networks have considerably improved the art of\nanimating a still image without operating in 3D domain. Whereas, prior arts can\nonly animate small images (typically no larger than 512x512) due to memory\nlimitations, difficulty of training and lack of high-resolution (HD) training\ndatasets, which significantly reduce their potential for applications in movie\nproduction and interactive systems. Motivated by the idea that HD images can be\ngenerated by adding high-frequency residuals to low-resolution results produced\nby a neural network, we propose a novel framework known as Animating Through\nWarping (ATW) to enable efficient animation of HD images.\n  Specifically, the proposed framework consists of two modules, a novel\ntwo-stage neural-network generator and a novel post-processing module known as\nAnimating Through Warping (ATW). It only requires the generator to be trained\non small images and can do inference on an image of any size. During inference,\nan HD input image is decomposed into a low-resolution component(128x128) and\nits corresponding high-frequency residuals. The generator predicts the\nlow-resolution result as well as the motion field that warps the input face to\nthe desired status (e.g., expressions categories or action units). Finally, the\nResWarp module warps the residuals based on the motion field and adding the\nwarped residuals to generates the final HD results from the naively up-sampled\nlow-resolution results. Experiments show the effectiveness and efficiency of\nour method in generating high-resolution animations. Our proposed framework\nsuccessfully animates a 4K facial image, which has never been achieved by prior\nneural models. In addition, our method generally guarantee the temporal\ncoherency of the generated animations. Source codes will be made publicly\navailable.\n",
    "topics": "{}",
    "score": 0.8655993069
  },
  {
    "id": "2003.02438",
    "title": "Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging",
    "abstract": "  Light Field (LF) offers unique advantages such as post-capture refocusing and\ndepth estimation, but low-light conditions limit these capabilities. To restore\nlow-light LFs we should harness the geometric cues present in different LF\nviews, which is not possible using single-frame low-light enhancement\ntechniques. We, therefore, propose a deep neural network for Low-Light Light\nField (L3F) restoration, which we refer to as L3Fnet. The proposed L3Fnet not\nonly performs the necessary visual enhancement of each LF view but also\npreserves the epipolar geometry across views. We achieve this by adopting a\ntwo-stage architecture for L3Fnet. Stage-I looks at all the LF views to encode\nthe LF geometry. This encoded information is then used in Stage-II to\nreconstruct each LF view. To facilitate learning-based techniques for low-light\nLF imaging, we collected a comprehensive LF dataset of various scenes. For each\nscene, we captured four LFs, one with near-optimal exposure and ISO settings\nand the others at different levels of low-light conditions varying from low to\nextreme low-light settings. The effectiveness of the proposed L3Fnet is\nsupported by both visual and numerical comparisons on this dataset. To further\nanalyze the performance of low-light reconstruction methods, we also propose an\nL3F-wild dataset that contains LF captured late at night with almost zero lux\nvalues. No ground truth is available in this dataset. To perform well on the\nL3F-wild dataset, any method must adapt to the light level of the captured\nscene. To do this we propose a novel pre-processing block that makes L3Fnet\nrobust to various degrees of low-light conditions. Lastly, we show that L3Fnet\ncan also be used for low-light enhancement of single-frame images, despite it\nbeing engineered for LF data. We do so by converting the single-frame DSLR\nimage into a form suitable to L3Fnet, which we call as pseudo-LF.\n",
    "topics": "{'Depth Estimation': 0.8559304}",
    "score": 0.8653068032
  },
  {
    "id": "1902.07880",
    "title": "Evaluation of Algorithms for Multi-Modality Whole Heart Segmentation: An\n  Open-Access Grand Challenge",
    "abstract": "  Knowledge of whole heart anatomy is a prerequisite for many clinical\napplications. Whole heart segmentation (WHS), which delineates substructures of\nthe heart, can be very valuable for modeling and analysis of the anatomy and\nfunctions of the heart. However, automating this segmentation can be arduous\ndue to the large variation of the heart shape, and different image qualities of\nthe clinical data. To achieve this goal, a set of training data is generally\nneeded for constructing priors or for training. In addition, it is difficult to\nperform comparisons between different methods, largely due to differences in\nthe datasets and evaluation metrics used. This manuscript presents the\nmethodologies and evaluation results for the WHS algorithms selected from the\nsubmissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge,\nin conjunction with MICCAI 2017. The challenge provides 120 three-dimensional\ncardiac images covering the whole heart, including 60 CT and 60 MRI volumes,\nall acquired in clinical environments with manual delineation. Ten algorithms\nfor CT data and eleven algorithms for MRI data, submitted from twelve groups,\nhave been evaluated. The results show that many of the deep learning (DL) based\nmethods achieved high accuracy, even though the number of training datasets was\nlimited. A number of them also reported poor results in the blinded evaluation,\nprobably due to overfitting in their training. The conventional algorithms,\nmainly based on multi-atlas segmentation, demonstrated robust and stable\nperformance, even though the accuracy is not as good as the best DL method in\nCT segmentation. The challenge, including the provision of the annotated\ntraining data and the blinded evaluation for submitted algorithms on the test\ndata, continues as an ongoing benchmarking resource via its homepage\n(\\url{www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/}).\n",
    "topics": "{}",
    "score": 0.8651940519
  },
  {
    "id": "1901.00751",
    "title": "Low-Cost Device Prototype for Automatic Medical Diagnosis Using Deep\n  Learning Methods",
    "abstract": "  This paper introduces a novel low-cost device prototype for the automatic\ndiagnosis of diseases, utilizing inputted symptoms and personal background. The\nengineering goal is to solve the problem of limited healthcare access with a\nsingle device. Diagnosing diseases automatically is an immense challenge, owing\nto their variable properties and symptoms. On the other hand, Neural Networks\nhave developed into a powerful tool in the field of machine learning, one that\nis showing to be extremely promising at computing diagnosis even with\ninconsistent variables.\n  In this research, a cheap device was created to allow for straightforward\ndiagnosis and treatment of human diseases. By utilizing Deep Neural Networks\n(DNNs) and Convolutional Neural Networks (CNNs), outfitted on a Raspberry Pi\nZero processor ($5), the device is able to detect up to 1537 different diseases\nand conditions and utilize a CNN for on-device visual diagnostics. The user can\ninput the symptoms using the buttons on the device and can take pictures using\nthe same mechanism. The algorithm processes inputted symptoms, providing\ndiagnosis and possible treatment options for common conditions. The purpose of\nthis work was to be able to diagnose diseases through an affordable processor\nwith high accuracy, as it is currently achieving an accuracy of 90% for Top-5\nsymptom-based diagnoses, and 91% for visual skin diseases. The NNs achieve\nperformance far above any other tested system, and its efficiency and ease of\nuse will prove it to be a helpful tool for people around the world. This device\ncould potentially provide low-cost universal access to vital diagnostics and\ntreatment options.\n",
    "topics": "{'Medical Diagnosis': 0.9999995}",
    "score": 0.8651583015
  },
  {
    "id": "2009.05796",
    "title": "Revisiting the Threat Space for Vision-based Keystroke Inference Attacks",
    "abstract": "  A vision-based keystroke inference attack is a side-channel attack in which\nan attacker uses an optical device to record users on their mobile devices and\ninfer their keystrokes. The threat space for these attacks has been studied in\nthe past, but we argue that the defining characteristics for this threat space,\nnamely the strength of the attacker, are outdated. Previous works do not study\nadversaries with vision systems that have been trained with deep neural\nnetworks because these models require large amounts of training data and\ncurating such a dataset is expensive. To address this, we create a large-scale\nsynthetic dataset to simulate the attack scenario for a keystroke inference\nattack. We show that first pre-training on synthetic data, followed by adopting\ntransfer learning techniques on real-life data, increases the performance of\nour deep learning models. This indicates that these models are able to learn\nrich, meaningful representations from our synthetic data and that training on\nthe synthetic data can help overcome the issue of having small, real-life\ndatasets for vision-based key stroke inference attacks. For this work, we focus\non single keypress classification where the input is a frame of a keypress and\nthe output is a predicted key. We are able to get an accuracy of 95.6% after\npre-training a CNN on our synthetic data and training on a small set of\nreal-life data in an adversarial domain adaptation framework. Source Code for\nSimulator:\nhttps://github.com/jlim13/keystroke-inference-attack-synthetic-dataset-generator-\n",
    "topics": "{'Domain Adaptation': 0.9912747, 'Transfer Learning': 0.8208755}",
    "score": 0.8647334277
  },
  {
    "id": "2007.08223",
    "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19\n  and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
    "abstract": "  Clinicians in the frontline need to assess quickly whether a patient with\nsymptoms indeed has COVID-19 or not. The difficulty of this task is exacerbated\nin low resource settings that may not have access to biotechnology tests.\nFurthermore, Tuberculosis (TB) remains a major health problem in several low-\nand middle-income countries and its common symptoms include fever, cough and\ntiredness, similarly to COVID-19. In order to help in the detection of\nCOVID-19, we propose the extraction of deep features (DF) from chest X-ray\nimages, a technology available in most hospitals, and their subsequent\nclassification using machine learning methods that do not require large\ncomputational resources. We compiled a five-class dataset of X-ray chest images\nincluding a balanced number of COVID-19, viral pneumonia, bacterial pneumonia,\nTB, and healthy cases. We compared the performance of pipelines combining 14\nindividual state-of-the-art pre-trained deep networks for DF extraction with\ntraditional machine learning classifiers. A pipeline consisting of ResNet-50\nfor DF computation and ensemble of subspace discriminant classifier was the\nbest performer in the classification of the five classes, achieving a detection\naccuracy of 91.6+ 2.6% (accuracy + 95% Confidence Interval). Furthermore, the\nsame pipeline achieved accuracies of 98.6+1.4% and 99.9+0.5% in simpler\nthree-class and two-class classification problems focused on distinguishing\nCOVID-19, TB and healthy cases; and COVID-19 and healthy images, respectively.\nThe pipeline was computationally efficient requiring just 0.19 second to\nextract DF per X-ray image and 2 minutes for training a traditional classifier\nwith more than 2000 images on a CPU machine. The results suggest the potential\nbenefits of using our pipeline in the detection of COVID-19, particularly in\nresource-limited settings and it can run with limited computational resources.\n",
    "topics": "{'COVID-19 Diagnosis': 0.9540308}",
    "score": 0.8645827389
  },
  {
    "id": "1804.09458",
    "title": "Dynamic Few-Shot Visual Learning without Forgetting",
    "abstract": "  The human visual system has the remarkably ability to be able to effortlessly\nlearn novel concepts from only a few examples. Mimicking the same behavior on\nmachine learning vision systems is an interesting and very challenging research\nproblem with many practical advantages on real world vision applications. In\nthis context, the goal of our work is to devise a few-shot visual learning\nsystem that during test time it will be able to efficiently learn novel\ncategories from only a few training data while at the same time it will not\nforget the initial categories on which it was trained (here called base\ncategories). To achieve that goal we propose (a) to extend an object\nrecognition system with an attention based few-shot classification weight\ngenerator, and (b) to redesign the classifier of a ConvNet model as the cosine\nsimilarity function between feature representations and classification weight\nvectors. The latter, apart from unifying the recognition of both novel and base\ncategories, it also leads to feature representations that generalize better on\n\"unseen\" categories. We extensively evaluate our approach on Mini-ImageNet\nwhere we manage to improve the prior state-of-the-art on few-shot recognition\n(i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings\nrespectively) while at the same time we do not sacrifice any accuracy on the\nbase categories, which is a characteristic that most prior approaches lack.\nFinally, we apply our approach on the recently introduced few-shot benchmark of\nBharath and Girshick [4] where we also achieve state-of-the-art results. The\ncode and models of our paper will be published on:\nhttps://github.com/gidariss/FewShotWithoutForgetting\n",
    "topics": "{'Few-Shot Image Classification': 0.9998098, 'One-Shot Learning': 0.9980045, 'Object Recognition': 0.99219745, 'Few-Shot Learning': 0.9905794}",
    "score": 0.864509734
  },
  {
    "id": "1704.04251",
    "title": "Visual Recognition of Paper Analytical Device Images for Detection of\n  Falsified Pharmaceuticals",
    "abstract": "  Falsification of medicines is a big problem in many developing countries,\nwhere technological infrastructure is inadequate to detect these harmful\nproducts. We have developed a set of inexpensive paper cards, called Paper\nAnalytical Devices (PADs), which can efficiently classify drugs based on their\nchemical composition, as a potential solution to the problem. These cards have\ndifferent reagents embedded in them which produce a set of distinctive color\ndescriptors upon reacting with the chemical compounds that constitute\npharmaceutical dosage forms. If a falsified version of the medicine lacks the\nactive ingredient or includes substitute fillers, the difference in color is\nperceivable by humans. However, reading the cards with accuracy takes training\nand practice, which may hamper their scaling and implementation in low resource\nsettings. To deal with this, we have developed an automatic visual recognition\nsystem to read the results from the PAD images. At first, the optimal set of\nreagents was found by running singular value decomposition on the intensity\nvalues of the color tones in the card images. A dataset of cards embedded with\nthese reagents is produced to generate the most distinctive results for a set\nof 26 different active pharmaceutical ingredients (APIs) and excipients. Then,\nwe train two popular convolutional neural network (CNN) models, with the card\nimages. We also extract some \"hand-crafted\" features from the images and train\na nearest neighbor classifier and a non-linear support vector machine with\nthem. On testing, higher-level features performed much better in accurately\nclassifying the PAD images, with the CNN models reaching the highest average\naccuracy of over 94\\%.\n",
    "topics": "{}",
    "score": 0.8642602652
  },
  {
    "id": "2009.06184",
    "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and\n  Visualization of Highly Sparse and Noisy Image Data",
    "abstract": "  The motivation of our work is to present a new visualization-guided computing\nparadigm to combine direct 3D volume processing and volume rendered clues for\neffective 3D exploration such as extracting and visualizing microstructures\nin-vivo. However, it is still challenging to extract and visualize high\nfidelity 3D vessel structure due to its high sparseness, noisiness, and complex\ntopology variations. In this paper, we present an end-to-end deep learning\nmethod, VC-Net, for robust extraction of 3D microvasculature through embedding\nthe image composition, generated by maximum intensity projection (MIP), into 3D\nvolume image learning to enhance the performance. The core novelty is to\nautomatically leverage the volume visualization technique (MIP) to enhance the\n3D data exploration at deep learning level. The MIP embedding features can\nenhance the local vessel signal and are adaptive to the geometric variability\nand scalability of vessels, which is crucial in microvascular tracking. A\nmulti-stream convolutional neural network is proposed to learn the 3D volume\nand 2D MIP features respectively and then explore their inter-dependencies in a\njoint volume-composition embedding space by unprojecting the MIP features into\n3D volume embedding space. The proposed framework can better capture small /\nmicro vessels and improve vessel connectivity. To our knowledge, this is the\nfirst deep learning framework to construct a joint convolutional embedding\nspace, where the computed vessel probabilities from volume rendering based 2D\nprojection and 3D volume can be explored and integrated synergistically.\nExperimental results are compared with the traditional 3D vessel segmentation\nmethods and the deep learning state-of-the-art on public and real patient\n(micro-)cerebrovascular image datasets. Our method demonstrates the potential\nin a powerful MR arteriogram and venogram diagnosis of vascular diseases.\n",
    "topics": "{'Retinal Vessel Segmentation': 0.98560876}",
    "score": 0.8638455868
  },
  {
    "id": "1905.02758",
    "title": "Generalization ability of region proposal networks for multispectral\n  person detection",
    "abstract": "  Multispectral person detection aims at automatically localizing humans in\nimages that consist of multiple spectral bands. Usually, the visual-optical\n(VIS) and the thermal infrared (IR) spectra are combined to achieve higher\nrobustness for person detection especially in insufficiently illuminated\nscenes. This paper focuses on analyzing existing detection approaches for their\ngeneralization ability. Generalization is a key feature for machine learning\nbased detection algorithms that are supposed to perform well across different\ndatasets. Inspired by recent literature regarding person detection in the VIS\nspectrum, we perform a cross-validation study to empirically determine the most\npromising dataset to train a well-generalizing detector. Therefore, we pick one\nreference Deep Convolutional Neural Network (DCNN) architecture and three\ndifferent multispectral datasets. The Region Proposal Network (RPN) originally\nintroduced for object detection within the popular Faster R-CNN is chosen as a\nreference DCNN. The reason is that a stand-alone RPN is able to serve as a\ncompetitive detector for two-class problems such as person detection.\nFurthermore, current state-of-the-art approaches initially apply an RPN\nfollowed by individual classifiers. The three considered datasets are the KAIST\nMultispectral Pedestrian Benchmark including recently published improved\nannotations for training and testing, the Tokyo Multi-spectral Semantic\nSegmentation dataset, and the OSU Color-Thermal dataset including recently\nreleased annotations. The experimental results show that the KAIST\nMultispectral Pedestrian Benchmark with its improved annotations provides the\nbest basis to train a DCNN with good generalization ability compared to the\nother two multispectral datasets. On average, this detection model achieves a\nlog-average Miss Rate (MR) of 29.74 % evaluated on the reasonable test subsets\nof the three datasets.\n",
    "topics": "{'Region Proposal': 1.0, 'Human Detection': 0.99994004, 'Object Detection': 0.98698306, 'Semantic Segmentation': 0.9574881}",
    "score": 0.8638144007
  },
  {
    "id": "1903.06249",
    "title": "Learning Representations from Persian Handwriting for Offline Signature\n  Verification, a Deep Transfer Learning Approach",
    "abstract": "  Offline Signature Verification (OSV) is a challenging pattern recognition\ntask, especially when it is expected to generalize well on the skilled\nforgeries that are not available during the training. Its challenges also\ninclude small training sample and large intra-class variations. Considering the\nlimitations, we suggest a novel transfer learning approach from Persian\nhandwriting domain to multi-language OSV domain. We train two Residual CNNs on\nthe source domain separately based on two different tasks of word\nclassification and writer identification. Since identifying a person signature\nresembles identifying ones handwriting, it seems perfectly convenient to use\nhandwriting for the feature learning phase. The learned representation on the\nmore varied and plentiful handwriting dataset can compensate for the lack of\ntraining data in the original task, i.e. OSV, without sacrificing the\ngeneralizability. Our proposed OSV system includes two steps: learning\nrepresentation and verification of the input signature. For the first step, the\nsignature images are fed into the trained Residual CNNs. The output\nrepresentations are then used to train SVMs for the verification. We test our\nOSV system on three different signature datasets, including MCYT (a Spanish\nsignature dataset), UTSig (a Persian one) and GPDS-Synthetic (an artificial\ndataset). On UT-SIG, we achieved 9.80% Equal Error Rate (EER) which showed\nsubstantial improvement over the best EER in the literature, 17.45%. Our\nproposed method surpassed state-of-the-arts by 6% on GPDS-Synthetic, achieving\n6.81%. On MCYT, EER of 3.98% was obtained which is comparable to the best\npreviously reported results.\n",
    "topics": "{'Handwriting Recognition': 1.0, 'Transfer Learning': 0.94945526}",
    "score": 0.8636517509
  },
  {
    "id": "1912.04391",
    "title": "Semi-supervised Learning Approach to Generate Neuroimaging Modalities\n  with Adversarial Training",
    "abstract": "  Magnetic Resonance Imaging (MRI) of the brain can come in the form of\ndifferent modalities such as T1-weighted and Fluid Attenuated Inversion\nRecovery (FLAIR) which has been used to investigate a wide range of\nneurological disorders. Current state-of-the-art models for brain tissue\nsegmentation and disease classification require multiple modalities for\ntraining and inference. However, the acquisition of all of these modalities are\nexpensive, time-consuming, inconvenient and the required modalities are often\nnot available. As a result, these datasets contain large amounts of\n\\emph{unpaired} data, where examples in the dataset do not contain all\nmodalities. On the other hand, there is smaller fraction of examples that\ncontain all modalities (\\emph{paired} data) and furthermore each modality is\nhigh dimensional when compared to number of datapoints. In this work, we\ndevelop a method to address these issues with semi-supervised learning in\ntranslating between two neuroimaging modalities. Our proposed model,\nSemi-Supervised Adversarial CycleGAN (SSA-CGAN), uses an adversarial loss to\nlearn from \\emph{unpaired} data points, cycle loss to enforce consistent\nreconstructions of the mappings and another adversarial loss to take advantage\nof \\emph{paired} data points. Our experiments demonstrate that our proposed\nframework produces an improvement in reconstruction error and reduced variance\nfor the pairwise translation of multiple modalities and is more robust to\nthermal noise when compared to existing methods.\n",
    "topics": "{}",
    "score": 0.8636383734
  },
  {
    "id": "2004.03597",
    "title": "JHU-CROWD++: Large-Scale Crowd Counting Dataset and A Benchmark Method",
    "abstract": "  Due to its variety of applications in the real-world, the task of single\nimage-based crowd counting has received a lot of interest in the recent years.\nRecently, several approaches have been proposed to address various problems\nencountered in crowd counting. These approaches are essentially based on\nconvolutional neural networks that require large amounts of data to train the\nnetwork parameters. Considering this, we introduce a new large scale\nunconstrained crowd counting dataset (JHU-CROWD++) that contains \"4,372\" images\nwith \"1.51 million\" annotations. In comparison to existing datasets, the\nproposed dataset is collected under a variety of diverse scenarios and\nenvironmental conditions. Specifically, the dataset includes several images\nwith weather-based degradations and illumination variations, making it a very\nchallenging dataset. Additionally, the dataset consists of a rich set of\nannotations at both image-level and head-level. Several recent methods are\nevaluated and compared on this dataset. The dataset can be downloaded from\nhttp://www.crowd-counting.com .\n  Furthermore, we propose a novel crowd counting network that progressively\ngenerates crowd density maps via residual error estimation. The proposed method\nuses VGG16 as the backbone network and employs density map generated by the\nfinal layer as a coarse prediction to refine and generate finer density maps in\na progressive fashion using residual learning. Additionally, the residual\nlearning is guided by an uncertainty-based confidence weighting mechanism that\npermits the flow of only high-confidence residuals in the refinement path. The\nproposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is\nevaluated on recent complex datasets, and it achieves significant improvements\nin errors.\n",
    "topics": "{'Crowd Counting': 1.0}",
    "score": 0.8630255804
  },
  {
    "id": "1912.09395",
    "title": "Neural Networks-based Regularization for Large-Scale Medical Image\n  Reconstruction",
    "abstract": "  In this paper we present a generalized Deep Learning-based approach for\nsolving ill-posed large-scale inverse problems occuring in medical image\nreconstruction. Recently, Deep Learning methods using iterative neural networks\nand cascaded neural networks have been reported to achieve state-of-the-art\nresults with respect to various quantitative quality measures as PSNR, NRMSE\nand SSIM across different imaging modalities. However, the fact that these\napproaches employ the forward and adjoint operators repeatedly in the network\narchitecture requires the network to process the whole images or volumes at\nonce, which for some applications is computationally infeasible. In this work,\nwe follow a different reconstruction strategy by decoupling the regularization\nof the solution from ensuring consistency with the measured data. The\nregularization is given in the form of an image prior obtained by the output of\na previously trained neural network which is used in a Tikhonov regularization\nframework. By doing so, more complex and sophisticated network architectures\ncan be used for the removal of the artefacts or noise than it is usually the\ncase in iterative networks. Due to the large scale of the considered problems\nand the resulting computational complexity of the employed networks, the priors\nare obtained by processing the images or volumes as patches or slices. We\nevaluated the method for the cases of 3D cone-beam low dose CT and undersampled\n2D radial cine MRI and compared it to a total variation-minimization-based\nreconstruction algorithm as well as to a method with regularization based on\nlearned overcomplete dictionaries. The proposed method outperformed all the\nreported methods with respect to all chosen quantitative measures and further\naccelerates the regularization step in the reconstruction by several orders of\nmagnitude.\n",
    "topics": "{'Image Reconstruction': 1.0, 'SSIM': 0.9999999}",
    "score": 0.862938908
  },
  {
    "id": "1901.10112",
    "title": "Evaluating Generalization Ability of Convolutional Neural Networks and\n  Capsule Networks for Image Classification via Top-2 Classification",
    "abstract": "  Image classification is a challenging problem which aims to identify the\ncategory of object in the image. In recent years, deep Convolutional Neural\nNetworks (CNNs) have been applied to handle this task, and impressive\nimprovement has been achieved. However, some research showed the output of CNNs\ncan be easily altered by adding relatively small perturbations to the input\nimage, such as modifying few pixels. Recently, Capsule Networks (CapsNets) are\nproposed, which can help eliminating this limitation. Experiments on MNIST\ndataset revealed that capsules can better characterize the features of object\nthan CNNs. But it's hard to find a suitable quantitative method to compare the\ngeneralization ability of CNNs and CapsNets. In this paper, we propose a new\nimage classification task called Top-2 classification to evaluate the\ngeneralization ability of CNNs and CapsNets. The models are trained on single\nlabel image samples same as the traditional image classification task. But in\nthe test stage, we randomly concatenate two test image samples which contain\ndifferent labels, and then use the trained models to predict the top-2 labels\non the unseen newly-created two label image samples. This task can provide us\nprecise quantitative results to compare the generalization ability of CNNs and\nCapsNets. Back to the CapsNet, because it uses Full Connectivity (FC) mechanism\namong all capsules, it requires many parameters. To reduce the number of\nparameters, we introduce the Parameter-Sharing (PS) mechanism between capsules.\nExperiments on five widely used benchmark image datasets demonstrate the method\nsignificantly reduces the number of parameters, without losing the\neffectiveness of extracting features. Further, on the Top-2 classification\ntask, the proposed PS CapsNets obtain impressive higher accuracy compared to\nthe traditional CNNs and FC CapsNets by a large margin.\n",
    "topics": "{'Image Classification': 1.0}",
    "score": 0.8628359369
  },
  {
    "id": "2002.01155",
    "title": "Simultaneous Enhancement and Super-Resolution of Underwater Imagery for\n  Improved Visual Perception",
    "abstract": "  In this paper, we introduce and tackle the simultaneous enhancement and\nsuper-resolution (SESR) problem for underwater robot vision and provide an\nefficient solution for near real-time applications. We present Deep SESR, a\nresidual-in-residual network-based generative model that can learn to restore\nperceptual image qualities at 2x, 3x, or 4x higher spatial resolution. We\nsupervise its training by formulating a multi-modal objective function that\naddresses the chrominance-specific underwater color degradation, lack of image\nsharpness, and loss in high-level feature representation. It is also supervised\nto learn salient foreground regions in the image, which in turn guides the\nnetwork to learn global contrast enhancement. We design an end-to-end training\npipeline to jointly learn the saliency prediction and SESR on a shared\nhierarchical feature space for fast inference. Moreover, we present UFO-120,\nthe first dataset to facilitate large-scale SESR learning; it contains over\n1500 training samples and a benchmark test set of 120 samples. By thorough\nexperimental evaluation on the UFO-120 and other standard datasets, we\ndemonstrate that Deep SESR outperforms the existing solutions for underwater\nimage enhancement and super-resolution. We also validate its generalization\nperformance on several test cases that include underwater images with diverse\nspectral and spatial degradation levels, and also terrestrial images with\nunseen natural objects. Lastly, we analyze its computational feasibility for\nsingle-board deployments and demonstrate its operational benefits for\nvisually-guided underwater robots. The model and dataset information will be\navailable at: https://github.com/xahidbuffon/Deep-SESR.\n",
    "topics": "{'Image Enhancement': 1.0, 'Super Resolution': 0.99999905, 'Super-Resolution': 0.9999988, 'Saliency Prediction': 0.9999938}",
    "score": 0.8627498882
  },
  {
    "id": "1908.10417",
    "title": "Complex Deep Learning Models for Denoising of Human Heart ECG signals",
    "abstract": "  Effective and powerful methods for denoising real electrocardiogram (ECG)\nsignals are important for wearable sensors and devices. Deep Learning (DL)\nmodels have been used extensively in image processing and other domains with\ngreat success but only very recently have been used in processing ECG signals.\nThis paper presents several DL models namely Convolutional Neural Networks\n(CNNs), Long Short-Term Memory (LSTM), Restricted Boltzmann Machine (RBM)\ntogether with the more conventional filtering methods (low pass filtering, high\npass filtering, Notch filtering) and the standard wavelet-based technique for\ndenoising EEG signals. These methods are trained, tested and evaluated on\ndifferent synthetic and real ECG datasets taken from the MIT PhysioNet database\nand for different simulation conditions (i.e. various lengths of the ECG\nsignals, single or multiple records). The results show the CNN model is a\nperformant model that can be used for off-line denoising ECG applications where\nit is satisfactory to train on a clean part of an ECG signal from an ECG\nrecord, and then to test on the same ECG signal, which would have some high\nlevel of noise added to it. However, for real-time applications or near-real\ntime applications, this task becomes more cumbersome, as the clean part of an\nECG signal is very probable to be very limited in size. Therefore the solution\nput forth in this work is to train a CNN model on 1 second ECG noisy artificial\nmultiple heartbeat data (i.e. ECG at effort), which was generated in a first\ninstance based on few sequences of real signal heartbeat ECG data (i.e. ECG at\nrest). Afterwards it would be possible to use the trained CNN model in real\nlife situations to denoise the ECG signal.\n",
    "topics": "{'Electrocardiography (ECG)': 0.99999726, 'Denoising': 0.9998971}",
    "score": 0.8626767673
  },
  {
    "id": "2006.03394",
    "title": "Detection of prostate cancer in whole-slide images through end-to-end\n  training with image-level labels",
    "abstract": "  Prostate cancer is the most prevalent cancer among men in Western countries,\nwith 1.1 million new diagnoses every year. The gold standard for the diagnosis\nof prostate cancer is a pathologists' evaluation of prostate tissue.\n  To potentially assist pathologists deep-learning-based cancer detection\nsystems have been developed. Many of the state-of-the-art models are\npatch-based convolutional neural networks, as the use of entire scanned slides\nis hampered by memory limitations on accelerator cards. Patch-based systems\ntypically require detailed, pixel-level annotations for effective training.\nHowever, such annotations are seldom readily available, in contrast to the\nclinical reports of pathologists, which contain slide-level labels. As such,\ndeveloping algorithms which do not require manual pixel-wise annotations, but\ncan learn using only the clinical report would be a significant advancement for\nthe field.\n  In this paper, we propose to use a streaming implementation of convolutional\nlayers, to train a modern CNN (ResNet-34) with 21 million parameters end-to-end\non 4712 prostate biopsies. The method enables the use of entire biopsy images\nat high-resolution directly by reducing the GPU memory requirements by 2.4 TB.\nWe show that modern CNNs, trained using our streaming approach, can extract\nmeaningful features from high-resolution images without additional heuristics,\nreaching similar performance as state-of-the-art patch-based and\nmultiple-instance learning methods. By circumventing the need for manual\nannotations, this approach can function as a blueprint for other tasks in\nhistopathological diagnosis.\n  The source code to reproduce the streaming models is available at\nhttps://github.com/DIAGNijmegen/pathology-streaming-pipeline .\n",
    "topics": "{}",
    "score": 0.8624526603
  },
  {
    "id": "1905.07767",
    "title": "Phish-IRIS: A New Approach for Vision Based Brand Prediction of Phishing\n  Web Pages via Compact Visual Descriptors",
    "abstract": "  Phishing, a continuously growing cyber threat, aims to obtain innocent users'\ncredentials by deceiving them via presenting fake web pages which mimic their\nlegitimate targets. To date, various attempts have been carried out in order to\ndetect phishing pages. In this study, we treat the problem of phishing web page\nidentification as an image classification task and propose a machine learning\naugmented pure vision based approach which extracts and classifies compact\nvisual features from web page screenshots. For this purpose, we employed\nseveral MPEG7 and MPEG7-like compact visual descriptors (SCD, CLD, CEDD, FCTH\nand JCD) to reveal color and edge based discriminative visual cues. Throughout\nthe feature extraction process we have followed two different schemes working\non either whole screenshots in a \"holistic\" manner or equal sized \"patches\"\nconstructing a coarse-to-fine \"pyramidal\" representation. Moreover, for the\ntask of image classification, we have built SVM and Random Forest based machine\nlearning models. In order to assess the performance and generalization\ncapability of the proposed approach, we have collected a mid-sized corpus\ncovering 14 distinct brands and involving 2852 samples. According to the\nconducted experiments, our approach reaches up to 90.5% F1 score via SCD. As a\nresult, compared to other studies, the suggested approach presents a\nlightweight schema serving competitive accuracy and superior feature extraction\nand inferring speed that enables it to be used as a browser plugin.\n",
    "topics": "{'Image Classification': 0.9999517}",
    "score": 0.8623342216
  },
  {
    "id": "1912.08178",
    "title": "AeroRIT: A New Scene for Hyperspectral Image Analysis",
    "abstract": "  We investigate applying convolutional neural network (CNN) architecture to\nfacilitate aerial hyperspectral scene understanding and present a new\nhyperspectral dataset-AeroRIT-that is large enough for CNN training. To date\nthe majority of hyperspectral airborne have been confined to various\nsub-categories of vegetation and roads and this scene introduces two new\ncategories: buildings and cars. To the best of our knowledge, this is the first\ncomprehensive large-scale hyperspectral scene with nearly seven million pixel\nannotations for identifying cars, roads, and buildings. We compare the\nperformance of three popular architectures - SegNet, U-Net, and Res-U-Net, for\nscene understanding and object identification via the task of dense semantic\nsegmentation to establish a benchmark for the scene. To further strengthen the\nnetwork, we add squeeze and excitation blocks for better channel interactions\nand use self-supervised learning for better encoder initialization. Aerial\nhyperspectral image analysis has been restricted to small datasets with limited\ntrain/test splits capabilities and we believe that AeroRIT will help advance\nthe research in the field with a more complex object distribution to perform\nwell on. The full dataset, with flight lines in radiance and reflectance\ndomain, is available for download at https://github.com/aneesh3108/AeroRIT.\nThis dataset is the first step towards developing robust algorithms for\nhyperspectral airborne sensing that can robustly perform advanced tasks like\nvehicle tracking and occlusion handling.\n",
    "topics": "{'Scene Understanding': 1.0, 'Self-Supervised Learning': 0.99997985, 'Image Super-Resolution': 0.98887616, 'Semantic Segmentation': 0.9801722, 'Super-Resolution': 0.43657574}",
    "score": 0.862274234
  },
  {
    "id": "2007.11993",
    "title": "CVR-Net: A deep convolutional neural network for coronavirus recognition\n  from chest radiography images",
    "abstract": "  The novel Coronavirus Disease 2019 (COVID-19) is a global pandemic disease\nspreading rapidly around the world. A robust and automatic early recognition of\nCOVID-19, via auxiliary computer-aided diagnostic tools, is essential for\ndisease cure and control. The chest radiography images, such as Computed\nTomography (CT) and X-ray, and deep Convolutional Neural Networks (CNNs), can\nbe a significant and useful material for designing such tools. However,\ndesigning such an automated tool is challenging as a massive number of manually\nannotated datasets are not publicly available yet, which is the core\nrequirement of supervised learning systems. In this article, we propose a\nrobust CNN-based network, called CVR-Net (Coronavirus Recognition Network), for\nthe automatic recognition of the coronavirus from CT or X-ray images. The\nproposed end-to-end CVR-Net is a multi-scale-multi-encoder ensemble model,\nwhere we have aggregated the outputs from two different encoders and their\ndifferent scales to obtain the final prediction probability. We train and test\nthe proposed CVR-Net on three different datasets, where the images have\ncollected from different open-source repositories. We compare our proposed\nCVR-Net with state-of-the-art methods, which are trained and tested on the same\ndatasets. We split three datasets into five different tasks, where each task\nhas a different number of classes, to evaluate the multi-tasking CVR-Net. Our\nmodel achieves an overall F1-score & accuracy of 0.997 & 0.998; 0.963 & 0.964;\n0.816 & 0.820; 0.961 & 0.961; and 0.780 & 0.780, respectively, for task-1 to\ntask-5. As the CVR-Net provides promising results on the small datasets, it can\nbe an auspicious computer-aided diagnostic tool for the diagnosis of\ncoronavirus to assist the clinical practitioners and radiologists. Our source\ncodes and model are publicly available at\nhttps://github.com/kamruleee51/CVR-Net.\n",
    "topics": "{'Computed Tomography (CT)': 0.9999565}",
    "score": 0.8621417157
  },
  {
    "id": "2008.09647",
    "title": "Generating synthetic photogrammetric data for training deep learning\n  based 3D point cloud segmentation models",
    "abstract": "  At I/ITSEC 2019, the authors presented a fully-automated workflow to segment\n3D photogrammetric point-clouds/meshes and extract object information,\nincluding individual tree locations and ground materials (Chen et al., 2019).\nThe ultimate goal is to create realistic virtual environments and provide the\nnecessary information for simulation. We tested the generalizability of the\npreviously proposed framework using a database created under the U.S. Army's\nOne World Terrain (OWT) project with a variety of landscapes (i.e., various\nbuildings styles, types of vegetation, and urban density) and different data\nqualities (i.e., flight altitudes and overlap between images). Although the\ndatabase is considerably larger than existing databases, it remains unknown\nwhether deep-learning algorithms have truly achieved their full potential in\nterms of accuracy, as sizable data sets for training and validation are\ncurrently lacking. Obtaining large annotated 3D point-cloud databases is\ntime-consuming and labor-intensive, not only from a data annotation perspective\nin which the data must be manually labeled by well-trained personnel, but also\nfrom a raw data collection and processing perspective. Furthermore, it is\ngenerally difficult for segmentation models to differentiate objects, such as\nbuildings and tree masses, and these types of scenarios do not always exist in\nthe collected data set. Thus, the objective of this study is to investigate\nusing synthetic photogrammetric data to substitute real-world data in training\ndeep-learning algorithms. We have investigated methods for generating synthetic\nUAV-based photogrammetric data to provide a sufficiently sized database for\ntraining a deep-learning algorithm with the ability to enlarge the data size\nfor scenarios in which deep-learning models have difficulties.\n",
    "topics": "{}",
    "score": 0.862120668
  },
  {
    "id": "1904.06083",
    "title": "DNN-based Acoustic-to-Articulatory Inversion using Ultrasound Tongue\n  Imaging",
    "abstract": "  Speech sounds are produced as the coordinated movement of the speaking\norgans. There are several available methods to model the relation of\narticulatory movements and the resulting speech signal. The reverse problem is\noften called as acoustic-to-articulatory inversion (AAI). In this paper we have\nimplemented several different Deep Neural Networks (DNNs) to estimate the\narticulatory information from the acoustic signal. There are several previous\nworks related to performing this task, but most of them are using\nElectroMagnetic Articulography (EMA) for tracking the articulatory movement.\nCompared to EMA, Ultrasound Tongue Imaging (UTI) is a technique of higher\ncost-benefit if we take into account equipment cost, portability, safety and\nvisualized structures. Seeing that, our goal is to train a DNN to obtain UT\nimages, when using speech as input. We also test two approaches to represent\nthe articulatory information: 1) the EigenTongue space and 2) the raw\nultrasound image. As an objective quality measure for the reconstructed UT\nimages, we use MSE, Structural Similarity Index (SSIM) and Complex-Wavelet SSIM\n(CW-SSIM). Our experimental results show that CW-SSIM is the most useful error\nmeasure in the UTI context. We tested three different system configurations: a)\nsimple DNN composed of 2 hidden layers with 64x64 pixels of an UTI file as\ntarget; b) the same simple DNN but with ultrasound images projected to the\nEigenTongue space as the target; c) and a more complex DNN composed of 5 hidden\nlayers with UTI files projected to the EigenTongue space. In a subjective\nexperiment the subjects found that the neural networks with two hidden layers\nwere more suitable for this inversion task.\n",
    "topics": "{'SSIM': 1.0}",
    "score": 0.8620058146
  },
  {
    "id": "1708.09427",
    "title": "Deep Learning to Improve Breast Cancer Early Detection on Screening\n  Mammography",
    "abstract": "  The rapid development of deep learning, a family of machine learning\ntechniques, has spurred much interest in its application to medical imaging\nproblems. Here, we develop a deep learning algorithm that can accurately detect\nbreast cancer on screening mammograms using an \"end-to-end\" training approach\nthat efficiently leverages training datasets with either complete clinical\nannotation or only the cancer status (label) of the whole image. In this\napproach, lesion annotations are required only in the initial training stage,\nand subsequent stages require only image-level labels, eliminating the reliance\non rarely available lesion annotations. Our all convolutional network method\nfor classifying screening mammograms attained excellent performance in\ncomparison with previous methods. On an independent test set of digitized film\nmammograms from Digital Database for Screening Mammography (DDSM), the best\nsingle model achieved a per-image AUC of 0.88, and four-model averaging\nimproved the AUC to 0.91 (sensitivity: 86.1%, specificity: 80.1%). On a\nvalidation set of full-field digital mammography (FFDM) images from the\nINbreast database, the best single model achieved a per-image AUC of 0.95, and\nfour-model averaging improved the AUC to 0.98 (sensitivity: 86.7%, specificity:\n96.1%). We also demonstrate that a whole image classifier trained using our\nend-to-end approach on the DDSM digitized film mammograms can be transferred to\nINbreast FFDM images using only a subset of the INbreast data for fine-tuning\nand without further reliance on the availability of lesion annotations. These\nfindings show that automatic deep learning methods can be readily trained to\nattain high accuracy on heterogeneous mammography platforms, and hold\ntremendous promise for improving clinical tools to reduce false positive and\nfalse negative screening mammography results.\n",
    "topics": "{}",
    "score": 0.8618992229
  },
  {
    "id": "2009.10549",
    "title": "CA-Net: Comprehensive Attention Convolutional Neural Networks for\n  Explainable Medical Image Segmentation",
    "abstract": "  Accurate medical image segmentation is essential for diagnosis and treatment\nplanning of diseases. Convolutional Neural Networks (CNNs) have achieved\nstate-of-the-art performance for automatic medical image segmentation. However,\nthey are still challenged by complicated conditions where the segmentation\ntarget has large variations of position, shape and scale, and existing CNNs\nhave a poor explainability that limits their application to clinical decisions.\nIn this work, we make extensive use of multiple attentions in a CNN\narchitecture and propose a comprehensive attention-based CNN (CA-Net) for more\naccurate and explainable medical image segmentation that is aware of the most\nimportant spatial positions, channels and scales at the same time. In\nparticular, we first propose a joint spatial attention module to make the\nnetwork focus more on the foreground region. Then, a novel channel attention\nmodule is proposed to adaptively recalibrate channel-wise feature responses and\nhighlight the most relevant feature channels. Also, we propose a scale\nattention module implicitly emphasizing the most salient feature maps among\nmultiple scales so that the CNN is adaptive to the size of an object. Extensive\nexperiments on skin lesion segmentation from ISIC 2018 and multi-class\nsegmentation of fetal MRI found that our proposed CA-Net significantly improved\nthe average segmentation Dice score from 87.77% to 92.08% for skin lesion,\n84.79% to 87.08% for the placenta and 93.20% to 95.88% for the fetal brain\nrespectively compared with U-Net. It reduced the model size to around 15 times\nsmaller with close or even better accuracy compared with state-of-the-art\nDeepLabv3+. In addition, it has a much higher explainability than existing\nnetworks by visualizing the attention weight maps. Our code is available at\nhttps://github.com/HiLab-git/CA-Net\n",
    "topics": "{'Semantic Segmentation': 1.0, 'Medical Image Segmentation': 1.0, 'Lesion Segmentation': 0.9861748}",
    "score": 0.8618984812
  },
  {
    "id": "1708.03307",
    "title": "Cell Detection in Microscopy Images with Deep Convolutional Neural\n  Network and Compressed Sensing",
    "abstract": "  The ability to automatically detect certain types of cells or cellular\nsubunits in microscopy images is of significant interest to a wide range of\nbiomedical research and clinical practices. Cell detection methods have evolved\nfrom employing hand-crafted features to deep learning-based techniques. The\nessential idea of these methods is that their cell classifiers or detectors are\ntrained in the pixel space, where the locations of target cells are labeled. In\nthis paper, we seek a different route and propose a convolutional neural\nnetwork (CNN)-based cell detection method that uses encoding of the output\npixel space. For the cell detection problem, the output space is the sparsely\nlabeled pixel locations indicating cell centers. We employ random projections\nto encode the output space to a compressed vector of fixed dimension. Then, CNN\nregresses this compressed vector from the input pixels. Furthermore, it is\npossible to stably recover sparse cell locations on the output pixel space from\nthe predicted compressed vector using $L_1$-norm optimization. In the past,\noutput space encoding using compressed sensing (CS) has been used in\nconjunction with linear and non-linear predictors. To the best of our\nknowledge, this is the first successful use of CNN with CS-based output space\nencoding. We made substantial experiments on several benchmark datasets, where\nthe proposed CNN + CS framework (referred to as CNNCS) achieved the highest or\nat least top-3 performance in terms of F1-score, compared with other\nstate-of-the-art methods.\n",
    "topics": "{'Cell Segmentation': 0.9861501}",
    "score": 0.8618781663
  },
  {
    "id": "2009.10474",
    "title": "Classification of COVID-19 in CT Scans using Multi-Source Transfer\n  Learning",
    "abstract": "  Since December of 2019, novel coronavirus disease COVID-19 has spread around\nthe world infecting millions of people and upending the global economy. One of\nthe driving reasons behind its high rate of infection is due to the\nunreliability and lack of RT-PCR testing. At times the turnaround results span\nas long as a couple of days, only to yield a roughly 70% sensitivity rate. As\nan alternative, recent research has investigated the use of Computer Vision\nwith Convolutional Neural Networks (CNNs) for the classification of COVID-19\nfrom CT scans. Due to an inherent lack of available COVID-19 CT data, these\nresearch efforts have been forced to leverage the use of Transfer Learning.\nThis commonly employed Deep Learning technique has shown to improve model\nperformance on tasks with relatively small amounts of data, as long as the\nSource feature space somewhat resembles the Target feature space.\nUnfortunately, a lack of similarity is often encountered in the classification\nof medical images as publicly available Source datasets usually lack the visual\nfeatures found in medical images. In this study, we propose the use of\nMulti-Source Transfer Learning (MSTL) to improve upon traditional Transfer\nLearning for the classification of COVID-19 from CT scans. With our\nmulti-source fine-tuning approach, our models outperformed baseline models\nfine-tuned with ImageNet. We additionally, propose an unsupervised label\ncreation process, which enhances the performance of our Deep Residual Networks.\nOur best performing model was able to achieve an accuracy of 0.893 and a Recall\nscore of 0.897, outperforming its baseline Recall score by 9.3%.\n",
    "topics": "{'Transfer Learning': 0.99999976}",
    "score": 0.8618682594
  },
  {
    "id": "2010.03341",
    "title": "Deep Learning in Diabetic Foot Ulcers Detection: A Comprehensive\n  Evaluation",
    "abstract": "  There has been a substantial amount of research on computer methods and\ntechnology for the detection and recognition of diabetic foot ulcers (DFUs),\nbut there is a lack of systematic comparisons of state-of-the-art deep learning\nobject detection frameworks applied to this problem. With recent development\nand data sharing performed as part of the DFU Challenge (DFUC2020) such a\ncomparison becomes possible: DFUC2020 provided participants with a\ncomprehensive dataset consisting of 2,000 images for training each method and\n2,000 images for testing them. The following deep learning-based algorithms are\ncompared in this paper: Faster R-CNN, three variants of Faster R-CNN and an\nensemble method; YOLOv3; YOLOv5; EfficientDet; and a new Cascade Attention\nNetwork. For each deep learning method, we provide a detailed description of\nmodel architecture, parameter settings for training and additional stages\nincluding pre-processing, data augmentation and post-processing. We provide a\ncomprehensive evaluation for each method. All the methods required a data\naugmentation stage to increase the number of images available for training and\na post-processing stage to remove false positives. The best performance is\nobtained Deformable Convolution, a variant of Faster R-CNN, with a mAP of\n0.6940 and an F1-Score of 0.7434. Finally, we demonstrate that the ensemble\nmethod based on different deep learning methods can enhanced the F1-Score but\nnot the mAP. Our results show that state-of-the-art deep learning methods can\ndetect DFU with some accuracy, but there are many challenges ahead before they\ncan be implemented in real world settings.\n",
    "topics": "{'Data Augmentation': 0.9999924, 'Object Detection': 0.997264}",
    "score": 0.8618551161
  },
  {
    "id": "1909.09541",
    "title": "A Transfer Learning Approach for Automated Segmentation of Prostate\n  Whole Gland and Transition Zone in Diffusion Weighted MRI",
    "abstract": "  The segmentation of prostate whole gland and transition zone in Diffusion\nWeighted MRI (DWI) are the first step in designing computer-aided detection\nalgorithms for prostate cancer. However, variations in MRI acquisition\nparameters and scanner manufacturing result in different appearances of\nprostate tissue in the images. Convolutional neural networks (CNNs) which have\nshown to be successful in various medical image analysis tasks including\nsegmentation are typically sensitive to the variations in imaging parameters.\nThis sensitivity leads to poor segmentation performance of CNNs trained on a\nsource cohort and tested on a target cohort from a different scanner and hence,\nit limits the applicability of CNNs for cross-cohort training and testing.\nContouring prostate whole gland and transition zone in DWI images are\ntime-consuming and expensive. Thus, it is important to enable CNNs pretrained\non images of source domain, to segment images of target domain with minimum\nrequirement for manual segmentation of images from the target domain. In this\nwork, we propose a transfer learning method based on a modified U-net\narchitecture and loss function, for segmentation of prostate whole gland and\ntransition zone in DWIs using a CNN pretrained on a source dataset and tested\non the target dataset. We explore the effect of the size of subset of target\ndataset used for fine-tuning the pre-trained CNN on the overall segmentation\naccuracy. Our results show that with a fine-tuning data as few as 30 patients\nfrom the target domain, the proposed transfer learning-based algorithm can\nreach dice score coefficient of 0.80 for both prostate whole gland and\ntransition zone segmentation. Using a fine-tuning data of 115 patients from the\ntarget domain, dice score coefficient of 0.85 and 0.84 are achieved for\nsegmentation of whole gland and transition zone, respectively, in the target\ndomain.\n",
    "topics": "{'Transfer Learning': 0.9943832, 'Semantic Segmentation': 0.3019026}",
    "score": 0.8618473636
  },
  {
    "id": "1801.01443",
    "title": "A semi-supervised fuzzy GrowCut algorithm to segment and classify\n  regions of interest of mammographic images",
    "abstract": "  According to the World Health Organization, breast cancer is the most common\nform of cancer in women. It is the second leading cause of death among women\nround the world, becoming the most fatal form of cancer. Mammographic image\nsegmentation is a fundamental task to support image analysis and diagnosis,\ntaking into account shape analysis of mammary lesions and their borders.\nHowever, mammogram segmentation is a very hard process, once it is highly\ndependent on the types of mammary tissues. In this work we present a new\nsemi-supervised segmentation algorithm based on the modification of the GrowCut\nalgorithm to perform automatic mammographic image segmentation once a region of\ninterest is selected by a specialist. In our proposal, we used fuzzy Gaussian\nmembership functions to modify the evolution rule of the original GrowCut\nalgorithm, in order to estimate the uncertainty of a pixel being object or\nbackground. The main impact of the proposed method is the significant reduction\nof expert effort in the initialization of seed points of GrowCut to perform\naccurate segmentation, once it removes the need of selection of background\nseeds. We also constructed an automatic point selection process based on the\nsimulated annealing optimization method, avoiding the need of human\nintervention. The proposed approach was qualitatively compared with other\nstate-of-the-art segmentation techniques, considering the shape of segmented\nregions. In order to validate our proposal, we built an image classifier using\na classical multilayer perceptron. We used Zernike moments to extract segmented\nimage features. This analysis employed 685 mammograms from IRMA breast cancer\ndatabase, using fat and fibroid tissues. Results show that the proposed\ntechnique could achieve a classification rate of 91.28\\% for fat tissues,\nevidencing the feasibility of our approach.\n",
    "topics": "{'Semantic Segmentation': 0.9998042}",
    "score": 0.8618132601
  },
  {
    "id": "1808.04228",
    "title": "DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human\n  Activity Recognition",
    "abstract": "  Deep Convolutional Neural Networks (DCNNs) are currently popular in human\nactivity recognition applications. However, in the face of modern artificial\nintelligence sensor-based games, many research achievements cannot be\npractically applied on portable devices. DCNNs are typically resource-intensive\nand too large to be deployed on portable devices, thus this limits the\npractical application of complex activity detection. In addition, since\nportable devices do not possess high-performance Graphic Processing Units\n(GPUs), there is hardly any improvement in Action Game (ACT) experience.\nBesides, in order to deal with multi-sensor collaboration, all previous human\nactivity recognition models typically treated the representations from\ndifferent sensor signal sources equally. However, distinct types of activities\nshould adopt different fusion strategies. In this paper, a novel scheme is\nproposed. This scheme is used to train 2-bit Convolutional Neural Networks with\nweights and activations constrained to {-0.5,0,0.5}. It takes into account the\ncorrelation between different sensor signal sources and the activity types.\nThis model, which we refer to as DFTerNet, aims at producing a more reliable\ninference and better trade-offs for practical applications. Our basic idea is\nto exploit quantization of weights and activations directly in pre-trained\nfilter banks and adopt dynamic fusion strategies for different activity types.\nExperiments demonstrate that by using dynamic fusion strategy can exceed the\nbaseline model performance by up to ~5% on activity recognition like\nOPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we\nwere able to achieve performances closer to that of full-precision counterpart.\nThese results were also verified using the UniMiB-SHAR dataset. In addition,\nthe proposed method can achieve ~9x acceleration on CPUs and ~11x memory\nsaving.\n",
    "topics": "{'Activity Recognition': 1.0, 'Activity Detection': 0.99985194, 'Action Detection': 0.9560626, 'Quantization': 0.64266336}",
    "score": 0.8616705519
  },
  {
    "id": "1911.05542",
    "title": "IStego100K: Large-scale Image Steganalysis Dataset",
    "abstract": "  In order to promote the rapid development of image steganalysis technology,\nin this paper, we construct and release a multivariable large-scale image\nsteganalysis dataset called IStego100K. It contains 208,104 images with the\nsame size of 1024*1024. Among them, 200,000 images (100,000 cover-stego image\npairs) are divided as the training set and the remaining 8,104 as testing set.\nIn addition, we hope that IStego100K can help researchers further explore the\ndevelopment of universal image steganalysis algorithms, so we try to reduce\nlimits on the images in IStego100K. For each image in IStego100K, the quality\nfactors is randomly set in the range of 75-95, the steganographic algorithm is\nrandomly selected from three well-known steganographic algorithms, which are\nJ-uniward, nsF5 and UERD, and the embedding rate is also randomly set to be a\nvalue of 0.1-0.4. In addition, considering the possible mismatch between\ntraining samples and test samples in real environment, we add a test set\n(DS-Test) whose source of samples are different from the training set. We hope\nthat this test set can help to evaluate the robustness of steganalysis\nalgorithms. We tested the performance of some latest steganalysis algorithms on\nIStego100K, with specific results and analysis details in the experimental\npart. We hope that the IStego100K dataset will further promote the development\nof universal image steganalysis technology. The description of IStego100K and\ninstructions for use can be found at https://github.com/YangzlTHU/IStego100K\n",
    "topics": "{}",
    "score": 0.8614037913
  },
  {
    "id": "1806.04422",
    "title": "Sample Dropout for Audio Scene Classification Using Multi-Scale Dense\n  Connected Convolutional Neural Network",
    "abstract": "  Acoustic scene classification is an intricate problem for a machine. As an\nemerging field of research, deep Convolutional Neural Networks (CNN) achieve\nconvincing results. In this paper, we explore the use of multi-scale Dense\nconnected convolutional neural network (DenseNet) for the classification task,\nwith the goal to improve the classification performance as multi-scale features\ncan be extracted from the time-frequency representation of the audio signal. On\nthe other hand, most of previous CNN-based audio scene classification\napproaches aim to improve the classification accuracy, by employing different\nregularization techniques, such as the dropout of hidden units and data\naugmentation, to reduce overfitting. It is widely known that outliers in the\ntraining set have a high negative influence on the trained model, and culling\nthe outliers may improve the classification performance, while it is often\nunder-explored in previous studies. In this paper, inspired by the silence\nremoval in the speech signal processing, a novel sample dropout approach is\nproposed, which aims to remove outliers in the training dataset. Using the\nDCASE 2017 audio scene classification datasets, the experimental results\ndemonstrates the proposed multi-scale DenseNet providing a superior performance\nthan the traditional single-scale DenseNet, while the sample dropout method can\nfurther improve the classification robustness of multi-scale DenseNet.\n",
    "topics": "{'Scene Classification': 1.0, 'Data Augmentation': 0.95401204}",
    "score": 0.8610732775
  },
  {
    "id": "2007.15963",
    "title": "Disentangling Human Error from the Ground Truth in Segmentation of\n  Medical Images",
    "abstract": "  Recent years have seen increasing use of supervised learning methods for\nsegmentation tasks. However, the predictive performance of these algorithms\ndepends on the quality of labels. This problem is particularly pertinent in the\nmedical image domain, where both the annotation cost and inter-observer\nvariability are high. In a typical label acquisition process, different human\nexperts provide their estimates of the 'true' segmentation labels under the\ninfluence of their own biases and competence levels. Treating these noisy\nlabels blindly as the ground truth limits the performance that automatic\nsegmentation algorithms can achieve. In this work, we present a method for\njointly learning, from purely noisy observations alone, the reliability of\nindividual annotators and the true segmentation label distributions, using two\ncoupled CNNs. The separation of the two is achieved by encouraging the\nestimated annotators to be maximally unreliable while achieving high fidelity\nwith the noisy training data. We first define a toy segmentation dataset based\non MNIST and study the properties of the proposed algorithm. We then\ndemonstrate the utility of the method on three public medical imaging\nsegmentation datasets with simulated (when necessary) and real diverse\nannotations: 1) MSLSC (multiple-sclerosis lesions); 2) BraTS (brain tumours);\n3) LIDC-IDRI (lung abnormalities). In all cases, our method outperforms\ncompeting methods and relevant baselines particularly in cases where the number\nof annotations is small and the amount of disagreement is large. The\nexperiments also show strong ability to capture the complex spatial\ncharacteristics of annotators' mistakes.\n",
    "topics": "{'Medical Image Segmentation': 0.9846864}",
    "score": 0.8610137225
  },
  {
    "id": "1910.00696",
    "title": "Improvement of Multiparametric MR Image Segmentation by Augmenting the\n  Data with Generative Adversarial Networks for Glioma Patients",
    "abstract": "  Every year thousands of patients are diagnosed with a glioma, a type of\nmalignant brain tumor. Physicians use MR images as a key tool in the diagnosis\nand treatment of these patients. Neural networks show great potential to aid\nphysicians in the medical image analysis. This study investigates the use of\nvarying amounts of synthetic brain T1-weighted (T1), post-contrast T1-weighted\n(T1Gd), T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (FLAIR) MR\nimages created by a generative adversarial network to overcome the lack of\nannotated medical image data in training separate 2D U-Nets to segment\nenhancing tumor, peritumoral edema, and necrosis (non-enhancing tumor core)\nregions on gliomas. These synthetic MR images were assessed quantitively\n(SSIM=0.79) and qualitatively by a physician who found that the synthetic\nimages seem stronger for delineation of structural boundaries but struggle more\nwhen gradient is significant, (e.g. edema signal in T2 modalities). Multiple 2D\nU-Nets were trained with original BraTS data and differing subsets of a\nquarter, half, three-quarters, and all synthetic MR images. There was not an\nobvious correlation between the improvement of values of the metrics in\nseparate validation dataset for each structure and amount of synthetic data\nadded, there is a strong correlation between the amount of synthetic data added\nand the number of best overall validation metrics. In summary, this study\nshowed ability to generate high quality synthetic Flair, T2, T1, and T1CE MR\nimages using the GAN. Using the synthetic MR images showed encouraging results\nto improve the U-Net segmentation performance which has the potential to\naddress the scarcity of readily available medical images.\n",
    "topics": "{'SSIM': 0.9974631, 'Semantic Segmentation': 0.8673661}",
    "score": 0.8609282669
  },
  {
    "id": "1910.04760",
    "title": "A cost-effective method for improving and re-purposing large,\n  pre-trained GANs by fine-tuning their class-embeddings",
    "abstract": "  Large, pre-trained generative models have been increasingly popular and\nuseful to both the research and wider communities. Specifically, BigGANs a\nclass-conditional Generative Adversarial Networks trained on\nImageNet---achieved excellent, state-of-the-art capability in generating\nrealistic photos. However, fine-tuning or training BigGANs from scratch is\npractically impossible for most researchers and engineers because (1) GAN\ntraining is often unstable and suffering from mode-collapse; and (2) the\ntraining requires a significant amount of computation, 256 Google TPUs for 2\ndays or 8xV100 GPUs for 15 days. Importantly, many pre-trained generative\nmodels both in NLP and image domains were found to contain biases that are\nharmful to society. Thus, we need computationally-feasible methods for\nmodifying and re-purposing these huge, pre-trained models for downstream tasks.\nIn this paper, we propose a cost-effective optimization method for improving\nand re-purposing BigGANs by fine-tuning only the class-embedding layer. We show\nthe effectiveness of our model-editing approach in three tasks: (1)\nsignificantly improving the realism and diversity of samples of complete\nmode-collapse classes; (2) re-purposing ImageNet BigGANs for generating images\nfor Places365; and (3) de-biasing or improving the sample diversity for\nselected ImageNet classes.\n",
    "topics": "{}",
    "score": 0.8608669698
  },
  {
    "id": "2002.05534",
    "title": "Abnormal respiratory patterns classifier may contribute to large-scale\n  screening of people infected with COVID-19 in an accurate and unobtrusive\n  manner",
    "abstract": "  Research significance: During the epidemic prevention and control period, our\nstudy can be helpful in prognosis, diagnosis and screening for the patients\ninfected with COVID-19 (the novel coronavirus) based on breathing\ncharacteristics. According to the latest clinical research, the respiratory\npattern of COVID-19 is different from the respiratory patterns of flu and the\ncommon cold. One significant symptom that occurs in the COVID-19 is Tachypnea.\nPeople infected with COVID-19 have more rapid respiration. Our study can be\nutilized to distinguish various respiratory patterns and our device can be\npreliminarily put to practical use. Demo videos of this method working in\nsituations of one subject and two subjects can be downloaded online. Research\ndetails: Accurate detection of the unexpected abnormal respiratory pattern of\npeople in a remote and unobtrusive manner has great significance. In this work,\nwe innovatively capitalize on depth camera and deep learning to achieve this\ngoal. The challenges in this task are twofold: the amount of real-world data is\nnot enough for training to get the deep model; and the intra-class variation of\ndifferent types of respiratory patterns is large and the outer-class variation\nis small. In this paper, considering the characteristics of actual respiratory\nsignals, a novel and efficient Respiratory Simulation Model (RSM) is first\nproposed to fill the gap between the large amount of training data and scarce\nreal-world data. Subsequently, we first apply a GRU neural network with\nbidirectional and attentional mechanisms (BI-AT-GRU) to classify 6 clinically\nsignificant respiratory patterns (Eupnea, Tachypnea, Bradypnea, Biots,\nCheyne-Stokes and Central-Apnea). The proposed deep model and the modeling\nideas have the great potential to be extended to large scale applications such\nas public places, sleep scenario, and office environment.\n",
    "topics": "{'COVID-19 Diagnosis': 0.6995767}",
    "score": 0.860623043
  },
  {
    "id": "1905.07419",
    "title": "Dynamic Vision Sensor integration on FPGA-based CNN accelerators for\n  high-speed visual classification",
    "abstract": "  Deep-learning is a cutting edge theory that is being applied to many fields.\nFor vision applications the Convolutional Neural Networks (CNN) are demanding\nsignificant accuracy for classification tasks. Numerous hardware accelerators\nhave populated during the last years to improve CPU or GPU based solutions.\nThis technology is commonly prototyped and tested over FPGAs before being\nconsidered for ASIC fabrication for mass production. The use of commercial\ntypical cameras (30fps) limits the capabilities of these systems for high speed\napplications. The use of dynamic vision sensors (DVS) that emulate the behavior\nof a biological retina is taking an incremental importance to improve this\napplications due to its nature, where the information is represented by a\ncontinuous stream of spikes and the frames to be processed by the CNN are\nconstructed collecting a fixed number of these spikes (called events). The\nfaster an object is, the more events are produced by DVS, so the higher is the\nequivalent frame rate. Therefore, these DVS utilization allows to compute a\nframe at the maximum speed a CNN accelerator can offer. In this paper we\npresent a VHDL/HLS description of a pipelined design for FPGA able to collect\nevents from an Address-Event-Representation (AER) DVS retina to obtain a\nnormalized histogram to be used by a particular CNN accelerator, called\nNullHop. VHDL is used to describe the circuit, and HLS for computation blocks,\nwhich are used to perform the normalization of a frame needed for the CNN.\nResults outperform previous implementations of frames collection and\nnormalization using ARM processors running at 800MHz on a Zynq7100 in both\nlatency and power consumption. A measured 67% speedup factor is presented for a\nRoshambo CNN real-time experiment running at 160fps peak rate.\n",
    "topics": "{}",
    "score": 0.8605568777
  },
  {
    "id": "1805.10628",
    "title": "A Simple Riemannian Manifold Network for Image Set Classification",
    "abstract": "  In the domain of image-set based classification, a considerable advance has\nbeen made by representing original image sets as covariance matrices which\ntypical lie in a Riemannian manifold. Specifically, it is a Symmetric Positive\nDefinite (SPD) manifold. Traditional manifold learning methods inevitably have\nthe property of high computational complexity or weak performance of the\nfeature representation. In order to overcome these limitations, we propose a\nvery simple Riemannian manifold network for image set classification. Inspired\nby deep learning architectures, we design a fully connected layer to generate\nmore novel, more powerful SPD matrices. However we exploit the rectifying layer\nto prevent the input SPD matrices from being singular. We also introduce a\nnon-linear learning of the proposed network with an innovative objective\nfunction. Furthermore we devise a pooling layer to further reduce the\nredundancy of the input SPD matrices, and the log-map layer to project the SPD\nmanifold to the Euclidean space. For learning the connection weights between\nthe input layer and the fully connected layer, we use Two-directional\ntwo-dimensional Principal Component Analysis ((2D)2PCA) algorithm. The proposed\nRiemannian manifold network (RieMNet) avoids complex computing and can be built\nand trained extremely easy and efficient. We have also developed a deep version\nof RieMNet, named as DRieMNet. The proposed RieMNet and DRieMNet are evaluated\non three tasks: video-based face recognition, set-based object categorization,\nand set-based cell identification. Extensive experimental results show the\nsuperiority of our method over the state-of-the-art.\n",
    "topics": "{'Face Recognition': 0.9817655}",
    "score": 0.8604946535
  },
  {
    "id": "1806.11430",
    "title": "Towards real-time unsupervised monocular depth estimation on CPU",
    "abstract": "  Unsupervised depth estimation from a single image is a very attractive\ntechnique with several implications in robotic, autonomous navigation,\naugmented reality and so on. This topic represents a very challenging task and\nthe advent of deep learning enabled to tackle this problem with excellent\nresults. However, these architectures are extremely deep and complex. Thus,\nreal-time performance can be achieved only by leveraging power-hungry GPUs that\ndo not allow to infer depth maps in application fields characterized by\nlow-power constraints. To tackle this issue, in this paper we propose a novel\narchitecture capable to quickly infer an accurate depth map on a CPU, even of\nan embedded system, using a pyramid of features extracted from a single input\nimage. Similarly to state-of-the-art, we train our network in an unsupervised\nmanner casting depth estimation as an image reconstruction problem. Extensive\nexperimental results on the KITTI dataset show that compared to the top\nperforming approach our network has similar accuracy but a much lower\ncomplexity (about 6% of parameters) enabling to infer a depth map for a KITTI\nimage in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard\nCPU. Moreover, by trading accuracy for efficiency, our network allows to infer\nmaps at about 2 Hz and 40 Hz respectively, still being more accurate than most\nstate-of-the-art slower methods. To the best of our knowledge, it is the first\nmethod enabling such performance on CPUs paving the way for effective\ndeployment of unsupervised monocular depth estimation even on embedded systems.\n",
    "topics": "{'Monocular Depth Estimation': 1.0, 'Depth Estimation': 1.0, 'Autonomous Navigation': 0.9999993, 'Image Reconstruction': 0.9993468}",
    "score": 0.8604587926
  },
  {
    "id": "1812.03405",
    "title": "AutoGAN: Robust Classifier Against Adversarial Attacks",
    "abstract": "  Classifiers fail to classify correctly input images that have been\npurposefully and imperceptibly perturbed to cause misclassification. This\nsusceptability has been shown to be consistent across classifiers, regardless\nof their type, architecture or parameters. Common defenses against adversarial\nattacks modify the classifer boundary by training on additional adversarial\nexamples created in various ways. In this paper, we introduce AutoGAN, which\ncounters adversarial attacks by enhancing the lower-dimensional manifold\ndefined by the training data and by projecting perturbed data points onto it.\nAutoGAN mitigates the need for knowing the attack type and magnitude as well as\nthe need for having adversarial samples of the attack. Our approach uses a\nGenerative Adversarial Network (GAN) with an autoencoder generator and a\ndiscriminator that also serves as a classifier. We test AutoGAN against\nadversarial samples generated with state-of-the-art Fast Gradient Sign Method\n(FGSM) as well as samples generated with random Gaussian noise, both using the\nMNIST dataset. For different magnitudes of perturbation in training and\ntesting, AutoGAN can surpass the accuracy of FGSM method by up to 25\\% points\non samples perturbed using FGSM. Without an augmented training dataset, AutoGAN\nachieves an accuracy of 89\\% compared to 1\\% achieved by FGSM method on FGSM\ntesting adversarial samples.\n",
    "topics": "{}",
    "score": 0.8604399765
  },
  {
    "id": "1807.11583",
    "title": "Testing the Efficient Network TRaining (ENTR) Hypothesis: initially\n  reducing training image size makes Convolutional Neural Network training for\n  image recognition tasks more efficient",
    "abstract": "  Convolutional Neural Networks (CNN) for image recognition tasks are seeing\nrapid advances in the available architectures and how networks are trained\nbased on large computational infrastructure and standard datasets with millions\nof images. In contrast, performance and time constraints for example, of small\ndevices and free cloud GPUs necessitate efficient network training (i.e.,\nhighest accuracy in the shortest inference time possible), often on small\ndatasets. Here, we hypothesize that initially decreasing image size during\ntraining makes the training process more efficient, because pre-shaping weights\nwith small images and later utilizing these weights with larger images reduces\ninitial network parameters and total inference time. We test this Efficient\nNetwork TRaining (ENTR) Hypothesis by training pre-trained Residual Network\n(ResNet) models (ResNet18, 34, & 50) on three small datasets (steel\nmicrostructures, bee images, and geographic aerial images) with a free cloud\nGPU. Based on three training regimes of i) not, ii) gradually or iii) in one\nstep increasing image size over the training process, we show that initially\nreducing image size increases training efficiency consistently across datasets\nand networks. We interpret these results mechanistically in the framework of\nregularization theory. Support for the ENTR hypothesis is an important\ncontribution, because network efficiency improvements for image recognition\ntasks are needed for practical applications. In the future, it will be exciting\nto see how the ENTR hypothesis holds for large standard datasets like ImageNet\nor CIFAR, to better understand the underlying mechanisms, and how these results\ncompare to other fields such as structural learning.\n",
    "topics": "{}",
    "score": 0.8604354861
  },
  {
    "id": "2009.11458",
    "title": "BWCFace: Open-set Face Recognition using Body-worn Camera",
    "abstract": "  With computer vision reaching an inflection point in the past decade, face\nrecognition technology has become pervasive in policing, intelligence\ngathering, and consumer applications. Recently, face recognition technology has\nbeen deployed on bodyworn cameras to keep officers safe, enabling situational\nawareness and providing evidence for trial. However, limited academic research\nhas been conducted on this topic using traditional techniques on datasets with\nsmall sample size. This paper aims to bridge the gap in the state-of-the-art\nface recognition using bodyworn cameras (BWC). To this aim, the contribution of\nthis work is two-fold: (1) collection of a dataset called BWCFace consisting of\na total of 178K facial images of 132 subjects captured using the body-worn\ncamera in in-door and daylight conditions, and (2) open-set evaluation of the\nlatest deep-learning-based Convolutional Neural Network (CNN) architectures\ncombined with five different loss functions for face identification, on the\ncollected dataset. Experimental results on our BWCFace dataset suggest a\nmaximum of 33.89% Rank-1 accuracy obtained when facial features are extracted\nusing SENet-50 trained on a large scale VGGFace2 facial image dataset. However,\nperformance improved up to a maximum of 99.00% Rank-1 accuracy when pretrained\nCNN models are fine-tuned on a subset of identities in our BWCFace dataset.\nEquivalent performances were obtained across body-worn camera sensor models\nused in existing face datasets. The collected BWCFace dataset and the\npretrained/ fine-tuned algorithms are publicly available to promote further\nresearch and development in this area. A downloadable link of this dataset and\nthe algorithms is available by contacting the authors.\n",
    "topics": "{'Face Recognition': 1.0, 'Face Identification': 0.99980026}",
    "score": 0.8603828674
  },
  {
    "id": "1708.01405",
    "title": "{\\mu}-MAR: Multiplane 3D Marker based Registration for Depth-sensing\n  Cameras",
    "abstract": "  Many applications including object reconstruction, robot guidance, and scene\nmapping require the registration of multiple views from a scene to generate a\ncomplete geometric and appearance model of it. In real situations,\ntransformations between views are unknown an it is necessary to apply expert\ninference to estimate them. In the last few years, the emergence of low-cost\ndepth-sensing cameras has strengthened the research on this topic, motivating a\nplethora of new applications. Although they have enough resolution and accuracy\nfor many applications, some situations may not be solved with general\nstate-of-the-art registration methods due to the Signal-to-Noise ratio (SNR)\nand the resolution of the data provided. The problem of working with low SNR\ndata, in general terms, may appear in any 3D system, then it is necessary to\npropose novel solutions in this aspect. In this paper, we propose a method,\n{\\mu}-MAR, able to both coarse and fine register sets of 3D points provided by\nlow-cost depth-sensing cameras, despite it is not restricted to these sensors,\ninto a common coordinate system. The method is able to overcome the noisy data\nproblem by means of using a model-based solution of multiplane registration.\nSpecifically, it iteratively registers 3D markers composed by multiple planes\nextracted from points of multiple views of the scene. As the markers and the\nobject of interest are static in the scenario, the transformations obtained for\nthe markers are applied to the object in order to reconstruct it. Experiments\nhave been performed using synthetic and real data. The synthetic data allows a\nqualitative and quantitative evaluation by means of visual inspection and\nHausdorff distance respectively. The real data experiments show the performance\nof the proposal using data acquired by a Primesense Carmine RGB-D sensor. The\nmethod has been compared to several state-of-the-art methods. The ...\n",
    "topics": "{'Object Reconstruction': 0.99954754}",
    "score": 0.8602540069
  },
  {
    "id": "1805.09305",
    "title": "DeepToF: Off-the-Shelf Real-Time Correction of Multipath Interference in\n  Time-of-Flight Imaging",
    "abstract": "  Time-of-flight (ToF) imaging has become a widespread technique for depth\nestimation, allowing affordable off-the-shelf cameras to provide depth maps in\nreal time. However, multipath interference (MPI) resulting from indirect\nillumination significantly degrades the captured depth. Most previous works\nhave tried to solve this problem by means of complex hardware modifications or\ncostly computations. In this work we avoid these approaches, and propose a new\ntechnique that corrects errors in depth caused by MPI that requires no camera\nmodifications, and corrects depth in just 10 milliseconds per frame. By\nobserving that most MPI information can be expressed as a function of the\ncaptured depth, we pose MPI removal as a convolutional approach, and model it\nusing a convolutional neural network. In particular, given that the input and\noutput data present similar structure, we base our network in an autoencoder,\nwhich we train in two stages: first, we use the encoder (convolution filters)\nto learn a suitable basis to represent corrupted range images; then, we train\nthe decoder (deconvolution filters) to correct depth from the learned basis\nfrom synthetically generated scenes. This approach allows us to tackle the lack\nof reference data, by using a large-scale captured training set with corrupted\ndepth to train the encoder, and a smaller synthetic training set with ground\ntruth depth to train the corrector stage of the network, which we generate by\nusing a physically-based, time-resolved rendering. We demonstrate and validate\nour method on both synthetic and real complex scenarios, using an off-the-shelf\nToF camera, and with only the captured incorrect depth as input.\n",
    "topics": "{'Depth Estimation': 0.9696853}",
    "score": 0.8601887519
  },
  {
    "id": "2005.03341",
    "title": "Scene Text Image Super-Resolution in the Wild",
    "abstract": "  Low-resolution text images are often seen in natural scenes such as documents\ncaptured by mobile phones. Recognizing low-resolution text images is\nchallenging because they lose detailed content information, leading to poor\nrecognition accuracy. An intuitive solution is to introduce super-resolution\n(SR) techniques as pre-processing. However, previous single image\nsuper-resolution (SISR) methods are trained on synthetic low-resolution images\n(e.g.Bicubic down-sampling), which is simple and not suitable for real\nlow-resolution text recognition. To this end, we pro-pose a real scene text SR\ndataset, termed TextZoom. It contains paired real low-resolution and\nhigh-resolution images which are captured by cameras with different focal\nlength in the wild. It is more authentic and challenging than synthetic data,\nas shown in Fig. 1. We argue improv-ing the recognition accuracy is the\nultimate goal for Scene Text SR. In this purpose, a new Text Super-Resolution\nNetwork termed TSRN, with three novel modules is developed. (1) A sequential\nresidual block is proposed to extract the sequential information of the text\nimages. (2) A boundary-aware loss is designed to sharpen the character\nboundaries. (3) A central alignment module is proposed to relieve the\nmisalignment problem in TextZoom. Extensive experiments on TextZoom demonstrate\nthat our TSRN largely improves the recognition accuracy by over 13%of CRNN, and\nby nearly 9.0% of ASTER and MORAN compared to synthetic SR data. Furthermore,\nour TSRN clearly outperforms 7 state-of-the-art SR methods in boosting the\nrecognition accuracy of LR images in TextZoom. For example, it outperforms\nLapSRN by over 5% and 8%on the recognition accuracy of ASTER and CRNN. Our\nresults suggest that low-resolution text recognition in the wild is far from\nbeing solved, thus more research effort is needed.\n",
    "topics": "{'Scene Text': 1.0, 'Super Resolution': 0.99999976, 'Image Super-Resolution': 0.99999905, 'Super-Resolution': 0.99999475, 'Scene Text Recognition': 0.97155094}",
    "score": 0.8601639158
  },
  {
    "id": "2008.04488",
    "title": "ARPM-net: A novel CNN-based adversarial method with Markov Random Field\n  enhancement for prostate and organs at risk segmentation in pelvic CT images",
    "abstract": "  Purpose: The research is to develop a novel CNN-based adversarial deep\nlearning method to improve and expedite the multi-organ semantic segmentation\nof CT images, and to generate accurate contours on pelvic CT images. Methods:\nPlanning CT and structure datasets for 120 patients with intact prostate cancer\nwere retrospectively selected and divided for 10-fold cross-validation. The\nproposed adversarial multi-residual multi-scale pooling Markov Random Field\n(MRF) enhanced network (ARPM-net) implements an adversarial training scheme. A\nsegmentation network and a discriminator network were trained jointly, and only\nthe segmentation network was used for prediction. The segmentation network\nintegrates a newly designed MRF block into a variation of multi-residual U-net.\nThe discriminator takes the product of the original CT and the\nprediction/ground-truth as input and classifies the input into fake/real. The\nsegmentation network and discriminator network can be trained jointly as a\nwhole, or the discriminator can be used for fine-tuning after the segmentation\nnetwork is coarsely trained. Multi-scale pooling layers were introduced to\npreserve spatial resolution during pooling using less memory compared to atrous\nconvolution layers. An adaptive loss function was proposed to enhance the\ntraining on small or low contrast organs. The accuracy of modeled contours was\nmeasured with the Dice similarity coefficient (DSC), Average Hausdorff Distance\n(AHD), Average Surface Hausdorff Distance (ASHD), and relative Volume\nDifference (VD) using clinical contours as references to the ground-truth. The\nproposed ARPM-net method was compared to several stateof-the-art deep learning\nmethods.\n",
    "topics": "{'Semantic Segmentation': 0.9879753}",
    "score": 0.8598118777
  },
  {
    "id": "2004.13652",
    "title": "Event-based Robotic Grasping Detection with Neuromorphic Vision Sensor\n  and Event-Stream Dataset",
    "abstract": "  Robotic grasping plays an important role in the field of robotics. The\ncurrent state-of-the-art robotic grasping detection systems are usually built\non the conventional vision, such as RGB-D camera. Compared to traditional\nframe-based computer vision, neuromorphic vision is a small and young community\nof research. Currently, there are limited event-based datasets due to the\ntroublesome annotation of the asynchronous event stream. Annotating large scale\nvision dataset often takes lots of computation resources, especially the\ntroublesome data for video-level annotation. In this work, we consider the\nproblem of detecting robotic grasps in a moving camera view of a scene\ncontaining objects. To obtain more agile robotic perception, a neuromorphic\nvision sensor (DAVIS) attaching to the robot gripper is introduced to explore\nthe potential usage in grasping detection. We construct a robotic grasping\ndataset named Event-Stream Dataset with 91 objects. A spatio-temporal mixed\nparticle filter (SMP Filter) is proposed to track the led-based grasp\nrectangles which enables video-level annotation of a single grasp rectangle per\nobject. As leds blink at high frequency, the Event-Stream dataset is annotated\nin a high frequency of 1 kHz. Based on the Event-Stream dataset, we develop a\ndeep neural network for grasping detection which consider the angle learning\nproblem as classification instead of regression. The method performs high\ndetection accuracy on our Event-Stream dataset with 93% precision at\nobject-wise level. This work provides a large-scale and well-annotated dataset,\nand promotes the neuromorphic vision applications in agile robot.\n",
    "topics": "{'Robotic Grasping': 1.0}",
    "score": 0.8595802768
  },
  {
    "id": "1708.06724",
    "title": "VIGAN: Missing View Imputation with Generative Adversarial Networks",
    "abstract": "  In an era when big data are becoming the norm, there is less concern with the\nquantity but more with the quality and completeness of the data. In many\ndisciplines, data are collected from heterogeneous sources, resulting in\nmulti-view or multi-modal datasets. The missing data problem has been\nchallenging to address in multi-view data analysis. Especially, when certain\nsamples miss an entire view of data, it creates the missing view problem.\nClassic multiple imputations or matrix completion methods are hardly effective\nhere when no information can be based on in the specific view to impute data\nfor such samples. The commonly-used simple method of removing samples with a\nmissing view can dramatically reduce sample size, thus diminishing the\nstatistical power of a subsequent analysis. In this paper, we propose a novel\napproach for view imputation via generative adversarial networks (GANs), which\nwe name by VIGAN. This approach first treats each view as a separate domain and\nidentifies domain-to-domain mappings via a GAN using randomly-sampled data from\neach view, and then employs a multi-modal denoising autoencoder (DAE) to\nreconstruct the missing view from the GAN outputs based on paired data across\nthe views. Then, by optimizing the GAN and DAE jointly, our model enables the\nknowledge integration for domain mappings and view correspondences to\neffectively recover the missing view. Empirical results on benchmark datasets\nvalidate the VIGAN approach by comparing against the state of the art. The\nevaluation of VIGAN in a genetic study of substance use disorders further\nproves the effectiveness and usability of this approach in life science.\n",
    "topics": "{'Imputation': 0.9999682, 'Denoising': 0.42327586, 'Novel View Synthesis': 0.37146673}",
    "score": 0.8590178268
  },
  {
    "id": "2003.03824",
    "title": "No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT\n  Scans by Augmenting with Adversarial Attacks",
    "abstract": "  Detecting malignant pulmonary nodules at an early stage can allow medical\ninterventions which increases the survival rate of lung cancer patients. Using\ncomputer vision techniques to detect nodules can improve the sensitivity and\nthe speed of interpreting chest CT for lung cancer screening. Many studies have\nused CNNs to detect nodule candidates. Though such approaches have been shown\nto outperform the conventional image processing based methods regarding the\ndetection accuracy, CNNs are also known to be limited to generalize on\nunder-represented samples in the training set and prone to imperceptible noise\nperturbations. Such limitations can not be easily addressed by scaling up the\ndataset or the models. In this work, we propose to add adversarial synthetic\nnodules and adversarial attack samples to the training data to improve the\ngeneralization and the robustness of the lung nodule detection systems. In\norder to generate hard examples of nodules from a differentiable nodule\nsynthesizer, we use projected gradient descent (PGD) to search the latent code\nwithin a bounded neighbourhood that would generate nodules to decrease the\ndetector response. To make the network more robust to unanticipated noise\nperturbations, we use PGD to search for noise patterns that can trigger the\nnetwork to give over-confident mistakes. By evaluating on two different\nbenchmark datasets containing consensus annotations from three radiologists, we\nshow that the proposed techniques can improve the detection performance on real\nCT data. To understand the limitations of both the conventional networks and\nthe proposed augmented networks, we also perform stress-tests on the false\npositive reduction networks by feeding different types of artificially produced\npatches. We show that the augmented networks are more robust to both\nunder-represented nodules as well as resistant to noise perturbations.\n",
    "topics": "{'Adversarial Attack': 0.9999894}",
    "score": 0.8590112369
  },
  {
    "id": "1506.03365",
    "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning\n  with Humans in the Loop",
    "abstract": "  While there has been remarkable progress in the performance of visual\nrecognition algorithms, the state-of-the-art models tend to be exceptionally\ndata-hungry. Large labeled training datasets, expensive and tedious to produce,\nare required to optimize millions of parameters in deep network models. Lagging\nbehind the growth in model capacity, the available datasets are quickly\nbecoming outdated in terms of size and density. To circumvent this bottleneck,\nwe propose to amplify human effort through a partially automated labeling\nscheme, leveraging deep learning with humans in the loop. Starting from a large\nset of candidate images for each category, we iteratively sample a subset, ask\npeople to label them, classify the others with a trained model, split the set\ninto positives, negatives, and unlabeled based on the classification\nconfidence, and then iterate with the unlabeled set. To assess the\neffectiveness of this cascading procedure and enable further progress in visual\nrecognition research, we construct a new image dataset, LSUN. It contains\naround one million labeled images for each of 10 scene categories and 20 object\ncategories. We experiment with training popular convolutional networks and find\nthat they achieve substantial performance gains when trained on this dataset.\n",
    "topics": "{}",
    "score": 0.8589306748
  },
  {
    "id": "1511.00561",
    "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image\n  Segmentation",
    "abstract": "  We present a novel and practical deep fully convolutional neural network\narchitecture for semantic pixel-wise segmentation termed SegNet. This core\ntrainable segmentation engine consists of an encoder network, a corresponding\ndecoder network followed by a pixel-wise classification layer. The architecture\nof the encoder network is topologically identical to the 13 convolutional\nlayers in the VGG16 network. The role of the decoder network is to map the low\nresolution encoder feature maps to full input resolution feature maps for\npixel-wise classification. The novelty of SegNet lies is in the manner in which\nthe decoder upsamples its lower resolution input feature map(s). Specifically,\nthe decoder uses pooling indices computed in the max-pooling step of the\ncorresponding encoder to perform non-linear upsampling. This eliminates the\nneed for learning to upsample. The upsampled maps are sparse and are then\nconvolved with trainable filters to produce dense feature maps. We compare our\nproposed architecture with the widely adopted FCN and also with the well known\nDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory\nversus accuracy trade-off involved in achieving good segmentation performance.\n  SegNet was primarily motivated by scene understanding applications. Hence, it\nis designed to be efficient both in terms of memory and computational time\nduring inference. It is also significantly smaller in the number of trainable\nparameters than other competing architectures. We also performed a controlled\nbenchmark of SegNet and other architectures on both road scenes and SUN RGB-D\nindoor scene segmentation tasks. We show that SegNet provides good performance\nwith competitive inference time and more efficient inference memory-wise as\ncompared to other architectures. We also provide a Caffe implementation of\nSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.\n",
    "topics": "{'Scene Understanding': 0.99999905, 'Scene Segmentation': 0.99983823, 'Crowd Counting': 0.99971634, 'Real-Time Semantic Segmentation': 0.99888474, 'Semantic Segmentation': 0.9982426, 'Lesion Segmentation': 0.36880112}",
    "score": 0.8588719298
  },
  {
    "id": "2003.00682",
    "title": "Hybrid Deep Learning for Detecting Lung Diseases from X-ray Images",
    "abstract": "  Lung disease is common throughout the world. These include chronic\nobstructive pulmonary disease, pneumonia, asthma, tuberculosis, fibrosis, etc.\nTimely diagnosis of lung disease is essential. Many image processing and\nmachine learning models have been developed for this purpose. Different forms\nof existing deep learning techniques including convolutional neural network\n(CNN), vanilla neural network, visual geometry group based neural network\n(VGG), and capsule network are applied for lung disease prediction.The basic\nCNN has poor performance for rotated, tilted, or other abnormal image\norientation. Therefore, we propose a new hybrid deep learning framework by\ncombining VGG, data augmentation and spatial transformer network (STN) with\nCNN. This new hybrid method is termed here as VGG Data STN with CNN (VDSNet).\nAs implementation tools, Jupyter Notebook, Tensorflow, and Keras are used. The\nnew model is applied to NIH chest X-ray image dataset collected from Kaggle\nrepository. Full and sample versions of the dataset are considered. For both\nfull and sample datasets, VDSNet outperforms existing methods in terms of a\nnumber of metrics including precision, recall, F0.5 score and validation\naccuracy. For the case of full dataset, VDSNet exhibits a validation accuracy\nof 73%, while vanilla gray, vanilla RGB, hybrid CNN and VGG, and modified\ncapsule network have accuracy values of 67.8%, 69%, 69.5%, 60.5% and 63.8%,\nrespectively. When sample dataset rather than full dataset is used, VDSNet\nrequires much lower training time at the expense of a slightly lower validation\naccuracy. Hence, the proposed VDSNet framework will simplify the detection of\nlung disease for experts as well as for doctors.\n",
    "topics": "{'Data Augmentation': 0.9858409}",
    "score": 0.8588182589
  },
  {
    "id": "1811.00628",
    "title": "Independent Vector Analysis for Data Fusion Prior to Molecular Property\n  Prediction with Machine Learning",
    "abstract": "  Due to its high computational speed and accuracy compared to ab-initio\nquantum chemistry and forcefield modeling, the prediction of molecular\nproperties using machine learning has received great attention in the fields of\nmaterials design and drug discovery. A main ingredient required for machine\nlearning is a training dataset consisting of molecular features\\textemdash for\nexample fingerprint bits, chemical descriptors, etc. that adequately\ncharacterize the corresponding molecules. However, choosing features for any\napplication is highly non-trivial. No \"universal\" method for feature selection\nexists. In this work, we propose a data fusion framework that uses Independent\nVector Analysis to exploit underlying complementary information contained in\ndifferent molecular featurization methods, bringing us a step closer to\nautomated feature generation. Our approach takes an arbitrary number of\nindividual feature vectors and automatically generates a single, compact (low\ndimensional) set of molecular features that can be used to enhance the\nprediction performance of regression models. At the same time our methodology\nretains the possibility of interpreting the generated features to discover\nrelationships between molecular structures and properties. We demonstrate this\non the QM7b dataset for the prediction of several properties such as\natomization energy, polarizability, frontier orbital eigenvalues, ionization\npotential, electron affinity, and excitation energies. In addition, we show how\nour method helps improve the prediction of experimental binding affinities for\na set of human BACE-1 inhibitors. To encourage more widespread use of IVA we\nhave developed the PyIVA Python package, an open source code which is available\nfor download on Github.\n",
    "topics": "{'Drug Discovery': 1.0, 'Feature Selection': 0.9565396}",
    "score": 0.8587865282
  },
  {
    "id": "1705.04932",
    "title": "GeneGAN: Learning Object Transfiguration and Attribute Subspace from\n  Unpaired Data",
    "abstract": "  Object Transfiguration replaces an object in an image with another object\nfrom a second image. For example it can perform tasks like \"putting exactly\nthose eyeglasses from image A on the nose of the person in image B\". Usage of\nexemplar images allows more precise specification of desired modifications and\nimproves the diversity of conditional image generation. However, previous\nmethods that rely on feature space operations, require paired data and/or\nappearance models for training or disentangling objects from background. In\nthis work, we propose a model that can learn object transfiguration from two\nunpaired sets of images: one set containing images that \"have\" that kind of\nobject, and the other set being the opposite, with the mild constraint that the\nobjects be located approximately at the same place. For example, the training\ndata can be one set of reference face images that have eyeglasses, and another\nset of images that have not, both of which spatially aligned by face landmarks.\nDespite the weak 0/1 labels, our model can learn an \"eyeglasses\" subspace that\ncontain multiple representatives of different types of glasses. Consequently,\nwe can perform fine-grained control of generated images, like swapping the\nglasses in two images by swapping the projected components in the \"eyeglasses\"\nsubspace, to create novel images of people wearing eyeglasses.\n  Overall, our deterministic generative model learns disentangled attribute\nsubspaces from weakly labeled data by adversarial training. Experiments on\nCelebA and Multi-PIE datasets validate the effectiveness of the proposed model\non real world data, in generating images with specified eyeglasses, smiling,\nhair styles, and lighting conditions etc. The code is available online.\n",
    "topics": "{'Image Generation': 0.7062619}",
    "score": 0.8587608699
  },
  {
    "id": "1807.04058",
    "title": "Presentation Attack Detection for Cadaver Iris",
    "abstract": "  This paper presents a deep-learning-based method for iris presentation attack\ndetection (PAD) when iris images are obtained from deceased people. Our\napproach is based on the VGG-16 architecture fine-tuned with a database of 574\npost-mortem, near-infrared iris images from the\nWarsaw-BioBase-PostMortem-Iris-v1 database, complemented by a dataset of 256\nimages of live irises, collected within the scope of this study. Experiments\ndescribed in this paper show that our approach is able to correctly classify\niris images as either representing a live or a dead eye in almost 99% of the\ntrials, averaged over 20 subject-disjoint, train/test splits. We also show that\nthe post-mortem iris detection accuracy increases as time since death elapses,\nand that we are able to construct a classification system with\nAPCER=0%@BPCER=1% (Attack Presentation and Bona Fide Presentation\nClassification Error Rates, respectively) when only post-mortem samples\ncollected at least 16 hours post-mortem are considered. Since acquisitions of\nante- and post-mortem samples differ significantly, we applied countermeasures\nto minimize bias in our classification methodology caused by image properties\nthat are not related to the PAD. This included using the same iris sensor in\ncollection of ante- and post-mortem samples, and analysis of class activation\nmaps to ensure that discriminant iris regions utilized by our classifier are\nrelated to properties of the eye, and not to those of the acquisition protocol.\nThis paper offers the first known to us PAD method in a post-mortem setting,\ntogether with an explanation of the decisions made by the convolutional neural\nnetwork. Along with the paper we offer source codes, weights of the trained\nnetwork, and a dataset of live iris images to facilitate reproducibility and\nfurther research.\n",
    "topics": "{'Iris Recognition': 1.0}",
    "score": 0.8587231414
  },
  {
    "id": "1806.05724",
    "title": "Action Learning for 3D Point Cloud Based Organ Segmentation",
    "abstract": "  We propose a novel point cloud based 3D organ segmentation pipeline utilizing\ndeep Q-learning. In order to preserve shape properties, the learning process is\nguided using a statistical shape model. The trained agent directly predicts\npiece-wise linear transformations for all vertices in each iteration. This\nmapping between the ideal transformation for an object outline estimation is\nlearned based on image features. To this end, we introduce aperture features\nthat extract gray values by sampling the 3D volume within the cone centered\naround the associated vertex and its normal vector. Our approach is also\ncapable of estimating a hierarchical pyramid of non rigid deformations for\nmulti-resolution meshes. In the application phase, we use a marginal approach\nto gradually estimate affine as well as non-rigid transformations. We performed\nextensive evaluations to highlight the robust performance of our approach on a\nvariety of challenge data as well as clinical data. Additionally, our method\nhas a run time ranging from 0.3 to 2.7 seconds to segment each organ. In\naddition, we show that the proposed method can be applied to different organs,\nX-ray based modalities, and scanning protocols without the need of transfer\nlearning. As we learn actions, even unseen reference meshes can be processed as\ndemonstrated in an example with the Visible Human. From this we conclude that\nour method is robust, and we believe that our method can be successfully\napplied to many more applications, in particular, in the interventional imaging\nspace.\n",
    "topics": "{'Q-Learning': 0.98154587, 'Transfer Learning': 0.9397628}",
    "score": 0.8581497082
  },
  {
    "id": "1703.07047",
    "title": "High-Resolution Breast Cancer Screening with Multi-View Deep\n  Convolutional Neural Networks",
    "abstract": "  Advances in deep learning for natural images have prompted a surge of\ninterest in applying similar techniques to medical images. The majority of the\ninitial attempts focused on replacing the input of a deep convolutional neural\nnetwork with a medical image, which does not take into consideration the\nfundamental differences between these two types of images. Specifically, fine\ndetails are necessary for detection in medical images, unlike in natural images\nwhere coarse structures matter most. This difference makes it inadequate to use\nthe existing network architectures developed for natural images, because they\nwork on heavily downscaled images to reduce the memory requirements. This hides\ndetails necessary to make accurate predictions. Additionally, a single exam in\nmedical imaging often comes with a set of views which must be fused in order to\nreach a correct conclusion. In our work, we propose to use a multi-view deep\nconvolutional neural network that handles a set of high-resolution medical\nimages. We evaluate it on large-scale mammography-based breast cancer screening\n(BI-RADS prediction) using 886,000 images. We focus on investigating the impact\nof the training set size and image size on the prediction accuracy. Our results\nhighlight that performance increases with the size of training set, and that\nthe best performance can only be achieved using the original resolution. In the\nreader study, performed on a random subset of the test set, we confirmed the\nefficacy of our model, which achieved performance comparable to a committee of\nradiologists when presented with the same data.\n",
    "topics": "{}",
    "score": 0.8581387968
  },
  {
    "id": "1812.00033",
    "title": "Learning from a tiny dataset of manual annotations: a teacher/student\n  approach for surgical phase recognition",
    "abstract": "  Vision algorithms capable of interpreting scenes from a real-time video\nstream are necessary for computer-assisted surgery systems to achieve\ncontext-aware behavior. In laparoscopic procedures one particular algorithm\nneeded for such systems is the identification of surgical phases, for which the\ncurrent state of the art is a model based on a CNN-LSTM. A number of previous\nworks using models of this kind have trained them in a fully supervised manner,\nrequiring a fully annotated dataset. Instead, our work confronts the problem of\nlearning surgical phase recognition in scenarios presenting scarce amounts of\nannotated data (under 25% of all available video recordings). We propose a\nteacher/student type of approach, where a strong predictor called the teacher,\ntrained beforehand on a small dataset of ground truth-annotated videos,\ngenerates synthetic annotations for a larger dataset, which another model - the\nstudent - learns from. In our case, the teacher features a novel CNN-biLSTM-CRF\narchitecture, designed for offline inference only. The student, on the other\nhand, is a CNN-LSTM capable of making real-time predictions. Results for\nvarious amounts of manually annotated videos demonstrate the superiority of the\nnew CNN-biLSTM-CRF predictor as well as improved performance from the CNN-LSTM\ntrained using synthetic labels generated for unannotated videos. For both\noffline and online surgical phase recognition with very few annotated\nrecordings available, this new teacher/student strategy provides a valuable\nperformance improvement by efficiently leveraging the unannotated data.\n",
    "topics": "{'Question Answering': 0.44124493}",
    "score": 0.8579823469
  },
  {
    "id": "1812.11092",
    "title": "Multi-resolution neural networks for tracking seismic horizons from few\n  training images",
    "abstract": "  Detecting a specific horizon in seismic images is a valuable tool for\ngeological interpretation. Because hand-picking the locations of the horizon is\na time-consuming process, automated computational methods were developed\nstarting three decades ago. Older techniques for such picking include\ninterpolation of control points however, in recent years neural networks have\nbeen used for this task. Until now, most networks trained on small patches from\nlarger images. This limits the networks ability to learn from large-scale\ngeologic structures. Moreover, currently available networks and training\nstrategies require label patches that have full and continuous annotations,\nwhich are also time-consuming to generate.\n  We propose a projected loss-function for training convolutional networks with\na multi-resolution structure, including variants of the U-net. Our networks\nlearn from a small number of large seismic images without creating patches. The\nprojected loss-function enables training on labels with just a few annotated\npixels and has no issue with the other unknown label pixels. Training uses all\ndata without reserving some for validation. Only the labels are split into\ntraining/testing. Contrary to other work on horizon tracking, we train the\nnetwork to perform non-linear regression, and not classification. As such, we\npropose labels as the convolution of a Gaussian kernel and the known horizon\nlocations that indicate uncertainty in the labels. The network output is the\nprobability of the horizon location. We demonstrate the proposed computational\ningredients on two different datasets, for horizon extrapolation and\ninterpolation. We show that the predictions of our methodology are accurate\neven in areas far from known horizon locations because our learning strategy\nexploits all data in large seismic images.\n",
    "topics": "{}",
    "score": 0.8578940013
  },
  {
    "id": "1909.04913",
    "title": "Distortion-adaptive Salient Object Detection in 360$^\\circ$\n  Omnidirectional Images",
    "abstract": "  Image-based salient object detection (SOD) has been extensively explored in\nthe past decades. However, SOD on 360$^\\circ$ omnidirectional images is less\nstudied owing to the lack of datasets with pixel-level annotations. Toward this\nend, this paper proposes a 360$^\\circ$ image-based SOD dataset that contains\n500 high-resolution equirectangular images. We collect the representative\nequirectangular images from five mainstream 360$^\\circ$ video datasets and\nmanually annotate all objects and regions over these images with precise masks\nwith a free-viewpoint way. To the best of our knowledge, it is the first public\navailable dataset for salient object detection on 360$^\\circ$ scenes. By\nobserving this dataset, we find that distortion from projection, large-scale\ncomplex scene and small salient objects are the most prominent characteristics.\nInspired by these foundings, this paper proposes a baseline model for SOD on\nequirectangular images. In the proposed approach, we construct a\ndistortion-adaptive module to deal with the distortion caused by the\nequirectangular projection. In addition, a multi-scale contextual integration\nblock is introduced to perceive and distinguish the rich scenes and objects in\nomnidirectional scenes. The whole network is organized in a progressively\nmanner with deep supervision. Experimental results show the proposed baseline\napproach outperforms the top-performanced state-of-the-art methods on\n360$^\\circ$ SOD dataset. Moreover, benchmarking results of the proposed\nbaseline approach and other methods on 360$^\\circ$ SOD dataset show the\nproposed dataset is very challenging, which also validate the usefulness of the\nproposed dataset and approach to boost the development of SOD on 360$^\\circ$\nomnidirectional scenes.\n",
    "topics": "{'RGB Salient Object Detection': 1.0, 'Object Detection': 0.9999658}",
    "score": 0.8578381349
  },
  {
    "id": "1906.06972",
    "title": "EnlightenGAN: Deep Light Enhancement without Paired Supervision",
    "abstract": "  Deep learning-based methods have achieved remarkable success in image\nrestoration and enhancement, but are they still competitive when there is a\nlack of paired training data? As one such example, this paper explores the\nlow-light image enhancement problem, where in practice it is extremely\nchallenging to simultaneously take a low-light and a normal-light photo of the\nsame visual scene. We propose a highly effective unsupervised generative\nadversarial network, dubbed EnlightenGAN, that can be trained without\nlow/normal-light image pairs, yet proves to generalize very well on various\nreal-world test images. Instead of supervising the learning using ground truth\ndata, we propose to regularize the unpaired training using the information\nextracted from the input itself, and benchmark a series of innovations for the\nlow-light image enhancement problem, including a global-local discriminator\nstructure, a self-regularized perceptual loss fusion, and attention mechanism.\nThrough extensive experiments, our proposed approach outperforms recent methods\nunder a variety of metrics in terms of visual quality and subjective user\nstudy. Thanks to the great flexibility brought by unpaired training,\nEnlightenGAN is demonstrated to be easily adaptable to enhancing real-world\nimages from various domains. The code is available at\n\\url{https://github.com/yueruchen/EnlightenGAN}\n",
    "topics": "{'Image Enhancement': 1.0, 'Image Restoration': 0.9999734}",
    "score": 0.8578088681
  },
  {
    "id": "2005.13759",
    "title": "Stereo Vision Based Single-Shot 6D Object Pose Estimation for\n  Bin-Picking by a Robot Manipulator",
    "abstract": "  We propose a fast and accurate method of 6D object pose estimation for\nbin-picking of mechanical parts by a robot manipulator. We extend the\nsingle-shot approach to stereo vision by application of attention architecture.\nOur convolutional neural network model regresses to object locations and\nrotations from either a left image or a right image without depth information.\nThen, a stereo feature matching module, designated as Stereo Grid Attention,\ngenerates stereo grid matching maps. The important point of our method is only\nto calculate disparity of the objects found by the attention from stereo\nimages, instead of calculating a point cloud over the entire image. The\ndisparity value is then used to calculate the depth to the objects by the\nprinciple of triangulation. Our method also achieves a rapid processing speed\nof pose estimation by the single-shot architecture and it is possible to\nprocess a 1024 x 1024 pixels image in 75 milliseconds on the Jetson AGX Xavier\nimplemented with half-float model. Weakly textured mechanical parts are used to\nexemplify the method. First, we create original synthetic datasets for training\nand evaluating of the proposed model. This dataset is created by capturing and\nrendering numerous 3D models of several types of mechanical parts in virtual\nspace. Finally, we use a robotic manipulator with an electromagnetic gripper to\npick up the mechanical parts in a cluttered state to verify the validity of our\nmethod in an actual scene. When a raw stereo image is used by the proposed\nmethod from our stereo camera to detect black steel screws, stainless screws,\nand DC motor parts, i.e., cases, rotor cores and commutator caps, the\nbin-picking tasks are successful with 76.3%, 64.0%, 50.5%, 89.1% and 64.2%\nprobability, respectively.\n",
    "topics": "{'Pose Estimation': 0.99996495, '6D Pose Estimation using RGB': 0.54824984}",
    "score": 0.8574939585
  },
  {
    "id": "2004.02234",
    "title": "Feature Super-Resolution Based Facial Expression Recognition for\n  Multi-scale Low-Resolution Faces",
    "abstract": "  Facial Expressions Recognition(FER) on low-resolution images is necessary for\napplications like group expression recognition in crowd scenarios(station,\nclassroom etc.). Classifying a small size facial image into the right\nexpression category is still a challenging task. The main cause of this problem\nis the loss of discriminative feature due to reduced resolution.\nSuper-resolution method is often used to enhance low-resolution images, but the\nperformance on FER task is limited when on images of very low resolution. In\nthis work, inspired by feature super-resolution methods for object detection,\nwe proposed a novel generative adversary network-based feature level\nsuper-resolution method for robust facial expression recognition(FSR-FER). In\nparticular, a pre-trained FER model was employed as feature extractor, and a\ngenerator network G and a discriminator network D are trained with features\nextracted from images of low resolution and original high resolution. Generator\nnetwork G tries to transform features of low-resolution images to more\ndiscriminative ones by making them closer to the ones of corresponding\nhigh-resolution images. For better classification performance, we also proposed\nan effective classification-aware loss re-weighting strategy based on the\nclassification probability calculated by a fixed FER model to make our model\nfocus more on samples that are easily misclassified. Experiment results on\nReal-World Affective Faces (RAF) Database demonstrate that our method achieves\nsatisfying results on various down-sample factors with a single model and has\nbetter performance on low-resolution images compared with methods using image\nsuper-resolution and expression recognition separately.\n",
    "topics": "{'Super-Resolution': 1.0, 'Super Resolution': 1.0, 'Image Super-Resolution': 0.99985385, 'Facial Expression Recognition': 0.9960509}",
    "score": 0.8574619035
  },
  {
    "id": "1901.00512",
    "title": "Real-Time EEG Classification via Coresets for BCI Applications",
    "abstract": "  A brain-computer interface (BCI) based on the motor imagery (MI) paradigm\ntranslates one's motor intention into a control signal by classifying the\nElectroencephalogram (EEG) signal of different tasks. However, most existing\nsystems either (i) use a high-quality algorithm to train the data off-line and\nrun only classification in real-time, since the off-line algorithm is too slow,\nor (ii) use low-quality heuristics that are sufficiently fast for real-time\ntraining but introduces relatively large classification error. In this work, we\npropose a novel processing pipeline that allows real-time and parallel learning\nof EEG signals using high-quality but possibly inefficient algorithms. This is\ndone by forging a link between BCI and core-sets, a technique that originated\nin computational geometry for handling streaming data via data summarization.\n  We suggest an algorithm that maintains the representation such coreset\ntailored to handle the EEG signal which enables: (i) real time and continuous\ncomputation of the Common Spatial Pattern (CSP) feature extraction method on a\ncoreset representation of the signal (instead on the signal itself) , (ii)\nimprovement of the CSP algorithm efficiency with provable guarantees by\napplying CSP algorithm on the coreset, and (iii) real time addition of the data\ntrials (EEG data windows) to the coreset.\n  For simplicity, we focus on the CSP algorithm, which is a classic algorithm.\nNevertheless, we expect that our coreset will be extended to other algorithms\nin future papers. In the experimental results we show that our system can\nindeed learn EEG signals in real-time for example a 64 channels setup with\nhundreds of time samples per second. Full open source is provided to reproduce\nthe experiment and in the hope that it will be used and extended to more\ncoresets and BCI applications in the future.\n",
    "topics": "{'EEG': 1.0}",
    "score": 0.8570020646
  },
  {
    "id": "1905.06803",
    "title": "How is Gaze Influenced by Image Transformations? Dataset and Model",
    "abstract": "  Data size is the bottleneck for developing deep saliency models, because\ncollecting eye-movement data is very time consuming and expensive. Most of\ncurrent studies on human attention and saliency modeling have used high quality\nstereotype stimuli. In real world, however, captured images undergo various\ntypes of transformations. Can we use these transformations to augment existing\nsaliency datasets? Here, we first create a novel saliency dataset including\nfixations of 10 observers over 1900 images degraded by 19 types of\ntransformations. Second, by analyzing eye movements, we find that observers\nlook at different locations over transformed versus original images. Third, we\nutilize the new data over transformed images, called data augmentation\ntransformation (DAT), to train deep saliency models. We find that label\npreserving DATs with negligible impact on human gaze boost saliency prediction,\nwhereas some other DATs that severely impact human gaze degrade the\nperformance. These label preserving valid augmentation transformations provide\na solution to enlarge existing saliency datasets. Finally, we introduce a novel\nsaliency model based on generative adversarial network (dubbed GazeGAN). A\nmodified UNet is proposed as the generator of the GazeGAN, which combines\nclassic skip connections with a novel center-surround connection (CSC), in\norder to leverage multi level features. We also propose a histogram loss based\non Alternative Chi Square Distance (ACS HistLoss) to refine the saliency map in\nterms of luminance distribution. Extensive experiments and comparisons over 3\ndatasets indicate that GazeGAN achieves the best performance in terms of\npopular saliency evaluation metrics, and is more robust to various\nperturbations. Our code and data are available at:\nhttps://github.com/CZHQuality/Sal-CFS-GAN.\n",
    "topics": "{'Saliency Prediction': 0.9999989, 'Data Augmentation': 0.9882349}",
    "score": 0.8568284425
  },
  {
    "id": "1710.09860",
    "title": "DoShiCo Challenge: Domain Shift in Control Prediction",
    "abstract": "  Training deep neural network policies end-to-end for real-world applications\nso far requires big demonstration datasets in the real world or big sets\nconsisting of a large variety of realistic and closely related 3D CAD models.\nThese real or virtual data should, moreover, have very similar characteristics\nto the conditions expected at test time. These stringent requirements and the\ntime consuming data collection processes that they entail, are currently the\nmost important impediment that keeps deep reinforcement learning from being\ndeployed in real-world applications. Therefore, in this work we advocate an\nalternative approach, where instead of avoiding any domain shift by carefully\nselecting the training data, the goal is to learn a policy that can cope with\nit. To this end, we propose the DoShiCo challenge: to train a model in very\nbasic synthetic environments, far from realistic, in a way that it can be\napplied in more realistic environments as well as take the control decisions on\nreal-world data. In particular, we focus on the task of collision avoidance for\ndrones. We created a set of simulated environments that can be used as\nbenchmark and implemented a baseline method, exploiting depth prediction as an\nauxiliary task to help overcome the domain shift. Even though the policy is\ntrained in very basic environments, it can learn to fly without collisions in a\nvery different realistic simulated environment. Of course several benchmarks\nfor reinforcement learning already exist - but they never include a large\ndomain shift. On the other hand, several benchmarks in computer vision focus on\nthe domain shift, but they take the form of a static datasets instead of\nsimulated environments. In this work we claim that it is crucial to take the\ntwo challenges together in one benchmark.\n",
    "topics": "{}",
    "score": 0.8567959273
  },
  {
    "id": "1910.11006",
    "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale\n  Dataset and Methods Comparison",
    "abstract": "  Vision-based sign language recognition aims at helping deaf people to\ncommunicate with others. However, most existing sign language datasets are\nlimited to a small number of words. Due to the limited vocabulary size, models\nlearned from those datasets cannot be applied in practice. In this paper, we\nintroduce a new large-scale Word-Level American Sign Language (WLASL) video\ndataset, containing more than 2000 words performed by over 100 signers. This\ndataset will be made publicly available to the research community. To our\nknowledge, it is by far the largest public ASL dataset to facilitate word-level\nsign recognition research.\n  Based on this new large-scale dataset, we are able to experiment with several\ndeep learning methods for word-level sign recognition and evaluate their\nperformances in large scale scenarios. Specifically we implement and compare\ntwo different models,i.e., (i) holistic visual appearance-based approach, and\n(ii) 2D human pose based approach. Both models are valuable baselines that will\nbenefit the community for method benchmarking. Moreover, we also propose a\nnovel pose-based temporal graph convolution networks (Pose-TGCN) that models\nspatial and temporal dependencies in human pose trajectories simultaneously,\nwhich has further boosted the performance of the pose-based method. Our results\nshow that pose-based and appearance-based models achieve comparable\nperformances up to 66% at top-10 accuracy on 2,000 words/glosses, demonstrating\nthe validity and challenges of our dataset. Our dataset and baseline deep\nmodels are available at \\url{https://dxli94.github.io/WLASL/}.\n",
    "topics": "{'Sign Language Recognition': 0.9999999}",
    "score": 0.8567726884
  },
  {
    "id": "2009.00584",
    "title": "Quality-aware semi-supervised learning for CMR segmentation",
    "abstract": "  One of the challenges in developing deep learning algorithms for medical\nimage segmentation is the scarcity of annotated training data. To overcome this\nlimitation, data augmentation and semi-supervised learning (SSL) methods have\nbeen developed. However, these methods have limited effectiveness as they\neither exploit the existing data set only (data augmentation) or risk negative\nimpact by adding poor training examples (SSL). Segmentations are rarely the\nfinal product of medical image analysis - they are typically used in downstream\ntasks to infer higher-order patterns to evaluate diseases. Clinicians take into\naccount a wealth of prior knowledge on biophysics and physiology when\nevaluating image analysis results. We have used these clinical assessments in\nprevious works to create robust quality-control (QC) classifiers for automated\ncardiac magnetic resonance (CMR) analysis. In this paper, we propose a novel\nscheme that uses QC of the downstream task to identify high quality outputs of\nCMR segmentation networks, that are subsequently utilised for further network\ntraining. In essence, this provides quality-aware augmentation of training data\nin a variant of SSL for segmentation networks (semiQCSeg). We evaluate our\napproach in two CMR segmentation tasks (aortic and short axis cardiac volume\nsegmentation) using UK Biobank data and two commonly used network architectures\n(U-net and a Fully Convolutional Network) and compare against supervised and\nSSL strategies. We show that semiQCSeg improves training of the segmentation\nnetworks. It decreases the need for labelled data, while outperforming the\nother methods in terms of Dice and clinical metrics. SemiQCSeg can be an\nefficient approach for training segmentation networks for medical image data\nwhen labelled datasets are scarce.\n",
    "topics": "{'Data Augmentation': 0.9999896, 'Medical Image Segmentation': 0.99994767, 'Semantic Segmentation': 0.9915906}",
    "score": 0.8566039264
  },
  {
    "id": "2007.00977",
    "title": "PerceptionGAN: Real-world Image Construction from Provided Text through\n  Perceptual Understanding",
    "abstract": "  Generating an image from a provided descriptive text is quite a challenging\ntask because of the difficulty in incorporating perceptual information (object\nshapes, colors, and their interactions) along with providing high relevancy\nrelated to the provided text. Current methods first generate an initial\nlow-resolution image, which typically has irregular object shapes, colors, and\ninteraction between objects. This initial image is then improved by\nconditioning on the text. However, these methods mainly address the problem of\nusing text representation efficiently in the refinement of the initially\ngenerated image, while the success of this refinement process depends heavily\non the quality of the initially generated image, as pointed out in the DM-GAN\npaper. Hence, we propose a method to provide good initialized images by\nincorporating perceptual understanding in the discriminator module. We improve\nthe perceptual information at the first stage itself, which results in\nsignificant improvement in the final generated image. In this paper, we have\napplied our approach to the novel StackGAN architecture. We then show that the\nperceptual information included in the initial image is improved while modeling\nimage distribution at multiple stages. Finally, we generated realistic\nmulti-colored images conditioned by text. These images have good quality along\nwith containing improved basic perceptual information. More importantly, the\nproposed method can be integrated into the pipeline of other state-of-the-art\ntext-based-image-generation models to generate initial low-resolution images.\nWe also worked on improving the refinement process in StackGAN by augmenting\nthe third stage of the generator-discriminator pair in the StackGAN\narchitecture. Our experimental analysis and comparison with the\nstate-of-the-art on a large but sparse dataset MS COCO further validate the\nusefulness of our proposed approach.\n",
    "topics": "{'Image Generation': 0.9112545}",
    "score": 0.8565598967
  },
  {
    "id": "2004.08522",
    "title": "Super-Resolution-based Snake Model -- An Unsupervised Method for\n  Large-Scale Building Extraction using Airborne LiDAR Data and Optical Image",
    "abstract": "  Automatic extraction of buildings in urban and residential scenes has become\na subject of growing interest in the domain of photogrammetry and remote\nsensing, particularly since mid-1990s. Active contour model, colloquially known\nas snake model, has been studied to extract buildings from aerial and satellite\nimagery. However, this task is still very challenging due to the complexity of\nbuilding size, shape, and its surrounding environment. This complexity leads to\na major obstacle for carrying out a reliable large-scale building extraction,\nsince the involved prior information and assumptions on building such as shape,\nsize, and color cannot be generalized over large areas. This paper presents an\nefficient snake model to overcome such challenge, called Super-Resolution-based\nSnake Model (SRSM). The SRSM operates on high-resolution LiDAR-based elevation\nimages -- called z-images -- generated by a super-resolution process applied to\nLiDAR data. The involved balloon force model is also improved to shrink or\ninflate adaptively, instead of inflating the snake continuously. This method is\napplicable for a large scale such as city scale and even larger, while having a\nhigh level of automation and not requiring any prior knowledge nor training\ndata from the urban scenes (hence unsupervised). It achieves high overall\naccuracy when tested on various datasets. For instance, the proposed SRSM\nyields an average area-based Quality of 86.57% and object-based Quality of\n81.60% on the ISPRS Vaihingen benchmark datasets. Compared to other methods\nusing this benchmark dataset, this level of accuracy is highly desirable even\nfor a supervised method. Similarly desirable outcomes are obtained when\ncarrying out the proposed SRSM on the whole City of Quebec (total area of 656\nkm2), yielding an area-based Quality of 62.37% and an object-based Quality of\n63.21%.\n",
    "topics": "{'Super-Resolution': 0.95962316, 'Super Resolution': 0.82242507}",
    "score": 0.8565155545
  },
  {
    "id": "2007.04645",
    "title": "Learning to Switch CNNs with Model Agnostic Meta Learning for Fine\n  Precision Visual Servoing",
    "abstract": "  Convolutional Neural Networks (CNNs) have been successfully applied for\nrelative camera pose estimation from labeled image-pair data, without requiring\nany hand-engineered features, camera intrinsic parameters or depth information.\nThe trained CNN can be utilized for performing pose based visual servo control\n(PBVS). One of the ways to improve the quality of visual servo output is to\nimprove the accuracy of the CNN for estimating the relative pose estimation.\nWith a given state-of-the-art CNN for relative pose regression, how can we\nachieve an improved performance for visual servo control? In this paper, we\nexplore switching of CNNs to improve the precision of visual servo control. The\nidea of switching a CNN is due to the fact that the dataset for training a\nrelative camera pose regressor for visual servo control must contain variations\nin relative pose ranging from a very small scale to eventually a larger scale.\nWe found that, training two different instances of the CNN, one for\nlarge-scale-displacements (LSD) and another for small-scale-displacements (SSD)\nand switching them during the visual servo execution yields better results than\ntraining a single CNN with the combined LSD+SSD data. However, it causes extra\nstorage overhead and switching decision is taken by a manually set threshold\nwhich may not be optimal for all the scenes. To eliminate these drawbacks, we\npropose an efficient switching strategy based on model agnostic meta learning\n(MAML) algorithm. In this, a single model is trained to learn parameters which\nare simultaneously good for multiple tasks, namely a binary classification for\nswitching decision, a 6DOF pose regression for LSD data and also a 6DOF pose\nregression for SSD data. The proposed approach performs far better than the\nnaive approach, while storage and run-time overheads are almost negligible.\n",
    "topics": "{'Meta-Learning': 0.9988513, 'Pose Estimation': 0.9986155}",
    "score": 0.8564464942
  },
  {
    "id": "1808.06452",
    "title": "Reproducible evaluation of classification methods in Alzheimer's\n  disease: framework and application to MRI and PET data",
    "abstract": "  A large number of papers have introduced novel machine learning and feature\nextraction methods for automatic classification of AD. However, they are\ndifficult to reproduce because key components of the validation are often not\nreadily available. These components include selected participants and input\ndata, image preprocessing and cross-validation procedures. The performance of\nthe different approaches is also difficult to compare objectively. In\nparticular, it is often difficult to assess which part of the method provides a\nreal improvement, if any. We propose a framework for reproducible and objective\nclassification experiments in AD using three publicly available datasets (ADNI,\nAIBL and OASIS). The framework comprises: i) automatic conversion of the three\ndatasets into BIDS format, ii) a modular set of preprocessing pipelines,\nfeature extraction and classification methods, together with an evaluation\nframework, that provide a baseline for benchmarking the different components.\nWe demonstrate the use of the framework for a large-scale evaluation on 1960\nparticipants using T1 MRI and FDG PET data. In this evaluation, we assess the\ninfluence of different modalities, preprocessing, feature types, classifiers,\ntraining set sizes and datasets. Performances were in line with the\nstate-of-the-art. FDG PET outperformed T1 MRI for all classification tasks. No\ndifference in performance was found for the use of different atlases, image\nsmoothing, partial volume correction of FDG PET images, or feature type. Linear\nSVM and L2-logistic regression resulted in similar performance and both\noutperformed random forests. The classification performance increased along\nwith the number of subjects used for training. Classifiers trained on ADNI\ngeneralized well to AIBL and OASIS. All the code of the framework and the\nexperiments is publicly available at:\nhttps://gitlab.icm-institute.org/aramislab/AD-ML.\n",
    "topics": "{}",
    "score": 0.8562871445
  },
  {
    "id": "1811.00656",
    "title": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
    "abstract": "  In this work, we describe a new deep learning based method that can\neffectively distinguish AI-generated fake videos (referred to as {\\em DeepFake}\nvideos hereafter) from real videos. Our method is based on the observations\nthat current DeepFake algorithm can only generate images of limited\nresolutions, which need to be further warped to match the original faces in the\nsource video. Such transforms leave distinctive artifacts in the resulting\nDeepFake videos, and we show that they can be effectively captured by\nconvolutional neural networks (CNNs). Compared to previous methods which use a\nlarge amount of real and DeepFake generated images to train CNN classifier, our\nmethod does not need DeepFake generated images as negative training examples\nsince we target the artifacts in affine face warping as the distinctive feature\nto distinguish real and fake images. The advantages of our method are two-fold:\n(1) Such artifacts can be simulated directly using simple image processing\noperations on a image to make it as negative example. Since training a DeepFake\nmodel to generate negative examples is time-consuming and resource-demanding,\nour method saves a plenty of time and resources in training data collection;\n(2) Since such artifacts are general existed in DeepFake videos from different\nsources, our method is more robust compared to others. Our method is evaluated\non two sets of DeepFake video datasets for its effectiveness in practice.\n",
    "topics": "{'Face Swapping': 0.99980706}",
    "score": 0.8561548708
  },
  {
    "id": "2005.01577",
    "title": "COVID-DA: Deep Domain Adaptation from Typical Pneumonia to COVID-19",
    "abstract": "  The outbreak of novel coronavirus disease 2019 (COVID-19) has already\ninfected millions of people and is still rapidly spreading all over the globe.\nMost COVID-19 patients suffer from lung infection, so one important diagnostic\nmethod is to screen chest radiography images, e.g., X-Ray or CT images.\nHowever, such examinations are time-consuming and labor-intensive, leading to\nlimited diagnostic efficiency. To solve this issue, AI-based technologies, such\nas deep learning, have been used recently as effective computer-aided means to\nimprove diagnostic efficiency. However, one practical and critical difficulty\nis the limited availability of annotated COVID-19 data, due to the prohibitive\nannotation costs and urgent work of doctors to fight against the pandemic. This\nmakes the learning of deep diagnosis models very challenging. To address this,\nmotivated by that typical pneumonia has similar characteristics with COVID-19\nand many pneumonia datasets are publicly available, we propose to conduct\ndomain knowledge adaptation from typical pneumonia to COVID-19. There are two\nmain challenges: 1) the discrepancy of data distributions between domains; 2)\nthe task difference between the diagnosis of typical pneumonia and COVID-19. To\naddress them, we propose a new deep domain adaptation method for COVID-19\ndiagnosis, namely COVID-DA. Specifically, we alleviate the domain discrepancy\nvia feature adversarial adaptation and handle the task difference issue via a\nnovel classifier separation scheme. In this way, COVID-DA is able to diagnose\nCOVID-19 effectively with only a small number of COVID-19 annotations.\nExtensive experiments verify the effectiveness of COVID-DA and its great\npotential for real-world applications.\n",
    "topics": "{'COVID-19 Diagnosis': 0.99999917, 'Domain Adaptation': 0.9901438}",
    "score": 0.8561510364
  },
  {
    "id": "1710.07110",
    "title": "Meta-Learning via Feature-Label Memory Network",
    "abstract": "  Deep learning typically requires training a very capable architecture using\nlarge datasets. However, many important learning problems demand an ability to\ndraw valid inferences from small size datasets, and such problems pose a\nparticular challenge for deep learning. In this regard, various researches on\n\"meta-learning\" are being actively conducted. Recent work has suggested a\nMemory Augmented Neural Network (MANN) for meta-learning. MANN is an\nimplementation of a Neural Turing Machine (NTM) with the ability to rapidly\nassimilate new data in its memory, and use this data to make accurate\npredictions. In models such as MANN, the input data samples and their\nappropriate labels from previous step are bound together in the same memory\nlocations. This often leads to memory interference when performing a task as\nthese models have to retrieve a feature of an input from a certain memory\nlocation and read only the label information bound to that location. In this\npaper, we tried to address this issue by presenting a more robust MANN. We\nrevisited the idea of meta-learning and proposed a new memory augmented neural\nnetwork by explicitly splitting the external memory into feature and label\nmemories. The feature memory is used to store the features of input data\nsamples and the label memory stores their labels. Hence, when predicting the\nlabel of a given input, our model uses its feature memory unit as a reference\nto extract the stored feature of the input, and based on that feature, it\nretrieves the label information of the input from the label memory unit. In\norder for the network to function in this framework, a new memory-writingmodule\nto encode label information into the label memory in accordance with the\nmeta-learning task structure is designed. Here, we demonstrate that our model\noutperforms MANN by a large margin in supervised one-shot classification tasks\nusing Omniglot and MNIST datasets.\n",
    "topics": "{'Meta-Learning': 1.0, 'Omniglot': 0.9995938}",
    "score": 0.8558846975
  },
  {
    "id": "2006.04868",
    "title": "DoubleU-Net: A Deep Convolutional Neural Network for Medical Image\n  Segmentation",
    "abstract": "  Semantic image segmentation is the process of labeling each pixel of an image\nwith its corresponding class. An encoder-decoder based approach, like U-Net and\nits variants, is a popular strategy for solving medical image segmentation\ntasks. To improve the performance of U-Net on various segmentation tasks, we\npropose a novel architecture called DoubleU-Net, which is a combination of two\nU-Net architectures stacked on top of each other. The first U-Net uses a\npre-trained VGG-19 as the encoder, which has already learned features from\nImageNet and can be transferred to another task easily. To capture more\nsemantic information efficiently, we added another U-Net at the bottom. We also\nadopt Atrous Spatial Pyramid Pooling (ASPP) to capture contextual information\nwithin the network. We have evaluated DoubleU-Net using four medical\nsegmentation datasets, covering various imaging modalities such as colonoscopy,\ndermoscopy, and microscopy. Experiments on the MICCAI 2015 segmentation\nchallenge, the CVC-ClinicDB, the 2018 Data Science Bowl challenge, and the\nLesion boundary segmentation datasets demonstrate that the DoubleU-Net\noutperforms U-Net and the baseline models. Moreover, DoubleU-Net produces more\naccurate segmentation masks, especially in the case of the CVC-ClinicDB and\nMICCAI 2015 segmentation challenge datasets, which have challenging images such\nas smaller and flat polyps. These results show the improvement over the\nexisting U-Net model. The encouraging results, produced on various medical\nimage segmentation datasets, show that DoubleU-Net can be used as a strong\nbaseline for both medical image segmentation and cross-dataset evaluation\ntesting to measure the generalizability of Deep Learning (DL) models.\n",
    "topics": "{'Semantic Segmentation': 1.0, 'Medical Image Segmentation': 1.0, 'Lesion Segmentation': 0.6783763}",
    "score": 0.8557971753
  },
  {
    "id": "2009.11429",
    "title": "Automatic identification of fossils and abiotic grains during carbonate\n  microfacies analysis using deep convolutional neural networks",
    "abstract": "  Petrographic analysis based on microfacies identification in thin sections is\nwidely used in sedimentary environment interpretation and paleoecological\nreconstruction. Fossil recognition from microfacies is an essential procedure\nfor petrographers to complete this task. Distinguishing the morphological and\nmicrostructural diversity of skeletal fragments requires extensive prior\nknowledge of fossil morphotypes in microfacies and long training sessions under\nthe microscope. This requirement engenders certain challenges for\nsedimentologists and paleontologists, especially novices. However, a machine\nclassifier can help address this challenge. We collected a microfacies image\ndataset comprising both public data from 1,149 references and our own materials\n(including a total of 30,815 images of 22 fossil and abiotic grain groups). We\nemployed a high-performance workstation to implement four classic deep\nconvolutional neural networks (DCNNs), which have proven to be highly efficient\nin computer vision over the last several years. Our framework uses a transfer\nlearning technique, which reuses the pre-trained parameters that are trained on\na larger ImageNet dataset as initialization for the network to achieve high\naccuracy with low computing costs. We obtained up to 95% of the top one and 99%\nof the top three test accuracies in the Inception ResNet v2 architecture. The\nmachine classifier exhibited 0.99 precision on minerals, such as dolomite and\npyrite. Although it had some difficulty on samples having similar morphologies,\nsuch as the bivalve, brachiopod, and ostracod, it nevertheless obtained 0.88\nprecision. Our machine learning framework demonstrated high accuracy with\nreproducibility and bias avoidance that was comparable to those of human\nclassifiers. Its application can thus eliminate much of the tedious, manually\nintensive efforts by human experts conducting routine identification.\n",
    "topics": "{'Transfer Learning': 0.7232882}",
    "score": 0.8557726765
  },
  {
    "id": "1905.01102",
    "title": "Generating Classification Weights with GNN Denoising Autoencoders for\n  Few-Shot Learning",
    "abstract": "  Given an initial recognition model already trained on a set of base classes,\nthe goal of this work is to develop a meta-model for few-shot learning. The\nmeta-model, given as input some novel classes with few training examples per\nclass, must properly adapt the existing recognition model into a new model that\ncan correctly classify in a unified way both the novel and the base classes. To\naccomplish this goal it must learn to output the appropriate classification\nweight vectors for those two types of classes. To build our meta-model we make\nuse of two main innovations: we propose the use of a Denoising Autoencoder\nnetwork (DAE) that (during training) takes as input a set of classification\nweights corrupted with Gaussian noise and learns to reconstruct the\ntarget-discriminative classification weights. In this case, the injected noise\non the classification weights serves the role of regularizing the weight\ngenerating meta-model. Furthermore, in order to capture the co-dependencies\nbetween different classes in a given task instance of our meta-model, we\npropose to implement the DAE model as a Graph Neural Network (GNN). In order to\nverify the efficacy of our approach, we extensively evaluate it on ImageNet\nbased few-shot benchmarks and we report strong results that surpass prior\napproaches. The code and models of our paper will be published on:\nhttps://github.com/gidariss/wDAE_GNN_FewShot\n",
    "topics": "{'Few-Shot Learning': 1.0, 'Denoising': 0.999985}",
    "score": 0.8555217744
  },
  {
    "id": "1912.06540",
    "title": "CIS-Net: A Novel CNN Model for Spatial Image Steganalysis via Cover\n  Image Suppression",
    "abstract": "  Image steganalysis is a special binary classification problem that aims to\nclassify natural cover images and suspected stego images which are the results\nof embedding very weak secret message signals into covers. How to effectively\nsuppress cover image content and thus make the classification of cover images\nand stego images easier is the key of this task. Recent researches show that\nConvolutional Neural Networks (CNN) are very effective to detect steganography\nby learning discriminative features between cover images and their stegos.\nSeveral deep CNN models have been proposed via incorporating domain knowledge\nof image steganography/steganalysis into the design of the network and achieve\nstate of the art performance on standard database. Following such direction, we\npropose a novel model called Cover Image Suppression Network (CIS-Net), which\nimproves the performance of spatial image steganalysis by suppressing cover\nimage content as much as possible in model learning. Two novel layers, the\nSingle-value Truncation Layer (STL) and Sub-linear Pooling Layer (SPL), are\nproposed in this work. Specifically, STL truncates input values into a same\nthreshold when they are out of a predefined interval. Theoretically, we have\nproved that STL can reduce the variance of input feature map without\ndeteriorating useful information. For SPL, it utilizes sub-linear power\nfunction to suppress large valued elements introduced by cover image contents\nand aggregates weak embedded signals via average pooling. Extensive experiments\ndemonstrate the proposed network equipped with STL and SPL achieves better\nperformance than rich model classifiers and existing CNN models on challenging\nsteganographic algorithms.\n",
    "topics": "{'Temporal Action Localization': 0.46680453}",
    "score": 0.8554228504
  },
  {
    "id": "2003.01874",
    "title": "A Deep Learning Method for Complex Human Activity Recognition Using\n  Virtual Wearable Sensors",
    "abstract": "  Sensor-based human activity recognition (HAR) is now a research hotspot in\nmultiple application areas. With the rise of smart wearable devices equipped\nwith inertial measurement units (IMUs), researchers begin to utilize IMU data\nfor HAR. By employing machine learning algorithms, early IMU-based research for\nHAR can achieve accurate classification results on traditional classical HAR\ndatasets, containing only simple and repetitive daily activities. However,\nthese datasets rarely display a rich diversity of information in real-scene. In\nthis paper, we propose a novel method based on deep learning for complex HAR in\nthe real-scene. Specially, in the off-line training stage, the AMASS dataset,\ncontaining abundant human poses and virtual IMU data, is innovatively adopted\nfor enhancing the variety and diversity. Moreover, a deep convolutional neural\nnetwork with an unsupervised penalty is proposed to automatically extract the\nfeatures of AMASS and improve the robustness. In the on-line testing stage, by\nleveraging advantages of the transfer learning, we obtain the final result by\nfine-tuning the partial neural network (optimizing the parameters in the\nfully-connected layers) using the real IMU data. The experimental results show\nthat the proposed method can surprisingly converge in a few iterations and\nachieve an accuracy of 91.15% on a real IMU dataset, demonstrating the\nefficiency and effectiveness of the proposed method.\n",
    "topics": "{'Activity Recognition': 1.0, 'Transfer Learning': 0.91661775}",
    "score": 0.8554182482
  },
  {
    "id": "2005.01468",
    "title": "A cascade network for Detecting COVID-19 using chest x-rays",
    "abstract": "  The worldwide spread of pneumonia caused by a novel coronavirus poses an\nunprecedented challenge to the world's medical resources and prevention and\ncontrol measures. Covid-19 attacks not only the lungs, making it difficult to\nbreathe and life-threatening, but also the heart, kidneys, brain and other\nvital organs of the body, with possible sequela. At present, the detection of\nCOVID-19 needs to be realized by the reverse transcription-polymerase Chain\nReaction (RT-PCR). However, many countries are in the outbreak period of the\nepidemic, and the medical resources are very limited. They cannot provide\nsufficient numbers of gene sequence detection, and many patients may not be\nisolated and treated in time. Given this situation, we researched the\nanalytical and diagnostic capabilities of deep learning on chest radiographs\nand proposed Cascade-SEMEnet which is cascaded with SEME-ResNet50 and\nSEME-DenseNet169. The two cascade networks of Cascade - SEMEnet both adopt\nlarge input sizes and SE-Structure and use MoEx and histogram equalization to\nenhance the data. We first used SEME-ResNet50 to screen chest X-ray and\ndiagnosed three classes: normal, bacterial, and viral pneumonia. Then we used\nSEME-DenseNet169 for fine-grained classification of viral pneumonia and\ndetermined if it is caused by COVID-19. To exclude the influence of\nnon-pathological features on the network, we preprocessed the data with U-Net\nduring the training of SEME-DenseNet169. The results showed that our network\nachieved an accuracy of 85.6\\% in determining the type of pneumonia infection\nand 97.1\\% in the fine-grained classification of COVID-19. We used Grad-CAM to\nvisualize the judgment based on the model and help doctors understand the chest\nradiograph while verifying the effectivene.\n",
    "topics": "{'COVID-19 Diagnosis': 0.8461884}",
    "score": 0.8553916124
  },
  {
    "id": "1904.02768",
    "title": "Biometric Fish Classification of Temperate Species Using Convolutional\n  Neural Network with Squeeze-and-Excitation",
    "abstract": "  Our understanding and ability to effectively monitor and manage coastal\necosystems are severely limited by observation methods. Automatic recognition\nof species in natural environment is a promising tool which would revolutionize\nvideo and image analysis for a wide range of applications in marine ecology.\nHowever, classifying fish from images captured by underwater cameras is in\ngeneral very challenging due to noise and illumination variations in water.\nPrevious classification methods in the literature relies on filtering the\nimages to separate the fish from the background or sharpening the images by\nremoving background noise. This pre-filtering process may negatively impact the\nclassification accuracy. In this work, we propose a Convolutional Neural\nNetwork (CNN) using the Squeeze-and-Excitation (SE) architecture for\nclassifying images of fish without pre-filtering. Different from conventional\nschemes, this scheme is divided into two steps. The first step is to train the\nfish classifier via a public data set, i.e., Fish4Knowledge, without using\nimage augmentation, named as pre-training. The second step is to train the\nclassifier based on a new data set consisting of species that we are interested\nin for classification, named as post-training. The weights obtained from\npre-training are applied to post-training as a priori. This is also known as\ntransfer learning. Our solution achieves the state-of-the-art accuracy of\n99.27% accuracy on the pre-training. The accuracy on the post-training is\n83.68%. Experiments on the post-training with image augmentation yields an\naccuracy of 87.74%, indicating that the solution is viable with a larger data\nset.\n",
    "topics": "{'Image Augmentation': 0.99998546, 'Transfer Learning': 0.959244}",
    "score": 0.8553714536
  },
  {
    "id": "2006.07827",
    "title": "PCAAE: Principal Component Analysis Autoencoder for organising the\n  latent space of generative networks",
    "abstract": "  Autoencoders and generative models produce some of the most spectacular deep\nlearning results to date. However, understanding and controlling the latent\nspace of these models presents a considerable challenge. Drawing inspiration\nfrom principal component analysis and autoencoder, we propose the Principal\nComponent Analysis Autoencoder (PCAAE). This is a novel autoencoder whose\nlatent space verifies two properties. Firstly, the dimensions are organised in\ndecreasing importance with respect to the data at hand. Secondly, the\ncomponents of the latent space are statistically independent. We achieve this\nby progressively increasing the latent space during training, and with a\ncovariance loss applied to the latent codes. The resulting autoencoder produces\na latent space which separates the intrinsic attributes of the data into\ndifferent components of the latent space, in a completely unsupervised manner.\nWe also describe an extension of our approach to the case of powerful,\npre-trained GANs. We show results on both synthetic examples of shapes and on a\nstate-of-the-art GAN. For example, we are able to separate the color shade\nscale of hair and skin, pose of faces and the gender in the CelebA, without\naccessing any labels. We compare the PCAAE with other state-of-the-art\napproaches, in particular with respect to the ability to disentangle attributes\nin the latent space. We hope that this approach will contribute to better\nunderstanding of the intrinsic latent spaces of powerful deep generative\nmodels.\n",
    "topics": "{}",
    "score": 0.8551830839
  },
  {
    "id": "1910.05909",
    "title": "Density-Aware Convolutional Networks with Context Encoding for Airborne\n  LiDAR Point Cloud Classification",
    "abstract": "  To better address challenging issues of the irregularity and inhomogeneity\ninherently present in 3D point clouds, researchers have been shifting their\nfocus from the design of hand-craft point feature towards the learning of 3D\npoint signatures using deep neural networks for 3D point cloud classification.\nRecent proposed deep learning based point cloud classification methods either\napply 2D CNN on projected feature images or apply 1D convolutional layers\ndirectly on raw point sets. These methods cannot adequately recognize\nfine-grained local structures caused by the uneven density distribution of the\npoint cloud data. In this paper, to address this challenging issue, we\nintroduced a density-aware convolution module which uses the point-wise density\nto re-weight the learnable weights of convolution kernels. The proposed\nconvolution module is able to fully approximate the 3D continuous convolution\non unevenly distributed 3D point sets. Based on this convolution module, we\nfurther developed a multi-scale fully convolutional neural network with\ndownsampling and upsampling blocks to enable hierarchical point feature\nlearning. In addition, to regularize the global semantic context, we\nimplemented a context encoding module to predict a global context encoding and\nformulated a context encoding regularizer to enforce the predicted context\nencoding to be aligned with the ground truth one. The overall network can be\ntrained in an end-to-end fashion with the raw 3D coordinates as well as the\nheight above ground as inputs. Experiments on the International Society for\nPhotogrammetry and Remote Sensing (ISPRS) 3D labeling benchmark demonstrated\nthe superiority of the proposed method for point cloud classification. Our\nmodel achieved a new state-of-the-art performance with an average F1 score of\n71.2% and improved the performance by a large margin on several categories.\n",
    "topics": "{}",
    "score": 0.855043988
  },
  {
    "id": "1907.06922",
    "title": "Human Pose Estimation for Real-World Crowded Scenarios",
    "abstract": "  Human pose estimation has recently made significant progress with the\nadoption of deep convolutional neural networks. Its many applications have\nattracted tremendous interest in recent years. However, many practical\napplications require pose estimation for human crowds, which still is a rarely\naddressed problem. In this work, we explore methods to optimize pose estimation\nfor human crowds, focusing on challenges introduced with dense crowds, such as\nocclusions, people in close proximity to each other, and partial visibility of\npeople. In order to address these challenges, we evaluate three aspects of a\npose detection approach: i) a data augmentation method to introduce robustness\nto occlusions, ii) the explicit detection of occluded body parts, and iii) the\nuse of the synthetic generated datasets. The first approach to improve the\naccuracy in crowded scenarios is to generate occlusions at training time using\nperson and object cutouts from the object recognition dataset COCO (Common\nObjects in Context). Furthermore, the synthetically generated dataset JTA\n(Joint Track Auto) is evaluated for the use in real-world crowd applications.\nIn order to overcome the transfer gap of JTA originating from a low pose\nvariety and less dense crowds, an extension dataset is created to ease the use\nfor real-world applications. Additionally, the occlusion flags provided with\nJTA are utilized to train a model, which explicitly distinguishes between\noccluded and visible body parts in two distinct branches. The combination of\nthe proposed additions to the baseline method help to improve the overall\naccuracy by 4.7% AP and thereby provide comparable results to current\nstate-of-the-art approaches on the respective dataset.\n",
    "topics": "{'Pose Estimation': 1.0, 'Data Augmentation': 0.9959117, 'Object Recognition': 0.98845005}",
    "score": 0.8548486606
  },
  {
    "id": "1901.11369",
    "title": "Cross-modality (CT-MRI) prior augmented deep learning for robust lung\n  tumor segmentation from small MR datasets",
    "abstract": "  Lack of large expert annotated MR datasets makes training deep learning\nmodels difficult. Therefore, a cross-modality (MR-CT) deep learning\nsegmentation approach that augments training data using pseudo MR images\nproduced by transforming expert-segmented CT images was developed. Eighty-One\nT2-weighted MRI scans from 28 patients with non-small cell lung cancers were\nanalyzed. Cross-modality prior encoding the transformation of CT to pseudo MR\nimages resembling T2w MRI was learned as a generative adversarial deep learning\nmodel. This model augmented training data arising from 6 expert-segmented T2w\nMR patient scans with 377 pseudo MRI from non-small cell lung cancer CT patient\nscans with obtained from the Cancer Imaging Archive. A two-dimensional Unet\nimplemented with batch normalization was trained to segment the tumors from T2w\nMRI. This method was benchmarked against (a) standard data augmentation and two\nstate-of-the art cross-modality pseudo MR-based augmentation and (b) two\nsegmentation networks. Segmentation accuracy was computed using Dice similarity\ncoefficient (DSC), Hausdroff distance metrics, and volume ratio. The proposed\napproach produced the lowest statistical variability in the intensity\ndistribution between pseudo and T2w MR images measured as Kullback-Leibler\ndivergence of 0.069. This method produced the highest segmentation accuracy\nwith a DSC of 0.75 and the lowest Hausdroff distance on the test dataset. This\napproach produced highly similar estimations of tumor growth as an expert (P =\n0.37). A novel deep learning MR segmentation was developed that overcomes the\nlimitation of learning robust models from small datasets by leveraging learned\ncross-modality priors to augment training. The results show the feasibility of\nthe approach and the corresponding improvement over the state-of-the-art\nmethods.\n",
    "topics": "{'Tumor Segmentation': 0.9999937, 'Data Augmentation': 0.9903442}",
    "score": 0.854847769
  },
  {
    "id": "2002.09635",
    "title": "Towards Label-Free 3D Segmentation of Optical Coherence Tomography\n  Images of the Optic Nerve Head Using Deep Learning",
    "abstract": "  Since the introduction of optical coherence tomography (OCT), it has been\npossible to study the complex 3D morphological changes of the optic nerve head\n(ONH) tissues that occur along with the progression of glaucoma. Although\nseveral deep learning (DL) techniques have been recently proposed for the\nautomated extraction (segmentation) and quantification of these morphological\nchanges, the device specific nature and the difficulty in preparing manual\nsegmentations (training data) limit their clinical adoption. With several new\nmanufacturers and next-generation OCT devices entering the market, the\ncomplexity in deploying DL algorithms clinically is only increasing. To address\nthis, we propose a DL based 3D segmentation framework that is easily\ntranslatable across OCT devices in a label-free manner (i.e. without the need\nto manually re-segment data for each device). Specifically, we developed 2 sets\nof DL networks. The first (referred to as the enhancer) was able to enhance OCT\nimage quality from 3 OCT devices, and harmonized image-characteristics across\nthese devices. The second performed 3D segmentation of 6 important ONH tissue\nlayers. We found that the use of the enhancer was critical for our segmentation\nnetwork to achieve device independency. In other words, our 3D segmentation\nnetwork trained on any of 3 devices successfully segmented ONH tissue layers\nfrom the other two devices with high performance (Dice coefficients > 0.92).\nWith such an approach, we could automatically segment images from new OCT\ndevices without ever needing manual segmentation data from such devices.\n",
    "topics": "{}",
    "score": 0.8544429042
  },
  {
    "id": "1910.12574",
    "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in\n  Online Social Media",
    "abstract": "  Generated hateful and toxic content by a portion of users in social media is\na rising phenomenon that motivated researchers to dedicate substantial efforts\nto the challenging direction of hateful content identification. We not only\nneed an efficient automatic hate speech detection model based on advanced\nmachine learning and natural language processing, but also a sufficiently large\namount of annotated data to train a model. The lack of a sufficient amount of\nlabelled hate speech data, along with the existing biases, has been the main\nissue in this domain of research. To address these needs, in this study we\nintroduce a novel transfer learning approach based on an existing pre-trained\nlanguage model called BERT (Bidirectional Encoder Representations from\nTransformers). More specifically, we investigate the ability of BERT at\ncapturing hateful context within social media content by using new fine-tuning\nmethods based on transfer learning. To evaluate our proposed approach, we use\ntwo publicly available datasets that have been annotated for racism, sexism,\nhate, or offensive content on Twitter. The results show that our solution\nobtains considerable performance on these datasets in terms of precision and\nrecall in comparison to existing approaches. Consequently, our model can\ncapture some biases in data annotation and collection process and can\npotentially lead us to a more accurate model.\n",
    "topics": "{'Hate Speech Detection': 0.99973005, 'Transfer Learning': 0.99154764, 'Language Modelling': 0.93358755}",
    "score": 0.8542563124
  },
  {
    "id": "2004.09788",
    "title": "Deep Cerebellar Nuclei Segmentation via Semi-Supervised Deep\n  Context-Aware Learning from 7T Diffusion MRI",
    "abstract": "  Deep cerebellar nuclei are a key structure of the cerebellum that are\ninvolved in processing motor and sensory information. It is thus a crucial step\nto accurately segment deep cerebellar nuclei for the understanding of the\ncerebellum system and its utility in deep brain stimulation treatment. However,\nit is challenging to clearly visualize such small nuclei under standard\nclinical magnetic resonance imaging (MRI) protocols and therefore precise\nsegmentation is not feasible. Recent advances in 7 Tesla (T) MRI technology and\ngreat potential of deep neural networks facilitate automatic patient-specific\nsegmentation. In this paper, we propose a novel deep learning framework\n(referred to as DCN-Net) for fast, accurate, and robust patient-specific\nsegmentation of deep cerebellar dentate and interposed nuclei on 7T diffusion\nMRI. DCN-Net effectively encodes contextual information on the patch images\nwithout consecutive pooling operations and adding complexity via proposed\ndilated dense blocks. During the end-to-end training, label probabilities of\ndentate and interposed nuclei are independently learned with a hybrid loss,\nhandling highly imbalanced data. Finally, we utilize self-training strategies\nto cope with the problem of limited labeled data. To this end, auxiliary\ndentate and interposed nuclei labels are created on unlabeled data by using\nDCN-Net trained on manual labels. We validate the proposed framework using 7T\nB0 MRIs from 60 subjects. Experimental results demonstrate that DCN-Net\nprovides better segmentation than atlas-based deep cerebellar nuclei\nsegmentation tools and other state-of-the-art deep neural networks in terms of\naccuracy and consistency. We further prove the effectiveness of the proposed\ncomponents within DCN-Net in dentate and interposed nuclei segmentation.\n",
    "topics": "{}",
    "score": 0.8541205352
  },
  {
    "id": "2009.14110",
    "title": "Learning to Compress Videos without Computing Motion",
    "abstract": "  With the development of higher resolution contents and displays, its\nsignificant volume poses significant challenges to the goals of acquiring,\ntransmitting, compressing and displaying high quality video content. In this\npaper, we propose a new deep learning video compression architecture that does\nnot require motion estimation, which is the most expensive element of modern\nhybrid video compression codecs like H.264 and HEVC. Our framework exploits the\nregularities inherent to video motion, which we capture by using displaced\nframe differences as video representations to train the neural network. In\naddition, we propose a new space-time reconstruction network based on both an\nLSTM model and a UNet model, which we call LSTM-UNet. The combined network is\nable to efficiently capture both temporal and spatial video information, making\nit highly amenable for our purposes. The new video compression framework has\nthree components: a Displacement Calculation Unit (DCU), a Displacement\nCompression Network (DCN), and a Frame Reconstruction Network (FRN), all of\nwhich are jointly optimized against a single perceptual loss function. The DCU\nobviates the need for motion estimation as in hybrid codecs, and is less\nexpensive. In the DCN, an RNN-based network is utilized to conduct variable\nbit-rate encoding based on a single round of training. The LSTM-UNet is used in\nthe FRN to learn space time differential representations of videos. Our\nexperimental results show that our compression model, which we call the\nMOtionless VIdeo Codec (MOVI-Codec), learns how to efficiently compress videos\nwithout computing motion. Our experiments show that MOVI-Codec outperforms the\nvideo coding standard H.264, and is highly competitive with, and sometimes\nexceeds the performance of the modern global standard HEVC codec, as measured\nby MS-SSIM.\n",
    "topics": "{'Motion Estimation': 1.0, 'Video Compression': 0.9986985, 'Motion Capture': 0.9856307}",
    "score": 0.8540525956
  },
  {
    "id": "1907.12720",
    "title": "Exploring large scale public medical image datasets",
    "abstract": "  Rationale and Objectives: Medical artificial intelligence systems are\ndependent on well characterised large scale datasets. Recently released public\ndatasets have been of great interest to the field, but pose specific challenges\ndue to the disconnect they cause between data generation and data usage,\npotentially limiting the utility of these datasets.\n  Materials and Methods: We visually explore two large public datasets, to\ndetermine how accurate the provided labels are and whether other subtle\nproblems exist. The ChestXray14 dataset contains 112,120 frontal chest films,\nand the MURA dataset contains 40,561 upper limb radiographs. A subset of around\n700 images from both datasets was reviewed by a board-certified radiologist,\nand the quality of the original labels was determined.\n  Results: The ChestXray14 labels did not accurately reflect the visual content\nof the images, with positive predictive values mostly between 10% and 30% lower\nthan the values presented in the original documentation. There were other\nsignificant problems, with examples of hidden stratification and label\ndisambiguation failure. The MURA labels were more accurate, but the original\nnormal/abnormal labels were inaccurate for the subset of cases with\ndegenerative joint disease, with a sensitivity of 60% and a specificity of 82%.\n  Conclusion: Visual inspection of images is a necessary component of\nunderstanding large image datasets. We recommend that teams producing public\ndatasets should perform this important quality control procedure and include a\nthorough description of their findings, along with an explanation of the data\ngenerating procedures and labelling rules, in the documentation for their\ndatasets.\n",
    "topics": "{}",
    "score": 0.8537124951
  },
  {
    "id": "1905.01118",
    "title": "Group Emotion Recognition Using Machine Learning",
    "abstract": "  Automatic facial emotion recognition is a challenging task that has gained\nsignificant scientific interest over the past few years, but the problem of\nemotion recognition for a group of people has been less extensively studied.\nHowever, it is slowly gaining popularity due to the massive amount of data\navailable on social networking sites containing images of groups of people\nparticipating in various social events. Group emotion recognition is a\nchallenging problem due to obstructions like head and body pose variations,\nocclusions, variable lighting conditions, variance of actors, varied indoor and\noutdoor settings and image quality. The objective of this task is to classify a\ngroup's perceived emotion as Positive, Neutral or Negative. In this report, we\ndescribe our solution which is a hybrid machine learning system that\nincorporates deep neural networks and Bayesian classifiers. Deep Convolutional\nNeural Networks (CNNs) work from bottom to top, analysing facial expressions\nexpressed by individual faces extracted from the image. The Bayesian network\nworks from top to bottom, inferring the global emotion for the image, by\nintegrating the visual features of the contents of the image obtained through a\nscene descriptor. In the final pipeline, the group emotion category predicted\nby an ensemble of CNNs in the bottom-up module is passed as input to the\nBayesian Network in the top-down module and an overall prediction for the image\nis obtained. Experimental results show that the stated system achieves 65.27%\naccuracy on the validation set which is in line with state-of-the-art results.\nAs an outcome of this project, a Progressive Web Application and an\naccompanying Android app with a simple and intuitive user interface are\npresented, allowing users to test out the system with their own pictures.\n",
    "topics": "{'Emotion Recognition': 0.99999964}",
    "score": 0.8533431758
  },
  {
    "id": "1909.01068",
    "title": "CGC-Net: Cell Graph Convolutional Network for Grading of Colorectal\n  Cancer Histology Images",
    "abstract": "  Colorectal cancer (CRC) grading is typically carried out by assessing the\ndegree of gland formation within histology images. To do this, it is important\nto consider the overall tissue micro-environment by assessing the cell-level\ninformation along with the morphology of the gland. However, current automated\nmethods for CRC grading typically utilise small image patches and therefore\nfail to incorporate the entire tissue micro-architecture for grading purposes.\nTo overcome the challenges of CRC grading, we present a novel cell-graph\nconvolutional neural network (CGC-Net) that converts each large histology image\ninto a graph, where each node is represented by a nucleus within the original\nimage and cellular interactions are denoted as edges between these nodes\naccording to node similarity. The CGC-Net utilises nuclear appearance features\nin addition to the spatial location of nodes to further boost the performance\nof the algorithm. To enable nodes to fuse multi-scale information, we introduce\nAdaptive GraphSage, which is a graph convolution technique that combines\nmulti-level features in a data-driven way. Furthermore, to deal with redundancy\nin the graph, we propose a sampling technique that removes nodes in areas of\ndense nuclear activity. We show that modeling the image as a graph enables us\nto effectively consider a much larger image (around 16$\\times$ larger) than\ntraditional patch-based approaches and model the complex structure of the\ntissue micro-environment. We construct cell graphs with an average of over\n3,000 nodes on a large CRC histology image dataset and report state-of-the-art\nresults as compared to recent patch-based as well as contextual patch-based\ntechniques, demonstrating the effectiveness of our method.\n",
    "topics": "{}",
    "score": 0.8532759876
  },
  {
    "id": "1904.00790",
    "title": "Retinal OCT disease classification with variational autoencoder\n  regularization",
    "abstract": "  According to the World Health Organization, 285 million people worldwide live\nwith visual impairment. The most commonly used imaging technique for diagnosis\nin ophthalmology is optical coherence tomography (OCT). However, analysis of\nretinal OCT requires trained ophthalmologists and time, making a comprehensive\nearly diagnosis unlikely. A recent study established a diagnostic tool based on\nconvolutional neural networks (CNN), which was trained on a large database of\nretinal OCT images. The performance of the tool in classifying retinal\nconditions was on par to that of trained medical experts. However, the training\nof these networks is based on an enormous amount of labeled data, which is\nexpensive and difficult to obtain. Therefore, this paper describes a method\nbased on variational autoencoder regularization that improves classification\nperformance when using a limited amount of labeled data. This work uses a\ntwo-path CNN model combining a classification network with an autoencoder (AE)\nfor regularization. The key idea behind this is to prevent overfitting when\nusing a limited training dataset size with small number of patients. Results\nshow superior classification performance compared to a pre-trained and fully\nfine-tuned baseline ResNet-34. Clustering of the latent space in relation to\nthe disease class is distinct. Neural networks for disease classification on\nOCTs can benefit from regularization using variational autoencoders when\ntrained with limited amount of patient data. Especially in the medical imaging\ndomain, data annotated by experts is expensive to obtain.\n",
    "topics": "{}",
    "score": 0.8531539558
  },
  {
    "id": "2004.04116",
    "title": "DeepStreamCE: A Streaming Approach to Concept Evolution Detection in\n  Deep Neural Networks",
    "abstract": "  Deep neural networks have experimentally demonstrated superior performance\nover other machine learning approaches in decision-making predictions. However,\none major concern is the closed set nature of the classification decision on\nthe trained classes, which can have serious consequences in safety critical\nsystems. When the deep neural network is in a streaming environment, fast\ninterpretation of this classification is required to determine if the\nclassification result is trusted. Un-trusted classifications can occur when the\ninput data to the deep neural network changes over time. One type of change\nthat can occur is concept evolution, where a new class is introduced that the\ndeep neural network was not trained on. In the majority of deep neural network\narchitectures, the only option is to assign this instance to one of the classes\nit was trained on, which would be incorrect. The aim of this research is to\ndetect the arrival of a new class in the stream. Existing work on interpreting\ndeep neural networks often focuses on neuron activations to provide visual\ninterpretation and feature extraction. Our novel approach, coined DeepStreamCE,\nuses streaming approaches for real-time concept evolution detection in deep\nneural networks. DeepStreamCE applies neuron activation reduction using an\nautoencoder and MCOD stream-based clustering in the offline phase. Both outputs\nare used in the online phase to analyse the neuron activations in the evolving\nstream in order to detect concept evolution occurrence in real time. We\nevaluate DeepStreamCE by training VGG16 convolutional neural networks on\ncombinations of data from the CIFAR-10 dataset, holding out some classes to be\nused as concept evolution. For comparison, we apply the data and VGG16 networks\nto an open-set deep network solution - OpenMax. DeepStreamCE outperforms\nOpenMax when identifying concept evolution for our datasets.\n",
    "topics": "{}",
    "score": 0.8530024323
  },
  {
    "id": "1807.11428",
    "title": "Efficient feature learning and multi-size image steganalysis based on\n  CNN",
    "abstract": "  For steganalysis, many studies showed that convolutional neural network has\nbetter performances than the two-part structure of traditional machine learning\nmethods. However, there are still two problems to be resolved: cutting down\nsignal to noise ratio of the steganalysis feature map and steganalyzing images\nof arbitrary size. Some algorithms required fixed size images as the input and\nhad low accuracy due to the underutilization of the noise residuals obtained by\nvarious types of filters. In this paper, we focus on designing an improved\nnetwork structure based on CNN to resolve the above problems. First, we use 3x3\nkernels instead of the traditional 5x5 kernels and optimize convolution kernels\nin the preprocessing layer. The smaller convolution kernels are used to reduce\nthe number of parameters and model the features in a small local region. Next,\nwe use separable convolutions to utilize channel correlation of the residuals,\ncompress the image content and increase the signal-to-noise ratio (between the\nstego signal and the image signal). Then, we use spatial pyramid pooling (SPP)\nto aggregate the local features, enhance the representation ability of\nfeatures, and steganalyze arbitrary size image. Finally, data augmentation is\nadopted to further improve network performance. The experimental results show\nthat the proposed CNN structure is significantly better than other four methods\nsuch as SRM, Ye-Net, Xu-Net, and Yedroudj-Net, when it is used to detect two\nspatial algorithms such as WOW and S-UNIWARAD with a wide variety of datasets\nand payloads.\n",
    "topics": "{'Data Augmentation': 0.9670633}",
    "score": 0.8528713132
  },
  {
    "id": "2007.06749",
    "title": "Water level prediction from social media images with a multi-task\n  ranking approach",
    "abstract": "  Floods are among the most frequent and catastrophic natural disasters and\naffect millions of people worldwide. It is important to create accurate flood\nmaps to plan (offline) and conduct (real-time) flood mitigation and flood\nrescue operations. Arguably, images collected from social media can provide\nuseful information for that task, which would otherwise be unavailable. We\nintroduce a computer vision system that estimates water depth from social media\nimages taken during flooding events, in order to build flood maps in (near)\nreal-time. We propose a multi-task (deep) learning approach, where a model is\ntrained using both a regression and a pairwise ranking loss. Our approach is\nmotivated by the observation that a main bottleneck for image-based flood level\nestimation is training data: it is diffcult and requires a lot of effort to\nannotate uncontrolled images with the correct water depth. We demonstrate how\nto effciently learn a predictor from a small set of annotated water levels and\na larger set of weaker annotations that only indicate in which of two images\nthe water level is higher, and are much easier to obtain. Moreover, we provide\na new dataset, named DeepFlood, with 8145 annotated ground-level images, and\nshow that the proposed multi-task approach can predict the water level from a\nsingle, crowd-sourced image with ~11 cm root mean square error.\n",
    "topics": "{}",
    "score": 0.8527748556
  },
  {
    "id": "1712.04695",
    "title": "UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face\n  Recognition",
    "abstract": "  Recently proposed robust 3D face alignment methods establish either dense or\nsparse correspondence between a 3D face model and a 2D facial image. The use of\nthese methods presents new challenges as well as opportunities for facial\ntexture analysis. In particular, by sampling the image using the fitted model,\na facial UV can be created. Unfortunately, due to self-occlusion, such a UV map\nis always incomplete. In this paper, we propose a framework for training Deep\nConvolutional Neural Network (DCNN) to complete the facial UV map extracted\nfrom in-the-wild images. To this end, we first gather complete UV maps by\nfitting a 3D Morphable Model (3DMM) to various multiview image and video\ndatasets, as well as leveraging on a new 3D dataset with over 3,000 identities.\nSecond, we devise a meticulously designed architecture that combines local and\nglobal adversarial DCNNs to learn an identity-preserving facial UV completion\nmodel. We demonstrate that by attaching the completed UV to the fitted mesh and\ngenerating instances of arbitrary poses, we can increase pose variations for\ntraining deep face recognition/verification models, and minimise pose\ndiscrepancy during testing, which lead to better performance. Experiments on\nboth controlled and in-the-wild UV datasets prove the effectiveness of our\nadversarial UV completion model. We achieve state-of-the-art verification\naccuracy, $94.05\\%$, under the CFP frontal-profile protocol only by combining\npose augmentation during training and pose discrepancy reduction during\ntesting. We will release the first in-the-wild UV dataset (we refer as WildUV)\nthat comprises of complete facial UV maps from 1,892 identities for research\npurposes.\n",
    "topics": "{'Face Model': 0.99993265, 'Texture Classification': 0.9996872, 'Robust Face Recognition': 0.99967897, 'Face Recognition': 0.99672866, 'Face Alignment': 0.9505824}",
    "score": 0.8526579549
  },
  {
    "id": "2004.04968",
    "title": "Would Mega-scale Datasets Further Enhance Spatiotemporal 3D CNNs?",
    "abstract": "  How can we collect and use a video dataset to further improve spatiotemporal\n3D Convolutional Neural Networks (3D CNNs)? In order to positively answer this\nopen question in video recognition, we have conducted an exploration study\nusing a couple of large-scale video datasets and 3D CNNs. In the early era of\ndeep neural networks, 2D CNNs have been better than 3D CNNs in the context of\nvideo recognition. Recent studies revealed that 3D CNNs can outperform 2D CNNs\ntrained on a large-scale video dataset. However, we heavily rely on\narchitecture exploration instead of dataset consideration. Therefore, in the\npresent paper, we conduct exploration study in order to improve spatiotemporal\n3D CNNs as follows: (i) Recently proposed large-scale video datasets help\nimprove spatiotemporal 3D CNNs in terms of video classification accuracy. We\nreveal that a carefully annotated dataset (e.g., Kinetics-700) effectively\npre-trains a video representation for a video classification task. (ii) We\nconfirm the relationships between #category/#instance and video classification\naccuracy. The results show that #category should initially be fixed, and then\n#instance is increased on a video dataset in case of dataset construction.\n(iii) In order to practically extend a video dataset, we simply concatenate\npublicly available datasets, such as Kinetics-700 and Moments in Time (MiT)\ndatasets. Compared with Kinetics-700 pre-training, we further enhance\nspatiotemporal 3D CNNs with the merged dataset, e.g., +0.9, +3.4, and +1.1 on\nUCF-101, HMDB-51, and ActivityNet datasets, respectively, in terms of\nfine-tuning. (iv) In terms of recognition architecture, the Kinetics-700 and\nmerged dataset pre-trained models increase the recognition performance to 200\nlayers with the Residual Network (ResNet), while the Kinetics-400 pre-trained\nmodel cannot successfully optimize the 200-layer architecture.\n",
    "topics": "{'Video Classification': 1.0, 'Video Recognition': 0.9999684}",
    "score": 0.8525973888
  },
  {
    "id": "2010.01532",
    "title": "Towards Cross-modality Medical Image Segmentation with Online Mutual\n  Knowledge Distillation",
    "abstract": "  The success of deep convolutional neural networks is partially attributed to\nthe massive amount of annotated training data. However, in practice, medical\ndata annotations are usually expensive and time-consuming to be obtained.\nConsidering multi-modality data with the same anatomic structures are widely\navailable in clinic routine, in this paper, we aim to exploit the prior\nknowledge (e.g., shape priors) learned from one modality (aka., assistant\nmodality) to improve the segmentation performance on another modality (aka.,\ntarget modality) to make up annotation scarcity. To alleviate the learning\ndifficulties caused by modality-specific appearance discrepancy, we first\npresent an Image Alignment Module (IAM) to narrow the appearance gap between\nassistant and target modality data.We then propose a novel Mutual Knowledge\nDistillation (MKD) scheme to thoroughly exploit the modality-shared knowledge\nto facilitate the target-modality segmentation. To be specific, we formulate\nour framework as an integration of two individual segmentors. Each segmentor\nnot only explicitly extracts one modality knowledge from corresponding\nannotations, but also implicitly explores another modality knowledge from its\ncounterpart in mutual-guided manner. The ensemble of two segmentors would\nfurther integrate the knowledge from both modalities and generate reliable\nsegmentation results on target modality. Experimental results on the public\nmulti-class cardiac segmentation data, i.e., MMWHS 2017, show that our method\nachieves large improvements on CT segmentation by utilizing additional MRI data\nand outperforms other state-of-the-art multi-modality learning methods.\n",
    "topics": "{'Medical Image Segmentation': 0.9997874, 'Semantic Segmentation': 0.9784532}",
    "score": 0.8521521097
  },
  {
    "id": "1602.00955",
    "title": "Unsupervised High-level Feature Learning by Ensemble Projection for\n  Semi-supervised Image Classification and Image Clustering",
    "abstract": "  This paper investigates the problem of image classification with limited or\nno annotations, but abundant unlabeled data. The setting exists in many tasks\nsuch as semi-supervised image classification, image clustering, and image\nretrieval. Unlike previous methods, which develop or learn sophisticated\nregularizers for classifiers, our method learns a new image representation by\nexploiting the distribution patterns of all available data for the task at\nhand. Particularly, a rich set of visual prototypes are sampled from all\navailable data, and are taken as surrogate classes to train discriminative\nclassifiers; images are projected via the classifiers; the projected values,\nsimilarities to the prototypes, are stacked to build the new feature vector.\nThe training set is noisy. Hence, in the spirit of ensemble learning we create\na set of such training sets which are all diverse, leading to diverse\nclassifiers. The method is dubbed Ensemble Projection (EP). EP captures not\nonly the characteristics of individual images, but also the relationships among\nimages. It is conceptually simple and computationally efficient, yet effective\nand flexible. Experiments on eight standard datasets show that: (1) EP\noutperforms previous methods for semi-supervised image classification; (2) EP\nproduces promising results for self-taught image classification, where\nunlabeled samples are a random collection of images rather than being from the\nsame distribution as the labeled ones; and (3) EP improves over the original\nfeatures for image clustering. The code of the method is available on the\nproject page.\n",
    "topics": "{'Image Classification': 1.0, 'Fine-Grained Image Classification': 0.7209549, 'Image Retrieval': 0.39793262}",
    "score": 0.8521460975
  },
  {
    "id": "1903.00705",
    "title": "Deep Optimization model for Screen Content Image Quality Assessment\n  using Neural Networks",
    "abstract": "  In this paper, we propose a novel quadratic optimized model based on the deep\nconvolutional neural network (QODCNN) for full-reference and no-reference\nscreen content image (SCI) quality assessment. Unlike traditional CNN methods\ntaking all image patches as training data and using average quality pooling,\nour model is optimized to obtain a more effective model including three steps.\nIn the first step, an end-to-end deep CNN is trained to preliminarily predict\nthe image visual quality, and batch normalized (BN) layers and l2\nregularization are employed to improve the speed and performance of network\nfitting. For second step, the pretrained model is fine-tuned to achieve better\nperformance under analysis of the raw training data. An adaptive weighting\nmethod is proposed in the third step to fuse local quality inspired by the\nperceptual property of the human visual system (HVS) that the HVS is sensitive\nto image patches containing texture and edge information. The novelty of our\nalgorithm can be concluded as follows: 1) with the consideration of correlation\nbetween local quality and subjective differential mean opinion score (DMOS),\nthe Euclidean distance is utilized to measure effectiveness of image patches,\nand the pretrained model is fine-tuned with more effective training data; 2) an\nadaptive pooling approach is employed to fuse patch quality of textual and\npictorial regions, whose feature only extracted from distorted images owns\nstrong noise robust and effects on both FR and NR IQA; 3) Considering the\ncharacteristics of SCIs, a deep and valid network architecture is designed for\nboth NR and FR visual quality evaluation of SCIs. Experimental results verify\nthat our model outperforms both current no-reference and full-reference image\nquality assessment methods on the benchmark screen content image quality\nassessment database (SIQAD).\n",
    "topics": "{'Image Quality Assessment': 1.0}",
    "score": 0.8521307635
  },
  {
    "id": "1412.3161",
    "title": "Object-centric Sampling for Fine-grained Image Classification",
    "abstract": "  This paper proposes to go beyond the state-of-the-art deep convolutional\nneural network (CNN) by incorporating the information from object detection,\nfocusing on dealing with fine-grained image classification. Unfortunately, CNN\nsuffers from over-fiting when it is trained on existing fine-grained image\nclassification benchmarks, which typically only consist of less than a few tens\nof thousands training images. Therefore, we first construct a large-scale\nfine-grained car recognition dataset that consists of 333 car classes with more\nthan 150 thousand training images. With this large-scale dataset, we are able\nto build a strong baseline for CNN with top-1 classification accuracy of 81.6%.\nOne major challenge in fine-grained image classification is that many classes\nare very similar to each other while having large within-class variation. One\ncontributing factor to the within-class variation is cluttered image\nbackground. However, the existing CNN training takes uniform window sampling\nover the image, acting as blind on the location of the object of interest. In\ncontrast, this paper proposes an \\emph{object-centric sampling} (OCS) scheme\nthat samples image windows based on the object location information. The\nchallenge in using the location information lies in how to design powerful\nobject detector and how to handle the imperfectness of detection results. To\nthat end, we design a saliency-aware object detection approach specific for the\nsetting of fine-grained image classification, and the uncertainty of detection\nresults are naturally handled in our OCS scheme. Our framework is demonstrated\nto be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on the\nlarge-scale fine-grained car classification dataset.\n",
    "topics": "{'Image Classification': 1.0, 'Fine-Grained Image Classification': 0.9999988, 'Object Detection': 0.99558616}",
    "score": 0.8520948703
  },
  {
    "id": "1909.09432",
    "title": "Genetic Neural Architecture Search for automatic assessment of human\n  sperm images",
    "abstract": "  Male infertility is a disease which affects approximately 7% of men. Sperm\nmorphology analysis (SMA) is one of the main diagnosis methods for this\nproblem. Manual SMA is an inexact, subjective, non-reproducible, and hard to\nteach process. As a result, in this paper, we introduce a novel automatic SMA\nbased on a neural architecture search algorithm termed Genetic Neural\nArchitecture Search (GeNAS). For this purpose, we used a collection of images\ncalled MHSMA dataset contains 1,540 sperm images which have been collected from\n235 patients with infertility problems. GeNAS is a genetic algorithm that acts\nas a meta-controller which explores the constrained search space of plain\nconvolutional neural network architectures. Every individual of the genetic\nalgorithm is a convolutional neural network trained to predict morphological\ndeformities in different segments of human sperm (head, vacuole, and acrosome),\nand its fitness is calculated by a novel proposed method named GeNAS-WF\nespecially designed for noisy, low resolution, and imbalanced datasets. Also, a\nhashing method is used to save each trained neural architecture fitness, so we\ncould reuse them during fitness evaluation and speed up the algorithm. Besides,\nin terms of running time and computation power, our proposed architecture\nsearch method is far more efficient than most of the other existing neural\narchitecture search algorithms. Additionally, other proposed methods have been\nevaluated on balanced datasets, whereas GeNAS is built specifically for noisy,\nlow quality, and imbalanced datasets which are common in the field of medical\nimaging. In our experiments, the best neural architecture found by GeNAS has\nreached an accuracy of 91.66%, 77.33%, and 77.66% in the vacuole, head, and\nacrosome abnormality detection, respectively. In comparison to other proposed\nalgorithms for MHSMA dataset, GeNAS achieved state-of-the-art results.\n",
    "topics": "{'Neural Architecture Search': 1.0, 'Anomaly Detection': 0.9739932}",
    "score": 0.8520535747
  },
  {
    "id": "2007.05729",
    "title": "Usefulness of interpretability methods to explain deep learning based\n  plant stress phenotyping",
    "abstract": "  Deep learning techniques have been successfully deployed for automating plant\nstress identification and quantification. In recent years, there is a growing\npush towards training models that are interpretable -i.e. that justify their\nclassification decisions by visually highlighting image features that were\ncrucial for classification decisions. The expectation is that trained network\nmodels utilize image features that mimic visual cues used by plant\npathologists. In this work, we compare some of the most popular\ninterpretability methods: Saliency Maps, SmoothGrad, Guided Backpropogation,\nDeep Taylor Decomposition, Integrated Gradients, Layer-wise Relevance\nPropagation and Gradient times Input, for interpreting the deep learning model.\nWe train a DenseNet-121 network for the classification of eight different\nsoybean stresses (biotic and abiotic). Using a dataset consisting of 16,573 RGB\nimages of healthy and stressed soybean leaflets captured under controlled\nconditions, we obtained an overall classification accuracy of 95.05 \\%. For a\ndiverse subset of the test data, we compared the important features with those\nidentified by a human expert. We observed that most interpretability methods\nidentify the infected regions of the leaf as important features for some -- but\nnot all -- of the correctly classified images. For some images, the output of\nthe interpretability methods indicated that spurious feature correlations may\nhave been used to correctly classify them. Although the output explanation maps\nof these interpretability methods may be different from each other for a given\nimage, we advocate the use of these interpretability methods as `hypothesis\ngeneration' mechanisms that can drive scientific insight.\n",
    "topics": "{}",
    "score": 0.8520218266
  },
  {
    "id": "2008.01882",
    "title": "Synthetic to Real Unsupervised Domain Adaptation for Single-Stage\n  Artwork Recognition in Cultural Sites",
    "abstract": "  Recognizing artworks in a cultural site using images acquired from the user's\npoint of view (First Person Vision) allows to build interesting applications\nfor both the visitors and the site managers. However, current object detection\nalgorithms working in fully supervised settings need to be trained with large\nquantities of labeled data, whose collection requires a lot of times and high\ncosts in order to achieve good performance. Using synthetic data generated from\nthe 3D model of the cultural site to train the algorithms can reduce these\ncosts. On the other hand, when these models are tested with real images, a\nsignificant drop in performance is observed due to the differences between real\nand synthetic images. In this study we consider the problem of Unsupervised\nDomain Adaptation for object detection in cultural sites. To address this\nproblem, we created a new dataset containing both synthetic and real images of\n16 different artworks. We hence investigated different domain adaptation\ntechniques based on one-stage and two-stage object detector, image-to-image\ntranslation and feature alignment. Based on the observation that single-stage\ndetectors are more robust to the domain shift in the considered settings, we\nproposed a new method based on RetinaNet and feature alignment that we called\nDA-RetinaNet. The proposed approach achieves better results than compared\nmethods. To support research in this field we release the dataset at the\nfollowing link https://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/ and the code of the\nproposed architecture at https://github.com/fpv-iplab/DA-RetinaNet.\n",
    "topics": "{'Unsupervised Domain Adaptation': 0.9999956, 'Object Detection': 0.99779236, 'Domain Adaptation': 0.99302316, 'Image-to-Image Translation': 0.88047284}",
    "score": 0.851893066
  },
  {
    "id": "2009.14677",
    "title": "SoRC -- Evaluation of Computational Molecular Co-Localization Analysis\n  in Mass Spectrometry Images",
    "abstract": "  The computational analysis of Mass Spectrometry Imaging (MSI) data aims at\nthe identification of interesting mass co-localizations and the visualization\nof their lateral distribution in the sample, usually a tissue cross section.\nBut as the morphological structure of tissues and the different kinds of mass\nco-localization naturally show a huge diversity, the selection and tuning of\nthe computational method is a time-consuming effort. In this work we address\nthe special problem of computationally grouping mass channel images according\nto their similarities in their lateral distribution patterns. Such an analysis\nis driven by the idea, that groups of molecules that feature a similar\ndistribution pattern may have a functional relation. But the selection of the\nsimilarity function and other parameters is often done by a time-consuming and\nunsatsifactory trial and error. We propose a new flexible workflow scheme\ncalled SoRC (sum of ranked cluster indices) for automating this tuning step and\nmaking it much more efficient. We test SoRC using three different data sets\nacquired from the lab for three different kinds of samples (barley seed, mouse\nbladder tissue, human PXE skin). We show, that SORC can be applied to score and\nvisualize the results obtained with the applied methods in short time without\ntoo much effort. In our application example, the SoRC results for the three\ndata sets reveal that a) some well-known similarity functions are suited to\nachieve good results for all three data sets and b) for the MSI data featuring\na higher degree of irregularity improved results can be achieved by applying\nnon-standard similarity functions. The SoRC scores computed with our approach\nindicate that an automated testing and scoring of different methods for mass\nchannel image grouping can improve the final outcome of a study by finally\nselecting the methods of the highest scores.\n",
    "topics": "{}",
    "score": 0.8517667957
  },
  {
    "id": "2003.01395",
    "title": "DeepSperm: A robust and real-time bull sperm-cell detection in densely\n  populated semen videos",
    "abstract": "  Background and Objective: Object detection is a primary research interest in\ncomputer vision. Sperm-cell detection in a densely populated bull semen\nmicroscopic observation video presents challenges such as partial occlusion,\nvast number of objects in a single video frame, tiny size of the object,\nartifacts, low contrast, and blurry objects because of the rapid movement of\nthe sperm cells. This study proposes an architecture, called DeepSperm, that\nsolves the aforementioned challenges and is more accurate and faster than\nstate-of-the-art architectures. Methods: In the proposed architecture, we use\nonly one detection layer, which is specific for small object detection. For\nhandling overfitting and increasing accuracy, we set a higher network\nresolution, use a dropout layer, and perform data augmentation on hue,\nsaturation, and exposure. Several hyper-parameters are tuned to achieve better\nperformance. We compare our proposed method with those of a conventional image\nprocessing-based object-detection method, you only look once (YOLOv3), and mask\nregion-based convolutional neural network (Mask R-CNN). Results: In our\nexperiment, we achieve 86.91 mAP on the test dataset and a processing speed of\n50.3 fps. In comparison with YOLOv3, we achieve an increase of 16.66 mAP point,\n3.26 x faster on testing, and 1.4 x faster on training with a small training\ndataset, which contains 40 video frames. The weights file size was also reduced\nsignificantly, with 16.94 x smaller than that of YOLOv3. Moreover, it requires\n1.3 x less graphical processing unit (GPU) memory than YOLOv3. Conclusions:\nThis study proposes DeepSperm, which is a simple, effective, and efficient\narchitecture with its hyper-parameters and configuration to detect bull sperm\ncells robustly in real time. In our experiment, we surpass the state of the art\nin terms of accuracy, speed, and resource needs.\n",
    "topics": "{'Object Detection': 0.9993787, 'Data Augmentation': 0.9993309}",
    "score": 0.8516361948
  },
  {
    "id": "2008.11662",
    "title": "Attr2Style: A Transfer Learning Approach for Inferring Fashion Styles\n  via Apparel Attributes",
    "abstract": "  Popular fashion e-commerce platforms mostly provide details about low-level\nattributes of an apparel (for example, neck type, dress length, collar type,\nprint etc) on their product detail pages. However, customers usually prefer to\nbuy apparels based on their style information, or simply put, occasion (for\nexample, party wear, sports wear, casual wear etc). Application of a supervised\nimage-captioning model to generate style-based image captions is limited\nbecause obtaining ground-truth annotations in the form of style-based captions\nis difficult. This is because annotating style-based captions requires a\ncertain amount of fashion domain expertise, and also adds to the costs and\nmanual effort. On the contrary, low-level attribute based annotations are much\nmore easily available. To address this issue, we propose a transfer-learning\nbased image captioning model that is trained on a source dataset with\nsufficient attribute-based ground-truth captions, and used to predict\nstyle-based captions on a target dataset. The target dataset has only a limited\namount of images with style-based ground-truth captions. The main motivation of\nour approach comes from the fact that most often there are correlations among\nthe low-level attributes and the higher-level styles for an apparel. We\nleverage this fact and train our model in an encoder-decoder based framework\nusing attention mechanism. In particular, the encoder of the model is first\ntrained on the source dataset to obtain latent representations capturing the\nlow-level attributes. The trained model is fine-tuned to generate style-based\ncaptions for the target dataset. To highlight the effectiveness of our method,\nwe qualitatively demonstrate that the captions generated by our approach are\nclose to the actual style information for the evaluated apparels.\n",
    "topics": "{'Image Captioning': 0.99991655, 'Transfer Learning': 0.797414}",
    "score": 0.8512459635
  },
  {
    "id": "1904.00785",
    "title": "Question Embeddings Based on Shannon Entropy: Solving intent\n  classification task in goal-oriented dialogue system",
    "abstract": "  Question-answering systems and voice assistants are becoming major part of\nclient service departments of many organizations, helping them to reduce the\nlabor costs of staff. In many such systems, there is always natural language\nunderstanding module that solves intent classification task. This task is\ncomplicated because of its case-dependency - every subject area has its own\nsemantic kernel. The state of art approaches for intent classification are\ndifferent machine learning and deep learning methods that use text vector\nrepresentations as input. The basic vector representation models such as Bag of\nwords and TF-IDF generate sparse matrixes, which are becoming very big as the\namount of input data grows. Modern methods such as word2vec and FastText use\nneural networks to evaluate word embeddings with fixed dimension size. As we\nare developing a question-answering system for students and enrollees of the\nPerm National Research Polytechnic University, we have faced the problem of\nuser's intent detection. The subject area of our system is very specific, that\nis why there is a lack of training data. This aspect makes intent\nclassification task more challenging for using state of the art deep learning\nmethods. In this paper, we propose an approach of the questions embeddings\nrepresentation based on calculation of Shannon entropy.The goal of the approach\nis to produce low dimensional question vectors as neural approaches do and to\noutperform related methods, described above in condition of small dataset. We\nevaluate and compare our model with existing ones using logistic regression and\ndataset that contains questions asked by students and enrollees. The data is\nlabeled into six classes. Experimental comparison of proposed approach and\nother models revealed that proposed model performed better in the given task.\n",
    "topics": "{'Natural Language Understanding': 0.99999774, 'Intent Classification': 0.9999864, 'Intent Detection': 0.9996717, 'Question Answering': 0.9745558, 'Word Embeddings': 0.8910454}",
    "score": 0.8511416147
  },
  {
    "id": "1912.02084",
    "title": "Mining Domain Knowledge: Improved Framework towards Automatically\n  Standardizing Anatomical Structure Nomenclature in Radiotherapy",
    "abstract": "  The automatic standardization of nomenclature for anatomical structures in\nradiotherapy (RT) clinical data is a critical prerequisite for data curation\nand data-driven research in the era of big data and artificial intelligence,\nbut it is currently an unmet need. Existing methods either cannot handle\ncross-institutional datasets or suffer from heavy imbalance and poor-quality\ndelineation in clinical RT datasets. To solve these problems, we propose an\nautomated structure nomenclature standardization framework, 3D Non-local\nNetwork with Voting (3DNNV). This framework consists of an improved data\nprocessing strategy, namely, adaptive sampling and adaptive cropping (ASAC)\nwith voting, and an optimized feature extraction module. The framework\nsimulates clinicians' domain knowledge and recognition mechanisms to identify\nsmall-volume organs at risk (OARs) with heavily imbalanced data better than\nother methods. We used partial data from an open-source head-and-neck cancer\ndataset to train the model, then tested the model on three cross-institutional\ndatasets to demonstrate its generalizability. 3DNNV outperformed the baseline\nmodel, achieving higher average true positive rates (TPR) overall categories on\nthe three test datasets (+8.27%, +2.39%, and +5.53%, respectively). More\nimportantly, the 3DNNV outperformed the baseline on the test dataset, 28.63% to\n91.17%, in terms of F1 score for a small-volume OAR with only 9 training\nsamples. The results show that 3DNNV can be applied to identify OARs, even\nerror-prone ones. Furthermore, we discussed the limitations and applicability\nof the framework in practical scenarios. The framework we developed can assist\nin standardizing structure nomenclature to facilitate data-driven clinical\nresearch in cancer radiotherapy.\n",
    "topics": "{}",
    "score": 0.8510778537
  },
  {
    "id": "1908.06912",
    "title": "Models Genesis: Generic Autodidactic Models for 3D Medical Image\n  Analysis",
    "abstract": "  Transfer learning from natural image to medical image has established as one\nof the most practical paradigms in deep learning for medical image analysis.\nHowever, to fit this paradigm, 3D imaging tasks in the most prominent imaging\nmodalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing\nrich 3D anatomical information and inevitably compromising the performance. To\novercome this limitation, we have built a set of models, called Generic\nAutodidactic Models, nicknamed Models Genesis, because they are created ex\nnihilo (with no manual labeling), self-taught (learned by self-supervision),\nand generic (served as source models for generating application-specific target\nmodels). Our extensive experiments demonstrate that our Models Genesis\nsignificantly outperform learning from scratch in all five target 3D\napplications covering both segmentation and classification. More importantly,\nlearning a model from scratch simply in 3D may not necessarily yield\nperformance better than transfer learning from ImageNet in 2D, but our Models\nGenesis consistently top any 2D approaches including fine-tuning the models\npre-trained from ImageNet as well as fine-tuning the 2D versions of our Models\nGenesis, confirming the importance of 3D anatomical information and\nsignificance of our Models Genesis for 3D medical imaging. This performance is\nattributed to our unified self-supervised learning framework, built on a simple\nyet powerful observation: the sophisticated yet recurrent anatomy in medical\nimages can serve as strong supervision signals for deep models to learn common\nanatomical representation automatically via self-supervision. As open science,\nall pre-trained Models Genesis are available at\nhttps://github.com/MrGiovanni/ModelsGenesis.\n",
    "topics": "{'Self-Supervised Learning': 0.99999726, 'Transfer Learning': 0.9969681, 'Brain Tumor Segmentation': 0.9294189}",
    "score": 0.8510303962
  },
  {
    "id": "2009.09247",
    "title": "Bias Field Poses a Threat to DNN-based X-Ray Recognition",
    "abstract": "  The chest X-ray plays a key role in screening and diagnosis of many lung\ndiseases including the COVID-19. More recently, many works construct deep\nneural networks (DNNs) for chest X-ray images to realize automated and\nefficient diagnosis of lung diseases. However, bias field caused by the\nimproper medical image acquisition process widely exists in the chest X-ray\nimages while the robustness of DNNs to the bias field is rarely explored, which\ndefinitely poses a threat to the X-ray-based automated diagnosis system. In\nthis paper, we study this problem based on the recent adversarial attack and\npropose a brand new attack, i.e., the adversarial bias field attack where the\nbias field instead of the additive noise works as the adversarial perturbations\nfor fooling the DNNs. This novel attack posts a key problem: how to locally\ntune the bias field to realize high attack success rate while maintaining its\nspatial smoothness to guarantee high realisticity. These two goals contradict\neach other and thus has made the attack significantly challenging. To overcome\nthis challenge, we propose the adversarial-smooth bias field attack that can\nlocally tune the bias field with joint smooth & adversarial constraints. As a\nresult, the adversarial X-ray images can not only fool the DNNs effectively but\nalso retain very high level of realisticity. We validate our method on real\nchest X-ray datasets with powerful DNNs, e.g., ResNet50, DenseNet121, and\nMobileNet, and show different properties to the state-of-the-art attacks in\nboth image realisticity and attack transferability. Our method reveals the\npotential threat to the DNN-based X-ray automated diagnosis and can definitely\nbenefit the development of bias-field-robust automated diagnosis system.\n",
    "topics": "{'Adversarial Attack': 0.9994518}",
    "score": 0.8509262901
  },
  {
    "id": "1811.12222",
    "title": "ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for\n  Autonomous Driving",
    "abstract": "  Autonomous driving has attracted remarkable attention from both industry and\nacademia. An important task is to estimate 3D properties(e.g.translation,\nrotation and shape) of a moving or parked vehicle on the road. This task, while\ncritical, is still under-researched in the computer vision community -\npartially owing to the lack of large scale and fully-annotated 3D car database\nsuitable for autonomous driving research. In this paper, we contribute the\nfirst large-scale database suitable for 3D car instance understanding -\nApolloCar3D. The dataset contains 5,277 driving images and over 60K car\ninstances, where each car is fitted with an industry-grade 3D CAD model with\nabsolute model size and semantically labelled keypoints. This dataset is above\n20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art. To\nenable efficient labelling in 3D, we build a pipeline by considering 2D-3D\nkeypoint correspondences for a single instance and 3D relationship among\nmultiple instances. Equipped with such dataset, we build various baseline\nalgorithms with the state-of-the-art deep convolutional neural networks.\nSpecifically, we first segment each car with a pre-trained Mask R-CNN, and then\nregress towards its 3D pose and shape based on a deformable 3D car model with\nor without using semantic keypoints. We show that using keypoints significantly\nimproves fitting performance. Finally, we develop a new 3D metric jointly\nconsidering 3D pose and 3D shape, allowing for comprehensive evaluation and\nablation study. By comparing with human performance we suggest several future\ndirections for further improvements.\n",
    "topics": "{'Autonomous Driving': 1.0}",
    "score": 0.8507106176
  },
  {
    "id": "1907.09679",
    "title": "Effortless Deep Training for Traffic Sign Detection Using Templates and\n  Arbitrary Natural Images",
    "abstract": "  Deep learning has been successfully applied to several problems related to\nautonomous driving. Often, these solutions rely on large networks that require\ndatabases of real image samples of the problem (i.e., real world) for proper\ntraining. The acquisition of such real-world data sets is not always possible\nin the autonomous driving context, and sometimes their annotation is not\nfeasible (e.g., takes too long or is too expensive). Moreover, in many tasks,\nthere is an intrinsic data imbalance that most learning-based methods struggle\nto cope with. It turns out that traffic sign detection is a problem in which\nthese three issues are seen altogether. In this work, we propose a novel\ndatabase generation method that requires only (i) arbitrary natural images,\ni.e., requires no real image from the domain of interest, and (ii) templates of\nthe traffic signs, i.e., templates synthetically created to illustrate the\nappearance of the category of a traffic sign. The effortlessly generated\ntraining database is shown to be effective for the training of a deep detector\n(such as Faster R-CNN) on German traffic signs, achieving 95.66% of mAP on\naverage. In addition, the proposed method is able to detect traffic signs with\nan average precision, recall and F1-score of about 94%, 91% and 93%,\nrespectively. The experiments surprisingly show that detectors can be trained\nwith simple data generation methods and without problem domain data for the\nbackground, which is in the opposite direction of the common sense for deep\nlearning.\n",
    "topics": "{'Autonomous Driving': 0.9999989, 'Data Augmentation': 0.90970224}",
    "score": 0.8505875427
  },
  {
    "id": "1809.01687",
    "title": "Breast Tumor Segmentation and Shape Classification in Mammograms using\n  Generative Adversarial and Convolutional Neural Network",
    "abstract": "  Mammogram inspection in search of breast tumors is a tough assignment that\nradiologists must carry out frequently. Therefore, image analysis methods are\nneeded for the detection and delineation of breast masses, which portray\ncrucial morphological information that will support reliable diagnosis. In this\npaper, we proposed a conditional Generative Adversarial Network (cGAN) devised\nto segment a breast mass within a region of interest (ROI) in a mammogram. The\ngenerative network learns to recognize the breast mass area and to create the\nbinary mask that outlines the breast mass. In turn, the adversarial network\nlearns to distinguish between real (ground truth) and synthetic segmentations,\nthus enforcing the generative network to create binary masks as realistic as\npossible. The cGAN works well even when the number of training samples are\nlimited. Therefore, the proposed method outperforms several state-of-the-art\napproaches. This hypothesis is corroborated by diverse experiments performed on\ntwo datasets, the public INbreast and a private in-house dataset. The proposed\nsegmentation model provides a high Dice coefficient and Intersection over Union\n(IoU) of 94% and 87%, respectively. In addition, a shape descriptor based on a\nConvolutional Neural Network (CNN) is proposed to classify the generated masks\ninto four mass shapes: irregular, lobular, oval and round. The proposed shape\ndescriptor was trained on Digital Database for Screening Mammography (DDSM)\nyielding an overall accuracy of 80%, which outperforms the current\nstate-of-the-art.\n",
    "topics": "{'Tumor Segmentation': 0.9999999}",
    "score": 0.8503965654
  },
  {
    "id": "1407.8518",
    "title": "Beyond KernelBoost",
    "abstract": "  In this Technical Report we propose a set of improvements with respect to the\nKernelBoost classifier presented in [Becker et al., MICCAI 2013]. We start with\na scheme inspired by Auto-Context, but that is suitable in situations where the\nlack of large training sets poses a potential problem of overfitting. The aim\nis to capture the interactions between neighboring image pixels to better\nregularize the boundaries of segmented regions. As in Auto-Context [Tu et al.,\nPAMI 2009] the segmentation process is iterative and, at each iteration, the\nsegmentation results for the previous iterations are taken into account in\nconjunction with the image itself. However, unlike in [Tu et al., PAMI 2009],\nwe organize our recursion so that the classifiers can progressively focus on\ndifficult-to-classify locations. This lets us exploit the power of the\ndecision-tree paradigm while avoiding over-fitting. In the context of this\narchitecture, KernelBoost represents a powerful building block due to its\nability to learn on the score maps coming from previous iterations. We first\nintroduce two important mechanisms to empower the KernelBoost classifier,\nnamely pooling and the clustering of positive samples based on the appearance\nof the corresponding ground-truth. These operations significantly contribute to\nincrease the effectiveness of the system on biomedical images, where texture\nplays a major role in the recognition of the different image components. We\nthen present some other techniques that can be easily integrated in the\nKernelBoost framework to further improve the accuracy of the final\nsegmentation. We show extensive results on different medical image datasets,\nincluding some multi-label tasks, on which our method is shown to outperform\nstate-of-the-art approaches. The resulting segmentations display high accuracy,\nneat contours, and reduced noise.\n",
    "topics": "{}",
    "score": 0.8502581356
  },
  {
    "id": "2003.08002",
    "title": "AMIL: Adversarial Multi Instance Learning for Human Pose Estimation",
    "abstract": "  Human pose estimation has an important impact on a wide range of applications\nfrom human-computer interface to surveillance and content-based video\nretrieval. For human pose estimation, joint obstructions and overlapping upon\nhuman bodies result in departed pose estimation. To address these problems, by\nintegrating priors of the structure of human bodies, we present a novel\nstructure-aware network to discreetly consider such priors during the training\nof the network. Typically, learning such constraints is a challenging task.\nInstead, we propose generative adversarial networks as our learning model in\nwhich we design two residual multiple instance learning (MIL) models with the\nidentical architecture, one is used as the generator and the other one is used\nas the discriminator. The discriminator task is to distinguish the actual poses\nfrom the fake ones. If the pose generator generates the results that the\ndiscriminator is not able to distinguish from the real ones, the model has\nsuccessfully learnt the priors. In the proposed model, the discriminator\ndifferentiates the ground-truth heatmaps from the generated ones, and later the\nadversarial loss back-propagates to the generator. Such procedure assists the\ngenerator to learn reasonable body configurations and is proved to be\nadvantageous to improve the pose estimation accuracy. Meanwhile, we propose a\nnovel function for MIL. It is an adjustable structure for both instance\nselection and modeling to appropriately pass the information between instances\nin a single bag. In the proposed residual MIL neural network, the pooling\naction adequately updates the instance contribution to its bag. The proposed\nadversarial residual multi-instance neural network that is based on pooling has\nbeen validated on two datasets for the human pose estimation task and\nsuccessfully outperforms the other state-of-arts models.\n",
    "topics": "{'Pose Estimation': 1.0, 'Multiple Instance Learning': 1.0, 'Video Retrieval': 0.9914163}",
    "score": 0.8499930415
  },
  {
    "id": "2005.09226",
    "title": "Holistic Parameteric Reconstruction of Building Models from Point Clouds",
    "abstract": "  Building models are conventionally reconstructed by building roof points\nplanar segmentation and then using a topology graph to group the planes\ntogether. Roof edges and vertices are then mathematically represented by\nintersecting segmented planes. Technically, such solution is based on\nsequential local fitting, i.e., the entire data of one building are not\nsimultaneously participating in determining the building model. As a\nconsequence, the solution is lack of topological integrity and geometric rigor.\nFundamentally different from this traditional approach, we propose a holistic\nparametric reconstruction method which means taking into consideration the\nentire point clouds of one building simultaneously. In our work, building\nmodels are reconstructed from predefined parametric (roof) primitives. We first\nuse a well-designed deep neural network to segment and identify primitives in\nthe given building point clouds. A holistic optimization strategy is then\nintroduced to simultaneously determine the parameters of a segmented primitive.\nIn the last step, the optimal parameters are used to generate a watertight\nbuilding model in CityGML format. The airborne LiDAR dataset RoofN3D with\npredefined roof types is used for our test. It is shown that PointNet++ applied\nto the entire dataset can achieve an accuracy of 83% for primitive\nclassification. For a subset of 910 buildings in RoofN3D, the holistic approach\nis then used to determine the parameters of primitives and reconstruct the\nbuildings. The achieved overall quality of reconstruction is 0.08 meters for\npoint-surface-distance or 0.7 times RMSE of the input LiDAR points. The study\ndemonstrates the efficiency and capability of the proposed approach and its\npotential to handle large scale urban point clouds.\n",
    "topics": "{'3D Reconstruction': 0.5594485}",
    "score": 0.8499197306
  },
  {
    "id": "1503.07077",
    "title": "Rotation-invariant convolutional neural networks for galaxy morphology\n  prediction",
    "abstract": "  Measuring the morphological parameters of galaxies is a key requirement for\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\nSurvey (SDSS) have resulted in the availability of very large collections of\nimages, which have permitted population-wide analyses of galaxy morphology.\nMorphological analysis has traditionally been carried out mostly via visual\ninspection by trained experts, which is time-consuming and does not scale to\nlarge ($\\gtrsim10^4$) numbers of images.\n  Although attempts have been made to build automated classification systems,\nthese have not been able to achieve the desired level of accuracy. The Galaxy\nZoo project successfully applied a crowdsourcing strategy, inviting online\nusers to classify images by answering a series of questions. Unfortunately,\neven this approach does not scale well enough to keep up with the increasing\navailability of galaxy images.\n  We present a deep neural network model for galaxy morphology classification\nwhich exploits translational and rotational symmetry. It was developed in the\ncontext of the Galaxy Challenge, an international competition to build the best\nmodel for morphology classification based on annotated images from the Galaxy\nZoo project.\n  For images with high agreement among the Galaxy Zoo participants, our model\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\%$) for\nmost questions. Confident model predictions are highly accurate, which makes\nthe model suitable for filtering large collections of images and forwarding\nchallenging images to experts for manual annotation. This approach greatly\nreduces the experts' workload without affecting accuracy. The application of\nthese algorithms to larger sets of training data will be critical for analysing\nresults from future surveys such as the LSST.\n",
    "topics": "{'Morphological Analysis': 0.6151044}",
    "score": 0.8497876398
  },
  {
    "id": "1712.05969",
    "title": "Learning a Virtual Codec Based on Deep Convolutional Neural Network to\n  Compress Image",
    "abstract": "  Although deep convolutional neural network has been proved to efficiently\neliminate coding artifacts caused by the coarse quantization of traditional\ncodec, it's difficult to train any neural network in front of the encoder for\ngradient's back-propagation. In this paper, we propose an end-to-end image\ncompression framework based on convolutional neural network to resolve the\nproblem of non-differentiability of the quantization function in the standard\ncodec. First, the feature description neural network is used to get a valid\ndescription in the low-dimension space with respect to the ground-truth image\nso that the amount of image data is greatly reduced for storage or\ntransmission. After image's valid description, standard image codec such as\nJPEG is leveraged to further compress image, which leads to image's great\ndistortion and compression artifacts, especially blocking artifacts, detail\nmissing, blurring, and ringing artifacts. Then, we use a post-processing neural\nnetwork to remove these artifacts. Due to the challenge of directly learning a\nnon-linear function for a standard codec based on convolutional neural network,\nwe propose to learn a virtual codec neural network to approximate the\nprojection from the valid description image to the post-processed compressed\nimage, so that the gradient could be efficiently back-propagated from the\npost-processing neural network to the feature description neural network during\ntraining. Meanwhile, an advanced learning algorithm is proposed to train our\ndeep neural networks for compression. Obviously, the priority of the proposed\nmethod is compatible with standard existing codecs and our learning strategy\ncan be easily extended into these codecs based on convolutional neural network.\nExperimental results have demonstrated the advances of the proposed method as\ncompared to several state-of-the-art approaches, especially at very low\nbit-rate.\n",
    "topics": "{'Image Compression': 0.9999945, 'Quantization': 0.99583817}",
    "score": 0.8496061788
  },
  {
    "id": "1312.6082",
    "title": "Multi-digit Number Recognition from Street View Imagery using Deep\n  Convolutional Neural Networks",
    "abstract": "  Recognizing arbitrary multi-character text in unconstrained natural\nphotographs is a hard problem. In this paper, we address an equally hard\nsub-problem in this domain viz. recognizing arbitrary multi-digit numbers from\nStreet View imagery. Traditional approaches to solve this problem typically\nseparate out the localization, segmentation, and recognition steps. In this\npaper we propose a unified approach that integrates these three steps via the\nuse of a deep convolutional neural network that operates directly on the image\npixels. We employ the DistBelief implementation of deep neural networks in\norder to train large, distributed neural networks on high quality images. We\nfind that the performance of this approach increases with the depth of the\nconvolutional network, with the best performance occurring in the deepest\narchitecture we trained, with eleven hidden layers. We evaluate this approach\non the publicly available SVHN dataset and achieve over $96\\%$ accuracy in\nrecognizing complete street numbers. We show that on a per-digit recognition\ntask, we improve upon the state-of-the-art, achieving $97.84\\%$ accuracy. We\nalso evaluate this approach on an even more challenging dataset generated from\nStreet View imagery containing several tens of millions of street number\nannotations and achieve over $90\\%$ accuracy. To further explore the\napplicability of the proposed system to broader text recognition tasks, we\napply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the\nmost secure reverse turing tests that uses distorted text to distinguish humans\nfrom bots. We report a $99.8\\%$ accuracy on the hardest category of reCAPTCHA.\nOur evaluations on both tasks indicate that at specific operating thresholds,\nthe performance of the proposed system is comparable to, and in some cases\nexceeds, that of human operators.\n",
    "topics": "{'Image Classification': 0.7592604}",
    "score": 0.8495968952
  },
  {
    "id": "2001.05489",
    "title": "CDGAN: Cyclic Discriminative Generative Adversarial Networks for\n  Image-to-Image Transformation",
    "abstract": "  Image-to-image transformation is a kind of problem, where the input image\nfrom one visual representation is transformed into the output image of another\nvisual representation. Since 2014, Generative Adversarial Networks (GANs) have\nfacilitated a new direction to tackle this problem by introducing the generator\nand the discriminator networks in its architecture. Many recent works, like\nPix2Pix, CycleGAN, DualGAN, PS2MAN and CSGAN handled this problem with the\nrequired generator and discriminator networks and choice of the different\nlosses that are used in the objective functions. In spite of these works, still\nthere is a gap to fill in terms of both the quality of the images generated\nthat should look more realistic and as much as close to the ground truth\nimages. In this work, we introduce a new Image-to-Image Transformation network\nnamed Cyclic Discriminative Generative Adversarial Networks (CDGAN) that fills\nthe above mentioned gaps. The proposed CDGAN generates high quality and more\nrealistic images by incorporating the additional discriminator networks for\ncycled images in addition to the original architecture of the CycleGAN. To\ndemonstrate the performance of the proposed CDGAN, it is tested over three\ndifferent baseline image-to-image transformation datasets. The quantitative\nmetrics such as pixel-wise similarity, structural level similarity and\nperceptual level similarity are used to judge the performance. Moreover, the\nqualitative results are also analyzed and compared with the state-of-the-art\nmethods. The proposed CDGAN method clearly outperformed all the\nstate-of-the-art methods when compared over the three baseline Image-to-Image\ntransformation datasets.\n",
    "topics": "{'Image-to-Image Translation': 0.97679543}",
    "score": 0.8495433669
  },
  {
    "id": "1804.07690",
    "title": "Domain Adversarial for Acoustic Emotion Recognition",
    "abstract": "  The performance of speech emotion recognition is affected by the differences\nin data distributions between train (source domain) and test (target domain)\nsets used to build and evaluate the models. This is a common problem, as\nmultiple studies have shown that the performance of emotional classifiers drop\nwhen they are exposed to data that does not match the distribution used to\nbuild the emotion classifiers. The difference in data distributions becomes\nvery clear when the training and testing data come from different domains,\ncausing a large performance gap between validation and testing performance. Due\nto the high cost of annotating new data and the abundance of unlabeled data, it\nis crucial to extract as much useful information as possible from the available\nunlabeled data. This study looks into the use of adversarial multitask training\nto extract a common representation between train and test domains. The primary\ntask is to predict emotional attribute-based descriptors for arousal, valence,\nor dominance. The secondary task is to learn a common representation where the\ntrain and test domains cannot be distinguished. By using a gradient reversal\nlayer, the gradients coming from the domain classifier are used to bring the\nsource and target domain representations closer. We show that exploiting\nunlabeled data consistently leads to better emotion recognition performance\nacross all emotional dimensions. We visualize the effect of adversarial\ntraining on the feature representation across the proposed deep learning\narchitecture. The analysis shows that the data representations for the train\nand test domains converge as the data is passed to deeper layers of the\nnetwork. We also evaluate the difference in performance when we use a shallow\nneural network versus a \\emph{deep neural network} (DNN) and the effect of the\nnumber of shared layers used by the task and domain classifiers.\n",
    "topics": "{'Emotion Recognition': 0.9998097}",
    "score": 0.8493304819
  },
  {
    "id": "1905.03198",
    "title": "Unsupervised Domain Adaptation using Generative Adversarial Networks for\n  Semantic Segmentation of Aerial Images",
    "abstract": "  Segmenting aerial images is being of great potential in surveillance and\nscene understanding of urban areas. It provides a mean for automatic reporting\nof the different events that happen in inhabited areas. This remarkably\npromotes public safety and traffic management applications. After the wide\nadoption of convolutional neural networks methods, the accuracy of semantic\nsegmentation algorithms could easily surpass 80% if a robust dataset is\nprovided. Despite this success, the deployment of a pre-trained segmentation\nmodel to survey a new city that is not included in the training set\nsignificantly decreases the accuracy. This is due to the domain shift between\nthe source dataset on which the model is trained and the new target domain of\nthe new city images. In this paper, we address this issue and consider the\nchallenge of domain adaptation in semantic segmentation of aerial images. We\ndesign an algorithm that reduces the domain shift impact using Generative\nAdversarial Networks (GANs). In the experiments, we test the proposed\nmethodology on the International Society for Photogrammetry and Remote Sensing\n(ISPRS) semantic segmentation dataset and found that our method improves the\noverall accuracy from 35% to 52% when passing from Potsdam domain (considered\nas source domain) to Vaihingen domain (considered as target domain). In\naddition, the method allows recovering efficiently the inverted classes due to\nsensor variation. In particular, it improves the average segmentation accuracy\nof the inverted classes due to sensor variation from 14% to 61%.\n",
    "topics": "{'Scene Understanding': 0.9999995, 'Semantic Segmentation': 0.99998975, 'Unsupervised Domain Adaptation': 0.99647504, 'Domain Adaptation': 0.9882287}",
    "score": 0.8493284207
  },
  {
    "id": "1908.10192",
    "title": "Large Scale Landmark Recognition via Deep Metric Learning",
    "abstract": "  This paper presents a novel approach for landmark recognition in images that\nwe've successfully deployed at Mail ru. This method enables us to recognize\nfamous places, buildings, monuments, and other landmarks in user photos. The\nmain challenge lies in the fact that it's very complicated to give a precise\ndefinition of what is and what is not a landmark. Some buildings, statues and\nnatural objects are landmarks; others are not. There's also no database with a\nfairly large number of landmarks to train a recognition model. A key feature of\nusing landmark recognition in a production environment is that the number of\nphotos containing landmarks is extremely small. This is why the model should\nhave a very low false positive rate as well as high recognition accuracy.\n  We propose a metric learning-based approach that successfully deals with\nexisting challenges and efficiently handles a large number of landmarks. Our\nmethod uses a deep neural network and requires a single pass inference that\nmakes it fast to use in production. We also describe an algorithm for cleaning\nlandmarks database which is essential for training a metric learning model. We\nprovide an in-depth description of basic components of our method like neural\nnetwork architecture, the learning strategy, and the features of our metric\nlearning approach. We show the results of proposed solutions in tests that\nemulate the distribution of photos with and without landmarks from a user\ncollection. We compare our method with others during these tests. The described\nsystem has been deployed as a part of a photo recognition solution at Cloud\nMail ru, which is the photo sharing and storage service at Mail ru Group.\n",
    "topics": "{'Metric Learning': 0.9999956}",
    "score": 0.8493207895
  },
  {
    "id": "1909.01136",
    "title": "Neural Language Model for Automated Classification of Electronic Medical\n  Records at the Emergency Room. The Significant Benefit of Unsupervised\n  Generative Pre-training",
    "abstract": "  In order to build a national injury surveillance system based on emergency\nroom (ER) visits we are developing a coding system to classify their causes\nfrom clinical notes in free-text. Supervised learning techniques have shown\ngood results in this area but require large number of annotated dataset. New\nlevels of performance have been recently achieved in neural language models\n(NLM) with models based on the Transformer architecture incorporating an\nunsupervised generative pre-training step. Our hypothesis is that methods\ninvolving a generative self-supervised pre-training step can significantly\nreduce the required number of annotated samples for supervised fine-tuning. In\nthis case study, we assessed whether we could predict from free-text clinical\nnotes whether a visit was the consequence of a traumatic or non-traumatic\nevent. Using fully re-trained GPT-2 models (without OpenAI pre-trained\nweightings), we compared two scenarios: Scenario A (26 study cases of different\ntraining data sizes) consisted in training the GPT-2 on the trauma/non-trauma\nlabeled (up to 161 930) clinical notes. In Scenario B (19 study cases), a first\nstep of self-supervised pre-training phase with unlabeled (up to 151 930) notes\nand the second step of supervised fine-tuning with labeled (up to 10 000)\nnotes. Results showed that, Scenario A needed to process >6 000 notes to\nachieve good performance (AUC>0.95), Scenario B needed only 600 notes, gain of\na factor 10. At the end case of both scenarios, for 16 times more data (161 930\nvs. 10 000), the gain from Scenario A compared to Scenario B is only an\nimprovement of 0.89% in AUC and 2.12% in F1 score. To conclude, it is possible\nto adapt a multi-purpose NLM model such as the GPT-2 to create a powerful tool\nfor classification of free-text notes with only very small number of labeled\nsamples.\n",
    "topics": "{'Language Modelling': 0.95512176}",
    "score": 0.8492128565
  },
  {
    "id": "1604.04539",
    "title": "Unsupervised single-particle deep clustering via statistical manifold\n  learning",
    "abstract": "  Motivation: Structural heterogeneity in single-particle cryo-electron\nmicroscopy (cryo-EM) data represents a major challenge for high-resolution\nstructure determination. Unsupervised classification may serve as the first\nstep in the assessment of structural heterogeneity. Traditional algorithms for\nunsupervised classification, such as K-means clustering and maximum likelihood\noptimization, may classify images into wrong classes with decreasing\nsignal-to-noise-ratio (SNR) in the image data, yet demand increased cost in\ncomputation. Overcoming these limitations requires further development on\nclustering algorithms for high-performance cryo-EM data analysis. Results: Here\nwe introduce a statistical manifold learning algorithm for unsupervised\nsingle-particle deep clustering. We show that statistical manifold learning\nimproves classification accuracy by about 40% in the absence of input\nreferences for lower SNR data. Applications to several experimental datasets\nsuggest that our deep clustering approach can detect subtle structural\ndifference among classes. Through code optimization over the Intel\nhigh-performance computing (HPC) processors, our software implementation can\ngenerate thousands of reference-free class averages within several hours from\nhundreds of thousands of single-particle cryo-EM images, which allows\nsignificant improvement in ab initio 3D reconstruction resolution and quality.\nOur approach has been successfully applied in several structural determination\nprojects. We expect that it provides a powerful computational tool in analyzing\nhighly heterogeneous structural data and assisting in computational\npurification of single-particle datasets for high-resolution reconstruction.\n",
    "topics": "{'Deep Clustering': 0.99992824, 'Electron Microscopy': 0.999503, '3D Reconstruction': 0.99598515}",
    "score": 0.8491812555
  },
  {
    "id": "1908.04387",
    "title": "Mass Estimation from Images using Deep Neural Network and Sparse Ground\n  Truth",
    "abstract": "  Supervised learning is the workhorse for regression and classification tasks,\nbut the standard approach presumes ground truth for every measurement. In real\nworld applications, limitations due to expense or general in-feasibility due to\nthe specific application are common. In the context of agriculture\napplications, yield monitoring is one such example where simple-physics based\nmeasurements such as volume or force-impact have been used to quantify mass\nflow, which incur error due to sensor calibration. By utilizing semi-supervised\ndeep learning with gradient aggregation and a sequence of images, in this work\nwe can accurately estimate a physical quantity (mass) with complex data\nstructures and sparse ground truth. Using a vision system capturing images of a\nsugarcane elevator and running bamboo under controlled testing as a surrogate\nmaterial to harvesting sugarcane, mass is accurately predicted from images by\ntraining a DNN using only final load weights. The DNN succeeds in capturing the\ncomplex density physics of random stacking of slender rods internally as part\nof the mass prediction model, and surpasses older volumetric-based methods for\nmass prediction. Furthermore, by incorporating knowledge about the system\nphysics through the DNN architecture and penalty terms, improvements in\nprediction accuracy and stability, as well as faster learning are obtained. It\nis shown that the classic nonlinear regression optimization can be reformulated\nwith an aggregation term with some independence assumptions to achieve this\nfeat. Since the number of images for any given run are too large to fit on\ntypical GPU vRAM, an implementation is shown that compensates for the limited\nmemory but still achieve fast training times. The same approach presented\nherein could be applied to other applications like yield monitoring on grain\ncombines or other harvesters using vision or other instrumentation.\n",
    "topics": "{}",
    "score": 0.8491732176
  },
  {
    "id": "1609.03093",
    "title": "Using Spatial Pooler of Hierarchical Temporal Memory to classify noisy\n  videos with predefined complexity",
    "abstract": "  This paper examines the performance of a Spatial Pooler (SP) of a\nHierarchical Temporal Memory (HTM) in the task of noisy object recognition. To\naddress this challenge, a dedicated custom-designed system based on the SP,\nhistogram calculation module and SVM classifier was implemented. In addition to\nimplementing their own version of HTM, the authors also designed a profiler\nwhich is capable of tracing all of the key parameters of the system. This was\nnecessary, since an analysis and monitoring of the system performance turned\nout to be extremely difficult using conventional testing and debugging tools.\nThe system was initially trained on artificially prepared videos without noise\nand then tested with a set of noisy video streams. This approach was intended\nto mimic a real life scenario where an agent or a system trained to deal with\nideal objects faces a task of classifying distorted and noisy ones in its\nregular working conditions. The authors conducted a series of experiments for\nvarious macro parameters of HTM SP, as well as for different levels of video\nreduction ratios. The experiments allowed them to evaluate the performance of\ntwo different system setups (i.e. 'Multiple HTMs' and 'Single HTM') under\nvarious noise conditions with 32--frame video files. Results of all the tests\nwere compared to SVM baseline setup. It was determined that the system\nfeaturing SP is capable of achieving approximately 12 times the noise reduction\nfor a video signal with with distorted bits accounting for 13\\% of the total.\nFurthermore, the system featuring SP performed better also in the experiments\nwithout a noise component and achieved a max F1 score of 0.96. The experiments\nalso revealed that a rise of column and synapse number of SP has a substantial\nimpact on the performance of the system. Consequently, the highest F1 score\nvalues were obtained for 256 and 4096 synapses and columns respectively.\n",
    "topics": "{'Object Recognition': 0.99791485}",
    "score": 0.8487107377
  },
  {
    "id": "2007.15237",
    "title": "Unsupervised Event Detection, Clustering, and Use Case Exposition in\n  Micro-PMU Measurements",
    "abstract": "  Distribution-level phasor measurement units, a.k.a, micro-PMUs, report a\nlarge volume of high resolution phasor measurements which constitute a variety\nof event signatures of different phenomena that occur all across power\ndistribution feeders. In order to implement an event-based analysis that has\nuseful applications for the utility operator, one needs to extract these events\nfrom a large volume of micro-PMU data. However, due to the infrequent,\nunscheduled, and unknown nature of the events, it is often a challenge to even\nfigure out what kind of events are out there to capture and scrutinize. In this\npaper, we seek to address this open problem by developing an unsupervised\napproach, which requires minimal prior human knowledge. First, we develop an\nunsupervised event detection method based on the concept of Generative\nAdversarial Networks (GAN). It works by training deep neural networks that\nlearn the characteristics of the normal trends in micro-PMU measurements; and\naccordingly detect an event when there is any abnormality. We also propose a\ntwo-step unsupervised clustering method, based on a novel linear mixed integer\nprogramming formulation. It helps us categorize events based on their origin in\nthe first step and their similarity in the second step. The active nature of\nthe proposed clustering method makes it capable of identifying new clusters of\nevents on an ongoing basis. The proposed unsupervised event detection and\nclustering methods are applied to real-world micro-PMU data. Results show that\nthey can outperform the prevalent methods in the literature. These methods also\nfacilitate our further analysis to identify important clusters of events that\nlead to unmasking several use cases that could be of value to the utility\noperator.\n",
    "topics": "{}",
    "score": 0.8486387264
  },
  {
    "id": "1807.02152",
    "title": "Automatic deep learning-based normalization of breast dynamic\n  contrast-enhanced magnetic resonance images",
    "abstract": "  Objective: To develop an automatic image normalization algorithm for\nintensity correction of images from breast dynamic contrast-enhanced magnetic\nresonance imaging (DCE-MRI) acquired by different MRI scanners with various\nimaging parameters, using only image information. Methods: DCE-MR images of 460\nsubjects with breast cancer acquired by different scanners were used in this\nstudy. Each subject had one T1-weighted pre-contrast image and three\nT1-weighted post-contrast images available. Our normalization algorithm\noperated under the assumption that the same type of tissue in different\npatients should be represented by the same voxel value. We used four\ntissue/material types as the anchors for the normalization: 1) air, 2) fat\ntissue, 3) dense tissue, and 4) heart. The algorithm proceeded in the following\ntwo steps: First, a state-of-the-art deep learning-based algorithm was applied\nto perform tissue segmentation accurately and efficiently. Then, based on the\nsegmentation results, a subject-specific piecewise linear mapping function was\napplied between the anchor points to normalize the same type of tissue in\ndifferent patients into the same intensity ranges. We evaluated the algorithm\nwith 300 subjects used for training and the rest used for testing. Results: The\napplication of our algorithm to images with different scanning parameters\nresulted in highly improved consistency in pixel values and extracted radiomics\nfeatures. Conclusion: The proposed image normalization strategy based on tissue\nsegmentation can perform intensity correction fully automatically, without the\nknowledge of the scanner parameters. Significance: We have thoroughly tested\nour algorithm and showed that it successfully normalizes the intensity of\nDCE-MR images. We made our software publicly available for others to apply in\ntheir analyses.\n",
    "topics": "{}",
    "score": 0.8485490376
  },
  {
    "id": "1606.04797",
    "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image\n  Segmentation",
    "abstract": "  Convolutional Neural Networks (CNNs) have been recently employed to solve\nproblems from both the computer vision and medical image analysis fields.\nDespite their popularity, most approaches are only able to process 2D images\nwhile most medical data used in clinical practice consists of 3D volumes. In\nthis work we propose an approach to 3D image segmentation based on a\nvolumetric, fully convolutional, neural network. Our CNN is trained end-to-end\non MRI volumes depicting prostate, and learns to predict segmentation for the\nwhole volume at once. We introduce a novel objective function, that we optimise\nduring training, based on Dice coefficient. In this way we can deal with\nsituations where there is a strong imbalance between the number of foreground\nand background voxels. To cope with the limited number of annotated volumes\navailable for training, we augment the data applying random non-linear\ntransformations and histogram matching. We show in our experimental evaluation\nthat our approach achieves good performances on challenging test data while\nrequiring only a fraction of the processing time needed by other previous\nmethods.\n",
    "topics": "{'Semantic Segmentation': 0.9999889, 'Medical Image Segmentation': 0.99949646, 'Brain Segmentation': 0.44548813}",
    "score": 0.8485222759
  },
  {
    "id": "1502.03682",
    "title": "Applying deep learning techniques on medical corpora from the World Wide\n  Web: a prototypical system and evaluation",
    "abstract": "  BACKGROUND: The amount of biomedical literature is rapidly growing and it is\nbecoming increasingly difficult to keep manually curated knowledge bases and\nontologies up-to-date. In this study we applied the word2vec deep learning\ntoolkit to medical corpora to test its potential for identifying relationships\nfrom unstructured text. We evaluated the efficiency of word2vec in identifying\nproperties of pharmaceuticals based on mid-sized, unstructured medical text\ncorpora available on the web. Properties included relationships to diseases\n('may treat') or physiological processes ('has physiological effect'). We\ncompared the relationships identified by word2vec with manually curated\ninformation from the National Drug File - Reference Terminology (NDF-RT)\nontology as a gold standard. RESULTS: Our results revealed a maximum accuracy\nof 49.28% which suggests a limited ability of word2vec to capture linguistic\nregularities on the collected medical corpora compared with other published\nresults. We were able to document the influence of different parameter settings\non result accuracy and found and unexpected trade-off between ranking quality\nand accuracy. Pre-processing corpora to reduce syntactic variability proved to\nbe a good strategy for increasing the utility of the trained vector models.\nCONCLUSIONS: Word2vec is a very efficient implementation for computing vector\nrepresentations and for its ability to identify relationships in textual data\nwithout any prior domain knowledge. We found that the ranking and retrieved\nresults generated by word2vec were not of sufficient quality for automatic\npopulation of knowledge bases and ontologies, but could serve as a starting\npoint for further manual curation.\n",
    "topics": "{}",
    "score": 0.848506358
  },
  {
    "id": "2006.10712",
    "title": "Unsupervised out-of-distribution detection using kernel density\n  estimation",
    "abstract": "  Deep neural networks (DNNs) achieve substantial advancement to the\nstate-of-the-art in many computer vision tasks. However, accuracy of DNNs may\ndrop drastically when test data come from a different distribution than\ntraining data. Detecting out-of-distribution (OOD) samples before performing\ndownstream analysis on the predictions of a DNN thus arises as a crucial\nproblem for critical applications, such as medical diagnosis and autonomous\ndriving. The majority of the existing methods focus on OOD detection in the\nclassification problem. In this paper, we propose an unsupervised OOD detection\nmethod using kernel density estimation (KDE), which is a non-parametric method\nfor estimating probability density functions (pdfs). Specifically, we estimate\nthe pdfs of features for each channel of the network, by performing KDE on the\nin-distribution (InD) dataset. At test time, the pdfs are evaluated on the test\ndata to obtain a confidence score for each channel, which is expected to be\nhigher for InD and lower for OOD samples. These scores are combined into a\nfinal score using logistic regression. Crucially, the proposed method does not\nrequire class labels nor information on the output of a network. Thus, it can\nbe used for networks both for classification and non-classification problems.\nFurthermore, the use of KDE eliminates the need for making a parametric\nassumption (e.g. Gaussian) about feature densities. We performed experiments on\n2 different classification networks trained on CIFAR-10 and CIFAR-100, and 2\ndifferent non-classification networks (segmentation and detection) trained on\nCOCO dataset. The proposed method achieved detection accuracy on-par with the\nstate-of-the-art for classification networks and substantially outperformed the\ncompared alternatives for segmentation and detection networks in all the tests,\nthus exhibiting a larger scope of applications than existing methods.\n",
    "topics": "{'Medical Diagnosis': 1.0, 'Density Estimation': 1.0, 'Out-of-Distribution Detection': 0.999967, 'Autonomous Driving': 0.9982401}",
    "score": 0.8484861638
  },
  {
    "id": "1806.03412",
    "title": "Fully Convolutional Networks with Sequential Information for Robust Crop\n  and Weed Detection in Precision Farming",
    "abstract": "  Reducing the use of agrochemicals is an important component towards\nsustainable agriculture. Robots that can perform targeted weed control offer\nthe potential to contribute to this goal, for example, through specialized\nweeding actions such as selective spraying or mechanical weed removal. A\nprerequisite of such systems is a reliable and robust plant classification\nsystem that is able to distinguish crop and weed in the field. A major\nchallenge in this context is the fact that different fields show a large\nvariability. Thus, classification systems have to robustly cope with\nsubstantial environmental changes with respect to weed pressure and weed types,\ngrowth stages of the crop, visual appearance, and soil conditions. In this\npaper, we propose a novel crop-weed classification system that relies on a\nfully convolutional network with an encoder-decoder structure and incorporates\nspatial information by considering image sequences. Exploiting the crop\narrangement information that is observable from the image sequences enables our\nsystem to robustly estimate a pixel-wise labeling of the images into crop and\nweed, i.e., a semantic segmentation. We provide a thorough experimental\nevaluation, which shows that our system generalizes well to previously unseen\nfields under varying environmental conditions --- a key capability to actually\nuse such systems in precision framing. We provide comparisons to other\nstate-of-the-art approaches and show that our system substantially improves the\naccuracy of crop-weed classification without requiring a retraining of the\nmodel.\n",
    "topics": "{'Semantic Segmentation': 0.99670815}",
    "score": 0.8483377077
  },
  {
    "id": "2002.00139",
    "title": "Analysis of Deep Feature Loss based Enhancement for Speaker Verification",
    "abstract": "  Data augmentation is conventionally used to inject robustness in Speaker\nVerification systems. Several recently organized challenges focus on handling\nnovel acoustic environments. Deep learning based speech enhancement is a modern\nsolution for this. Recently, a study proposed to optimize the enhancement\nnetwork in the activation space of a pre-trained auxiliary network. This\nmethodology, called deep feature loss, greatly improved over the\nstate-of-the-art conventional x-vector based system on a children speech\ndataset called BabyTrain. This work analyzes various facets of that approach\nand asks few novel questions in that context. We first search for optimal\nnumber of auxiliary network activations, training data, and enhancement feature\ndimension. Experiments reveal the importance of Signal-to-Noise Ratio filtering\nthat we employ to create a large, clean, and naturalistic corpus for\nenhancement network training. To counter the \"mismatch\" problem in enhancement,\nwe find enhancing front-end (x-vector network) data helpful while harmful for\nthe back-end (Probabilistic Linear Discriminant Analysis (PLDA)). Importantly,\nwe find enhanced signals contain complementary information to original.\nEstablished by combining them in front-end, this gives ~40% relative\nimprovement over the baseline. We also do an ablation study to remove a noise\nclass from x-vector data augmentation and, for such systems, we establish the\nutility of enhancement regardless of whether it has seen that noise class\nitself during training. Finally, we design several dereverberation schemes to\nconclude ineffectiveness of deep feature loss enhancement scheme for this task.\n",
    "topics": "{'Speaker Verification': 1.0, 'Data Augmentation': 0.9999864, 'Speech Enhancement': 0.748461}",
    "score": 0.8478902249
  },
  {
    "id": "1702.08513",
    "title": "Learning Deep Visual Object Models From Noisy Web Data: How to Make it\n  Work",
    "abstract": "  Deep networks thrive when trained on large scale data collections. This has\ngiven ImageNet a central role in the development of deep architectures for\nvisual object classification. However, ImageNet was created during a specific\nperiod in time, and as such it is prone to aging, as well as dataset bias\nissues. Moving beyond fixed training datasets will lead to more robust visual\nsystems, especially when deployed on robots in new environments which must\ntrain on the objects they encounter there. To make this possible, it is\nimportant to break free from the need for manual annotators. Recent work has\nbegun to investigate how to use the massive amount of images available on the\nWeb in place of manual image annotations. We contribute to this research thread\nwith two findings: (1) a study correlating a given level of noisily labels to\nthe expected drop in accuracy, for two deep architectures, on two different\ntypes of noise, that clearly identifies GoogLeNet as a suitable architecture\nfor learning from Web data; (2) a recipe for the creation of Web datasets with\nminimal noise and maximum visual variability, based on a visual and natural\nlanguage processing concept expansion strategy. By combining these two results,\nwe obtain a method for learning powerful deep object models automatically from\nthe Web. We confirm the effectiveness of our approach through object\ncategorization experiments using our Web-derived version of ImageNet on a\npopular robot vision benchmark database, and on a lifelong object discovery\ntask on a mobile robot.\n",
    "topics": "{'Object Classification': 0.99994195, 'Object Discovery': 0.9999243}",
    "score": 0.8476457198
  },
  {
    "id": "1812.10240",
    "title": "Studying the Plasticity in Deep Convolutional Neural Networks using\n  Random Pruning",
    "abstract": "  Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations.The key idea is to rank the filters based on a certain criterion\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\nfilters are pruned away the remainder of the network is fine tuned and is shown\nto give performance comparable to the original unpruned network. In this work,\nwe report experiments which suggest that the comparable performance of the\npruned network is not due to the specific criterion chosen but due to the\ninherent plasticity of deep neural networks which allows them to recover from\nthe loss of pruned filters once the rest of the filters are fine-tuned.\nSpecifically we show counter-intuitive results wherein by randomly pruning\n25-50% filters from deep CNNs we are able to obtain the same performance as\nobtained by using state-of-the-art pruning methods. We empirically validate our\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\nneeds to be tested on only a small set of classes at test time (say, only\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\nclass specific pruning and show that even here a random pruning strategy gives\nclose to state-of-the-art performance. Unlike existing approaches which mainly\nfocus on the task of image classification, in this work we also report results\non object detection and image segmentation. We show that using a simple random\npruning strategy we can achieve significant speed up in object detection (74%\nimprovement in fps) while retaining the same accuracy as that of the original\nFaster RCNN model. Similarly we show that the performance of a pruned\nSegmentation Network (SegNet) is actually very similar to that of the original\nunpruned SegNet.\n",
    "topics": "{'Object Detection': 0.99780625, 'Image Classification': 0.9853388, 'Semantic Segmentation': 0.9491097}",
    "score": 0.8475476611
  },
  {
    "id": "1801.01539",
    "title": "DeepIso: A Deep Learning Model for Peptide Feature Detection",
    "abstract": "  Liquid chromatography with tandem mass spectrometry (LC-MS/MS) based\nproteomics is a well-established research field with major applications such as\nidentification of disease biomarkers, drug discovery, drug design and\ndevelopment. In proteomics, protein identification and quantification is a\nfundamental task, which is done by first enzymatically digesting it into\npeptides, and then analyzing peptides by LC-MS/MS instruments. The peptide\nfeature detection and quantification from an LC-MS map is the first step in\ntypical analysis workflows. In this paper we propose a novel deep learning\nbased model, DeepIso, that uses Convolutional Neural Networks (CNNs) to scan an\nLC-MS map to detect peptide features and estimate their abundance. Existing\ntools are often designed with limited engineered features based on domain\nknowledge, and depend on pretrained parameters which are hardly updated despite\nhuge amount of new coming proteomic data. Our proposed model, on the other\nhand, is capable of learning multiple levels of representation of high\ndimensional data through its many layers of neurons and continuously evolving\nwith newly acquired data. To evaluate our proposed model, we use an antibody\ndataset including a heavy and a light chain, each digested by Asp-N,\nChymotrypsin, Trypsin, thus giving six LC-MS maps for the experiment. Our model\nachieves 93.21% sensitivity with specificity of 99.44% on this dataset. Our\nresults demonstrate that novel deep learning tools are desirable to advance the\nstate-of-the-art in protein identification and quantification.\n",
    "topics": "{'Drug Discovery': 1.0}",
    "score": 0.8474813664
  },
  {
    "id": "1605.05543",
    "title": "A deep convolutional neural network approach to single-particle\n  recognition in cryo-electron microscopy",
    "abstract": "  Background: Single-particle cryo-electron microscopy (cryo-EM) has become a\npopular tool for structural determination of biological macromolecular\ncomplexes. High-resolution cryo-EM reconstruction often requires hundreds of\nthousands of single-particle images. Particle extraction from experimental\nmicrographs thus can be laborious and presents a major practical bottleneck in\ncryo-EM structural determination. Existing computational methods of particle\npicking often use low-resolution templates as inputs for particle matching,\nmaking it possible to cause reference-dependent bias. It is critical to develop\na highly efficient template-free method to automatically recognize particle\nimages from cryo-EM micrographs. Results: We developed a deep learning-based\nalgorithmic framework, DeepEM, for single-particle recognition from noisy\ncryo-EM micrographs, enabling automated particle picking, selection and\nverification in an integrated fashion. The kernel of DeepEM is built upon a\nconvolutional neural network (CNN) of eight layers, which can be recursively\ntrained to be highly \"knowledgeable\". Our approach exhibits improved\nperformance and high precision when tested on the standard KLH dataset.\nApplication of DeepEM to several challenging experimental cryo-EM datasets\ndemonstrates its capability in avoiding selection of un-wanted particles and\nnon-particles even when true particles contain fewer features. Conclusions: The\nDeepEM method derived from a deep CNN allows automated particle extraction from\nraw cryo-EM micrographs in the absence of templates, which demonstrated\nimproved performance, objectivity and accuracy. Application of this novel\napproach is expected to free the labor involved in single-particle\nverification, thus promoting the efficiency of cryo-EM data processing.\n",
    "topics": "{'Electron Microscopy': 0.8891458}",
    "score": 0.8474586725
  },
  {
    "id": "1701.06599",
    "title": "Unsupervised Joint Mining of Deep Features and Image Labels for\n  Large-scale Radiology Image Categorization and Scene Recognition",
    "abstract": "  The recent rapid and tremendous success of deep convolutional neural networks\n(CNN) on many challenging computer vision tasks largely derives from the\naccessibility of the well-annotated ImageNet and PASCAL VOC datasets.\nNevertheless, unsupervised image categorization (i.e., without the ground-truth\nlabeling) is much less investigated, yet critically important and difficult\nwhen annotations are extremely hard to obtain in the conventional way of\n\"Google Search\" and crowd sourcing. We address this problem by presenting a\nlooped deep pseudo-task optimization (LDPO) framework for joint mining of deep\nCNN features and image labels. Our method is conceptually simple and rests upon\nthe hypothesized \"convergence\" of better labels leading to better trained CNN\nmodels which in turn feed more discriminative image representations to\nfacilitate more meaningful clusters/labels. Our proposed method is validated in\ntackling two important applications: 1) Large-scale medical image annotation\nhas always been a prohibitively expensive and easily-biased task even for\nwell-trained radiologists. Significantly better image categorization results\nare achieved via our proposed approach compared to the previous\nstate-of-the-art method. 2) Unsupervised scene recognition on representative\nand publicly available datasets with our proposed technique is examined. The\nLDPO achieves excellent quantitative scene classification results. On the MIT\nindoor scene dataset, it attains a clustering accuracy of 75.3%, compared to\nthe state-of-the-art supervised classification accuracy of 81.0% (when both are\nbased on the VGG-VD model).\n",
    "topics": "{'Scene Recognition': 1.0, 'Scene Classification': 0.99999976}",
    "score": 0.8473897133
  },
  {
    "id": "1809.08495",
    "title": "SqueezeSegV2: Improved Model Structure and Unsupervised Domain\n  Adaptation for Road-Object Segmentation from a LiDAR Point Cloud",
    "abstract": "  Earlier work demonstrates the promise of deep-learning-based approaches for\npoint cloud segmentation; however, these approaches need to be improved to be\npractically useful. To this end, we introduce a new model SqueezeSegV2 that is\nmore robust to dropout noise in LiDAR point clouds. With improved model\nstructure, training loss, batch normalization and additional input channel,\nSqueezeSegV2 achieves significant accuracy improvement when trained on real\ndata. Training models for point cloud segmentation requires large amounts of\nlabeled point-cloud data, which is expensive to obtain. To sidestep the cost of\ncollection and annotation, simulators such as GTA-V can be used to create\nunlimited amounts of labeled, synthetic data. However, due to domain shift,\nmodels trained on synthetic data often do not generalize well to the real\nworld. We address this problem with a domain-adaptation training pipeline\nconsisting of three major components: 1) learned intensity rendering, 2)\ngeodesic correlation alignment, and 3) progressive domain calibration. When\ntrained on real data, our new model exhibits segmentation accuracy improvements\nof 6.0-8.6% over the original SqueezeSeg. When training our new model on\nsynthetic data using the proposed domain adaptation pipeline, we nearly double\ntest accuracy on real-world data, from 29.0% to 57.4%. Our source code and\nsynthetic dataset will be open-sourced.\n",
    "topics": "{'Domain Adaptation': 0.9986663, 'Unsupervised Domain Adaptation': 0.7920652}",
    "score": 0.8473637
  },
  {
    "id": "1909.12743",
    "title": "MRCNet: Crowd Counting and Density Map Estimation in Aerial and Ground\n  Imagery",
    "abstract": "  In spite of the many advantages of aerial imagery for crowd monitoring and\nmanagement at mass events, datasets of aerial images of crowds are still\nlacking in the field. As a remedy, in this work we introduce a novel crowd\ndataset, the DLR Aerial Crowd Dataset (DLR-ACD), which is composed of 33 large\naerial images acquired from 16 flight campaigns over mass events with 226,291\npersons annotated. To the best of our knowledge, DLR-ACD is the first aerial\ncrowd dataset and will be released publicly. To tackle the problem of accurate\ncrowd counting and density map estimation in aerial images of crowds, this work\nalso proposes a new encoder-decoder convolutional neural network, the so-called\nMulti-Resolution Crowd Network MRCNet. The encoder is based on the VGG-16\nnetwork and the decoder is composed of a set of bilinear upsampling and\nconvolutional layers. Using two losses, one at an earlier level and another at\nthe last level of the decoder, MRCNet estimates crowd counts and\nhigh-resolution crowd density maps as two different but interrelated tasks. In\naddition, MRCNet utilizes contextual and detailed local information by\ncombining high- and low-level features through a number of lateral connections\ninspired by the Feature Pyramid Network (FPN) technique. We evaluated MRCNet on\nthe proposed DLR-ACD dataset as well as on the ShanghaiTech dataset, a\nCCTV-based crowd counting benchmark. The results demonstrate that MRCNet\noutperforms the state-of-the-art crowd counting methods in estimating the crowd\ncounts and density maps for both aerial and CCTV-based images.\n",
    "topics": "{'Crowd Counting': 1.0}",
    "score": 0.8471191063
  },
  {
    "id": "2008.05948",
    "title": "Estimating Magnitude and Phase of Automotive Radar Signals under\n  Multiple Interference Sources with Fully Convolutional Networks",
    "abstract": "  Radar sensors are gradually becoming a wide-spread equipment for road\nvehicles, playing a crucial role in autonomous driving and road safety. The\nbroad adoption of radar sensors increases the chance of interference among\nsensors from different vehicles, generating corrupted range profiles and\nrange-Doppler maps. In order to extract distance and velocity of multiple\ntargets from range-Doppler maps, the interference affecting each range profile\nneeds to be mitigated. In this paper, we propose a fully convolutional neural\nnetwork for automotive radar interference mitigation. In order to train our\nnetwork in a real-world scenario, we introduce a new data set of realistic\nautomotive radar signals with multiple targets and multiple interferers. To our\nknowledge, this is the first work to mitigate interference from multiple\nsources. Furthermore, we introduce a new training regime that eliminates noisy\nweights, showing superior results compared to the widely-used dropout. While\nsome previous works successfully estimated the magnitude of automotive radar\nsignals, we are the first to propose a deep learning model that can accurately\nestimate the phase. For instance, our novel approach reduces the phase\nestimation error with respect to the commonly-adopted zeroing technique by\nhalf, from 12.55 degrees to 6.58 degrees. Considering the lack of databases for\nautomotive radar interference mitigation, we release as open source our\nlarge-scale data set that closely replicates the real-world automotive scenario\nfor multiple interference cases, allowing others to objectively compare their\nfuture work in this domain. Our data set is available for download at:\nhttp://github.com/ristea/arim-v2.\n",
    "topics": "{'Autonomous Driving': 0.97610414}",
    "score": 0.8471086136
  },
  {
    "id": "2005.03793",
    "title": "Federated Generative Adversarial Learning",
    "abstract": "  This work studies training generative adversarial networks under the\nfederated learning setting. Generative adversarial networks (GANs) have\nachieved advancement in various real-world applications, such as image editing,\nstyle transfer, scene generations, etc. However, like other deep learning\nmodels, GANs are also suffering from data limitation problems in real cases. To\nboost the performance of GANs in target tasks, collecting images as many as\npossible from different sources becomes not only important but also essential.\nFor example, to build a robust and accurate bio-metric verification system,\nhuge amounts of images might be collected from surveillance cameras, and/or\nuploaded from cellphones by users accepting agreements. In an ideal case,\nutilize all those data uploaded from public and private devices for model\ntraining is straightforward. Unfortunately, in the real scenarios, this is hard\ndue to a few reasons. At first, some data face the serious concern of leakage,\nand therefore it is prohibitive to upload them to a third-party server for\nmodel training; at second, the images collected by different kinds of devices,\nprobably have distinctive biases due to various factors, $\\textit{e.g.}$,\ncollector preferences, geo-location differences, which is also known as \"domain\nshift\". To handle those problems, we propose a novel generative learning scheme\nutilizing a federated learning framework. Following the configuration of\nfederated learning, we conduct model training and aggregation on one center and\na group of clients. Specifically, our method learns the distributed generative\nmodels in clients, while the models trained in each client are fused into one\nunified and versatile model in the center. We perform extensive experiments to\ncompare different federation strategies, and empirically examine the\neffectiveness of federation under different levels of parallelism and data\nskewness.\n",
    "topics": "{'Federated Learning': 0.999998, 'Style Transfer': 0.9973557}",
    "score": 0.8470651961
  },
  {
    "id": "2009.05750",
    "title": "Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision\n  Farming",
    "abstract": "  An effective perception system is a fundamental component for farming robots,\nas it enables them to properly perceive the surrounding environment and to\ncarry out targeted operations. The most recent approaches make use of\nstate-of-the-art machine learning techniques to learn an effective model for\nthe target task. However, those methods need a large amount of labelled data\nfor training. A recent approach to deal with this issue is data augmentation\nthrough Generative Adversarial Networks (GANs), where entire synthetic scenes\nare added to the training data, thus enlarging and diversifying their\ninformative content. In this work, we propose an alternative solution with\nrespect to the common data augmentation techniques, applying it to the\nfundamental problem of crop/weed segmentation in precision farming. Starting\nfrom real images, we create semi-artificial samples by replacing the most\nrelevant object classes (i.e., crop and weeds) with their synthesized\ncounterparts. To do that, we employ a conditional GAN (cGAN), where the\ngenerative model is trained by conditioning the shape of the generated object.\nMoreover, in addition to RGB data, we take into account also near-infrared\n(NIR) information, generating four channel multi-spectral synthetic images.\nQuantitative experiments, carried out on three publicly available datasets,\nshow that (i) our model is capable of generating realistic multi-spectral\nimages of plants and (ii) the usage of such synthetic images in the training\nprocess improves the segmentation performance of state-of-the-art semantic\nsegmentation Convolutional Networks.\n",
    "topics": "{'Data Augmentation': 0.99999976, 'Image Generation': 0.9960724, 'Semantic Segmentation': 0.9800488}",
    "score": 0.8468188073
  },
  {
    "id": "2003.13401",
    "title": "Context Based Emotion Recognition using EMOTIC Dataset",
    "abstract": "  In our everyday lives and social interactions we often try to perceive the\nemotional states of people. There has been a lot of research in providing\nmachines with a similar capacity of recognizing emotions. From a computer\nvision perspective, most of the previous efforts have been focusing in\nanalyzing the facial expressions and, in some cases, also the body pose. Some\nof these methods work remarkably well in specific settings. However, their\nperformance is limited in natural, unconstrained environments. Psychological\nstudies show that the scene context, in addition to facial expression and body\npose, provides important information to our perception of people's emotions.\nHowever, the processing of the context for automatic emotion recognition has\nnot been explored in depth, partly due to the lack of proper data. In this\npaper we present EMOTIC, a dataset of images of people in a diverse set of\nnatural situations, annotated with their apparent emotion. The EMOTIC dataset\ncombines two different types of emotion representation: (1) a set of 26\ndiscrete categories, and (2) the continuous dimensions Valence, Arousal, and\nDominance. We also present a detailed statistical and algorithmic analysis of\nthe dataset along with annotators' agreement analysis. Using the EMOTIC dataset\nwe train different CNN models for emotion recognition, combining the\ninformation of the bounding box containing the person with the contextual\ninformation extracted from the scene. Our results show how scene context\nprovides important information to automatically recognize emotional states and\nmotivate further research in this direction. Dataset and code is open-sourced\nand available at: https://github.com/rkosti/emotic and link for the\npeer-reviewed published article: https://ieeexplore.ieee.org/document/8713881\n",
    "topics": "{'Emotion Recognition': 0.99999976}",
    "score": 0.8467555944
  },
  {
    "id": "1910.07360",
    "title": "Conservation AI: Live Stream Analysis for the Detection of Endangered\n  Species Using Convolutional Neural Networks and Drone Technology",
    "abstract": "  Many different species are adversely affected by poaching. In response to\nthis escalating crisis, efforts to stop poaching using hidden cameras, drones\nand DNA tracking have been implemented with varying degrees of success. Limited\nresources, costs and logistical limitations are often the cause of most\nunsuccessful poaching interventions. The study presented in this paper outlines\na flexible and interoperable framework for the automatic detection of animals\nand poaching activity to facilitate early intervention practices. Using a\nrobust deep learning pipeline, a convolutional neural network is trained and\nimplemented to detect rhinos and cars (considered an important tool in poaching\nfor fast access and artefact transportation in natural habitats) in the study,\nthat are found within live video streamed from drones Transfer learning with\nthe Faster RCNN Resnet 101 is performed to train a custom model with 350 images\nof rhinos and 350 images of cars. Inference is performed using a frame sampling\ntechnique to address the required trade-off control precision and processing\nspeed and maintain synchronisation with the live feed. Inference models are\nhosted on a web platform using flask web serving, OpenCV and TensorFlow 1.13.\nVideo streams are transmitted from a DJI Mavic Pro 2 drone using the Real-Time\nMessaging Protocol (RMTP). The best trained Faster RCNN model achieved a mAP of\n0.83 @IOU 0.50 and 0.69 @IOU 0.75 respectively. In comparison an\nSSD-mobilenetmodel trained under the same experimental conditions achieved a\nmAP of 0.55 @IOU .50 and 0.27 @IOU 0.75.The results demonstrate that using a\nFRCNN and off-the-shelf drones is a promising and scalable option for a range\nof conservation projects.\n",
    "topics": "{'Transfer Learning': 0.95919365}",
    "score": 0.8464100443
  },
  {
    "id": "1707.09320",
    "title": "Z-checker: A Framework for Assessing Lossy Compression of Scientific\n  Data",
    "abstract": "  Because of vast volume of data being produced by today's scientific\nsimulations and experiments, lossy data compressor allowing user-controlled\nloss of accuracy during the compression is a relevant solution for\nsignificantly reducing the data size. However, lossy compressor developers and\nusers are missing a tool to explore the features of scientific datasets and\nunderstand the data alteration after compression in a systematic and reliable\nway. To address this gap, we have designed and implemented a generic framework\ncalled Z-checker. On the one hand, Z-checker combines a battery of data\nanalysis components for data compression. On the other hand, Z-checker is\nimplemented as an open-source community tool to which users and developers can\ncontribute and add new analysis components based on their additional analysis\ndemands. In this paper, we present a survey of existing lossy compressors. Then\nwe describe the design framework of Z-checker, in which we integrated\nevaluation metrics proposed in prior work as well as other analysis tools.\nSpecifically, for lossy compressor developers, Z-checker can be used to\ncharacterize critical properties of any dataset to improve compression\nstrategies. For lossy compression users, Z-checker can detect the compression\nquality, provide various global distortion analysis comparing the original data\nwith the decompressed data and statistical analysis of the compression error.\nZ-checker can perform the analysis with either coarse granularity or fine\ngranularity, such that the users and developers can select the best-fit,\nadaptive compressors for different parts of the dataset. Z-checker features a\nvisualization interface displaying all analysis results in addition to some\nbasic views of the datasets such as time series. To the best of our knowledge,\nZ-checker is the first tool designed to assess lossy compression\ncomprehensively for scientific datasets.\n",
    "topics": "{}",
    "score": 0.8459950618
  },
  {
    "id": "1706.03008",
    "title": "An Ensemble Deep Learning Based Approach for Red Lesion Detection in\n  Fundus Images",
    "abstract": "  Diabetic retinopathy is one of the leading causes of preventable blindness in\nthe world. Its earliest sign are red lesions, a general term that groups both\nmicroaneurysms and hemorrhages. In daily clinical practice, these lesions are\nmanually detected by physicians using fundus photographs. However, this task is\ntedious and time consuming, and requires an intensive effort due to the small\nsize of the lesions and their lack of contrast. Computer-assisted diagnosis of\nDR based on red lesion detection is being actively explored due to its\nimprovement effects both in clinicians consistency and accuracy. Several\nmethods for detecting red lesions have been proposed in the literature, most of\nthem based on characterizing lesion candidates using hand crafted features, and\nclassifying them into true or false positive detections. Deep learning based\napproaches, by contrast, are scarce in this domain due to the high expense of\nannotating the lesions manually. In this paper we propose a novel method for\nred lesion detection based on combining both deep learned and domain knowledge.\nFeatures learned by a CNN are augmented by incorporating hand crafted features.\nSuch ensemble vector of descriptors is used afterwards to identify true lesion\ncandidates using a Random Forest classifier. We empirically observed that\ncombining both sources of information significantly improve results with\nrespect to using each approach separately. Furthermore, our method reported the\nhighest performance on a per-lesion basis on DIARETDB1 and e-ophtha, and for\nscreening and need for referral on MESSIDOR compared to a second human expert.\nResults highlight the fact that integrating manually engineered approaches with\ndeep learned features is relevant to improve results when the networks are\ntrained from lesion-level annotated data. An open source implementation of our\nsystem is publicly available online.\n",
    "topics": "{}",
    "score": 0.8458935077
  },
  {
    "id": "2009.12698",
    "title": "COVID-19 Infection Map Generation and Detection from Chest X-Ray Images",
    "abstract": "  Computer-aided diagnosis has become a necessity for accurate and immediate\ncoronavirus disease 2019 (COVID-19) detection to aid treatment and prevent the\nspread of the virus. Compared to other diagnosis methodologies, chest X-ray\n(CXR) imaging is an advantageous tool since it is fast, low-cost, and easily\naccessible. Thus, CXR has a great potential not only to help diagnose COVID-19\nbut also to track the progression of the disease. Numerous studies have\nproposed to use Deep Learning techniques for COVID-19 diagnosis. However, they\nhave used very limited CXR image repositories for evaluation with a small\nnumber, a few hundreds, of COVID-19 samples. Moreover, these methods can\nneither localize nor grade the severity of COVID-19 infection. For this\npurpose, recent studies proposed to explore the activation maps of deep\nnetworks. However, they remain inaccurate for localizing the actual infestation\nmaking them unreliable for clinical use. This study proposes a novel method for\nthe joint localization, severity grading, and detection of COVID-19 from CXR\nimages by generating the so-called infection maps that can accurately localize\nand grade the severity of COVID-19 infection. To accomplish this, we have\ncompiled the largest COVID-19 dataset up to date with 2951 COVID-19 CXR images,\nwhere the annotation of the ground-truth segmentation masks is performed on\nCXRs by a novel collaborative expert human-machine approach. Furthermore, we\npublicly release the first CXR dataset with the ground-truth segmentation masks\nof the COVID-19 infected regions. A detailed set of experiments show that\nstate-of-the-art segmentation networks can learn to localize COVID-19 infection\nwith an F1-score of 85.81%, that is significantly superior to the activation\nmaps created by the previous methods. Finally, the proposed approach achieved a\nCOVID-19 detection performance with 98.37% sensitivity and 99.16% specificity.\n",
    "topics": "{'COVID-19 Diagnosis': 0.9999951}",
    "score": 0.8457453177
  },
  {
    "id": "1903.09344",
    "title": "Overcoming Small Minirhizotron Datasets Using Transfer Learning",
    "abstract": "  Minirhizotron technology is widely used for studying the development of\nroots. Such systems collect visible-wavelength color imagery of plant roots\nin-situ by scanning an imaging system within a clear tube driven into the soil.\nAutomated analysis of root systems could facilitate new scientific discoveries\nthat would be critical to address the world's pressing food, resource, and\nclimate issues. A key component of automated analysis of plant roots from\nimagery is the automated pixel-level segmentation of roots from their\nsurrounding soil. Supervised learning techniques appear to be an appropriate\ntool for the challenge due to varying local soil and root conditions, however,\nlack of enough annotated training data is a major limitation due to the\nerror-prone and time-consuming manually labeling process. In this paper, we\ninvestigate the use of deep neural networks based on the U-net architecture for\nautomated, precise pixel-wise root segmentation in minirhizotron imagery. We\ncompiled two minirhizotron image datasets to accomplish this study: one with\n17,550 peanut root images and another with 28 switchgrass root images. Both\ndatasets were paired with manually labeled ground truth masks. We trained three\nneural networks with different architectures on the larger peanut root dataset\nto explore the effect of the neural network depth on segmentation performance.\nTo tackle the more limited switchgrass root dataset, we showed that models\ninitialized with features pre-trained on the peanut dataset and then fine-tuned\non the switchgrass dataset can improve segmentation performance significantly.\nWe obtained 99\\% segmentation accuracy in switchgrass imagery using only 21\ntraining images. We also observed that features pre-trained on a closely\nrelated but relatively moderate size dataset like our peanut dataset are more\neffective than features pre-trained on the large but unrelated ImageNet\ndataset.\n",
    "topics": "{'Transfer Learning': 0.903149}",
    "score": 0.8456035702
  },
  {
    "id": "1910.01010",
    "title": "Design Space Exploration of Hardware Spiking Neurons for Embedded\n  Artificial Intelligence",
    "abstract": "  Machine learning is yielding unprecedented interest in research and industry,\ndue to recent success in many applied contexts such as image classification and\nobject recognition. However, the deployment of these systems requires huge\ncomputing capabilities, thus making them unsuitable for embedded systems. To\ndeal with this limitation, many researchers are investigating brain-inspired\ncomputing, which would be a perfect alternative to the conventional Von Neumann\narchitecture based computers (CPU/GPU) that meet the requirements for computing\nperformance, but not for energy-efficiency. Therefore, neuromorphic hardware\ncircuits that are adaptable for both parallel and distributed computations need\nto be designed. In this paper, we focus on Spiking Neural Networks (SNNs) with\na comprehensive study of information coding methods and hardware exploration.\nIn this context, we propose a framework for neuromorphic hardware design space\nexploration, which allows to define a suitable architecture based on\napplication-specific constraints and starting from a wide variety of possible\narchitectural choices. For this framework, we have developed a behavioral level\nsimulator for neuromorphic hardware architectural exploration named NAXT.\nMoreover, we propose modified versions of the standard Rate Coding technique to\nmake trade-offs with the Time Coding paradigm, which is characterized by the\nlow number of spikes propagating in the network. Thus, we are able to reduce\nthe number of spikes while keeping the same neuron's model, which results in an\nSNN with fewer events to process. By doing so, we seek to reduce the amount of\npower consumed by the hardware. Furthermore, we present three neuromorphic\nhardware architectures in order to quantitatively study the implementation of\nSNNs. These architectures are derived from a novel funnel-like Design Space\nExploration framework for neuromorphic hardware.\n",
    "topics": "{'Object Recognition': 0.7999502, 'Image Classification': 0.54325587}",
    "score": 0.8454341326
  },
  {
    "id": "1611.00068",
    "title": "RNN Approaches to Text Normalization: A Challenge",
    "abstract": "  This paper presents a challenge to the community: given a large corpus of\nwritten text aligned to its normalized spoken form, train an RNN to learn the\ncorrect normalization function. We present a data set of general text where the\nnormalizations were generated using an existing text normalization component of\na text-to-speech system. This data set will be released open-source in the near\nfuture.\n  We also present our own experiments with this data set with a variety of\ndifferent RNN architectures. While some of the architectures do in fact produce\nvery good results when measured in terms of overall accuracy, the errors that\nare produced are problematic, since they would convey completely the wrong\nmessage if such a system were deployed in a speech application. On the other\nhand, we show that a simple FST-based filter can mitigate those errors, and\nachieve a level of accuracy not achievable by the RNN alone.\n  Though our conclusions are largely negative on this point, we are actually\nnot arguing that the text normalization problem is intractable using an pure\nRNN approach, merely that it is not going to be something that can be solved\nmerely by having huge amounts of annotated text data and feeding that to a\ngeneral RNN model. And when we open-source our data, we will be providing a\nnovel data set for sequence-to-sequence modeling in the hopes that the the\ncommunity can find better solutions.\n  The data used in this work have been released and are available at:\nhttps://github.com/rwsproat/text-normalization-data\n",
    "topics": "{}",
    "score": 0.8454023005
  },
  {
    "id": "1812.06625",
    "title": "Semi-supervised mp-MRI Data Synthesis with StitchLayer and Auxiliary\n  Distance Maximization",
    "abstract": "  In this paper, we address the problem of synthesizing multi-parameter\nmagnetic resonance imaging (mp-MRI) data, i.e. Apparent Diffusion Coefficients\n(ADC) and T2-weighted (T2w), containing clinically significant (CS) prostate\ncancer (PCa) via semi-supervised adversarial learning. Specifically, our\nsynthesizer generates mp-MRI data in a sequential manner: first generating ADC\nmaps from 128-d latent vectors, followed by translating them to the T2w images.\nThe synthesizer is trained in a semisupervised manner. In the supervised\ntraining process, a limited amount of paired ADC-T2w images and the\ncorresponding ADC encodings are provided and the synthesizer learns the paired\nrelationship by explicitly minimizing the reconstruction losses between\nsynthetic and real images. To avoid overfitting limited ADC encodings, an\nunlimited amount of random latent vectors and unpaired ADC-T2w Images are\nutilized in the unsupervised training process for learning the marginal image\ndistributions of real images. To improve the robustness of synthesizing, we\ndecompose the difficult task of generating full-size images into several\nsimpler tasks which generate sub-images only. A StitchLayer is then employed to\nfuse sub-images together in an interlaced manner into a full-size image. To\nenforce the synthetic images to indeed contain distinguishable CS PCa lesions,\nwe propose to also maximize an auxiliary distance of Jensen-Shannon divergence\n(JSD) between CS and nonCS images. Experimental results show that our method\ncan effectively synthesize a large variety of mpMRI images which contain\nmeaningful CS PCa lesions, display a good visual quality and have the correct\npaired relationship. Compared to the state-of-the-art synthesis methods, our\nmethod achieves a significant improvement in terms of both visual and\nquantitative evaluation metrics.\n",
    "topics": "{}",
    "score": 0.8452258134
  },
  {
    "id": "1907.03217",
    "title": "Adaptive Weighting Depth-variant Deconvolution of Fluorescence\n  Microscopy Images with Convolutional Neural Network",
    "abstract": "  Fluorescence microscopy plays an important role in biomedical research. The\ndepth-variant point spread function (PSF) of a fluorescence microscope produces\nlow-quality images especially in the out-of-focus regions of thick specimens.\nTraditional deconvolution to restore the out-of-focus images is usually\ninsufficient since a depth-invariant PSF is assumed. This article aims at\nhandling fluorescence microscopy images by learning-based depth-variant PSF and\nreducing artifacts. We propose adaptive weighting depth-variant deconvolution\n(AWDVD) with defocus level prediction convolutional neural network (DelpNet) to\nrestore the out-of-focus images. Depth-variant PSFs of image patches can be\nobtained by DelpNet and applied in the afterward deconvolution. AWDVD is\nadopted for a whole image which is patch-wise deconvolved and appropriately\ncropped before deconvolution. DelpNet achieves the accuracy of 98.2%, which\noutperforms the best-ever one using the same microscopy dataset. Image patches\nof 11 defocus levels after deconvolution are validated with maximum improvement\nin the peak signal-to-noise ratio and structural similarity index of 6.6 dB and\n11%, respectively. The adaptive weighting of the patch-wise deconvolved image\ncan eliminate patch boundary artifacts and improve deconvolved image quality.\nThe proposed method can accurately estimate depth-variant PSF and effectively\nrecover out-of-focus microscopy images. To our acknowledge, this is the first\nstudy of handling out-of-focus microscopy images using learning-based\ndepth-variant PSF. Facing one of the most common blurs in fluorescence\nmicroscopy, the novel method provides a practical technology to improve the\nimage quality.\n",
    "topics": "{}",
    "score": 0.8451664111
  },
  {
    "id": "1909.03676",
    "title": "Joint, Partially-joint, and Individual Independent Component Analysis in\n  Multi-Subject fMRI Data",
    "abstract": "  Objective: Joint analysis of multi-subject brain imaging datasets has wide\napplications in biomedical engineering. In these datasets, some sources belong\nto all subjects (joint), a subset of subjects (partially-joint), or a single\nsubject (individual). In this paper, this source model is referred to as\njoint/partially-joint/individual multiple datasets multidimensional (JpJI-MDM),\nand accordingly, a source extraction method is developed. Method: We present a\ndeflation-based algorithm utilizing higher order cumulants to analyze the\nJpJI-MDM source model. The algorithm maximizes a cost function which leads to\nan eigenvalue problem solved with thin-SVD (singular value decomposition)\nfactorization. Furthermore, we introduce the JpJI-feature which indicates the\nspatial shape of each source and the amount of its jointness with other\nsubjects. We use this feature to determine the type of sources. Results: We\nevaluate our algorithm by analyzing simulated data and two real functional\nmagnetic resonance imaging (fMRI) datasets. In our simulation study, we will\nshow that the proposed algorithm determines the type of sources with the\naccuracy of 95% and 100% for 2-class and 3-class clustering scenarios,\nrespectively. Furthermore, our algorithm extracts meaningful joint and\npartially-joint sources from the two real datasets, which are consistent with\nthe existing neuroscience studies. Conclusion: Our results in analyzing the\nreal datasets reveal that both datasets follow the JpJI-MDM source model. This\nsource model improves the accuracy of source extraction methods developed for\nmulti-subject datasets. Significance: The proposed joint blind source\nseparation algorithm is robust and avoids parameters which are difficult to\nfine-tune.\n",
    "topics": "{}",
    "score": 0.8449827089
  },
  {
    "id": "1907.09449",
    "title": "Automatic detection of rare pathologies in fundus photographs using\n  few-shot learning",
    "abstract": "  In the last decades, large datasets of fundus photographs have been collected\nin diabetic retinopathy (DR) screening networks. Through deep learning, these\ndatasets were used to train automatic detectors for DR and a few other frequent\npathologies, with the goal to automate screening. One challenge limits the\nadoption of such systems so far: automatic detectors ignore rare conditions\nthat ophthalmologists currently detect, such as papilledema or anterior\nischemic optic neuropathy. The reason is that standard deep learning requires\ntoo many examples of these conditions. However, this limitation can be\naddressed with few-shot learning, a machine learning paradigm where a\nclassifier has to generalize to a new category not seen in training, given only\na few examples of this category. This paper presents a new few-shot learning\nframework that extends convolutional neural networks (CNNs), trained for\nfrequent conditions, with an unsupervised probabilistic model for rare\ncondition detection. It is based on the observation that CNNs often perceive\nphotographs containing the same anomalies as similar, even though these CNNs\nwere trained to detect unrelated conditions. This observation was based on the\nt-SNE visualization tool, which we decided to incorporate in our probabilistic\nmodel. Experiments on a dataset of 164,660 screening examinations from the\nOPHDIAT screening network show that 37 conditions, out of 41, can be detected\nwith an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938).\nIn particular, this framework significantly outperforms other frameworks for\ndetecting rare conditions, including multitask learning, transfer learning and\nSiamese networks, another few-shot learning solution. We expect these richer\npredictions to trigger the adoption of automated eye pathology screening, which\nwill revolutionize clinical practice in ophthalmology.\n",
    "topics": "{'Few-Shot Learning': 1.0, 'One-Shot Learning': 0.9973289, 'Transfer Learning': 0.824687}",
    "score": 0.8449240186
  },
  {
    "id": "2006.00304",
    "title": "SDCT-AuxNet$^{\\theta}$: DCT Augmented Stain Deconvolutional CNN with\n  Auxiliary Classifier for Cancer Diagnosis",
    "abstract": "  Acute lymphoblastic leukemia (ALL) is a pervasive pediatric white blood cell\ncancer across the globe. With the popularity of convolutional neural networks\n(CNNs), computer-aided diagnosis of cancer has attracted considerable\nattention. Such tools are easily deployable and are cost-effective. Hence,\nthese can enable extensive coverage of cancer diagnostic facilities. However,\nthe development of such a tool for ALL cancer was challenging so far due to the\nnon-availability of a large training dataset. The visual similarity between the\nmalignant and normal cells adds to the complexity of the problem. This paper\ndiscusses the recent release of a large dataset and presents a novel deep\nlearning architecture for the classification of cell images of ALL cancer. The\nproposed architecture, namely, SDCT-AuxNet$^{\\theta}$ is a 2-module framework\nthat utilizes a compact CNN as the main classifier in one module and a Kernel\nSVM as the auxiliary classifier in the other one. While CNN classifier uses\nfeatures through bilinear-pooling, spectral-averaged features are used by the\nauxiliary classifier. Further, this CNN is trained on the stain deconvolved\nquantity images in the optical density domain instead of the conventional RGB\nimages. A novel test strategy is proposed that exploits both the classifiers\nfor decision making using the confidence scores of their predicted class\nlabels. Elaborate experiments have been carried out on our recently released\npublic dataset of 15114 images of ALL cancer and healthy cells to establish the\nvalidity of the proposed methodology that is also robust to subject-level\nvariability. A weighted F1 score of 94.8$\\%$ is obtained that is best so far on\nthis challenging dataset.\n",
    "topics": "{'Decision Making': 0.98760986}",
    "score": 0.844894304
  },
  {
    "id": "1710.04835",
    "title": "Filmy Cloud Removal on Satellite Imagery with Multispectral Conditional\n  Generative Adversarial Nets",
    "abstract": "  In this paper, we propose a method for cloud removal from visible light RGB\nsatellite images by extending the conditional Generative Adversarial Networks\n(cGANs) from RGB images to multispectral images. Satellite images have been\nwidely utilized for various purposes, such as natural environment monitoring\n(pollution, forest or rivers), transportation improvement and prompt emergency\nresponse to disasters. However, the obscurity caused by clouds makes it\nunstable to monitor the situation on the ground with the visible light camera.\nImages captured by a longer wavelength are introduced to reduce the effects of\nclouds. Synthetic Aperture Radar (SAR) is such an example that improves\nvisibility even the clouds exist. On the other hand, the spatial resolution\ndecreases as the wavelength increases. Furthermore, the images captured by long\nwavelengths differs considerably from those captured by visible light in terms\nof their appearance. Therefore, we propose a network that can remove clouds and\ngenerate visible light images from the multispectral images taken as inputs.\nThis is achieved by extending the input channels of cGANs to be compatible with\nmultispectral images. The networks are trained to output images that are close\nto the ground truth using the images synthesized with clouds over the ground\ntruth as inputs. In the available dataset, the proportion of images of the\nforest or the sea is very high, which will introduce bias in the training\ndataset if uniformly sampled from the original dataset. Thus, we utilize the\nt-Distributed Stochastic Neighbor Embedding (t-SNE) to improve the problem of\nbias in the training dataset. Finally, we confirm the feasibility of the\nproposed network on the dataset of four bands images, which include three\nvisible light bands and one near-infrared (NIR) band.\n",
    "topics": "{}",
    "score": 0.8445699076
  },
  {
    "id": "2003.07640",
    "title": "EventSR: From Asynchronous Events to Image Reconstruction, Restoration,\n  and Super-Resolution via End-to-End Adversarial Learning",
    "abstract": "  Event cameras sense intensity changes and have many advantages over\nconventional cameras. To take advantage of event cameras, some methods have\nbeen proposed to reconstruct intensity images from event streams. However, the\noutputs are still in low resolution (LR), noisy, and unrealistic. The\nlow-quality outputs stem broader applications of event cameras, where high\nspatial resolution (HR) is needed as well as high temporal resolution, dynamic\nrange, and no motion blur. We consider the problem of reconstructing and\nsuper-resolving intensity images from LR events, when no ground truth (GT) HR\nimages and down-sampling kernels are available. To tackle the challenges, we\npropose a novel end-to-end pipeline that reconstructs LR images from event\nstreams, enhances the image qualities and upsamples the enhanced images, called\nEventSR. For the absence of real GT images, our method is primarily\nunsupervised, deploying adversarial learning. To train EventSR, we create an\nopen dataset including both real-world and simulated scenes. The use of both\ndatasets boosts up the network performance, and the network architectures and\nvarious loss functions in each phase help improve the image qualities. The\nwhole pipeline is trained in three phases. While each phase is mainly for one\nof the three tasks, the networks in earlier phases are fine-tuned by respective\nloss functions in an end-to-end manner. Experimental results show that EventSR\nreconstructs high-quality SR images from events for both simulated and\nreal-world data.\n",
    "topics": "{'Image Reconstruction': 0.9997309, 'Super-Resolution': 0.94849044, 'Super Resolution': 0.89610696}",
    "score": 0.8443178203
  },
  {
    "id": "1710.04681",
    "title": "Hyperspectral band selection using genetic algorithm and support vector\n  machines for early identification of charcoal rot disease in soybean",
    "abstract": "  Charcoal rot is a fungal disease that thrives in warm dry conditions and\naffects the yield of soybeans and other important agronomic crops worldwide.\nThere is a need for robust, automatic and consistent early detection and\nquantification of disease symptoms which are important in breeding programs for\nthe development of improved cultivars and in crop production for the\nimplementation of disease control measures for yield protection. Current\nmethods of plant disease phenotyping are predominantly visual and hence are\nslow and prone to human error and variation. There has been increasing interest\nin hyperspectral imaging applications for early detection of disease symptoms.\nHowever, the high dimensionality of hyperspectral data makes it very important\nto have an efficient analysis pipeline in place for the identification of\ndisease so that effective crop management decisions can be made. The focus of\nthis work is to determine the minimal number of most effective hyperspectral\nbands that can distinguish between healthy and diseased specimens early in the\ngrowing season. Healthy and diseased hyperspectral data cubes were captured at\n3, 6, 9, 12, and 15 days after inoculation. We utilized inoculated and control\nspecimens from 4 different genotypes. Each hyperspectral image was captured at\n240 different wavelengths in the range of 383 to 1032 nm. We used a combination\nof genetic algorithm as an optimizer and support vector machines as a\nclassifier for identification of maximally effective band combinations. A\nbinary classification between healthy and infected samples using six selected\nband combinations obtained a classification accuracy of 97% and a F1 score of\n0.97 for the infected class. The results demonstrated that these carefully\nchosen bands are more informative than RGB images, and could be used in a\nmultispectral camera for remote identification of charcoal rot infection in\nsoybean.\n",
    "topics": "{'Hyperspectral Image Classification': 0.9694873}",
    "score": 0.8442921036
  },
  {
    "id": "1901.09482",
    "title": "Bridging the Gap Between Computational Photography and Visual\n  Recognition",
    "abstract": "  What is the current state-of-the-art for image restoration and enhancement\napplied to degraded images acquired under less than ideal circumstances? Can\nthe application of such algorithms as a pre-processing step to improve image\ninterpretability for manual analysis or automatic visual recognition to\nclassify scene content? While there have been important advances in the area of\ncomputational photography to restore or enhance the visual quality of an image,\nthe capabilities of such techniques have not always translated in a useful way\nto visual recognition tasks. Consequently, there is a pressing need for the\ndevelopment of algorithms that are designed for the joint problem of improving\nvisual appearance and recognition, which will be an enabling factor for the\ndeployment of visual recognition tools in many real-world scenarios. To address\nthis, we introduce the UG^2 dataset as a large-scale benchmark composed of\nvideo imagery captured under challenging conditions, and two enhancement tasks\ndesigned to test algorithmic impact on visual quality and automatic object\nrecognition. Furthermore, we propose a set of metrics to evaluate the joint\nimprovement of such tasks as well as individual algorithmic advances, including\na novel psychophysics-based evaluation regime for human assessment and a\nrealistic set of quantitative measures for object recognition performance. We\nintroduce six new algorithms for image restoration or enhancement, which were\ncreated as part of the IARPA sponsored UG^2 Challenge workshop held at CVPR\n2018. Under the proposed evaluation regime, we present an in-depth analysis of\nthese algorithms and a host of deep learning-based and classic baseline\napproaches. From the observed results, it is evident that we are in the early\ndays of building a bridge between computational photography and visual\nrecognition, leaving many opportunities for innovation in this area.\n",
    "topics": "{'Image Restoration': 1.0, 'Object Recognition': 0.9999994}",
    "score": 0.8442590022
  },
  {
    "id": "1907.00437",
    "title": "INN: Inflated Neural Networks for IPMN Diagnosis",
    "abstract": "  Intraductal papillary mucinous neoplasm (IPMN) is a precursor to pancreatic\nductal adenocarcinoma. While over half of patients are diagnosed with\npancreatic cancer at a distant stage, patients who are diagnosed early enjoy a\nmuch higher 5-year survival rate of $34\\%$ compared to $3\\%$ in the former;\nhence, early diagnosis is key. Unique challenges in the medical imaging domain\nsuch as extremely limited annotated data sets and typically large 3D volumetric\ndata have made it difficult for deep learning to secure a strong foothold. In\nthis work, we construct two novel \"inflated\" deep network architectures,\n$\\textit{InceptINN}$ and $\\textit{DenseINN}$, for the task of diagnosing IPMN\nfrom multisequence (T1 and T2) MRI. These networks inflate their 2D layers to\n3D and bootstrap weights from their 2D counterparts (Inceptionv3 and\nDenseNet121 respectively) trained on ImageNet to the new 3D kernels. We also\nextend the inflation process by further expanding the pre-trained kernels to\nhandle any number of input modalities and different fusion strategies. This is\none of the first studies to train an end-to-end deep network on multisequence\nMRI for IPMN diagnosis, and shows that our proposed novel inflated network\narchitectures are able to handle the extremely limited training data (139 MRI\nscans), while providing an absolute improvement of $8.76\\%$ in accuracy for\ndiagnosing IPMN over the current state-of-the-art. Code is publicly available\nat https://github.com/lalonderodney/INN-Inflated-Neural-Nets.\n",
    "topics": "{}",
    "score": 0.844213028
  },
  {
    "id": "1702.08652",
    "title": "Scene Flow to Action Map: A New Representation for RGB-D based Action\n  Recognition with Convolutional Neural Networks",
    "abstract": "  Scene flow describes the motion of 3D objects in real world and potentially\ncould be the basis of a good feature for 3D action recognition. However, its\nuse for action recognition, especially in the context of convolutional neural\nnetworks (ConvNets), has not been previously studied. In this paper, we propose\nthe extraction and use of scene flow for action recognition from RGB-D data.\nPrevious works have considered the depth and RGB modalities as separate\nchannels and extract features for later fusion. We take a different approach\nand consider the modalities as one entity, thus allowing feature extraction for\naction recognition at the beginning. Two key questions about the use of scene\nflow for action recognition are addressed: how to organize the scene flow\nvectors and how to represent the long term dynamics of videos based on scene\nflow. In order to calculate the scene flow correctly on the available datasets,\nwe propose an effective self-calibration method to align the RGB and depth data\nspatially without knowledge of the camera parameters. Based on the scene flow\nvectors, we propose a new representation, namely, Scene Flow to Action Map\n(SFAM), that describes several long term spatio-temporal dynamics for action\nrecognition. We adopt a channel transform kernel to transform the scene flow\nvectors to an optimal color space analogous to RGB. This transformation takes\nbetter advantage of the trained ConvNets models over ImageNet. Experimental\nresults indicate that this new representation can surpass the performance of\nstate-of-the-art methods on two large public datasets.\n",
    "topics": "{'Action Recognition': 1.0, 'Temporal Action Localization': 0.9999956, '3D Action Recognition': 0.9948709}",
    "score": 0.8440618972
  },
  {
    "id": "1811.09782",
    "title": "Alternating Loss Correction for Preterm-Birth Prediction from EHR Data\n  with Noisy Labels",
    "abstract": "  In this paper we are interested in the prediction of preterm birth based on\ndiagnosis codes from longitudinal EHR. We formulate the prediction problem as a\nsupervised classification with noisy labels. Our base classifier is a Recurrent\nNeural Network with an attention mechanism. We assume the availability of a\ndata subset with both noisy and clean labels. For the cohort definition, most\nof the diagnosis codes on mothers' records related to pregnancy are ambiguous\nfor the definition of full-term and preterm classes. On the other hand,\ndiagnosis codes on babies' records provide fine-grained information on\nprematurity. Due to data de-identification, the links between mothers and\nbabies are not available. We developed a heuristic based on admission and\ndischarge times to match babies to their mothers and hence enrich mothers'\nrecords with additional information on delivery status. The obtained additional\ndataset from the matching heuristic has noisy labels and was used to leverage\nthe training of the deep learning model. We propose an Alternating Loss\nCorrection (ALC) method to train deep models with both clean and noisy labels.\nFirst, the label corruption matrix is estimated using the data subset with both\nnoisy and clean labels. Then it is used in the model as a dense output layer to\ncorrect for the label noise. The network is alternately trained on epochs with\nthe clean dataset with a simple cross-entropy loss and on next epoch with the\nnoisy dataset and a loss corrected with the estimated corruption matrix. The\nexperiments for the prediction of preterm birth at 90 days before delivery\nshowed an improvement in performance compared with baseline and state\nof-the-art methods.\n",
    "topics": "{}",
    "score": 0.8440529757
  },
  {
    "id": "2002.00569",
    "title": "DiverseDepth: Affine-invariant Depth Prediction Using Diverse Data",
    "abstract": "  We present a method for depth estimation with monocular images, which can\npredict high-quality depth on diverse scenes up to an affine transformation,\nthus preserving accurate shapes of a scene. Previous methods that predict\nmetric depth often work well only for a specific scene. In contrast, learning\nrelative depth (information of being closer or further) can enjoy better\ngeneralization, with the price of failing to recover the accurate geometric\nshape of the scene. In this work, we propose a dataset and methods to tackle\nthis dilemma, aiming to predict accurate depth up to an affine transformation\nwith good generalization to diverse scenes. First we construct a large-scale\nand diverse dataset, termed Diverse Scene Depth dataset (DiverseDepth), which\nhas a broad range of scenes and foreground contents. Compared with previous\nlearning objectives, i.e., learning metric depth or relative depth, we propose\nto learn the affine-invariant depth using our diverse dataset to ensure both\ngeneralization and high-quality geometric shapes of scenes. Furthermore, in\norder to train the model on the complex dataset effectively, we propose a\nmulti-curriculum learning method. Experiments show that our method outperforms\nprevious methods on 8 datasets by a large margin with the zero-shot test\nsetting, demonstrating the excellent generalization capacity of the learned\nmodel to diverse scenes. The reconstructed point clouds with the predicted\ndepth show that our method can recover high-quality 3D shapes. Code and dataset\nare available at: https://tinyurl.com/DiverseDepth\n",
    "topics": "{'Depth Estimation': 0.9997162, 'Curriculum Learning': 0.3236202}",
    "score": 0.8440295373
  },
  {
    "id": "2009.07536",
    "title": "Hybrid-Attention Guided Network with Multiple Resolution Features for\n  Person Re-Identification",
    "abstract": "  Extracting effective and discriminative features is very important for\naddressing the challenging person re-identification (re-ID) task. Prevailing\ndeep convolutional neural networks (CNNs) usually use high-level features for\nidentifying pedestrian. However, some essential spatial information resided in\nlow-level features such as shape, texture and color will be lost when learning\nthe high-level features, due to extensive padding and pooling operations in the\ntraining stage. In addition, most existing person re-ID methods are mainly\nbased on hand-craft bounding boxes where images are precisely aligned. It is\nunrealistic in practical applications, since the exploited object detection\nalgorithms often produce inaccurate bounding boxes. This will inevitably\ndegrade the performance of existing algorithms. To address these problems, we\nput forward a novel person re-ID model that fuses high- and low-level\nembeddings to reduce the information loss caused in learning high-level\nfeatures. Then we divide the fused embedding into several parts and reconnect\nthem to obtain the global feature and more significant local features, so as to\nalleviate the affect caused by the inaccurate bounding boxes. In addition, we\nalso introduce the spatial and channel attention mechanisms in our model, which\naims to mine more discriminative features related to the target. Finally, we\nreconstruct the feature extractor to ensure that our model can obtain more\nricher and robust features. Extensive experiments display the superiority of\nour approach compared with existing approaches. Our code is available at\nhttps://github.com/libraflower/MutipleFeature-for-PRID.\n",
    "topics": "{'Person Re-Identification': 1.0, '3D Object Detection': 0.66014063}",
    "score": 0.8439386298
  },
  {
    "id": "2001.06965",
    "title": "G2MF-WA: Geometric Multi-Model Fitting with Weakly Annotated Data",
    "abstract": "  In this paper we attempt to address the problem of geometric multi-model\nfitting with resorting to a few weakly annotated (WA) data points, which has\nbeen sparsely studied so far. In weak annotating, most of the manual\nannotations are supposed to be correct yet inevitably mixed with incorrect\nones. The WA data can be naturally obtained in an interactive way for specific\ntasks, for example, in the case of homography estimation, one can easily\nannotate points on the same plane/object with a single label by observing the\nimage. Motivated by this, we propose a novel method to make full use of the WA\ndata to boost the multi-model fitting performance. Specifically, a graph for\nmodel proposal sampling is first constructed using the WA data, given the prior\nthat the WA data annotated with the same weak label has a high probability of\nbeing assigned to the same model. By incorporating this prior knowledge into\nthe calculation of edge probabilities, vertices (i.e., data points) lie on/near\nthe latent model are likely to connect together and further form a\nsubset/cluster for effective proposals generation. With the proposals\ngenerated, the $\\alpha$-expansion is adopted for labeling, and our method in\nreturn updates the proposals. This works in an iterative way. Extensive\nexperiments validate our method and show that the proposed method produces\nnoticeably better results than state-of-the-art techniques in most cases.\n",
    "topics": "{}",
    "score": 0.8438090521
  },
  {
    "id": "1901.00204",
    "title": "Augmentation Scheme for Dealing with Imbalanced Network Traffic\n  Classification Using Deep Learning",
    "abstract": "  One of the most important tasks in network management is identifying\ndifferent types of traffic flows. As a result, a type of management service,\ncalled Network Traffic Classifier (NTC), has been introduced. One type of NTCs\nthat has gained huge attention in recent years applies deep learning on packets\nin order to classify flows. Internet is an imbalanced environment i.e., some\nclasses of applications are a lot more populated than others e.g., HTTP.\nAdditionally, one of the challenges in deep learning methods is that they do\nnot perform well in imbalanced environments in terms of evaluation metrics such\nas precision, recall, and $\\mathrm{F_1}$ measure. In order to solve this\nproblem, we recommend the use of augmentation methods to balance the dataset.\nIn this paper, we propose a novel data augmentation approach based on the use\nof Long Short Term Memory (LSTM) networks for generating traffic flow patterns\nand Kernel Density Estimation (KDE) for replicating the numerical features of\neach class. First, we use the LSTM network in order to learn and generate the\nsequence of packets in a flow for classes with less population. Then, we\ncomplete the features of the sequence with generating random values based on\nthe distribution of a certain feature, which will be estimated using KDE.\nFinally, we compare the training of a Convolutional Recurrent Neural Network\n(CRNN) in large-scale imbalanced, sampled, and augmented datasets. The\ncontribution of our augmentation scheme is then evaluated on all of the\ndatasets through measurements of precision, recall, and F1 measure for every\nclass of application. The results demonstrate that our scheme is well suited\nfor network traffic flow datasets and improves the performance of deep learning\nalgorithms when it comes to above-mentioned metrics.\n",
    "topics": "{'Density Estimation': 0.99999475, 'Data Augmentation': 0.9932318}",
    "score": 0.8437177529
  },
  {
    "id": "2010.02087",
    "title": "Machine learning on Crays to optimise petrophysical workflows in oil and\n  gas exploration",
    "abstract": "  The oil and gas industry is awash with sub-surface data, which is used to\ncharacterize the rock and fluid properties beneath the seabed. This in turn\ndrives commercial decision making and exploration, but the industry currently\nrelies upon highly manual workflows when processing data. A key question is\nwhether this can be improved using machine learning to complement the\nactivities of petrophysicists searching for hydrocarbons. In this paper we\npresent work done, in collaboration with Rock Solid Images (RSI), using\nsupervised machine learning on a Cray XC30 to train models that streamline the\nmanual data interpretation process. With a general aim of decreasing the\npetrophysical interpretation time down from over 7 days to 7 minutes, in this\npaper we describe the use of mathematical models that have been trained using\nraw well log data, for completing each of the four stages of a petrophysical\ninterpretation workflow, along with initial data cleaning. We explore how the\npredictions from these models compare against the interpretations of human\npetrophysicists, along with numerous options and techniques that were used to\noptimise the prediction of our models. The power provided by modern\nsupercomputers such as Cray machines is crucial here, but some popular machine\nlearning framework are unable to take full advantage of modern HPC machines. As\nsuch we will also explore the suitability of the machine learning tools we have\nused, and describe steps we took to work round their limitations. The result of\nthis work is the ability, for the first time, to use machine learning for the\nentire petrophysical workflow. Whilst there are numerous challenges,\nlimitations and caveats, we demonstrate that machine learning has an important\nrole to play in the processing of sub-surface data.\n",
    "topics": "{'Decision Making': 0.94812554}",
    "score": 0.8437165897
  },
  {
    "id": "1704.08803",
    "title": "Neural Ranking Models with Weak Supervision",
    "abstract": "  Despite the impressive improvements achieved by unsupervised deep neural\nnetworks in computer vision and NLP tasks, such improvements have not yet been\nobserved in ranking for information retrieval. The reason may be the complexity\nof the ranking problem, as it is not obvious how to learn from queries and\ndocuments when no supervised signal is available. Hence, in this paper, we\npropose to train a neural ranking model using weak supervision, where labels\nare obtained automatically without human annotators or any external resources\n(e.g., click data). To this aim, we use the output of an unsupervised ranking\nmodel, such as BM25, as a weak supervision signal. We further train a set of\nsimple yet effective ranking models based on feed-forward neural networks. We\nstudy their effectiveness under various learning scenarios (point-wise and\npair-wise models) and using different input representations (i.e., from\nencoding query-document pairs into dense/sparse vectors to using word embedding\nrepresentation). We train our networks using tens of millions of training\ninstances and evaluate it on two standard collections: a homogeneous news\ncollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).\nOur experiments indicate that employing proper objective functions and letting\nthe networks to learn the input representation based on weakly supervised data\nleads to impressive performance, with over 13% and 35% MAP improvements over\nthe BM25 model on the Robust and the ClueWeb collections. Our findings also\nsuggest that supervised neural ranking models can greatly benefit from\npre-training on large amounts of weakly labeled data that can be easily\nobtained from unsupervised IR models.\n",
    "topics": "{'Information Retrieval': 0.8247988}",
    "score": 0.8436852291
  },
  {
    "id": "1703.10480",
    "title": "A deep learning classification scheme based on augmented-enhanced\n  features to segment organs at risk on the optic region in brain cancer\n  patients",
    "abstract": "  Radiation therapy has emerged as one of the preferred techniques to treat\nbrain cancer patients. During treatment, a very high dose of radiation is\ndelivered to a very narrow area. Prescribed radiation therapy for brain cancer\nrequires precisely defining the target treatment area, as well as delineating\nvital brain structures which must be spared from radiotoxicity. Nevertheless,\ndelineation task is usually still manually performed, which is inefficient and\noperator-dependent. Several attempts of automatizing this process have\nreported. however, marginal results when analyzing organs in the optic region.\nIn this work we present a deep learning classification scheme based on\naugmented-enhanced features to automatically segment organs at risk (OARs) in\nthe optic region -optic nerves, optic chiasm, pituitary gland and pituitary\nstalk-. Fifteen MR images with various types of brain tumors were\nretrospectively collected to undergo manual and automatic segmentation. Mean\nDice Similarity coefficients around 0.80 were reported. Incorporation of\nproposed features yielded to improvements on the segmentation. Compared with\nsupport vector machines, our method achieved better performance with less\nvariation on the results, as well as a considerably reduction on the\nclassification time. Performance of the proposed approach was also evaluated\nwith respect to manual contours. In this case, results obtained from the\nautomatic contours mostly lie on the variability of the observers, showing no\nsignificant differences with respect to them. These results suggest therefore\nthat the proposed system is more accurate than other presented approaches, up\nto date, to segment these structures. The speed, reproducibility, and\nrobustness of the process make the proposed deep learning-based classification\nsystem a valuable tool for assisting in the delineation task of small OARs in\nbrain cancer.\n",
    "topics": "{}",
    "score": 0.8435584419
  },
  {
    "id": "1809.10816",
    "title": "Generative Adversarial Active Learning for Unsupervised Outlier\n  Detection",
    "abstract": "  Outlier detection is an important topic in machine learning and has been used\nin a wide range of applications. In this paper, we approach outlier detection\nas a binary-classification issue by sampling potential outliers from a uniform\nreference distribution. However, due to the sparsity of data in\nhigh-dimensional space, a limited number of potential outliers may fail to\nprovide sufficient information to assist the classifier in describing a\nboundary that can separate outliers from normal data effectively. To address\nthis, we propose a novel Single-Objective Generative Adversarial Active\nLearning (SO-GAAL) method for outlier detection, which can directly generate\ninformative potential outliers based on the mini-max game between a generator\nand a discriminator. Moreover, to prevent the generator from falling into the\nmode collapsing problem, the stop node of training should be determined when\nSO-GAAL is able to provide sufficient information. But without any prior\ninformation, it is extremely difficult for SO-GAAL. Therefore, we expand the\nnetwork structure of SO-GAAL from a single generator to multiple generators\nwith different objectives (MO-GAAL), which can generate a reasonable reference\ndistribution for the whole dataset. We empirically compare the proposed\napproach with several state-of-the-art outlier detection methods on both\nsynthetic and real-world datasets. The results show that MO-GAAL outperforms\nits competitors in the majority of cases, especially for datasets with various\ncluster types or high irrelevant variable ratio.\n",
    "topics": "{'Outlier Detection': 1.0, 'Active Learning': 0.9972862}",
    "score": 0.8434349591
  },
  {
    "id": "1810.04021",
    "title": "Deep Geodesic Learning for Segmentation and Anatomical Landmarking",
    "abstract": "  In this paper, we propose a novel deep learning framework for anatomy\nsegmentation and automatic landmark- ing. Specifically, we focus on the\nchallenging problem of mandible segmentation from cone-beam computed tomography\n(CBCT) scans and identification of 9 anatomical landmarks of the mandible on\nthe geodesic space. The overall approach employs three inter-related steps. In\nstep 1, we propose a deep neu- ral network architecture with carefully designed\nregularization, and network hyper-parameters to perform image segmentation\nwithout the need for data augmentation and complex post- processing refinement.\nIn step 2, we formulate the landmark localization problem directly on the\ngeodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose\nto use a long short-term memory (LSTM) network to identify closely- spaced\nlandmarks, which is rather difficult to obtain using other standard detection\nnetworks. The proposed fully automated method showed superior efficacy compared\nto the state-of-the- art mandible segmentation and landmarking approaches in\ncraniofacial anomalies and diseased states. We used a very challenging CBCT\ndataset of 50 patients with a high-degree of craniomaxillofacial (CMF)\nvariability that is realistic in clinical practice. Complementary to the\nquantitative analysis, the qualitative visual inspection was conducted for\ndistinct CBCT scans from 250 patients with high anatomical variability. We have\nalso shown feasibility of the proposed work in an independent dataset from\nMICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance.\nLastly, we present an in-depth analysis of the proposed deep networks with\nrespect to the choice of hyper-parameters such as pooling and activation\nfunctions.\n",
    "topics": "{'Computed Tomography (CT)': 0.9984939, 'Data Augmentation': 0.8727228}",
    "score": 0.8433491448
  },
  {
    "id": "1811.00472",
    "title": "Class-Agnostic Counting",
    "abstract": "  Nearly all existing counting methods are designed for a specific object\nclass. Our work, however, aims to create a counting model able to count any\nclass of object. To achieve this goal, we formulate counting as a matching\nproblem, enabling us to exploit the image self-similarity property that\nnaturally exists in object counting problems. We make the following three\ncontributions: first, a Generic Matching Network (GMN) architecture that can\npotentially count any object in a class-agnostic manner; second, by\nreformulating the counting problem as one of matching objects, we can take\nadvantage of the abundance of video data labeled for tracking, which contains\nnatural repetitions suitable for training a counting model. Such data enables\nus to train the GMN. Third, to customize the GMN to different user\nrequirements, an adapter module is used to specialize the model with minimal\neffort, i.e. using a few labeled examples, and adapting only a small fraction\nof the trained parameters. This is a form of few-shot learning, which is\npractical for domains where labels are limited due to requiring expert\nknowledge (e.g. microbiology). We demonstrate the flexibility of our method on\na diverse set of existing counting benchmarks: specifically cells, cars, and\nhuman crowds. The model achieves competitive performance on cell and crowd\ncounting datasets, and surpasses the state-of-the-art on the car dataset using\nonly three training images. When training on the entire dataset, the proposed\nmethod outperforms all previous methods by a large margin.\n",
    "topics": "{'Crowd Counting': 1.0, 'Few-Shot Learning': 0.996145}",
    "score": 0.8433245128
  },
  {
    "id": "1909.07072",
    "title": "A Real-Time Cross-modality Correlation Filtering Method for Referring\n  Expression Comprehension",
    "abstract": "  Referring expression comprehension aims to localize the object instance\ndescribed by a natural language expression. Current referring expression\nmethods have achieved good performance. However, none of them is able to\nachieve real-time inference without accuracy drop. The reason for the\nrelatively slow inference speed is that these methods artificially split the\nreferring expression comprehension into two sequential stages including\nproposal generation and proposal ranking. It does not exactly conform to the\nhabit of human cognition. To this end, we propose a novel Realtime\nCross-modality Correlation Filtering method (RCCF). RCCF reformulates the\nreferring expression comprehension as a correlation filtering process. The\nexpression is first mapped from the language domain to the visual domain and\nthen treated as a template (kernel) to perform correlation filtering on the\nimage feature map. The peak value in the correlation heatmap indicates the\ncenter points of the target box. In addition, RCCF also regresses a 2-D object\nsize and 2-D offset. The center point coordinates, object size and center point\noffset together to form the target bounding box. Our method runs at 40 FPS\nwhile achieving leading performance in RefClef, RefCOCO, RefCOCO+ and RefCOCOg\nbenchmarks. In the challenging RefClef dataset, our methods almost double the\nstate-of-the-art performance (34.70% increased to 63.79%). We hope this work\ncan arouse more attention and studies to the new cross-modality correlation\nfiltering framework as well as the one-stage framework for referring expression\ncomprehension.\n",
    "topics": "{}",
    "score": 0.8432727877
  },
  {
    "id": "1710.06854",
    "title": "Material Classification using Neural Networks",
    "abstract": "  The recognition and classification of the diversity of materials that exist\nin the environment around us are a key visual competence that computer vision\nsystems focus on in recent years. Understanding the identification of materials\nin distinct images involves a deep process that has made usage of the recent\nprogress in neural networks which has brought the potential to train\narchitectures to extract features for this challenging task. This project uses\nstate-of-the-art Convolutional Neural Network (CNN) techniques and Support\nVector Machine (SVM) classifiers in order to classify materials and analyze the\nresults. Building on various widely used material databases collected, a\nselection of CNN architectures is evaluated to understand which is the best\napproach to extract features in order to achieve outstanding results for the\ntask. The results gathered over four material datasets and nine CNNs outline\nthat the best overall performance of a CNN using a linear SVM can achieve up to\n~92.5% mean average precision, while applying a new relevant direction in\ncomputer vision, transfer learning. By limiting the amount of information\nextracted from the layer before the last fully connected layer, transfer\nlearning aims at analyzing the contribution of shading information and\nreflectance to identify which main characteristics decide the material category\nthe image belongs to. In addition to the main topic of my project, the\nevaluation of the nine different CNN architectures, it is questioned if, by\nusing the transfer learning instead of extracting the information from the last\nconvolutional layer, the total accuracy of the system created improves. The\nresults of the comparison emphasize the fact that the accuracy and performance\nof the system improve, especially in the datasets which consist of a large\nnumber of images.\n",
    "topics": "{'Transfer Learning': 0.9997584}",
    "score": 0.843269215
  },
  {
    "id": "1908.07201",
    "title": "Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from\n  Images \"In the Wild\"",
    "abstract": "  We present the first method to perform automatic 3D pose, shape and texture\ncapture of animals from images acquired in-the-wild. In particular, we focus on\nthe problem of capturing 3D information about Grevy's zebras from a collection\nof images. The Grevy's zebra is one of the most endangered species in Africa,\nwith only a few thousand individuals left. Capturing the shape and pose of\nthese animals can provide biologists and conservationists with information\nabout animal health and behavior. In contrast to research on human pose, shape\nand texture estimation, training data for endangered species is limited, the\nanimals are in complex natural scenes with occlusion, they are naturally\ncamouflaged, travel in herds, and look similar to each other. To overcome these\nchallenges, we integrate the recent SMAL animal model into a network-based\nregression pipeline, which we train end-to-end on synthetically generated\nimages with pose, shape, and background variation. Going beyond\nstate-of-the-art methods for human shape and pose estimation, our method learns\na shape space for zebras during training. Learning such a shape space from\nimages using only a photometric loss is novel, and the approach can be used to\nlearn shape in other settings with limited 3D supervision. Moreover, we couple\n3D pose and shape prediction with the task of texture synthesis, obtaining a\nfull texture map of the animal from a single image. We show that the predicted\ntexture map allows a novel per-instance unsupervised optimization over the\nnetwork features. This method, SMALST (SMAL with learned Shape and Texture)\ngoes beyond previous work, which assumed manual keypoints and/or segmentation,\nto regress directly from pixels to 3D animal shape, pose and texture. Code and\ndata are available at https://github.com/silviazuffi/smalst.\n",
    "topics": "{'Texture Synthesis': 0.9997907, 'Pose Estimation': 0.8237741}",
    "score": 0.8431932912
  },
  {
    "id": "1803.02315",
    "title": "Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray\n  Classification",
    "abstract": "  The increased availability of X-ray image archives (e.g. the ChestX-ray14\ndataset from the NIH Clinical Center) has triggered a growing interest in deep\nlearning techniques. To provide better insight into the different approaches,\nand their applications to chest X-ray classification, we investigate a powerful\nnetwork architecture in detail: the ResNet-50. Building on prior work in this\ndomain, we consider transfer learning with and without fine-tuning as well as\nthe training of a dedicated X-ray network from scratch. To leverage the high\nspatial resolution of X-ray data, we also include an extended ResNet-50\narchitecture, and a network integrating non-image data (patient age, gender and\nacquisition type) in the classification process. In a concluding experiment, we\nalso investigate multiple ResNet depths (i.e. ResNet-38 and ResNet-101). In a\nsystematic evaluation, using 5-fold re-sampling and a multi-label loss\nfunction, we compare the performance of the different approaches for pathology\nclassification by ROC statistics and analyze differences between the\nclassifiers using rank correlation. Overall, we observe a considerable spread\nin the achieved performance and conclude that the X-ray-specific ResNet-38,\nintegrating non-image data yields the best overall results. Furthermore, class\nactivation maps are used to understand the classification process, and a\ndetailed analysis of the impact of non-image features is provided.\n",
    "topics": "{'Transfer Learning': 0.9897653}",
    "score": 0.8431105235
  },
  {
    "id": "1411.2861",
    "title": "Computational Baby Learning",
    "abstract": "  Intuitive observations show that a baby may inherently possess the capability\nof recognizing a new visual concept (e.g., chair, dog) by learning from only\nvery few positive instances taught by parent(s) or others, and this recognition\ncapability can be gradually further improved by exploring and/or interacting\nwith the real instances in the physical world. Inspired by these observations,\nwe propose a computational model for slightly-supervised object detection,\nbased on prior knowledge modelling, exemplar learning and learning with video\ncontexts. The prior knowledge is modeled with a pre-trained Convolutional\nNeural Network (CNN). When very few instances of a new concept are given, an\ninitial concept detector is built by exemplar learning over the deep features\nfrom the pre-trained CNN. Simulating the baby's interaction with physical\nworld, the well-designed tracking solution is then used to discover more\ndiverse instances from the massive online unlabeled videos. Once a positive\ninstance is detected/identified with high score in each video, more variable\ninstances possibly from different view-angles and/or different distances are\ntracked and accumulated. Then the concept detector can be fine-tuned based on\nthese new instances. This process can be repeated again and again till we\nobtain a very mature concept detector. Extensive experiments on Pascal\nVOC-07/10/12 object detection datasets well demonstrate the effectiveness of\nour framework. It can beat the state-of-the-art full-training based\nperformances by learning from very few samples for each object category, along\nwith about 20,000 unlabeled videos.\n",
    "topics": "{'Object Detection': 0.9991767, 'Weakly Supervised Object Detection': 0.94595766}",
    "score": 0.8429428555
  },
  {
    "id": "1911.01705",
    "title": "A GMM based algorithm to generate point-cloud and its application to\n  neuroimaging",
    "abstract": "  Recent years have witnessed the emergence of 3D medical imaging techniques\nwith the development of 3D sensors and technology. Due to the presence of noise\nin image acquisition, registration researchers focused on an alternative way to\nrepresent medical images. An alternative way to analyze medical imaging is by\nunderstanding the 3D shapes represented in terms of point-cloud. Though in the\nmedical imaging community, 3D point-cloud processing is not a ``go-to'' choice,\nit is a ``natural'' way to capture 3D shapes. However, as the number of samples\nfor medical images are small, researchers have used pre-trained models to\nfine-tune on medical images. Furthermore, due to different modality in medical\nimages, standard generative models can not be used to generate new samples of\nmedical images. In this work, we use the advantage of point-cloud\nrepresentation of 3D structures of medical images and propose a Gaussian\nmixture model-based generation scheme. Our proposed method is robust to\noutliers. Experimental validation has been performed to show that the proposed\nscheme can generate new 3D structures using interpolation techniques, i.e.,\ngiven two 3D structures represented as point-clouds, we can generate\npoint-clouds in between. We have also generated new point-clouds for subjects\nwith and without dementia and show that the generated samples are indeed\nclosely matched to the respective training samples from the same class.\n",
    "topics": "{'Medical Image Segmentation': 0.3695366}",
    "score": 0.8429026221
  },
  {
    "id": "2001.11202",
    "title": "Image Embedded Segmentation: Uniting Supervised and Unsupervised\n  Objectives for Segmenting Histopathological Images",
    "abstract": "  This paper presents a new regularization method to train a fully\nconvolutional network for semantic tissue segmentation in histopathological\nimages. This method relies on the benefit of unsupervised learning, in the form\nof image reconstruction, for network training. To this end, it puts forward an\nidea of defining a new embedding that allows uniting the main supervised task\nof semantic segmentation and an auxiliary unsupervised task of image\nreconstruction into a single one and proposes to learn this united task by a\nsingle generative model. This embedding generates an output image by\nsuperimposing an input image on its segmentation map. Then, the method learns\nto translate the input image to this embedded output image using a conditional\ngenerative adversarial network, which is known as quite effective for\nimage-to-image translations. This proposal is different than the existing\napproach that uses image reconstruction for the same regularization purpose.\nThe existing approach considers segmentation and image reconstruction as two\nseparate tasks in a multi-task network, defines their losses independently, and\ncombines them in a joint loss function. However, the definition of such a\nfunction requires externally determining right contributions of the supervised\nand unsupervised losses that yield balanced learning between the segmentation\nand image reconstruction tasks. The proposed approach provides an easier\nsolution to this problem by uniting these two tasks into a single one, which\nintrinsically combines their losses. We test our approach on three datasets of\nhistopathological images. Our experiments demonstrate that it leads to better\nsegmentation results in these datasets, compared to its counterparts.\n",
    "topics": "{'Image Reconstruction': 1.0, 'Semantic Segmentation': 0.9952989}",
    "score": 0.8429003493
  },
  {
    "id": "1810.11973",
    "title": "Feature Bagging for Steganographer Identification",
    "abstract": "  Traditional steganalysis algorithms focus on detecting the existence of\nsteganography in a single object. In practice, one may face a complex scenario\nwhere one or some of multiple users also called actors are guilty of using\nsteganography, which is defined as the steganographer identification problem\n(SIP). This requires steganalysis experts to design effective and robust\ndetection algorithms to identify the guilty actor(s). The mainstream works use\nclustering, ensemble and anomaly detection, where distances in high dimensional\nspace between features of actors are determined to find out the outlier(s)\ncorresponding to steganographer(s). However, in high dimensional space, feature\npoints could be sparse such that distances between feature points may become\nrelatively similar to each other, which cannot benefit the detection. Moreover,\nit is well-known in machine learning that combining techniques such as boosting\nand bagging can be effective in improving detection performance. This motivates\nthe authors in this paper to present a feature bagging approach to SIP. The\nproposed work merges results from multiple detection sub-models, each of which\nfeature space is randomly sampled from the raw full dimensional space. We\ncreate a new dataset called ImgNetEase including 5108 images downloaded from a\nsocial website to mimic the real-world scenario. We extract PEV-274 features\nfrom images, and take nsF5 as the steganographic algorithm for evaluation.\nExperiments have shown that our work improves the detection accuracy\nsignificantly on created dataset in most cases, which has shown the superiority\nand applicability.\n",
    "topics": "{'Anomaly Detection': 0.9855629}",
    "score": 0.8426143095
  },
  {
    "id": "1803.11078",
    "title": "Asymmetric Loss Functions and Deep Densely Connected Networks for Highly\n  Imbalanced Medical Image Segmentation: Application to Multiple Sclerosis\n  Lesion Detection",
    "abstract": "  Fully convolutional deep neural networks have been asserted to be fast and\nprecise frameworks with great potential in image segmentation. One of the major\nchallenges in training such networks raises when data is unbalanced, which is\ncommon in many medical imaging applications such as lesion segmentation where\nlesion class voxels are often much lower in numbers than non-lesion voxels. A\ntrained network with unbalanced data may make predictions with high precision\nand low recall, being severely biased towards the non-lesion class which is\nparticularly undesired in most medical applications where FNs are more\nimportant than FPs. Various methods have been proposed to address this problem,\nmore recently similarity loss functions and focal loss. In this work we trained\nfully convolutional deep neural networks using an asymmetric similarity loss\nfunction to mitigate the issue of data imbalance and achieve much better\ntradeoff between precision and recall. To this end, we developed a 3D\nFC-DenseNet with large overlapping image patches as input and an asymmetric\nsimilarity loss layer based on Tversky index (using Fbeta scores). We used\nlarge overlapping image patches as inputs for intrinsic and extrinsic data\naugmentation, a patch selection algorithm, and a patch prediction fusion\nstrategy using B-spline weighted soft voting to account for the uncertainty of\nprediction in patch borders. We applied this method to MS lesion segmentation\nbased on two different datasets of MSSEG and ISBI longitudinal MS lesion\nsegmentation challenge, where we achieved top performance in both challenges.\nOur network trained with focal loss ranked first according to the ISBI\nchallenge overall score and resulted in the lowest reported lesion false\npositive rate among all submitted methods. Our network trained with the\nasymmetric similarity loss led to the lowest surface distance and the best\nlesion true positive rate.\n",
    "topics": "{'Lesion Segmentation': 1.0, 'Medical Image Segmentation': 0.99979633, 'Semantic Segmentation': 0.9885499, 'Data Augmentation': 0.9748585}",
    "score": 0.8424693143
  },
  {
    "id": "2008.02457",
    "title": "Graph Convolutional Networks for Hyperspectral Image Classification",
    "abstract": "  Convolutional neural networks (CNNs) have been attracting increasing\nattention in hyperspectral (HS) image classification, owing to their ability to\ncapture spatial-spectral feature representations. Nevertheless, their ability\nin modeling relations between samples remains limited. Beyond the limitations\nof grid sampling, graph convolutional networks (GCNs) have been recently\nproposed and successfully applied in irregular (or non-grid) data\nrepresentation and analysis. In this paper, we thoroughly investigate CNNs and\nGCNs (qualitatively and quantitatively) in terms of HS image classification.\nDue to the construction of the adjacency matrix on all the data, traditional\nGCNs usually suffer from a huge computational cost, particularly in large-scale\nremote sensing (RS) problems. To this end, we develop a new mini-batch GCN\n(called miniGCN hereinafter) which allows to train large-scale GCNs in a\nmini-batch fashion. More significantly, our miniGCN is capable of inferring\nout-of-sample data without re-training networks and improving classification\nperformance. Furthermore, as CNNs and GCNs can extract different types of HS\nfeatures, an intuitive solution to break the performance bottleneck of a single\nmodel is to fuse them. Since miniGCNs can perform batch-wise network training\n(enabling the combination of CNNs and GCNs) we explore three fusion strategies:\nadditive fusion, element-wise multiplicative fusion, and concatenation fusion\nto measure the obtained performance gain. Extensive experiments, conducted on\nthree HS datasets, demonstrate the advantages of miniGCNs over GCNs and the\nsuperiority of the tested fusion strategies with regards to the single CNN or\nGCN models. The codes of this work will be available at\nhttps://github.com/danfenghong/IEEE_TGRS_GCN for the sake of reproducibility.\n",
    "topics": "{'Image Classification': 0.9999982, 'Hyperspectral Image Classification': 0.8731196}",
    "score": 0.8424338856
  },
  {
    "id": "1808.01134",
    "title": "iSPA-Net: Iterative Semantic Pose Alignment Network",
    "abstract": "  Understanding and extracting 3D information of objects from monocular 2D\nimages is a fundamental problem in computer vision. In the task of 3D object\npose estimation, recent data driven deep neural network based approaches suffer\nfrom scarcity of real images with 3D keypoint and pose annotations. Drawing\ninspiration from human cognition, where the annotators use a 3D CAD model as\nstructural reference to acquire ground-truth viewpoints for real images; we\npropose an iterative Semantic Pose Alignment Network, called iSPA-Net. Our\napproach focuses on exploiting semantic 3D structural regularity to solve the\ntask of fine-grained pose estimation by predicting viewpoint difference between\na given pair of images. Such image comparison based approach also alleviates\nthe problem of data scarcity and hence enhances scalability of the proposed\napproach for novel object categories with minimal annotation. The fine-grained\nobject pose estimator is also aided by correspondence of learned spatial\ndescriptor of the input image pair. The proposed pose alignment framework\nenjoys the faculty to refine its initial pose estimation in consecutive\niterations by utilizing an online rendering setup along with effectiveness of a\nnon-uniform bin classification of pose-difference. This enables iSPA-Net to\nachieve state-of-the-art performance on various real image viewpoint estimation\ndatasets. Further, we demonstrate effectiveness of the approach for multiple\napplications. First, we show results for active object viewpoint localization\nto capture images from similar pose considering only a single image as pose\nreference. Second, we demonstrate the ability of the learned semantic\ncorrespondence to perform unsupervised part-segmentation transfer using only a\nsingle part-annotated 3D template model per object class. To encourage\nreproducible research, we have released the codes for our proposed algorithm.\n",
    "topics": "{'Pose Estimation': 0.99999654}",
    "score": 0.8424258506
  },
  {
    "id": "1907.08719",
    "title": "Cross-Domain Car Detection Using Unsupervised Image-to-Image\n  Translation: From Day to Night",
    "abstract": "  Deep learning techniques have enabled the emergence of state-of-the-art\nmodels to address object detection tasks. However, these techniques are\ndata-driven, delegating the accuracy to the training dataset which must\nresemble the images in the target task. The acquisition of a dataset involves\nannotating images, an arduous and expensive process, generally requiring time\nand manual effort. Thus, a challenging scenario arises when the target domain\nof application has no annotated dataset available, making tasks in such\nsituation to lean on a training dataset of a different domain. Sharing this\nissue, object detection is a vital task for autonomous vehicles where the large\namount of driving scenarios yields several domains of application requiring\nannotated data for the training process. In this work, a method for training a\ncar detection system with annotated data from a source domain (day images)\nwithout requiring the image annotations of the target domain (night images) is\npresented. For that, a model based on Generative Adversarial Networks (GANs) is\nexplored to enable the generation of an artificial dataset with its respective\nannotations. The artificial dataset (fake dataset) is created translating\nimages from day-time domain to night-time domain. The fake dataset, which\ncomprises annotated images of only the target domain (night images), is then\nused to train the car detector model. Experimental results showed that the\nproposed method achieved significant and consistent improvements, including the\nincreasing by more than 10% of the detection performance when compared to the\ntraining with only the available annotated data (i.e., day images).\n",
    "topics": "{'Autonomous Vehicles': 0.9999987, 'Unsupervised Image-To-Image Translation': 0.9990509, 'Object Detection': 0.9980337, 'Image-to-Image Translation': 0.9899138}",
    "score": 0.842415384
  },
  {
    "id": "1811.05304",
    "title": "Self-Supervised Learning of Depth and Camera Motion from 360{\\deg}\n  Videos",
    "abstract": "  As 360{\\deg} cameras become prevalent in many autonomous systems (e.g.,\nself-driving cars and drones), efficient 360{\\deg} perception becomes more and\nmore important. We propose a novel self-supervised learning approach for\npredicting the omnidirectional depth and camera motion from a 360{\\deg} video.\nIn particular, starting from the SfMLearner, which is designed for cameras with\nnormal field-of-view, we introduce three key features to process 360{\\deg}\nimages efficiently. Firstly, we convert each image from equirectangular\nprojection to cubic projection in order to avoid image distortion. In each\nnetwork layer, we use Cube Padding (CP), which pads intermediate features from\nadjacent faces, to avoid image boundaries. Secondly, we propose a novel\n\"spherical\" photometric consistency constraint on the whole viewing sphere. In\nthis way, no pixel will be projected outside the image boundary which typically\nhappens in images with normal field-of-view. Finally, rather than naively\nestimating six independent camera motions (i.e., naively applying SfM-Learner\nto each face on a cube), we propose a novel camera pose consistency loss to\nensure the estimated camera motions reaching consensus. To train and evaluate\nour approach, we collect a new PanoSUNCG dataset containing a large amount of\n360{\\deg} videos with groundtruth depth and camera motion. Our approach\nachieves state-of-the-art depth prediction and camera motion estimation on\nPanoSUNCG with faster inference speed comparing to equirectangular. In\nreal-world indoor videos, our approach can also achieve qualitatively\nreasonable depth prediction by acquiring model pre-trained on PanoSUNCG.\n",
    "topics": "{'Self-Supervised Learning': 1.0, 'Self-Driving Cars': 1.0, 'Motion Estimation': 0.9999999, 'Depth Estimation': 0.9993624}",
    "score": 0.8423953042
  },
  {
    "id": "1811.08837",
    "title": "Recognizing Disguised Faces in the Wild",
    "abstract": "  Research in face recognition has seen tremendous growth over the past couple\nof decades. Beginning from algorithms capable of performing recognition in\nconstrained environments, the current face recognition systems achieve very\nhigh accuracies on large-scale unconstrained face datasets. While upcoming\nalgorithms continue to achieve improved performance, a majority of the face\nrecognition systems are susceptible to failure under disguise variations, one\nof the most challenging covariate of face recognition. Most of the existing\ndisguise datasets contain images with limited variations, often captured in\ncontrolled settings. This does not simulate a real world scenario, where both\nintentional and unintentional unconstrained disguises are encountered by a face\nrecognition system. In this paper, a novel Disguised Faces in the Wild (DFW)\ndataset is proposed which contains over 11000 images of 1000 identities with\ndifferent types of disguise accessories. The dataset is collected from the\nInternet, resulting in unconstrained face images similar to real world\nsettings. This is the first-of-a-kind dataset with the availability of\nimpersonator and genuine obfuscated face images for each subject. The proposed\ndataset has been analyzed in terms of three levels of difficulty: (i) easy,\n(ii) medium, and (iii) hard in order to showcase the challenging nature of the\nproblem. It is our view that the research community can greatly benefit from\nthe DFW dataset in terms of developing algorithms robust to such adversaries.\nThe proposed dataset was released as part of the First International Workshop\nand Competition on Disguised Faces in the Wild at CVPR, 2018. This paper\npresents the DFW dataset in detail, including the evaluation protocols,\nbaseline results, performance analysis of the submissions received as part of\nthe competition, and three levels of difficulties of the DFW challenge dataset.\n",
    "topics": "{'Face Recognition': 1.0}",
    "score": 0.8422274881
  },
  {
    "id": "1906.07794",
    "title": "Convolutional neural network models for cancer type prediction based on\n  gene expression",
    "abstract": "  Background Precise prediction of cancer types is vital for cancer diagnosis\nand therapy. Important cancer marker genes can be inferred through predictive\nmodel. Several studies have attempted to build machine learning models for this\ntask however none has taken into consideration the effects of tissue of origin\nthat can potentially bias the identification of cancer markers. Results In this\npaper, we introduced several Convolutional Neural Network (CNN) models that\ntake unstructured gene expression inputs to classify tumor and non-tumor\nsamples into their designated cancer types or as normal. Based on different\ndesigns of gene embeddings and convolution schemes, we implemented three CNN\nmodels: 1D-CNN, 2D-Vanilla-CNN, and 2D-Hybrid-CNN. The models were trained and\ntested on combined 10,340 samples of 33 cancer types and 731 matched normal\ntissues of The Cancer Genome Atlas (TCGA). Our models achieved excellent\nprediction accuracies (93.9-95.0%) among 34 classes (33 cancers and normal).\nFurthermore, we interpreted one of the models, known as 1D-CNN model, with a\nguided saliency technique and identified a total of 2,090 cancer markers (108\nper class). The concordance of differential expression of these markers between\nthe cancer type they represent and others is confirmed. In breast cancer, for\ninstance, our model identified well-known markers, such as GATA3 and ESR1.\nFinally, we extended the 1D-CNN model for prediction of breast cancer subtypes\nand achieved an average accuracy of 88.42% among 5 subtypes. The codes can be\nfound at https://github.com/chenlabgccri/CancerTypePrediction.\n",
    "topics": "{}",
    "score": 0.8421542783
  },
  {
    "id": "1803.07728",
    "title": "Unsupervised Representation Learning by Predicting Image Rotations",
    "abstract": "  Over the last years, deep convolutional neural networks (ConvNets) have\ntransformed the field of computer vision thanks to their unparalleled capacity\nto learn high level semantic image features. However, in order to successfully\nlearn those features, they usually require massive amounts of manually labeled\ndata, which is both expensive and impractical to scale. Therefore, unsupervised\nsemantic feature learning, i.e., learning without requiring manual annotation\neffort, is of crucial importance in order to successfully harvest the vast\namount of visual data that are available today. In our work we propose to learn\nimage features by training ConvNets to recognize the 2d rotation that is\napplied to the image that it gets as input. We demonstrate both qualitatively\nand quantitatively that this apparently simple task actually provides a very\npowerful supervisory signal for semantic feature learning. We exhaustively\nevaluate our method in various unsupervised feature learning benchmarks and we\nexhibit in all of them state-of-the-art performance. Specifically, our results\non those benchmarks demonstrate dramatic improvements w.r.t. prior\nstate-of-the-art approaches in unsupervised representation learning and thus\nsignificantly close the gap with supervised feature learning. For instance, in\nPASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model\nachieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is\nonly 2.4 points lower from the supervised case. We get similarly striking\nresults when we transfer our unsupervised learned features on various other\ntasks, such as ImageNet classification, PASCAL classification, PASCAL\nsegmentation, and CIFAR-10 classification. The code and models of our paper\nwill be published on: https://github.com/gidariss/FeatureLearningRotNet .\n",
    "topics": "{'Unsupervised Representation Learning': 1.0, 'Representation Learning': 0.9582154}",
    "score": 0.8420749261
  },
  {
    "id": "1811.05531",
    "title": "Interactive dimensionality reduction using similarity projections",
    "abstract": "  Recent advances in machine learning allow us to analyze and describe the\ncontent of high-dimensional data like text, audio, images or other signals. In\norder to visualize that data in 2D or 3D, usually Dimensionality Reduction (DR)\ntechniques are employed. Most of these techniques, e.g., PCA or t-SNE, produce\nstatic projections without taking into account corrections from humans or other\ndata exploration scenarios. In this work, we propose the interactive Similarity\nProjection (iSP), a novel interactive DR framework based on similarity\nembeddings, where we form a differentiable objective based on the user\ninteractions and perform learning using gradient descent, with an end-to-end\ntrainable architecture. Two interaction scenarios are evaluated. First, a\ncommon methodology in multidimensional projection is to project a subset of\ndata, arrange them in classes or clusters, and project the rest unseen dataset\nbased on that manipulation, in a kind of semi-supervised interpolation. We\nreport results that outperform competitive baselines in a wide range of metrics\nand datasets. Second, we explore the scenario of manipulating some classes,\nwhile enriching the optimization with high-dimensional neighbor information.\nApart from improving classification precision and clustering on images and text\ndocuments, the new emerging structure of the projection unveils semantic\nmanifolds. For example, on the Head Pose dataset, by just dragging the faces\nlooking far left to the left and those looking far right to the right, all\nfaces are re-arranged on a continuum even on the vertical axis (face up and\ndown). This end-to-end framework can be used for fast, visual semi-supervised\nlearning, manifold exploration, interactive domain adaptation of neural\nembeddings and transfer learning.\n",
    "topics": "{'Dimensionality Reduction': 0.99998176, 'Domain Adaptation': 0.95752275, 'Transfer Learning': 0.91757596}",
    "score": 0.8420661022
  },
  {
    "id": "1711.08580",
    "title": "3D Anisotropic Hybrid Network: Transferring Convolutional Features from\n  2D Images to 3D Anisotropic Volumes",
    "abstract": "  While deep convolutional neural networks (CNN) have been successfully applied\nfor 2D image analysis, it is still challenging to apply them to 3D anisotropic\nvolumes, especially when the within-slice resolution is much higher than the\nbetween-slice resolution and when the amount of 3D volumes is relatively small.\nOn one hand, direct learning of CNN with 3D convolution kernels suffers from\nthe lack of data and likely ends up with poor generalization; insufficient GPU\nmemory limits the model size or representational power. On the other hand,\napplying 2D CNN with generalizable features to 2D slices ignores between-slice\ninformation. Coupling 2D network with LSTM to further handle the between-slice\ninformation is not optimal due to the difficulty in LSTM learning. To overcome\nthe above challenges, we propose a 3D Anisotropic Hybrid Network (AH-Net) that\ntransfers convolutional features learned from 2D images to 3D anisotropic\nvolumes. Such a transfer inherits the desired strong generalization capability\nfor within-slice information while naturally exploiting between-slice\ninformation for more effective modelling. The focal loss is further utilized\nfor more effective end-to-end learning. We experiment with the proposed 3D\nAH-Net on two different medical image analysis tasks, namely lesion detection\nfrom a Digital Breast Tomosynthesis volume, and liver and liver tumor\nsegmentation from a Computed Tomography volume and obtain the state-of-the-art\nresults.\n",
    "topics": "{'Tumor Segmentation': 0.99991786, 'Computed Tomography (CT)': 0.991447}",
    "score": 0.8419974153
  },
  {
    "id": "2008.00195",
    "title": "Joint Generative Learning and Super-Resolution For Real-World\n  Camera-Screen Degradation",
    "abstract": "  In real-world single image super-resolution (SISR) task, the low-resolution\nimage suffers more complicated degradations, not only downsampled by unknown\nkernels. However, existing SISR methods are generally studied with the\nsynthetic low-resolution generation such as bicubic interpolation (BI), which\ngreatly limits their performance. Recently, some researchers investigate\nreal-world SISR from the perspective of the camera and smartphone. However,\nexcept the acquisition equipment, the display device also involves more\ncomplicated degradations. In this paper, we focus on the camera-screen\ndegradation and build a real-world dataset (Cam-ScreenSR), where HR images are\noriginal ground truths from the previous DIV2K dataset and corresponding LR\nimages are camera-captured versions of HRs displayed on the screen. We conduct\nextensive experiments to demonstrate that involving more real degradations is\npositive to improve the generalization of SISR models. Moreover, we propose a\njoint two-stage model. Firstly, the downsampling degradation GAN(DD-GAN) is\ntrained to model the degradation and produces more various of LR images, which\nis validated to be efficient for data augmentation. Then the dual residual\nchannel attention network (DuRCAN) learns to recover the SR image. The weighted\ncombination of L1 loss and proposed Laplacian loss are applied to sharpen the\nhigh-frequency edges. Extensive experimental results in both typical synthetic\nand complicated real-world degradations validate the proposed method\noutperforms than existing SOTA models with less parameters, faster speed and\nbetter visual results. Moreover, in real captured photographs, our model also\ndelivers best visual quality with sharper edge, less artifacts, especially\nappropriate color enhancement, which has not been accomplished by previous\nmethods.\n",
    "topics": "{'Image Super-Resolution': 0.9999981, 'Super-Resolution': 0.99959034, 'Super Resolution': 0.99923325}",
    "score": 0.8419845978
  },
  {
    "id": "1912.05067",
    "title": "Wide-Area Land Cover Mapping with Sentinel-1 Imagery using Deep Learning\n  Semantic Segmentation Models",
    "abstract": "  Land cover mapping is essential for monitoring the environment and\nunderstanding the effects of human activities on it. The automatic approaches\nto land cover mapping (i.e., image segmentation) mostly used traditional\nmachine learning. On the natural images, deep learning has outperformed\ntraditional machine learning on a range of tasks, including the image\nsegmentation. On remote sensing images, recent studies are demonstrating\nsuccessful application of specific deep learning models or their adaptations to\nparticular small-scale mapping tasks (e.g., to classify wetland complexes).\nHowever, it is not readily clear which of the existing models for natural\nimages are the best candidates to be taken for the particular remote sensing\ntask and data. In this study, we answer that question for mapping the\nfundamental land cover classes using the satellite imaging radar data. We took\nESA Sentinel-1 C-band SAR images available at no cost to users as\nrepresentative data. CORINE land cover map was used as a reference, and the\nmodels were trained to distinguish between the 5 Level-1 CORINE classes. We\nselected seven among the state-of-the-art semantic segmentation models so that\nthey cover a diverse set of approaches. We used 14 ESA Sentinel-1 scenes\nacquired during the summer season in Finland, which are representative of the\nland cover in the country. Upon the benchmarking, all the models demonstrated\nsolid performance. The best model, FC-DenseNet (Fully Convolutional DenseNets),\nachieved the overall accuracy of 90.7%. Overall, our results indicate that the\nsemantic segmentation models are suitable for efficient wide-area mapping using\nsatellite SAR imagery. Our results also provide baseline accuracy against which\nthe newly proposed models should be evaluated and suggest the DenseNet-based\nmodels are the first candidate for this task.\n",
    "topics": "{'Semantic Segmentation': 0.9999999, 'Image Classification': 0.6537919}",
    "score": 0.8417398737
  },
  {
    "id": "1607.07034",
    "title": "Impact of Physical Activity on Sleep:A Deep Learning Based Exploration",
    "abstract": "  The importance of sleep is paramount for maintaining physical, emotional and\nmental wellbeing. Though the relationship between sleep and physical activity\nis known to be important, it is not yet fully understood. The explosion in\npopularity of actigraphy and wearable devices, provides a unique opportunity to\nunderstand this relationship. Leveraging this information source requires new\ntools to be developed to facilitate data-driven research for sleep and activity\npatient-recommendations.\n  In this paper we explore the use of deep learning to build sleep quality\nprediction models based on actigraphy data. We first use deep learning as a\npure model building device by performing human activity recognition (HAR) on\nraw sensor data, and using deep learning to build sleep prediction models. We\ncompare the deep learning models with those build using classical approaches,\ni.e. logistic regression, support vector machines, random forest and adaboost.\nSecondly, we employ the advantage of deep learning with its ability to handle\nhigh dimensional datasets. We explore several deep learning models on the raw\nwearable sensor output without performing HAR or any other feature extraction.\n  Our results show that using a convolutional neural network on the raw\nwearables output improves the predictive value of sleep quality from physical\nactivity, by an additional 8% compared to state-of-the-art non-deep learning\napproaches, which itself shows a 15% improvement over current practice.\nMoreover, utilizing deep learning on raw data eliminates the need for data\npre-processing and simplifies the overall workflow to analyze actigraphy data\nfor sleep and physical activity research.\n",
    "topics": "{'Activity Recognition': 0.99964404}",
    "score": 0.8416458742
  },
  {
    "id": "1801.00602",
    "title": "Accurate reconstruction of image stimuli from human fMRI based on the\n  decoding model with capsule network architecture",
    "abstract": "  In neuroscience, all kinds of computation models were designed to answer the\nopen question of how sensory stimuli are encoded by neurons and conversely, how\nsensory stimuli can be decoded from neuronal activities. Especially, functional\nMagnetic Resonance Imaging (fMRI) studies have made many great achievements\nwith the rapid development of the deep network computation. However, comparing\nwith the goal of decoding orientation, position and object category from\nactivities in visual cortex, accurate reconstruction of image stimuli from\nhuman fMRI is a still challenging work. In this paper, the capsule network\n(CapsNet) architecture based visual reconstruction (CNAVR) method is developed\nto reconstruct image stimuli. The capsule means containing a group of neurons\nto perform the better organization of feature structure and representation,\ninspired by the structure of cortical mini column including several hundred\nneurons in primates. The high-level capsule features in the CapsNet includes\ndiverse features of image stimuli such as semantic class, orientation, location\nand so on. We used these features to bridge between human fMRI and image\nstimuli. We firstly employed the CapsNet to train the nonlinear mapping from\nimage stimuli to high-level capsule features, and from high-level capsule\nfeatures to image stimuli again in an end-to-end manner. After estimating the\nserviceability of each voxel by encoding performance to accomplish the\nselecting of voxels, we secondly trained the nonlinear mapping from\ndimension-decreasing fMRI data to high-level capsule features. Finally, we can\npredict the high-level capsule features with fMRI data, and reconstruct image\nstimuli with the CapsNet. We evaluated the proposed CNAVR method on the dataset\nof handwritten digital images, and exceeded about 10% than the accuracy of all\nexisting state-of-the-art methods on the structural similarity index (SSIM).\n",
    "topics": "{'SSIM': 0.9998859}",
    "score": 0.841585042
  },
  {
    "id": "2004.02696",
    "title": "COVID-CAPS: A Capsule Network-based Framework for Identification of\n  COVID-19 cases from X-ray Images",
    "abstract": "  Novel Coronavirus disease (COVID-19) has abruptly and undoubtedly changed the\nworld as we know it at the end of the 2nd decade of the 21st century. COVID-19\nis extremely contagious and quickly spreading globally making its early\ndiagnosis of paramount importance. Early diagnosis of COVID-19 enables health\ncare professionals and government authorities to break the chain of transition\nand flatten the epidemic curve. The common type of COVID-19 diagnosis test,\nhowever, requires specific equipment and has relatively low sensitivity.\nComputed tomography (CT) scans and X-ray images, on the other hand, reveal\nspecific manifestations associated with this disease. Overlap with other lung\ninfections makes human-centered diagnosis of COVID-19 challenging.\nConsequently, there has been an urgent surge of interest to develop Deep Neural\nNetwork (DNN)-based diagnosis solutions, mainly based on Convolutional Neural\nNetworks (CNNs), to facilitate identification of positive COVID-19 cases. CNNs,\nhowever, are prone to lose spatial information between image instances and\nrequire large datasets. The paper presents an alternative modeling framework\nbased on Capsule Networks, referred to as the COVID-CAPS, being capable of\nhandling small datasets, which is of significant importance due to sudden and\nrapid emergence of COVID-19. Our results based on a dataset of X-ray images\nshow that COVID-CAPS has advantage over previous CNN-based models. COVID-CAPS\nachieved an Accuracy of 95.7%, Sensitivity of 90%, Specificity of 95.8%, and\nArea Under the Curve (AUC) of 0.97, while having far less number of trainable\nparameters in comparison to its counterparts. To further improve diagnosis\ncapabilities of the COVID-CAPS, pre-training based on a new dataset constructed\nfrom an external dataset of X-ray images. Pre-training with a dataset of\nsimilar nature further improved accuracy to 98.3% and specificity to 98.6%.\n",
    "topics": "{'COVID-19 Diagnosis': 0.99999535, 'Computed Tomography (CT)': 0.9994248}",
    "score": 0.8415158183
  },
  {
    "id": "1811.05660",
    "title": "MT-CGCNN: Integrating Crystal Graph Convolutional Neural Network with\n  Multitask Learning for Material Property Prediction",
    "abstract": "  Developing accurate, transferable and computationally inexpensive machine\nlearning models can rapidly accelerate the discovery and development of new\nmaterials. Some of the major challenges involved in developing such models are,\n(i) limited availability of materials data as compared to other fields, (ii)\nlack of universal descriptor of materials to predict its various properties.\nThe limited availability of materials data can be addressed through transfer\nlearning, while the generic representation was recently addressed by Xie and\nGrossman [1], where they developed a crystal graph convolutional neural network\n(CGCNN) that provides a unified representation of crystals. In this work, we\ndevelop a new model (MT-CGCNN) by integrating CGCNN with transfer learning\nbased on multi-task (MT) learning. We demonstrate the effectiveness of MT-CGCNN\nby simultaneous prediction of various material properties such as Formation\nEnergy, Band Gap and Fermi Energy for a wide range of inorganic crystals (46774\nmaterials). MT-CGCNN is able to reduce the test error when employed on\ncorrelated properties by upto 8%. The model prediction has lower test error\ncompared to CGCNN, even when the training data is reduced by 10%. We also\ndemonstrate our model's better performance through prediction of end user\nscenario related to metal/non-metal classification. These results encourage\nfurther development of machine learning approaches which leverage multi-task\nlearning to address the aforementioned challenges in the discovery of new\nmaterials. We make MT-CGCNN's source code available to encourage reproducible\nresearch.\n",
    "topics": "{'Multi-Task Learning': 0.9999995, 'Transfer Learning': 0.9915283}",
    "score": 0.8413936201
  },
  {
    "id": "2007.05946",
    "title": "Dual Adversarial Network: Toward Real-world Noise Removal and Noise\n  Generation",
    "abstract": "  Real-world image noise removal is a long-standing yet very challenging task\nin computer vision. The success of deep neural network in denoising stimulates\nthe research of noise generation, aiming at synthesizing more clean-noisy image\npairs to facilitate the training of deep denoisers. In this work, we propose a\nnovel unified framework to simultaneously deal with the noise removal and noise\ngeneration tasks. Instead of only inferring the posteriori distribution of the\nlatent clean image conditioned on the observed noisy image in traditional MAP\nframework, our proposed method learns the joint distribution of the clean-noisy\nimage pairs. Specifically, we approximate the joint distribution with two\ndifferent factorized forms, which can be formulated as a denoiser mapping the\nnoisy image to the clean one and a generator mapping the clean image to the\nnoisy one. The learned joint distribution implicitly contains all the\ninformation between the noisy and clean images, avoiding the necessity of\nmanually designing the image priors and noise assumptions as traditional.\nBesides, the performance of our denoiser can be further improved by augmenting\nthe original training dataset with the learned generator. Moreover, we propose\ntwo metrics to assess the quality of the generated noisy image, for which, to\nthe best of our knowledge, such metrics are firstly proposed along this\nresearch line. Extensive experiments have been conducted to demonstrate the\nsuperiority of our method over the state-of-the-arts both in the real noise\nremoval and generation tasks. The training and testing code is available at\nhttps://github.com/zsyOAOA/DANet.\n",
    "topics": "{'Denoising': 0.9858073}",
    "score": 0.8413738935
  },
  {
    "id": "1812.09232",
    "title": "Learning from Web Data: the Benefit of Unsupervised Object Localization",
    "abstract": "  Annotating a large number of training images is very time-consuming. In this\nbackground, this paper focuses on learning from easy-to-acquire web data and\nutilizes the learned model for fine-grained image classification in labeled\ndatasets. Currently, the performance gain from training with web data is\nincremental, like a common saying \"better than nothing, but not by much\".\nConventionally, the community looks to correcting the noisy web labels to\nselect informative samples. In this work, we first systematically study the\nbuilt-in gap between the web and standard datasets, i.e. different data\ndistributions between the two kinds of data. Then, in addition to using web\nlabels, we present an unsupervised object localization method, which provides\ncritical insights into the object density and scale in web images.\nSpecifically, we design two constraints on web data to substantially reduce the\ndifference of data distributions for the web and standard datasets. First, we\npresent a method to control the scale, localization and number of objects in\nthe detected region. Second, we propose to select the regions containing\nobjects that are consistent with the web tag. Based on the two constraints, we\nare able to process web images to reduce the gap, and the processed web data is\nused to better assist the standard dataset to train CNNs. Experiments on\nseveral fine-grained image classification datasets confirm that our method\nperforms favorably against the state-of-the-art methods.\n",
    "topics": "{'Object Localization': 1.0, 'Fine-Grained Image Classification': 0.9998945, 'Image Classification': 0.9986884}",
    "score": 0.8412614887
  },
  {
    "id": "1906.08095",
    "title": "PoseConvGRU: A Monocular Approach for Visual Ego-motion Estimation by\n  Learning",
    "abstract": "  While many visual ego-motion algorithm variants have been proposed in the\npast decade, learning based ego-motion estimation methods have seen an\nincreasing attention because of its desirable properties of robustness to image\nnoise and camera calibration independence. In this work, we propose a\ndata-driven approach of fully trainable visual ego-motion estimation for a\nmonocular camera. We use an end-to-end learning approach in allowing the model\nto map directly from input image pairs to an estimate of ego-motion\n(parameterized as 6-DoF transformation matrices). We introduce a novel\ntwo-module Long-term Recurrent Convolutional Neural Networks called\nPoseConvGRU, with an explicit sequence pose estimation loss to achieve this.\nThe feature-encoding module encodes the short-term motion feature in an image\npair, while the memory-propagating module captures the long-term motion feature\nin the consecutive image pairs. The visual memory is implemented with\nconvolutional gated recurrent units, which allows propagating information over\ntime. At each time step, two consecutive RGB images are stacked together to\nform a 6 channels tensor for module-1 to learn how to extract motion\ninformation and estimate poses. The sequence of output maps is then passed\nthrough a stacked ConvGRU module to generate the relative transformation pose\nof each image pair. We also augment the training data by randomly skipping\nframes to simulate the velocity variation which results in a better performance\nin turning and high-velocity situations. We evaluate the performance of our\nproposed approach on the KITTI Visual Odometry benchmark. The experiments show\na competitive performance of the proposed method to the geometric method and\nencourage further exploration of learning based methods for the purpose of\nestimating camera ego-motion even though geometrical methods demonstrate\npromising results.\n",
    "topics": "{'Motion Estimation': 0.99999774, 'Visual Odometry': 0.9999852, 'Pose Estimation': 0.9399415}",
    "score": 0.8412174075
  },
  {
    "id": "1908.01456",
    "title": "A Deep Learning Approach for Tweet Classification and Rescue Scheduling\n  for Effective Disaster Management",
    "abstract": "  It is a challenging and complex task to acquire information from different\nregions of a disaster-affected area in a timely fashion. The extensive spread\nand reach of social media and networks allow people to share information in\nreal-time. However, the processing of social media data and gathering of\nvaluable information require a series of operations such as (1) processing each\nspecific tweet for a text classification, (2) possible location determination\nof people needing help based on tweets, and (3) priority calculations of rescue\ntasks based on the classification of tweets. These are three primary challenges\nin developing an effective rescue scheduling operation using social media data.\nIn this paper, first, we propose a deep learning model combining attention\nbased Bi-directional Long Short-Term Memory (BLSTM) and Convolutional Neural\nNetwork (CNN) to classify the tweets under different categories. We use\npre-trained crisis word vectors and global vectors for word representation\n(GLoVe) for capturing semantic meaning from tweets. Next, we perform feature\nengineering to create an auxiliary feature map which dramatically increases the\nmodel accuracy. In our experiments using real data sets from Hurricanes Harvey\nand Irma, it is observed that our proposed approach performs better compared to\nother classification methods based on Precision, Recall, F1-score, and\nAccuracy, and is highly effective to determine the correct priority of a tweet.\nFurthermore, to evaluate the effectiveness and robustness of the proposed\nclassification model a merged dataset comprises of 4 different datasets from\nCrisisNLP and another 15 different disasters data from CrisisLex are used.\nFinally, we develop an adaptive multitask hybrid scheduling algorithm\nconsidering resource constraints to perform an effective rescue scheduling\noperation considering different rescue priorities.\n",
    "topics": "{'Feature Engineering': 0.99901307, 'Text Classification': 0.99398524}",
    "score": 0.84116537
  },
  {
    "id": "1912.01349",
    "title": "Asymmetric Co-Teaching for Unsupervised Cross Domain Person\n  Re-Identification",
    "abstract": "  Person re-identification (re-ID), is a challenging task due to the high\nvariance within identity samples and imaging conditions. Although recent\nadvances in deep learning have achieved remarkable accuracy in settled scenes,\ni.e., source domain, few works can generalize well on the unseen target domain.\nOne popular solution is assigning unlabeled target images with pseudo labels by\nclustering, and then retraining the model. However, clustering methods tend to\nintroduce noisy labels and discard low confidence samples as outliers, which\nmay hinder the retraining process and thus limit the generalization ability. In\nthis study, we argue that by explicitly adding a sample filtering procedure\nafter the clustering, the mined examples can be much more efficiently used. To\nthis end, we design an asymmetric co-teaching framework, which resists noisy\nlabels by cooperating two models to select data with possibly clean labels for\neach other. Meanwhile, one of the models receives samples as pure as possible,\nwhile the other takes in samples as diverse as possible. This procedure\nencourages that the selected training samples can be both clean and\nmiscellaneous, and that the two models can promote each other iteratively.\nExtensive experiments show that the proposed framework can consistently benefit\nmost clustering-based methods, and boost the state-of-the-art adaptation\naccuracy. Our code is available at\nhttps://github.com/FlyingRoastDuck/ACT_AAAI20.\n",
    "topics": "{'Person Re-Identification': 0.9999999, 'Unsupervised Domain Adaptation': 0.6430459}",
    "score": 0.8411215421
  },
  {
    "id": "1905.09282",
    "title": "Spatio-Temporal Deep Learning Models for Tip Force Estimation During\n  Needle Insertion",
    "abstract": "  Purpose. Precise placement of needles is a challenge in a number of clinical\napplications such as brachytherapy or biopsy. Forces acting at the needle cause\ntissue deformation and needle deflection which in turn may lead to misplacement\nor injury. Hence, a number of approaches to estimate the forces at the needle\nhave been proposed. Yet, integrating sensors into the needle tip is challenging\nand a careful calibration is required to obtain good force estimates.\n  Methods. We describe a fiber-optical needle tip force sensor design using a\nsingle OCT fiber for measurement. The fiber images the deformation of an epoxy\nlayer placed below the needle tip which results in a stream of 1D depth\nprofiles. We study different deep learning approaches to facilitate calibration\nbetween this spatio-temporal image data and the related forces. In particular,\nwe propose a novel convGRU-CNN architecture for simultaneous spatial and\ntemporal data processing.\n  Results. The needle can be adapted to different operating ranges by changing\nthe stiffness of the epoxy layer. Likewise, calibration can be adapted by\ntraining the deep learning models. Our novel convGRU-CNN architecture results\nin the lowest mean absolute error of 1.59 +- 1.3 mN and a cross-correlation\ncoefficient of 0.9997, and clearly outperforms the other methods. Ex vivo\nexperiments in human prostate tissue demonstrate the needle's application.\n  Conclusions. Our OCT-based fiber-optical sensor presents a viable alternative\nfor needle tip force estimation. The results indicate that the rich\nspatio-temporal information included in the stream of images showing the\ndeformation throughout the epoxy layer can be effectively used by deep learning\nmodels. Particularly, we demonstrate that the convGRU-CNN architecture performs\nfavorably, making it a promising approach for other spatio-temporal learning\nproblems.\n",
    "topics": "{}",
    "score": 0.8410362763
  },
  {
    "id": "2010.01305",
    "title": "Bounding Boxes Are All We Need: Street View Image Classification via\n  Context Encoding of Detected Buildings",
    "abstract": "  Street view images classification aiming at urban land use analysis is\ndifficult because the class labels (e.g., commercial area), are concepts with\nhigher abstract level compared to the ones of general visual tasks (e.g.,\npersons and cars). Therefore, classification models using only visual features\noften fail to achieve satisfactory performance. In this paper, a novel approach\nbased on a \"Detector-Encoder-Classifier\" framework is proposed. Instead of\nusing visual features of the whole image directly as common image-level models\nbased on convolutional neural networks (CNNs) do, the proposed framework\nfirstly obtains the bounding boxes of buildings in street view images from a\ndetector. Their contextual information such as the co-occurrence patterns of\nbuilding classes and their layout are then encoded into metadata by the\nproposed algorithm \"CODING\" (Context encOding of Detected buildINGs). Finally,\nthese bounding box metadata are classified by a recurrent neural network (RNN).\nIn addition, we made a dual-labeled dataset named \"BEAUTY\" (Building dEtection\nAnd Urban funcTional-zone portraYing) of 19,070 street view images and 38,857\nbuildings based on the existing BIC GSV [1]. The dataset can be used not only\nfor street view image classification, but also for multi-class building\ndetection. Experiments on \"BEAUTY\" show that the proposed approach achieves a\n12.65% performance improvement on macro-precision and 12% on macro-recall over\nimage-level CNN based models. Our code and dataset are available at\nhttps://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/\n",
    "topics": "{'Image Classification': 0.99749064}",
    "score": 0.8409303685
  },
  {
    "id": "1904.13007",
    "title": "Reconstruction of Natural Visual Scenes from Neural Spikes with Deep\n  Neural Networks",
    "abstract": "  Neural coding is one of the central questions in systems neuroscience for\nunderstanding how the brain processes stimulus from the environment, moreover,\nit is also a cornerstone for designing algorithms of brain-machine interface,\nwhere decoding incoming stimulus is highly demanded for better performance of\nphysical devices. Traditionally researchers have focused on functional magnetic\nresonance imaging (fMRI) data as the neural signals of interest for decoding\nvisual scenes. However, our visual perception operates in a fast time scale of\nmillisecond in terms of an event termed neural spike. There are few studies of\ndecoding by using spikes. Here we fulfill this aim by developing a novel\ndecoding framework based on deep neural networks, named spike-image decoder\n(SID), for reconstructing natural visual scenes, including static images and\ndynamic videos, from experimentally recorded spikes of a population of retinal\nganglion cells. The SID is an end-to-end decoder with one end as neural spikes\nand the other end as images, which can be trained directly such that visual\nscenes are reconstructed from spikes in a highly accurate fashion. Our SID also\noutperforms on the reconstruction of visual stimulus compared to existing fMRI\ndecoding models. In addition, with the aid of a spike encoder, we show that SID\ncan be generalized to arbitrary visual scenes by using the image datasets of\nMNIST, CIFAR10, and CIFAR100. Furthermore, with a pre-trained SID, one can\ndecode any dynamic videos to achieve real-time encoding and decoding of visual\nscenes by spikes. Altogether, our results shed new light on neuromorphic\ncomputing for artificial visual systems, such as event-based visual cameras and\nvisual neuroprostheses.\n",
    "topics": "{}",
    "score": 0.840923544
  },
  {
    "id": "1901.00449",
    "title": "Instant Automated Inference of Perceived Mental Stress through\n  Smartphone PPG and Thermal Imaging",
    "abstract": "  Background: A smartphone is a promising tool for daily cardiovascular\nmeasurement and mental stress monitoring. A smartphone camera-based\nPhotoPlethysmoGraphy (PPG) and a low-cost thermal camera can be used to create\ncheap, convenient and mobile monitoring systems. However, to ensure reliable\nmonitoring results, a person has to remain still for several minutes while a\nmeasurement is being taken. This is very cumbersome and makes its use in\nreal-life mobile situations quite impractical.\n  Objective: We propose a system which combines PPG and thermography with the\naim of improving cardiovascular signal quality and capturing stress responses\nquickly.\n  Methods: Using a smartphone camera with a low cost thermal camera added on,\nwe built a novel system which continuously and reliably measures two different\ntypes of cardiovascular events: i) blood volume pulse and ii)\nvasoconstriction/dilation-induced temperature changes of the nose tip. 17\nhealthy participants, involved in a series of stress-inducing mental workload\ntasks, measured their physiological responses to stressors over a short window\nof time (20 seconds) immediately after each task. Participants reported their\nlevel of perceived mental stress using a 10-cm Visual Analogue Scale (VAS). We\nused normalized K-means clustering to reduce interpersonal differences in the\nself-reported ratings. For the instant stress inference task, we built novel\nlow-level feature sets representing variability of cardiovascular patterns. We\nthen used the automatic feature learning capability of artificial Neural\nNetworks (NN) to improve the mapping between the extracted set of features and\nthe self-reported ratings. We compared our proposed method with existing\nhand-engineered features-based machine learning methods.\n  Results, Conclusions: ... due to limited space here, we refer to our\nmanuscript.\n",
    "topics": "{}",
    "score": 0.8408923985
  },
  {
    "id": "1806.07497",
    "title": "Fully Automatic Myocardial Segmentation of Contrast Echocardiography\n  Sequence Using Random Forests Guided by Shape Model",
    "abstract": "  Myocardial contrast echocardiography (MCE) is an imaging technique that\nassesses left ventricle function and myocardial perfusion for the detection of\ncoronary artery diseases. Automatic MCE perfusion quantification is challenging\nand requires accurate segmentation of the myocardium from noisy and\ntime-varying images. Random forests (RF) have been successfully applied to many\nmedical image segmentation tasks. However, the pixel-wise RF classifier ignores\ncontextual relationships between label outputs of individual pixels. RF which\nonly utilizes local appearance features is also susceptible to data suffering\nfrom large intensity variations. In this paper, we demonstrate how to overcome\nthe above limitations of classic RF by presenting a fully automatic\nsegmentation pipeline for myocardial segmentation in full-cycle 2D MCE data.\nSpecifically, a statistical shape model is used to provide shape prior\ninformation that guide the RF segmentation in two ways. First, a novel shape\nmodel (SM) feature is incorporated into the RF framework to generate a more\naccurate RF probability map. Second, the shape model is fitted to the RF\nprobability map to refine and constrain the final segmentation to plausible\nmyocardial shapes. We further improve the performance by introducing a bounding\nbox detection algorithm as a preprocessing step in the segmentation pipeline.\nOur approach on 2D image is further extended to 2D+t sequence which ensures\ntemporal consistency in the resultant sequence segmentations. When evaluated on\nclinical MCE data, our proposed method achieves notable improvement in\nsegmentation accuracy and outperforms other state-of-the-art methods including\nthe classic RF and its variants, active shape model and image registration.\n",
    "topics": "{'Image Registration': 0.99999845, 'Medical Image Segmentation': 0.9999782, 'Semantic Segmentation': 0.9907828}",
    "score": 0.8408085954
  },
  {
    "id": "1902.04487",
    "title": "Extended 2D Consensus Hippocampus Segmentation",
    "abstract": "  Hippocampus segmentation plays a key role in diagnosing various brain\ndisorders such as Alzheimer's disease, epilepsy, multiple sclerosis, cancer,\ndepression and others. Nowadays, segmentation is still mainly performed\nmanually by specialists. Segmentation done by experts is considered to be a\ngold-standard when evaluating automated methods, buts it is a time consuming\nand arduos task, requiring specialized personnel. In recent years, efforts have\nbeen made to achieve reliable automated segmentation. For years the best\nperforming authomatic methods were multi atlas based with around 80-85% Dice\ncoefficient and very time consuming, but machine learning methods are recently\nrising with promising time and accuracy performance. A method for volumetric\nhippocampus segmentation is presented, based on the consensus of tri-planar\nU-Net inspired fully convolutional networks (FCNNs), with some modifications,\nincluding residual connections, VGG weight transfers, batch normalization and a\npatch extraction technique employing data from neighbor patches. A study on the\nimpact of our modifications to the classical U-Net architecture was performed.\nOur method achieves cutting edge performance in our dataset, with around 96%\nvolumetric Dice accuracy in our test data. In a public validation dataset,\nHARP, we achieve 87.48% DICE. GPU execution time is in the order of seconds per\nvolume, and source code is publicly available. Also, masks are shown to be\nsimilar to other recent state-of-the-art hippocampus segmentation methods in a\nthird dataset, without manual annotations.\n",
    "topics": "{'Semantic Segmentation': 0.7714804, 'Brain Segmentation': 0.6283851}",
    "score": 0.8406933591
  },
  {
    "id": "2009.12569",
    "title": "DT-Net: A novel network based on multi-directional integrated\n  convolution and threshold convolution",
    "abstract": "  Since medical image data sets contain few samples and singular features,\nlesions are viewed as highly similar to other tissues. The traditional neural\nnetwork has a limited ability to learn features. Even if a host of feature maps\nis expanded to obtain more semantic information, the accuracy of segmenting the\nfinal medical image is slightly improved, and the features are excessively\nredundant. To solve the above problems, in this paper, we propose a novel\nend-to-end semantic segmentation algorithm, DT-Net, and use two new convolution\nstrategies to better achieve end-to-end semantic segmentation of medical\nimages. 1. In the feature mining and feature fusion stage, we construct a\nmulti-directional integrated convolution (MDIC). The core idea is to use the\nmulti-scale convolution to enhance the local multi-directional feature maps to\ngenerate enhanced feature maps and to mine the generated features that contain\nmore semantics without increasing the number of feature maps. 2. We also aim to\nfurther excavate and retain more meaningful deep features reduce a host of\nnoise features in the training process. Therefore, we propose a convolution\nthresholding strategy. The central idea is to set a threshold to eliminate a\nlarge number of redundant features and reduce computational complexity. Through\nthe two strategies proposed above, the algorithm proposed in this paper\nproduces state-of-the-art results on two public medical image datasets. We\nprove in detail that our proposed strategy plays an important role in feature\nmining and eliminating redundant features. Compared with the existing semantic\nsegmentation algorithms, our proposed algorithm has better robustness.\n",
    "topics": "{'Semantic Segmentation': 0.9997528}",
    "score": 0.8405952513
  },
  {
    "id": "2002.02650",
    "title": "What You See is What it Means! Semantic Representation Learning of Code\n  based on Visualization and Transfer Learning",
    "abstract": "  Recent successes in training word embeddings for NLP tasks have encouraged a\nwave of research on representation learning for source code, which builds on\nsimilar NLP methods. The overall objective is then to produce code embeddings\nthat capture the maximum of program semantics. State-of-the-art approaches\ninvariably rely on a syntactic representation (i.e., raw lexical tokens,\nabstract syntax trees, or intermediate representation tokens) to generate\nembeddings, which are criticized in the literature as non-robust or\nnon-generalizable. In this work, we investigate a novel embedding approach\nbased on the intuition that source code has visual patterns of semantics. We\nfurther use these patterns to address the outstanding challenge of identifying\nsemantic code clones. We propose the WYSIWIM (\"What You See Is What It Means\")\napproach where visual representations of source code are fed into powerful\npre-trained image classification neural networks from the field of computer\nvision to benefit from the practical advantages of transfer learning. We\nevaluate the proposed embedding approach on two variations of the task of\nsemantic code clone identification: code clone detection (a binary\nclassification problem), and code classification (a multi-classification\nproblem). We show with experiments on the BigCloneBench (Java) and Open Judge\n(C) datasets that although simple, our WYSIWIM approach performs as effectively\nas state of the art approaches such as ASTNN or TBCNN. We further explore the\ninfluence of different steps in our approach, such as the choice of visual\nrepresentations or the classification algorithm, to eventually discuss the\npromises and limitations of this research direction.\n",
    "topics": "{'Representation Learning': 0.9999255, 'Transfer Learning': 0.9908828, 'Image Classification': 0.5640286}",
    "score": 0.8405748936
  },
  {
    "id": "2007.13098",
    "title": "Towards Purely Unsupervised Disentanglement of Appearance and Shape for\n  Person Images Generation",
    "abstract": "  There have been a fairly of research interests in exploring the\ndisentanglement of appearance and shape from human images. Most existing\nendeavours pursuit this goal by either using training images with annotations\nor regulating the training process with external clues such as human skeleton,\nbody segmentation or cloth patches etc. In this paper, we aim to address this\nchallenge in a more unsupervised manner---we do not require any annotation nor\nany external task-specific clues. To this end, we formulate an\nencoder-decoder-like network to extract both the shape and appearance features\nfrom input images at the same time, and train the parameters by three losses:\nfeature adversarial loss, color consistency loss and reconstruction loss. The\nfeature adversarial loss mainly impose little to none mutual information\nbetween the extracted shape and appearance features, while the color\nconsistency loss is to encourage the invariance of person appearance\nconditioned on different shapes. More importantly, our unsupervised\n(Unsupervised learning has many interpretations in different tasks. To be\nclear, in this paper, we refer unsupervised learning as learning without\ntask-specific human annotations, pairs or any form of weak supervision.)\nframework utilizes learned shape features as masks which are applied to the\ninput itself in order to obtain clean appearance features. Without using fixed\ninput human skeleton, our network better preserves the conditional human\nposture while requiring less supervision. Experimental results on DeepFashion\nand Market1501 demonstrate that the proposed method achieves clean\ndisentanglement and is able to synthesis novel images of comparable quality\nwith state-of-the-art weakly-supervised or even supervised methods.\n",
    "topics": "{}",
    "score": 0.8404117141
  },
  {
    "id": "1812.05237",
    "title": "Code Failure Prediction and Pattern Extraction using LSTM Networks",
    "abstract": "  In this paper, we use a well-known Deep Learning technique called Long Short\nTerm Memory (LSTM) recurrent neural networks to find sessions that are prone to\ncode failure in applications that rely on telemetry data for system health\nmonitoring. We also use LSTM networks to extract telemetry patterns that lead\nto a specific code failure. For code failure prediction, we treat the telemetry\nevents, sequence of telemetry events and the outcome of each sequence as words,\nsentence and sentiment in the context of sentiment analysis, respectively. Our\nproposed method is able to process a large set of data and can automatically\nhandle edge cases in code failure prediction. We take advantage of Bayesian\noptimization technique to find the optimal hyper parameters as well as the type\nof LSTM cells that leads to the best prediction performance. We then introduce\nthe Contributors and Blockers concepts. In this paper, contributors are the set\nof events that cause a code failure, while blockers are the set of events that\neach of them individually prevents a code failure from happening, even in\npresence of one or multiple contributor(s). Once the proposed LSTM model is\ntrained, we use a greedy approach to find the contributors and blockers. To\ndevelop and test our proposed method, we use synthetic (simulated) data in the\nfirst step. The synthetic data is generated using a number of rules for code\nfailures, as well as a number of rules for preventing a code failure from\nhappening. The trained LSTM model shows over 99% accuracy for detecting code\nfailures in the synthetic data. The results from the proposed method outperform\nthe classical learning models such as Decision Tree and Random Forest. Using\nthe proposed greedy method, we are able to find the contributors and blockers\nin the synthetic data in more than 90% of the cases, with a performance better\nthan sequential rule and pattern mining algorithms.\n",
    "topics": "{'Sentiment Analysis': 0.64943844}",
    "score": 0.8401866308
  },
  {
    "id": "2006.00084",
    "title": "Clustering-informed Cinematic Astrophysical Data Visualization with\n  Application to the Moon-forming Terrestrial Synestia",
    "abstract": "  Scientific visualization tools are currently not optimized to create\ncinematic, production-quality representations of numerical data for the purpose\nof science communication. In our pipeline \\texttt{Estra}, we outline a\nstep-by-step process from a raw simulation into a finished render as a way to\nteach non-experts in the field of visualization how to achieve\nproduction-quality outputs on their own. We demonstrate feasibility of using\nthe visual effects software Houdini for cinematic astrophysical data\nvisualization, informed by machine learning clustering algorithms. To\ndemonstrate the capabilities of this pipeline, we used a post-impact,\nthermally-equilibrated Moon-forming synestia from \\cite{Lock18}. Our approach\naims to identify \"physically interpretable\" clusters, where clusters identified\nin an appropriate phase space (e.g. here we use a temperature-entropy\nphase-space) correspond to physically meaningful structures within the\nsimulation data. Clustering results can then be used to highlight these\nstructures by informing the color-mapping process in a simplified Houdini\nsoftware shading network, where dissimilar phase-space clusters are mapped to\ndifferent color values for easier visual identification. Cluster information\ncan also be used in 3D position space, via Houdini's Scene View, to aid in\nphysical cluster finding, simulation prototyping, and data exploration. Our\nclustering-based renders are compared to those created by the Advanced\nVisualization Lab (AVL) team for the full dome show \"Imagine the Moon\" as proof\nof concept. With \\texttt{Estra}, scientists have a tool to create their own\nproduction-quality, data-driven visualizations.\n",
    "topics": "{'Data Visualization': 0.99961203}",
    "score": 0.84007732
  },
  {
    "id": "1906.04301",
    "title": "Transfer Learning for Ultrasound Tongue Contour Extraction with\n  Different Domains",
    "abstract": "  Medical ultrasound technology is widely used in routine clinical applications\nsuch as disease diagnosis and treatment as well as other applications like\nreal-time monitoring of human tongue shapes and motions as visual feedback in\nsecond language training. Due to the low-contrast characteristic and noisy\nnature of ultrasound images, it might require expertise for non-expert users to\nrecognize tongue gestures. Manual tongue segmentation is a cumbersome,\nsubjective, and error-prone task. Furthermore, it is not a feasible solution\nfor real-time applications. In the last few years, deep learning methods have\nbeen used for delineating and tracking tongue dorsum. Deep convolutional neural\nnetworks (DCNNs), which have shown to be successful in medical image analysis\ntasks, are typically weak for the same task on different domains. In many\ncases, DCNNs trained on data acquired with one ultrasound device, do not\nperform well on data of varying ultrasound device or acquisition protocol.\nDomain adaptation is an alternative solution for this difficulty by\ntransferring the weights from the model trained on a large annotated legacy\ndataset to a new model for adapting on another different dataset using\nfine-tuning. In this study, after conducting extensive experiments, we\naddressed the problem of domain adaptation on small ultrasound datasets for\ntongue contour extraction. We trained a U-net network comprises of an\nencoder-decoder path from scratch, and then with several surrogate scenarios,\nsome parts of the trained network were fine-tuned on another dataset as the\ndomain-adapted networks. We repeat scenarios from target to source domains to\nfind a balance point for knowledge transfer from source to target and vice\nversa. The performance of new fine-tuned networks was evaluated on the same\ntask with images from different domains.\n",
    "topics": "{'Transfer Learning': 0.997474, 'Domain Adaptation': 0.40535098}",
    "score": 0.8399953435
  },
  {
    "id": "2002.06619",
    "title": "CRL: Class Representative Learning for Image Classification",
    "abstract": "  Building robust and real-time classifiers with diverse datasets are one of\nthe most significant challenges to deep learning researchers. It is because\nthere is a considerable gap between a model built with training (seen) data and\nreal (unseen) data in applications. Recent works including Zero-Shot Learning\n(ZSL), have attempted to deal with this problem of overcoming the apparent gap\nthrough transfer learning. In this paper, we propose a novel model, called\nClass Representative Learning Model (CRL), that can be especially effective in\nimage classification influenced by ZSL. In the CRL model, first, the learning\nstep is to build class representatives to represent classes in datasets by\naggregating prominent features extracted from a Convolutional Neural Network\n(CNN). Second, the inferencing step in CRL is to match between the class\nrepresentatives and new data. The proposed CRL model demonstrated superior\nperformance compared to the current state-of-the-art research in ZSL and mobile\ndeep learning. The proposed CRL model has been implemented and evaluated in a\nparallel environment, using Apache Spark, for both distributed learning and\nrecognition. An extensive experimental study on the benchmark datasets,\nImageNet-1K, CalTech-101, CalTech-256, CIFAR-100, shows that CRL can build a\nclass distribution model with drastic improvement in learning and recognition\nperformance without sacrificing accuracy compared to the state-of-the-art\nperformances in image classification.\n",
    "topics": "{'Image Classification': 0.9999993, 'Zero-Shot Learning': 0.9999137, 'Transfer Learning': 0.37274063}",
    "score": 0.8399643918
  },
  {
    "id": "2010.03735",
    "title": "Decamouflage: A Framework to Detect Image-Scaling Attacks on\n  Convolutional Neural Networks",
    "abstract": "  As an essential processing step in computer vision applications, image\nresizing or scaling, more specifically downsampling, has to be applied before\nfeeding a normally large image into a convolutional neural network (CNN) model\nbecause CNN models typically take small fixed-size images as inputs. However,\nimage scaling functions could be adversarially abused to perform a newly\nrevealed attack called image-scaling attack, which can affect a wide range of\ncomputer vision applications building upon image-scaling functions.\n  This work presents an image-scaling attack detection framework, termed as\nDecamouflage. Decamouflage consists of three independent detection methods: (1)\nrescaling, (2) filtering/pooling, and (3) steganalysis. While each of these\nthree methods is efficient standalone, they can work in an ensemble manner not\nonly to improve the detection accuracy but also to harden potential adaptive\nattacks. Decamouflage has a pre-determined detection threshold that is generic.\nMore precisely, as we have validated, the threshold determined from one dataset\nis also applicable to other different datasets. Extensive experiments show that\nDecamouflage achieves detection accuracy of 99.9\\% and 99.8\\% in the white-box\n(with the knowledge of attack algorithms) and the black-box (without the\nknowledge of attack algorithms) settings, respectively. To corroborate the\nefficiency of Decamouflage, we have also measured its run-time overhead on a\npersonal PC with an i5 CPU and found that Decamouflage can detect image-scaling\nattacks in milliseconds. Overall, Decamouflage can accurately detect image\nscaling attacks in both white-box and black-box settings with acceptable\nrun-time overhead.\n",
    "topics": "{}",
    "score": 0.839881672
  },
  {
    "id": "1906.05165",
    "title": "Stereoscopic Omnidirectional Image Quality Assessment Based on\n  Predictive Coding Theory",
    "abstract": "  Objective quality assessment of stereoscopic omnidirectional images is a\nchallenging problem since it is influenced by multiple aspects such as\nprojection deformation, field of view (FoV) range, binocular vision, visual\ncomfort, etc. Existing studies show that classic 2D or 3D image quality\nassessment (IQA) metrics are not able to perform well for stereoscopic\nomnidirectional images. However, very few research works have focused on\nevaluating the perceptual visual quality of omnidirectional images, especially\nfor stereoscopic omnidirectional images. In this paper, based on the predictive\ncoding theory of the human vision system (HVS), we propose a stereoscopic\nomnidirectional image quality evaluator (SOIQE) to cope with the\ncharacteristics of 3D 360-degree images. Two modules are involved in SOIQE:\npredictive coding theory based binocular rivalry module and multi-view fusion\nmodule. In the binocular rivalry module, we introduce predictive coding theory\nto simulate the competition between high-level patterns and calculate the\nsimilarity and rivalry dominance to obtain the quality scores of viewport\nimages. Moreover, we develop the multi-view fusion module to aggregate the\nquality scores of viewport images with the help of both content weight and\nlocation weight. The proposed SOIQE is a parametric model without necessary of\nregression learning, which ensures its interpretability and generalization\nperformance. Experimental results on our published stereoscopic omnidirectional\nimage quality assessment database (SOLID) demonstrate that our proposed SOIQE\nmethod outperforms state-of-the-art metrics. Furthermore, we also verify the\neffectiveness of each proposed module on both public stereoscopic image\ndatasets and panoramic image datasets.\n",
    "topics": "{'Image Quality Assessment': 1.0}",
    "score": 0.8398695777
  },
  {
    "id": "1701.03779",
    "title": "Tumour Ellipsification in Ultrasound Images for Treatment Prediction in\n  Breast Cancer",
    "abstract": "  Recent advances in using quantitative ultrasound (QUS) methods have provided\na promising framework to non-invasively and inexpensively monitor or predict\nthe effectiveness of therapeutic cancer responses. One of the earliest steps in\nusing QUS methods is contouring a region of interest (ROI) inside the tumour in\nultrasound B-mode images. While manual segmentation is a very time-consuming\nand tedious task for human experts, auto-contouring is also an extremely\ndifficult task for computers due to the poor quality of ultrasound B-mode\nimages. However, for the purpose of cancer response prediction, a rough\nboundary of the tumour as an ROI is only needed. In this research, a\nsemi-automated tumour localization approach is proposed for ROI estimation in\nultrasound B-mode images acquired from patients with locally advanced breast\ncancer (LABC). The proposed approach comprised several modules, including 1)\nfeature extraction using keypoint descriptors, 2) augmenting the feature\ndescriptors with the distance of the keypoints to the user-input pixel as the\ncentre of the tumour, 3) supervised learning using a support vector machine\n(SVM) to classify keypoints as \"tumour\" or \"non-tumour\", and 4) computation of\nan ellipse as an outline of the ROI representing the tumour. Experiments with\n33 B-mode images from 10 LABC patients yielded promising results with an\naccuracy of 76.7% based on the Dice coefficient performance measure. The\nresults demonstrated that the proposed method can potentially be used as the\nfirst stage in a computer-assisted cancer response prediction system for\nsemi-automated contouring of breast tumours.\n",
    "topics": "{}",
    "score": 0.8397999368
  },
  {
    "id": "1904.08254",
    "title": "USE-Net: incorporating Squeeze-and-Excitation blocks into U-Net for\n  prostate zonal segmentation of multi-institutional MRI datasets",
    "abstract": "  Prostate cancer is the most common malignant tumors in men but prostate\nMagnetic Resonance Imaging (MRI) analysis remains challenging. Besides whole\nprostate gland segmentation, the capability to differentiate between the blurry\nboundary of the Central Gland (CG) and Peripheral Zone (PZ) can lead to\ndifferential diagnosis, since tumor's frequency and severity differ in these\nregions. To tackle the prostate zonal segmentation task, we propose a novel\nConvolutional Neural Network (CNN), called USE-Net, which incorporates\nSqueeze-and-Excitation (SE) blocks into U-Net. Especially, the SE blocks are\nadded after every Encoder (Enc USE-Net) or Encoder-Decoder block (Enc-Dec\nUSE-Net). This study evaluates the generalization ability of CNN-based\narchitectures on three T2-weighted MRI datasets, each one consisting of a\ndifferent number of patients and heterogeneous image characteristics, collected\nby different institutions. The following mixed scheme is used for\ntraining/testing: (i) training on either each individual dataset or multiple\nprostate MRI datasets and (ii) testing on all three datasets with all possible\ntraining/testing combinations. USE-Net is compared against three\nstate-of-the-art CNN-based architectures (i.e., U-Net, pix2pix, and Mixed-Scale\nDense Network), along with a semi-automatic continuous max-flow model. The\nresults show that training on the union of the datasets generally outperforms\ntraining on each dataset separately, allowing for both intra-/cross-dataset\ngeneralization. Enc USE-Net shows good overall generalization under any\ntraining condition, while Enc-Dec USE-Net remarkably outperforms the other\nmethods when trained on all datasets. These findings reveal that the SE blocks'\nadaptive feature recalibration provides excellent cross-dataset generalization\nwhen testing is performed on samples of the datasets used during training.\n",
    "topics": "{}",
    "score": 0.839681243
  },
  {
    "id": "1711.07319",
    "title": "Cascaded Pyramid Network for Multi-Person Pose Estimation",
    "abstract": "  The topic of multi-person pose estimation has been largely improved recently,\nespecially with the development of convolutional neural network. However, there\nstill exist a lot of challenging cases, such as occluded keypoints, invisible\nkeypoints and complex background, which cannot be well addressed. In this\npaper, we present a novel network structure called Cascaded Pyramid Network\n(CPN) which targets to relieve the problem from these \"hard\" keypoints. More\nspecifically, our algorithm includes two stages: GlobalNet and RefineNet.\nGlobalNet is a feature pyramid network which can successfully localize the\n\"simple\" keypoints like eyes and hands but may fail to precisely recognize the\noccluded or invisible keypoints. Our RefineNet tries explicitly handling the\n\"hard\" keypoints by integrating all levels of feature representations from the\nGlobalNet together with an online hard keypoint mining loss. In general, to\naddress the multi-person pose estimation problem, a top-down pipeline is\nadopted to first generate a set of human bounding boxes based on a detector,\nfollowed by our CPN for keypoint localization in each human bounding box. Based\non the proposed algorithm, we achieve state-of-art results on the COCO keypoint\nbenchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1\non the COCO test-challenge dataset, which is a 19% relative improvement\ncompared with 60.5 from the COCO 2016 keypoint challenge.Code\n(https://github.com/chenyilun95/tf-cpn.git) and the detection results are\npublicly available for further research.\n",
    "topics": "{'Multi-Person Pose Estimation': 0.99999976, 'Pose Estimation': 0.9999956, 'Keypoint Detection': 0.99770135}",
    "score": 0.8395569755
  },
  {
    "id": "2003.01271",
    "title": "Med7: a transferable clinical natural language processing model for\n  electronic health records",
    "abstract": "  The field of clinical natural language processing has been advanced\nsignificantly since the introduction of deep learning models. The\nself-supervised representation learning and the transfer learning paradigm\nbecame the methods of choice in many natural language processing application,\nin particular in the settings with the dearth of high quality manually\nannotated data. Electronic health record systems are ubiquitous and the\nmajority of patients' data are now being collected electronically and in\nparticular in the form of free text. Identification of medical concepts and\ninformation extraction is a challenging task, yet important ingredient for\nparsing unstructured data into structured and tabulated format for downstream\nanalytical tasks. In this work we introduced a named-entity recognition model\nfor clinical natural language processing. The model is trained to recognise\nseven categories: drug names, route, frequency, dosage, strength, form,\nduration. The model was first self-supervisedly pre-trained by predicting the\nnext word, using a collection of 2 million free-text patients' records from\nMIMIC-III corpora and then fine-tuned on the named-entity recognition task. The\nmodel achieved a lenient (strict) micro-averaged F1 score of 0.957 (0.893)\nacross all seven categories. Additionally, we evaluated the transferability of\nthe developed model using the data from the Intensive Care Unit in the US to\nsecondary care mental health records (CRIS) in the UK. A direct application of\nthe trained NER model to CRIS data resulted in reduced performance of F1=0.762,\nhowever after fine-tuning on a small sample from CRIS, the model achieved a\nreasonable performance of F1=0.944. This demonstrated that despite a close\nsimilarity between the data sets and the NER tasks, it is essential to\nfine-tune on the target domain data in order to achieve more accurate results.\n",
    "topics": "{'Named Entity Recognition': 0.88778055, 'Representation Learning': 0.8204406, 'Transfer Learning': 0.7582096}",
    "score": 0.8392953446
  },
  {
    "id": "2005.00966",
    "title": "Boundary-aware Context Neural Network for Medical Image Segmentation",
    "abstract": "  Medical image segmentation can provide a reliable basis for further clinical\nanalysis and disease diagnosis. The performance of medical image segmentation\nhas been significantly advanced with the convolutional neural networks (CNNs).\nHowever, most existing CNNs-based methods often produce unsatisfactory\nsegmentation mask without accurate object boundaries. This is caused by the\nlimited context information and inadequate discriminative feature maps after\nconsecutive pooling and convolution operations. In that the medical image is\ncharacterized by the high intra-class variation, inter-class indistinction and\nnoise, extracting powerful context and aggregating discriminative features for\nfine-grained segmentation are still challenging today. In this paper, we\nformulate a boundary-aware context neural network (BA-Net) for 2D medical image\nsegmentation to capture richer context and preserve fine spatial information.\nBA-Net adopts encoder-decoder architecture. In each stage of encoder network,\npyramid edge extraction module is proposed for obtaining edge information with\nmultiple granularities firstly. Then we design a mini multi-task learning\nmodule for jointly learning to segment object masks and detect lesion\nboundaries. In particular, a new interactive attention is proposed to bridge\ntwo tasks for achieving information complementarity between different tasks,\nwhich effectively leverages the boundary information for offering a strong cue\nto better segmentation prediction. At last, a cross feature fusion module aims\nto selectively aggregate multi-level features from the whole encoder network.\nBy cascaded three modules, richer context and fine-grain features of each stage\nare encoded. Extensive experiments on five datasets show that the proposed\nBA-Net outperforms state-of-the-art approaches.\n",
    "topics": "{'Semantic Segmentation': 1.0, 'Medical Image Segmentation': 1.0, 'Multi-Task Learning': 0.9882919}",
    "score": 0.8391309058
  },
  {
    "id": "1511.06219",
    "title": "Knowledge Base Population using Semantic Label Propagation",
    "abstract": "  A crucial aspect of a knowledge base population system that extracts new\nfacts from text corpora, is the generation of training data for its relation\nextractors. In this paper, we present a method that maximizes the effectiveness\nof newly trained relation extractors at a minimal annotation cost. Manual\nlabeling can be significantly reduced by Distant Supervision, which is a method\nto construct training data automatically by aligning a large text corpus with\nan existing knowledge base of known facts. For example, all sentences\nmentioning both 'Barack Obama' and 'US' may serve as positive training\ninstances for the relation born_in(subject,object). However, distant\nsupervision typically results in a highly noisy training set: many training\nsentences do not really express the intended relation. We propose to combine\ndistant supervision with minimal manual supervision in a technique called\nfeature labeling, to eliminate noise from the large and noisy initial training\nset, resulting in a significant increase of precision. We further improve on\nthis approach by introducing the Semantic Label Propagation method, which uses\nthe similarity between low-dimensional representations of candidate training\ninstances, to extend the training set in order to increase recall while\nmaintaining high precision. Our proposed strategy for generating training data\nis studied and evaluated on an established test collection designed for\nknowledge base population tasks. The experimental results show that the\nSemantic Label Propagation strategy leads to substantial performance gains when\ncompared to existing approaches, while requiring an almost negligible manual\nannotation effort.\n",
    "topics": "{'Knowledge Base Population': 0.9871005}",
    "score": 0.8390637553
  },
  {
    "id": "2008.08698",
    "title": "Self-Supervised Ultrasound to MRI Fetal Brain Image Synthesis",
    "abstract": "  Fetal brain magnetic resonance imaging (MRI) offers exquisite images of the\ndeveloping brain but is not suitable for second-trimester anomaly screening,\nfor which ultrasound (US) is employed. Although expert sonographers are adept\nat reading US images, MR images which closely resemble anatomical images are\nmuch easier for non-experts to interpret. Thus in this paper we propose to\ngenerate MR-like images directly from clinical US images. In medical image\nanalysis such a capability is potentially useful as well, for instance for\nautomatic US-MRI registration and fusion. The proposed model is end-to-end\ntrainable and self-supervised without any external annotations. Specifically,\nbased on an assumption that the US and MRI data share a similar anatomical\nlatent space, we first utilise a network to extract the shared latent features,\nwhich are then used for MRI synthesis. Since paired data is unavailable for our\nstudy (and rare in practice), pixel-level constraints are infeasible to apply.\nWe instead propose to enforce the distributions to be statistically\nindistinguishable, by adversarial learning in both the image domain and feature\nspace. To regularise the anatomical structures between US and MRI during\nsynthesis, we further propose an adversarial structural constraint. A new\ncross-modal attention technique is proposed to utilise non-local spatial\ninformation, by encouraging multi-modal knowledge fusion and propagation. We\nextend the approach to consider the case where 3D auxiliary information (e.g.,\n3D neighbours and a 3D location index) from volumetric data is also available,\nand show that this improves image synthesis. The proposed approach is evaluated\nquantitatively and qualitatively with comparison to real fetal MR images and\nother approaches to synthesis, demonstrating its feasibility of synthesising\nrealistic MR images.\n",
    "topics": "{'Image Generation': 0.9989691}",
    "score": 0.8390418373
  },
  {
    "id": "1803.03434",
    "title": "Solving Fourier ptychographic imaging problems via neural network\n  modeling and TensorFlow",
    "abstract": "  Fourier ptychography is a recently developed imaging approach for large\nfield-of-view and high-resolution microscopy. Here we model the Fourier\nptychographic forward imaging process using a convolution neural network (CNN)\nand recover the complex object information in the network training process. In\nthis approach, the input of the network is the point spread function in the\nspatial domain or the coherent transfer function in the Fourier domain. The\nobject is treated as 2D learnable weights of a convolution or a multiplication\nlayer. The output of the network is modeled as the loss function we aim to\nminimize. The batch size of the network corresponds to the number of captured\nlow-resolution images in one forward / backward pass. We use a popular\nopen-source machine learning library, TensorFlow, for setting up the network\nand conducting the optimization process. We analyze the performance of\ndifferent learning rates, different solvers, and different batch sizes. It is\nshown that a large batch size with the Adam optimizer achieves the best\nperformance in general. To accelerate the phase retrieval process, we also\ndiscuss a strategy to implement Fourier-magnitude projection using a\nmultiplication neural network model. Since convolution and multiplication are\nthe two most-common operations in imaging modeling, the reported approach may\nprovide a new perspective to examine many coherent and incoherent systems. As a\ndemonstration, we discuss the extensions of the reported networks for modeling\nsingle-pixel imaging and structured illumination microscopy (SIM). 4-frame\nresolution doubling is demonstrated using a neural network for SIM. We have\nmade our implementation code open-source for the broad research community.\n",
    "topics": "{}",
    "score": 0.8390356578
  },
  {
    "id": "1812.10766",
    "title": "SMPLR: Deep SMPL reverse for 3D human pose and shape recovery",
    "abstract": "  Current state-of-the-art in 3D human pose and shape recovery relies on deep\nneural networks and statistical morphable body models, such as the Skinned\nMulti-Person Linear model (SMPL). However, regardless of the advantages of\nhaving both body pose and shape, SMPL-based solutions have shown difficulties\nto predict 3D bodies accurately. This is mainly due to the unconstrained nature\nof SMPL, which may generate unrealistic body meshes. Because of this,\nregression of SMPL parameters is a difficult task, often addressed with complex\nregularization terms. In this paper we propose to embed SMPL within a deep\nmodel to accurately estimate 3D pose and shape from a still RGB image. We use\nCNN-based 3D joint predictions as an intermediate representation to regress\nSMPL pose and shape parameters. Later, 3D joints are reconstructed again in the\nSMPL output. This module can be seen as an autoencoder where the encoder is a\ndeep neural network and the decoder is SMPL model. We refer to this as SMPL\nreverse (SMPLR). By implementing SMPLR as an encoder-decoder we avoid the need\nof complex constraints on pose and shape. Furthermore, given that in-the-wild\ndatasets usually lack accurate 3D annotations, it is desirable to lift 2D\njoints to 3D without pairing 3D annotations with RGB images. Therefore, we also\npropose a denoising autoencoder (DAE) module between CNN and SMPLR, able to\nlift 2D joints to 3D and partially recover from structured error. We evaluate\nour method on SURREAL and Human3.6M datasets, showing improvement over\nSMPL-based state-of-the-art alternatives by about 4 and 25 millimeters,\nrespectively.\n",
    "topics": "{'3D Human Pose Estimation': 0.8322125, 'Pose Prediction': 0.42709765, 'Denoising': 0.38054195}",
    "score": 0.8390244471
  },
  {
    "id": "2009.10858",
    "title": "Improving Medical Annotation Quality to Decrease Labeling Burden Using\n  Stratified Noisy Cross-Validation",
    "abstract": "  As machine learning has become increasingly applied to medical imaging data,\nnoise in training labels has emerged as an important challenge. Variability in\ndiagnosis of medical images is well established; in addition, variability in\ntraining and attention to task among medical labelers may exacerbate this\nissue. Methods for identifying and mitigating the impact of low quality labels\nhave been studied, but are not well characterized in medical imaging tasks. For\ninstance, Noisy Cross-Validation splits the training data into halves, and has\nbeen shown to identify low-quality labels in computer vision tasks; but it has\nnot been applied to medical imaging tasks specifically. In this work we\nintroduce Stratified Noisy Cross-Validation (SNCV), an extension of noisy cross\nvalidation. SNCV can provide estimates of confidence in model predictions by\nassigning a quality score to each example; stratify labels to handle class\nimbalance; and identify likely low-quality labels to analyze the causes. We\nassess performance of SNCV on diagnosis of glaucoma suspect risk from retinal\nfundus photographs, a clinically important yet nuanced labeling task. Using\ntraining data from a previously-published deep learning model, we compute a\ncontinuous quality score (QS) for each training example. We relabel 1,277\nlow-QS examples using a trained glaucoma specialist; the new labels agree with\nthe SNCV prediction over the initial label >85% of the time, indicating that\nlow-QS examples mostly reflect labeler errors. We then quantify the impact of\ntraining with only high-QS labels, showing that strong model performance may be\nobtained with many fewer examples. By applying the method to randomly\nsub-sampled training dataset, we show that our method can reduce labelling\nburden by approximately 50% while achieving model performance non-inferior to\nusing the full dataset on multiple held-out test sets.\n",
    "topics": "{}",
    "score": 0.8386655829
  },
  {
    "id": "1808.08093",
    "title": "Atherosclerotic carotid plaques on panoramic imaging: an automatic\n  detection using deep learning with small dataset",
    "abstract": "  Stroke is the second most frequent cause of death worldwide with a\nconsiderable economic burden on the health systems. In about 15% of strokes,\natherosclerotic carotid plaques (ACPs) constitute the main etiological factor.\nEarly detection of ACPs may have a key-role for preventing strokes by managing\nthe patient a-priory to the occurrence of the damage. ACPs can be detected on\npanoramic images. As these are one of the most common images performed for\nroutine dental practice, they can be used as a source of available data for\ncomputerized methods of automatic detection in order to significantly increase\ntimely diagnosis of ACPs. Recently, there has been a definite breakthrough in\nthe field of analysis of medical images due to the use of deep learning based\non neural networks. These methods, however have been barely used in dentistry.\nIn this study we used the Faster Region-based Convolutional Network (Faster\nR-CNN) for deep learning. We aimed to assess the operation of the algorithm on\na small database of 65 panoramic images. Due to a small amount of available\ntraining data, we had to use data augmentation by changing the brightness and\nrandomly flipping and rotating cropped regions of interest in multiple angles.\nReceiver Operating Characteristic (ROC) analysis was performed to calculate the\naccuracy of detection. ACP was detected with a sensitivity of 75%, specificity\nof 80% and an accuracy of 83%. The ROC analysis showed a significant Area Under\nCurve (AUC) difference from 0.5. Our novelty lies in that we have showed the\nefficiency of the Faster R-CNN algorithm in detecting ACPs on routine panoramic\nimages based on a small database. There is a need to further improve the\napplication of the algorithm to the level of introducing this methodology in\nroutine dental practice in order to enable us to prevent stroke events.\n",
    "topics": "{'Data Augmentation': 0.9972753}",
    "score": 0.8385457936
  },
  {
    "id": "2007.09438",
    "title": "Few-Shot Defect Segmentation Leveraging Abundant Normal Training Samples\n  Through Normal Background Regularization and Crop-and-Paste Operation",
    "abstract": "  In industrial product quality assessment, it is essential to determine\nwhether a product is defect-free and further analyze the severity of anomality.\nTo this end, accurate defect segmentation on images of products provides an\nimportant functionality. In industrial inspection tasks, it is common to\ncapture abundant defect-free image samples but very limited anomalous ones.\nTherefore, it is critical to develop automatic and accurate defect segmentation\nsystems using only a small number of annotated anomalous training images. This\npaper tackles the challenging few-shot defect segmentation task with sufficient\nnormal (defect-free) training images but very few anomalous ones. We present\ntwo effective regularization techniques via incorporating abundant defect-free\nimages into the training of a UNet-like encoder-decoder defect segmentation\nnetwork. We first propose a Normal Background Regularization (NBR) loss which\nis jointly minimized with the segmentation loss, enhancing the encoder network\nto produce distinctive representations for normal regions. Secondly, we\ncrop/paste defective regions to the randomly selected normal images for data\naugmentation and propose a weighted binary cross-entropy loss to enhance the\ntraining by emphasizing more realistic crop-and-pasted augmented images based\non feature-level similarity comparison. Both techniques are implemented on an\nencoder-decoder segmentation network backboned by ResNet-34 for few-shot defect\nsegmentation. Extensive experiments are conducted on the recently released\nMVTec Anomaly Detection dataset with high-resolution industrial images. Under\nboth 1-shot and 5-shot defect segmentation settings, the proposed method\nsignificantly outperforms several benchmarking methods.\n",
    "topics": "{'Data Augmentation': 0.9505925}",
    "score": 0.8383796825
  },
  {
    "id": "2008.05381",
    "title": "Improving the Performance of Fine-Grain Image Classifiers via Generative\n  Data Augmentation",
    "abstract": "  Recent advances in machine learning (ML) and computer vision tools have\nenabled applications in a wide variety of arenas such as financial analytics,\nmedical diagnostics, and even within the Department of Defense. However, their\nwidespread implementation in real-world use cases poses several challenges: (1)\nmany applications are highly specialized, and hence operate in a \\emph{sparse\ndata} domain; (2) ML tools are sensitive to their training sets and typically\nrequire cumbersome, labor-intensive data collection and data labelling\nprocesses; and (3) ML tools can be extremely \"black box,\" offering users little\nto no insight into the decision-making process or how new data might affect\nprediction performance. To address these challenges, we have designed and\ndeveloped Data Augmentation from Proficient Pre-Training of Robust Generative\nAdversarial Networks (DAPPER GAN), an ML analytics support tool that\nautomatically generates novel views of training images in order to improve\ndownstream classifier performance. DAPPER GAN leverages high-fidelity\nembeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to\ncreate novel imagery for previously unseen classes. We experimentally evaluate\nthis technique on the Stanford Cars dataset, demonstrating improved vehicle\nmake and model classification accuracy and reduced requirements for real data\nusing our GAN based data augmentation framework. The method's validity was\nsupported through an analysis of classifier performance on both augmented and\nnon-augmented datasets, achieving comparable or better accuracy with up to 30\\%\nless real data across visually similar classes. To support this method, we\ndeveloped a novel augmentation method that can manipulate semantically\nmeaningful dimensions (e.g., orientation) of the target object in the embedding\nspace.\n",
    "topics": "{'Data Augmentation': 1.0, 'Decision Making': 0.96160716}",
    "score": 0.8382839119
  },
  {
    "id": "1304.3992",
    "title": "GPU Acclerated Automated Feature Extraction from Satellite Images",
    "abstract": "  The availability of large volumes of remote sensing data insists on higher\ndegree of automation in feature extraction, making it a need of the hour.The\nhuge quantum of data that needs to be processed entails accelerated processing\nto be enabled.GPUs, which were originally designed to provide efficient\nvisualization, are being massively employed for computation intensive parallel\nprocessing environments. Image processing in general and hence automated\nfeature extraction, is highly computation intensive, where performance\nimprovements have a direct impact on societal needs. In this context, an\nalgorithm has been formulated for automated feature extraction from a\npanchromatic or multispectral image based on image processing techniques. Two\nLaplacian of Guassian (LoG) masks were applied on the image individually\nfollowed by detection of zero crossing points and extracting the pixels based\non their standard deviation with the surrounding pixels. The two extracted\nimages with different LoG masks were combined together which resulted in an\nimage with the extracted features and edges. Finally the user is at liberty to\napply the image smoothing step depending on the noise content in the extracted\nimage. The image is passed through a hybrid median filter to remove the salt\nand pepper noise from the image. This paper discusses the aforesaid algorithm\nfor automated feature extraction, necessity of deployment of GPUs for the same;\nsystem-level challenges and quantifies the benefits of integrating GPUs in such\nenvironment. The results demonstrate that substantial enhancement in\nperformance margin can be achieved with the best utilization of GPU resources\nand an efficient parallelization strategy. Performance results in comparison\nwith the conventional computing scenario have provided a speedup of 20x, on\nrealization of this parallelizing strategy.\n",
    "topics": "{}",
    "score": 0.8381912683
  },
  {
    "id": "1801.06940",
    "title": "MRI Cross-Modality NeuroImage-to-NeuroImage Translation",
    "abstract": "  We present a cross-modality generation framework that learns to generate\ntranslated modalities from given modalities in MR images without real\nacquisition. Our proposed method performs NeuroImage-to-NeuroImage translation\n(abbreviated as N2N) by means of a deep learning model that leverages\nconditional generative adversarial networks (cGANs). Our framework jointly\nexploits the low-level features (pixel-wise information) and high-level\nrepresentations (e.g. brain tumors, brain structure like gray matter, etc.)\nbetween cross modalities which are important for resolving the challenging\ncomplexity in brain structures. Our framework can serve as an auxiliary method\nin clinical diagnosis and has great application potential. Based on our\nproposed framework, we first propose a method for cross-modality registration\nby fusing the deformation fields to adopt the cross-modality information from\ntranslated modalities. Second, we propose an approach for MRI segmentation,\ntranslated multichannel segmentation (TMS), where given modalities, along with\ntranslated modalities, are segmented by fully convolutional networks (FCN) in a\nmultichannel manner. Both of these two methods successfully adopt the\ncross-modality information to improve the performance without adding any extra\ndata. Experiments demonstrate that our proposed framework advances the\nstate-of-the-art on five brain MRI datasets. We also observe encouraging\nresults in cross-modality registration and segmentation on some widely adopted\nbrain datasets. Overall, our work can serve as an auxiliary method in clinical\ndiagnosis and be applied to various tasks in medical fields.\n  Keywords: image-to-image, cross-modality, registration, segmentation, brain\nMRI\n",
    "topics": "{}",
    "score": 0.8381344247
  },
  {
    "id": "2002.06838",
    "title": "Hierarchical Rule Induction Network for Abstract Visual Reasoning",
    "abstract": "  Abstract reasoning refers to the ability to analyze information, discover\nrules at an intangible level, and solve problems in innovative ways. Raven's\nProgressive Matrices (RPM) test is typically used to examine the capability of\nabstract reasoning. In the test, the subject is asked to identify the correct\nchoice from the answer set to fill the missing panel at the bottom right of RPM\n(e.g., a 3$\\times$3 matrix), following the underlying rules inside the matrix.\nRecent studies, taking advantage of Convolutional Neural Networks (CNNs), have\nachieved encouraging progress to accomplish the RPM test problems.\nUnfortunately, simply relying on the relation extraction at the matrix level,\nthey fail to recognize the complex attribute patterns inside or across\nrows/columns of RPM. To address this problem, in this paper we propose a\nHierarchical Rule Induction Network (HriNet), by intimating human induction\nstrategies. HriNet extracts multiple granularity rule embeddings at different\nlevels and integrates them through a gated embedding fusion module. We further\nintroduce a rule similarity metric based on the embeddings, so that HriNet can\nnot only be trained using a tuplet loss but also infer the best answer\naccording to the similarity score. To comprehensively evaluate HriNet, we first\nfix the defects contained in the very recent RAVEN dataset and generate a new\none named Balanced-RAVEN. Then extensive experiments are conducted on the\nlarge-scale dataset PGM and our Balanced-RAVEN, the results of which show that\nHriNet outperforms the state-of-the-art models by a large margin.\n",
    "topics": "{'Visual Reasoning': 0.9998747, 'Relation Extraction': 0.99035513}",
    "score": 0.838113782
  },
  {
    "id": "2010.03516",
    "title": "Combination of digital signal processing and assembled predictive models\n  facilitates the rational design of proteins",
    "abstract": "  Predicting the effect of mutations in proteins is one of the most critical\nchallenges in protein engineering; by knowing the effect a substitution of one\n(or several) residues in the protein's sequence has on its overall properties,\ncould design a variant with a desirable function. New strategies and\nmethodologies to create predictive models are continually being developed.\nHowever, those that claim to be general often do not reach adequate\nperformance, and those that aim to a particular task improve their predictive\nperformance at the cost of the method's generality. Moreover, these approaches\ntypically require a particular decision to encode the amino acidic sequence,\nwithout an explicit methodological agreement in such endeavor. To address these\nissues, in this work, we applied clustering, embedding, and dimensionality\nreduction techniques to the AAIndex database to select meaningful combinations\nof physicochemical properties for the encoding stage. We then used the chosen\nset of properties to obtain several encodings of the same sequence, to\nsubsequently apply the Fast Fourier Transform (FFT) on them. We perform an\nexploratory stage of Machine-Learning models in the frequency space, using\ndifferent algorithms and hyperparameters. Finally, we select the best\nperforming predictive models in each set of properties and create an assembled\nmodel. We extensively tested the proposed methodology on different datasets and\ndemonstrated that the generated assembled model achieved notably better\nperformance metrics than those models based on a single encoding and, in most\ncases, better than those previously reported. The proposed method is available\nas a Python library for non-commercial use under the GNU General Public License\n(GPLv3) license.\n",
    "topics": "{'Dimensionality Reduction': 0.99863654}",
    "score": 0.8380901739
  },
  {
    "id": "2006.06704",
    "title": "End-to-end Sinkhorn Autoencoder with Noise Generator",
    "abstract": "  In this work, we propose a novel end-to-end sinkhorn autoencoder with noise\ngenerator for efficient data collection simulation. Simulating processes that\naim at collecting experimental data is crucial for multiple real-life\napplications, including nuclear medicine, astronomy and high energy physics.\nContemporary methods, such as Monte Carlo algorithms, provide high-fidelity\nresults at a price of high computational cost. Multiple attempts are taken to\nreduce this burden, e.g. using generative approaches based on Generative\nAdversarial Networks or Variational Autoencoders. Although such methods are\nmuch faster, they are often unstable in training and do not allow sampling from\nan entire data distribution. To address these shortcomings, we introduce a\nnovel method dubbed end-to-end Sinkhorn Autoencoder, that leverages sinkhorn\nalgorithm to explicitly align distribution of encoded real data examples and\ngenerated noise. More precisely, we extend autoencoder architecture by adding a\ndeterministic neural network trained to map noise from a known distribution\nonto autoencoder latent space representing data distribution. We optimise the\nentire model jointly. Our method outperforms competing approaches on a\nchallenging dataset of simulation data from Zero Degree Calorimeters of ALICE\nexperiment in LHC. as well as standard benchmarks, such as MNIST and CelebA.\n",
    "topics": "{}",
    "score": 0.8379780866
  },
  {
    "id": "2010.03990",
    "title": "UESegNet: Context Aware Unconstrained ROI Segmentation Networks for Ear\n  Biometric",
    "abstract": "  Biometric-based personal authentication systems have seen a strong demand\nmainly due to the increasing concern in various privacy and security\napplications. Although the use of each biometric trait is problem dependent,\nthe human ear has been found to have enough discriminating characteristics to\nallow its use as a strong biometric measure. To locate an ear in a 2D side face\nimage is a challenging task, numerous existing approaches have achieved\nsignificant performance, but the majority of studies are based on the\nconstrained environment. However, ear biometrics possess a great level of\ndifficulties in the unconstrained environment, where pose, scale, occlusion,\nilluminations, background clutter etc. varies to a great extent. To address the\nproblem of ear localization in the wild, we have proposed two high-performance\nregion of interest (ROI) segmentation models UESegNet-1 and UESegNet-2, which\nare fundamentally based on deep convolutional neural networks and primarily\nuses contextual information to localize ear in the unconstrained environment.\nAdditionally, we have applied state-of-the-art deep learning models viz; FRCNN\n(Faster Region Proposal Network) and SSD (Single Shot MultiBox Detecor) for ear\nlocalization task. To test the model's generalization, they are evaluated on\nsix different benchmark datasets viz; IITD, IITK, USTB-DB3, UND-E, UND-J2 and\nUBEAR, all of which contain challenging images. The performance of the models\nis compared on the basis of object detection performance measure parameters\nsuch as IOU (Intersection Over Union), Accuracy, Precision, Recall, and\nF1-Score. It has been observed that the proposed models UESegNet-1 and\nUESegNet-2 outperformed the FRCNN and SSD at higher values of IOUs i.e. an\naccuracy of 100\\% is achieved at IOU 0.5 on majority of the databases.\n",
    "topics": "{'Region Proposal': 1.0, 'Object Detection': 0.9799954}",
    "score": 0.8379405908
  },
  {
    "id": "1803.03963",
    "title": "BTS-DSN: Deeply Supervised Neural Network with Short Connections for\n  Retinal Vessel Segmentation",
    "abstract": "  Background and Objective: The condition of vessel of the human eye is an\nimportant factor for the diagnosis of ophthalmological diseases. Vessel\nsegmentation in fundus images is a challenging task due to complex vessel\nstructure, the presence of similar structures such as microaneurysms and\nhemorrhages, micro-vessel with only one to several pixels wide, and\nrequirements for finer results. Methods:In this paper, we present a multi-scale\ndeeply supervised network with short connections (BTS-DSN) for vessel\nsegmentation. We used short connections to transfer semantic information\nbetween side-output layers. Bottom-top short connections pass low level\nsemantic information to high level for refining results in high-level\nside-outputs, and top-bottom short connection passes much structural\ninformation to low level for reducing noises in low-level side-outputs. In\naddition, we employ cross-training to show that our model is suitable for real\nworld fundus images. Results: The proposed BTS-DSN has been verified on DRIVE,\nSTARE and CHASE_DB1 datasets, and showed competitive performance over other\nstate-of-the-art methods. Specially, with patch level input, the network\nachieved 0.7891/0.8212 sensitivity, 0.9804/0.9843 specificity, 0.9806/0.9859\nAUC, and 0.8249/0.8421 F1-score on DRIVE and STARE, respectively. Moreover, our\nmodel behaves better than other methods in cross-training experiments.\nConclusions: BTS-DSN achieves competitive performance in vessel segmentation\ntask on three public datasets. It is suitable for vessel segmentation. The\nsource code of our method is available at https://github.com/guomugong/BTS-DSN.\n",
    "topics": "{'Retinal Vessel Segmentation': 1.0}",
    "score": 0.8378650698
  },
  {
    "id": "2009.09445",
    "title": "Unsupervised Domain Adaptation for Person Re-Identification through\n  Source-Guided Pseudo-Labeling",
    "abstract": "  Person Re-Identification (re-ID) aims at retrieving images of the same person\ntaken by different cameras. A challenge for re-ID is the performance\npreservation when a model is used on data of interest (target data) which\nbelong to a different domain from the training data domain (source data).\nUnsupervised Domain Adaptation (UDA) is an interesting research direction for\nthis challenge as it avoids a costly annotation of the target data.\nPseudo-labeling methods achieve the best results in UDA-based re-ID.\nSurprisingly, labeled source data are discarded after this initialization step.\nHowever, we believe that pseudo-labeling could further leverage the labeled\nsource data in order to improve the post-initialization training steps. In\norder to improve robustness against erroneous pseudo-labels, we advocate the\nexploitation of both labeled source data and pseudo-labeled target data during\nall training iterations. To support our guideline, we introduce a framework\nwhich relies on a two-branch architecture optimizing classification and triplet\nloss based metric learning in source and target domains, respectively, in order\nto allow \\emph{adaptability to the target domain} while ensuring\n\\emph{robustness to noisy pseudo-labels}. Indeed, shared low and mid-level\nparameters benefit from the source classification and triplet loss signal while\nhigh-level parameters of the target branch learn domain-specific features. Our\nmethod is simple enough to be easily combined with existing pseudo-labeling UDA\napproaches. We show experimentally that it is efficient and improves\nperformance when the base method has no mechanism to deal with pseudo-label\nnoise or for hard adaptation tasks. Our approach reaches state-of-the-art\nperformance when evaluated on commonly used datasets, Market-1501 and\nDukeMTMC-reID, and outperforms the state of the art when targeting the bigger\nand more challenging dataset MSMT.\n",
    "topics": "{'Person Re-Identification': 0.9999999, 'Unsupervised Domain Adaptation': 0.99990034, 'Metric Learning': 0.96124136, 'Domain Adaptation': 0.9363473}",
    "score": 0.8375913747
  },
  {
    "id": "1812.01049",
    "title": "Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall\n  Survival Prediction using Radiomic Features",
    "abstract": "  Accurate segmentation of different sub-regions of gliomas including\nperitumoral edema, necrotic core, enhancing and non-enhancing tumor core from\nmultimodal MRI scans has important clinical relevance in diagnosis, prognosis\nand treatment of brain tumors. However, due to the highly heterogeneous\nappearance and shape, segmentation of the sub-regions is very challenging.\nRecent development using deep learning models has proved its effectiveness in\nthe past several brain segmentation challenges as well as other semantic and\nmedical image segmentation problems. Most models in brain tumor segmentation\nuse a 2D/3D patch to predict the class label for the center voxel and variant\npatch sizes and scales are used to improve the model performance. However, it\nhas low computation efficiency and also has limited receptive field. U-Net is a\nwidely used network structure for end-to-end segmentation and can be used on\nthe entire image or extracted patches to provide classification labels over the\nentire input voxels so that it is more efficient and expect to yield better\nperformance with larger input size. Furthermore, instead of picking the best\nnetwork structure, an ensemble of multiple models, trained on different dataset\nor different hyper-parameters, can generally improve the segmentation\nperformance. In this study we propose to use an ensemble of 3D U-Nets with\ndifferent hyper-parameters for brain tumor segmentation. Preliminary results\nshowed effectiveness of this model. In addition, we developed a linear model\nfor survival prediction using extracted imaging and non-imaging features,\nwhich, despite the simplicity, can effectively reduce overfitting and\nregression errors.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'Brain Tumor Segmentation': 1.0, 'Medical Image Segmentation': 0.99891496}",
    "score": 0.8373143883
  },
  {
    "id": "1911.10773",
    "title": "Fine-grained Attention and Feature-sharing Generative Adversarial\n  Networks for Single Image Super-Resolution",
    "abstract": "  The traditional super-resolution methods that aim to minimize the mean square\nerror usually produce the images with over-smoothed and blurry edges, due to\nthe lose of high-frequency details. In this paper, we propose two novel\ntechniques in the generative adversarial networks to produce photo-realistic\nimages for image super-resolution. Firstly, instead of producing a single score\nto discriminate images between real and fake, we propose a variant, called\nFine-grained Attention Generative Adversarial Network for image\nsuper-resolution (FASRGAN), to discriminate each pixel between real and fake.\nFASRGAN adopts a Unet-like network as the discriminator with two outputs: an\nimage score and an image score map. The score map has the same spatial size as\nthe HR/SR images, serving as the fine-grained attention to represent the degree\nof reconstruction difficulty for each pixel. Secondly, instead of using\ndifferent networks for the generator and the discriminator in the SR problem,\nwe use a feature-sharing network (Fs-SRGAN) for both the generator and the\ndiscriminator. By network sharing, certain information is shared between the\ngenerator and the discriminator, which in turn can improve the ability of\nproducing high-quality images. Quantitative and visual comparisons with the\nstate-of-the-art methods on the benchmark datasets demonstrate the superiority\nof our methods. The application of super-resolution images to object\nrecognition further proves that the proposed methods endow the power to\nreconstruction capabilities and the excellent super-resolution effects.\n",
    "topics": "{'Super-Resolution': 1.0, 'Super Resolution': 1.0, 'Image Super-Resolution': 1.0, 'Object Recognition': 0.98869497}",
    "score": 0.8372132359
  },
  {
    "id": "1909.11380",
    "title": "Beyond image classification: zooplankton identification with deep vector\n  space embeddings",
    "abstract": "  Zooplankton images, like many other real world data types, have intrinsic\nproperties that make the design of effective classification systems difficult.\nFor instance, the number of classes encountered in practical settings is\npotentially very large, and classes can be ambiguous or overlap. In addition,\nthe choice of taxonomy often differs between researchers and between\ninstitutions. Although high accuracy has been achieved in benchmarks using\nstandard classifier architectures, biases caused by an inflexible\nclassification scheme can have profound effects when the output is used in\necosystem assessments and monitoring.\n  Here, we propose using a deep convolutional network to construct a vector\nembedding of zooplankton images. The system maps (embeds) each image into a\nhigh-dimensional Euclidean space so that distances between vectors reflect\nsemantic relationships between images. We show that the embedding can be used\nto derive classifications with comparable accuracy to a specific classifier,\nbut that it simultaneously reveals important structures in the data.\nFurthermore, we apply the embedding to new classes previously unseen by the\nsystem, and evaluate its classification performance in such cases.\n  Traditional neural network classifiers perform well when the classes are\nclearly defined a priori and have sufficiently large labeled data sets\navailable. For practical cases in ecology as well as in many other fields this\nis not the case, and we argue that the vector embedding method presented here\nis a more appropriate approach.\n",
    "topics": "{'Image Classification': 0.97026676}",
    "score": 0.8370632441
  },
  {
    "id": "2001.09947",
    "title": "Near real-time map building with multi-class image set labelling and\n  classification of road conditions using convolutional neural networks",
    "abstract": "  Weather is an important factor affecting transportation and road safety. In\nthis paper, we leverage state-of-the-art convolutional neural networks in\nlabelling images taken by street and highway cameras located across across\nNorth America. Road camera snapshots were used in experiments with multiple\ndeep learning frameworks to classify images by road condition. The training\ndata for these experiments used images labelled as dry, wet, snow/ice, poor,\nand offline. The experiments tested different configurations of six\nconvolutional neural networks (VGG-16, ResNet50, Xception, InceptionResNetV2,\nEfficientNet-B0 and EfficientNet-B4) to assess their suitability to this\nproblem. The precision, accuracy, and recall were measured for each framework\nconfiguration. In addition, the training sets were varied both in overall size\nand by size of individual classes. The final training set included 47,000\nimages labelled using the five aforementioned classes. The EfficientNet-B4\nframework was found to be most suitable to this problem, achieving validation\naccuracy of 90.6%, although EfficientNet-B0 achieved an accuracy of 90.3% with\nhalf the execution time. It was observed that VGG-16 with transfer learning\nproved to be very useful for data acquisition and pseudo-labelling with limited\nhardware resources, throughout this project. The EfficientNet-B4 framework was\nthen placed into a real-time production environment, where images could be\nclassified in real-time on an ongoing basis. The classified images were then\nused to construct a map showing real-time road conditions at various camera\nlocations across North America. The choice of these frameworks and our analysis\ntake into account unique requirements of real-time map building functions. A\ndetailed analysis of the process of semi-automated dataset labelling using\nthese frameworks is also presented in this paper.\n",
    "topics": "{'Transfer Learning': 0.9175267}",
    "score": 0.8370180318
  },
  {
    "id": "2001.03493",
    "title": "A Two-step-training Deep Learning Framework for Real-time Computational\n  Imaging without Physics Priors",
    "abstract": "  Deep learning (DL) is a powerful tool in computational imaging for many\napplications. A common strategy is to reconstruct a preliminary image as the\ninput of a neural network to achieve an optimized image. Usually, the\npreliminary image is acquired with the prior knowledge of the imaging model.\nOne outstanding challenge, however, is the degree to which the actual imaging\nmodel deviates from the assumed model. Model mismatches degrade the quality of\nthe preliminary image and therefore affect the DL predictions. Another main\nchallenge is that since most imaging inverse problems are ill-posed and the\nnetworks are over-parameterized, DL networks have flexibility to extract\nfeatures from the data that are not directly related to the imaging model. To\nsolve these challenges, a two-step-training DL (TST-DL) framework is proposed\nfor real-time computational imaging without physics priors. First, a single\nfully-connected layer (FCL) is trained to directly learn the model. Then, this\nFCL is fixed and concatenated with an un-trained U-Net architecture for a\nsecond-step training to improve the output image fidelity, resulting in four\nmain advantages. First, it does not rely on an accurate representation of the\nimaging model since the model is directly learned. Second, real-time imaging\ncan be achieved. Third, the TST-DL network is trained in the desired direction\nand the predictions are improved since the first step is constrained to learn\nthe model and the second step improves the result by learning the optimal\nregularizer. Fourth, the approach accommodates any size and dimensionality of\ndata. We demonstrate this framework using a linear single-pixel camera imaging\nmodel. The results are quantitatively compared with those from other DL\nframeworks and model-based iterative optimization approaches. We further extend\nthis concept to nonlinear models in the application of image\nde-autocorrelation.\n",
    "topics": "{}",
    "score": 0.8369795502
  },
  {
    "id": "2003.11055",
    "title": "COVIDX-Net: A Framework of Deep Learning Classifiers to Diagnose\n  COVID-19 in X-Ray Images",
    "abstract": "  Background and Purpose: Coronaviruses (CoV) are perilous viruses that may\ncause Severe Acute Respiratory Syndrome (SARS-CoV), Middle East Respiratory\nSyndrome (MERS-CoV). The novel 2019 Coronavirus disease (COVID-19) was\ndiscovered as a novel disease pneumonia in the city of Wuhan, China at the end\nof 2019. Now, it becomes a Coronavirus outbreak around the world, the number of\ninfected people and deaths are increasing rapidly every day according to the\nupdated reports of the World Health Organization (WHO). Therefore, the aim of\nthis article is to introduce a new deep learning framework; namely COVIDX-Net\nto assist radiologists to automatically diagnose COVID-19 in X-ray images.\nMaterials and Methods: Due to the lack of public COVID-19 datasets, the study\nis validated on 50 Chest X-ray images with 25 confirmed positive COVID-19\ncases. The COVIDX-Net includes seven different architectures of deep\nconvolutional neural network models, such as modified Visual Geometry Group\nNetwork (VGG19) and the second version of Google MobileNet. Each deep neural\nnetwork model is able to analyze the normalized intensities of the X-ray image\nto classify the patient status either negative or positive COVID-19 case.\nResults: Experiments and evaluation of the COVIDX-Net have been successfully\ndone based on 80-20% of X-ray images for the model training and testing phases,\nrespectively. The VGG19 and Dense Convolutional Network (DenseNet) models\nshowed a good and similar performance of automated COVID-19 classification with\nf1-scores of 0.89 and 0.91 for normal and COVID-19, respectively. Conclusions:\nThis study demonstrated the useful application of deep learning models to\nclassify COVID-19 in X-ray images based on the proposed COVIDX-Net framework.\nClinical studies are the next milestone of this research work.\n",
    "topics": "{'COVID-19 Diagnosis': 0.997212}",
    "score": 0.8368575824
  },
  {
    "id": "1810.03031",
    "title": "Text-based Sentiment Analysis and Music Emotion Recognition",
    "abstract": "  Sentiment polarity of tweets, blog posts or product reviews has become highly\nattractive and is utilized in recommender systems, market predictions, business\nintelligence and more. Deep learning techniques are becoming top performers on\nanalyzing such texts. There are however several problems that need to be solved\nfor efficient use of deep neural networks on text mining and text polarity\nanalysis. First, deep neural networks need to be fed with data sets that are\nbig in size as well as properly labeled. Second, there are various\nuncertainties regarding the use of word embedding vectors: should they be\ngenerated from the same data set that is used to train the model or it is\nbetter to source them from big and popular collections? Third, to simplify\nmodel creation it is convenient to have generic neural network architectures\nthat are effective and can adapt to various texts, encapsulating much of design\ncomplexity. This thesis addresses the above problems to provide methodological\nand practical insights for utilizing neural networks on sentiment analysis of\ntexts and achieving state of the art results. Regarding the first problem, the\neffectiveness of various crowdsourcing alternatives is explored and two\nmedium-sized and emotion-labeled song data sets are created utilizing social\ntags. To address the second problem, a series of experiments with large text\ncollections of various contents and domains were conducted, trying word\nembeddings of various parameters. Regarding the third problem, a series of\nexperiments involving convolution and max-pooling neural layers were conducted.\nCombining convolutions of words, bigrams, and trigrams with regional\nmax-pooling layers in a couple of stacks produced the best results. The derived\narchitecture achieves competitive performance on sentiment polarity analysis of\nmovie, business and product reviews.\n",
    "topics": "{'Sentiment Analysis': 0.9217494, 'Recommendation Systems': 0.9162216, 'Emotion Recognition': 0.8947545, 'Word Embeddings': 0.8789156}",
    "score": 0.8367684242
  },
  {
    "id": "2006.03423",
    "title": "Generation of Differentially Private Heterogeneous Electronic Health\n  Records",
    "abstract": "  Electronic Health Records (EHRs) are commonly used by the machine learning\ncommunity for research on problems specifically related to health care and\nmedicine. EHRs have the advantages that they can be easily distributed and\ncontain many features useful for e.g. classification problems. What makes EHR\ndata sets different from typical machine learning data sets is that they are\noften very sparse, due to their high dimensionality, and often contain\nheterogeneous (mixed) data types. Furthermore, the data sets deal with\nsensitive information, which limits the distribution of any models learned\nusing them, due to privacy concerns. For these reasons, using EHR data in\npractice presents a real challenge. In this work, we explore using Generative\nAdversarial Networks to generate synthetic, heterogeneous EHRs with the goal of\nusing these synthetic records in place of existing data sets for downstream\nclassification tasks. We will further explore applying differential privacy\n(DP) preserving optimization in order to produce DP synthetic EHR data sets,\nwhich provide rigorous privacy guarantees, and are therefore shareable and\nusable in the real world. The performance (measured by AUROC, AUPRC and\naccuracy) of our model's synthetic, heterogeneous data is very close to the\noriginal data set (within 3 - 5% of the baseline) for the non-DP model when\ntested in a binary classification task. Using strong $(1, 10^{-5})$ DP, our\nmodel still produces data useful for machine learning tasks, albeit incurring a\nroughly 17% performance penalty in our tested classification task. We\nadditionally perform a sub-population analysis and find that our model does not\nintroduce any bias into the synthetic EHR data compared to the baseline in\neither male/female populations, or the 0-18, 19-50 and 51+ age groups in terms\nof classification performance for either the non-DP or DP variant.\n",
    "topics": "{}",
    "score": 0.8365883142
  },
  {
    "id": "1807.01864",
    "title": "Detecting Tiny Moving Vehicles in Satellite Videos",
    "abstract": "  In recent years, the satellite videos have been captured by a moving\nsatellite platform. In contrast to consumer, movie, and common surveillance\nvideos, satellite video can record the snapshot of the city-scale scene. In a\nbroad field-of-view of satellite videos, each moving target would be very tiny\nand usually composed of several pixels in frames. Even worse, the noise signals\nalso existed in the video frames, since the background of the video frame has\nthe subpixel-level and uneven moving thanks to the motion of satellites. We\nargue that this is a new type of computer vision task since previous\ntechnologies are unable to detect such tiny vehicles efficiently. This paper\nproposes a novel framework that can identify the small moving vehicles in\nsatellite videos. In particular, we offer a novel detecting algorithm based on\nthe local noise modeling. We differentiate the potential vehicle targets from\nnoise patterns by an exponential probability distribution. Subsequently, a\nmulti-morphological-cue based discrimination strategy is designed to\ndistinguish correct vehicle targets from a few existing noises further. Another\nsignificant contribution is to introduce a series of evaluation protocols to\nmeasure the performance of tiny moving vehicle detection systematically. We\nannotate a satellite video manually and use it to test our algorithms under\ndifferent evaluation criterion. The proposed algorithm is also compared with\nthe state-of-the-art baselines, and demonstrates the advantages of our\nframework over the benchmarks.\n",
    "topics": "{}",
    "score": 0.8365206465
  },
  {
    "id": "2005.03832",
    "title": "Synergistic Learning of Lung Lobe Segmentation and Hierarchical\n  Multi-Instance Classification for Automated Severity Assessment of COVID-19\n  in CT Images",
    "abstract": "  Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19)\nwill help detect infections early and assess the disease progression.\nEspecially, automated severity assessment of COVID-19 in CT images plays an\nessential role in identifying cases that are in great need of intensive\nclinical care. However, it is often challenging to accurately assess the\nseverity of this disease in CT images, due to variable infection regions in the\nlungs, similar imaging biomarkers, and large inter-case variations. To this\nend, we propose a synergistic learning framework for automated severity\nassessment of COVID-19 in 3D CT images, by jointly performing lung lobe\nsegmentation and multi-instance classification. Considering that only a few\ninfection regions in a CT image are related to the severity assessment, we\nfirst represent each input image by a bag that contains a set of 2D image\npatches (with each cropped from a specific slice). A multi-task multi-instance\ndeep network (called M$^2$UNet) is then developed to assess the severity of\nCOVID-19 patients and also segment the lung lobe simultaneously. Our M$^2$UNet\nconsists of a patch-level encoder, a segmentation sub-network for lung lobe\nsegmentation, and a classification sub-network for severity assessment (with a\nunique hierarchical multi-instance learning strategy). Here, the context\ninformation provided by segmentation can be implicitly employed to improve the\nperformance of severity assessment. Extensive experiments were performed on a\nreal COVID-19 CT image dataset consisting of 666 chest CT images, with results\nsuggesting the effectiveness of our proposed method compared to several\nstate-of-the-art methods.\n",
    "topics": "{'COVID-19 Diagnosis': 0.32630965}",
    "score": 0.8364207113
  },
  {
    "id": "2006.12709",
    "title": "CIE XYZ Net: Unprocessing Images for Low-Level Computer Vision Tasks",
    "abstract": "  Cameras currently allow access to two image states: (i) a minimally processed\nlinear raw-RGB image state (i.e., raw sensor data) or (ii) a highly-processed\nnonlinear image state (e.g., sRGB). There are many computer vision tasks that\nwork best with a linear image state, such as image deblurring and image\ndehazing. Unfortunately, the vast majority of images are saved in the nonlinear\nimage state. Because of this, a number of methods have been proposed to\n\"unprocess\" nonlinear images back to a raw-RGB state. However, existing\nunprocessing methods have a drawback because raw-RGB images are\nsensor-specific. As a result, it is necessary to know which camera produced the\nsRGB output and use a method or network tailored for that sensor to properly\nunprocess it. This paper addresses this limitation by exploiting another camera\nimage state that is not available as an output, but it is available inside the\ncamera pipeline. In particular, cameras apply a colorimetric conversion step to\nconvert the raw-RGB image to a device-independent space based on the CIE XYZ\ncolor space before they apply the nonlinear photo-finishing. Leveraging this\ncanonical image state, we propose a deep learning framework, CIE XYZ Net, that\ncan unprocess a nonlinear image back to the canonical CIE XYZ image. This image\ncan then be processed by any low-level computer vision operator and re-rendered\nback to the nonlinear image. We demonstrate the usefulness of the CIE XYZ Net\non several low-level vision tasks and show significant gains that can be\nobtained by this processing framework. Code and dataset are publicly available\nat https://github.com/mahmoudnafifi/CIE_XYZ_NET.\n",
    "topics": "{'Deblurring': 0.9996891}",
    "score": 0.8363995941
  },
  {
    "id": "1710.01766",
    "title": "DeepLesion: Automated Deep Mining, Categorization and Detection of\n  Significant Radiology Image Findings using Large-Scale Clinical Lesion\n  Annotations",
    "abstract": "  Extracting, harvesting and building large-scale annotated radiological image\ndatasets is a greatly important yet challenging problem. It is also the\nbottleneck to designing more effective data-hungry computing paradigms (e.g.,\ndeep learning) for medical image analysis. Yet, vast amounts of clinical\nannotations (usually associated with disease image findings and marked using\narrows, lines, lesion diameters, segmentation, etc.) have been collected over\nseveral decades and stored in hospitals' Picture Archiving and Communication\nSystems. In this paper, we mine and harvest one major type of clinical\nannotation data - lesion diameters annotated on bookmarked images - to learn an\neffective multi-class lesion detector via unsupervised and supervised deep\nConvolutional Neural Networks (CNN). Our dataset is composed of 33,688\nbookmarked radiology images from 10,825 studies of 4,477 unique patients. For\nevery bookmarked image, a bounding box is created to cover the target lesion\nbased on its measured diameters. We categorize the collection of lesions using\nan unsupervised deep mining scheme to generate clustered pseudo lesion labels.\nNext, we adopt a regional-CNN method to detect lesions of multiple categories,\nregardless of missing annotations (normally only one lesion is annotated,\ndespite the presence of multiple co-existing findings). Our integrated mining,\ncategorization and detection framework is validated with promising empirical\nresults, as a scalable, universal or multi-purpose CAD paradigm built upon\nabundant retrospective medical data. Furthermore, we demonstrate that detection\naccuracy can be significantly improved by incorporating pseudo lesion labels\n(e.g., Liver lesion/tumor, Lung nodule/tumor, Abdomen lesions, Chest lymph node\nand others). This dataset will be made publicly available (under the open\nscience initiative).\n",
    "topics": "{'Lesion Classification': 0.85811925}",
    "score": 0.836238314
  },
  {
    "id": "2002.01031",
    "title": "Superfast Diffusion Tensor Imaging and Fiber Tractography Using Deep\n  Learning",
    "abstract": "  Diffusion tensor imaging (DTI) is widely used to examine the human brain\nwhite matter structures, including their microarchitecture integrity and\nspatial fiber tract trajectories. It has clinical applications in several\nneurological disorders and neurosurgical guidance. However, a major factor that\nprevents DTI from being incorporated in clinical routines is its long scan time\ndue to the acquisition of a large number (typically 30 or more) of\ndiffusion-weighted images (DWIs) required for reliable tensor estimation. Here,\na deep learning-based technique has been developed to obtain tensor-derived\nquantitative maps and fiber tractography with only six DWIs, resulting in a\nsignificant reduction in imaging time. The method uses deep convolutional\nneural networks to learn the nonlinear relationship between DWIs and several\ntensor-derived maps, bypassing the conventional tensor fitting procedure, which\nis well known to be highly susceptible to noises in DWIs. The performance of\nthe method was evaluated using DWI datasets from the Human Connectome Project\nand patients with ischemic stroke. Our results demonstrate that the proposed\ntechnique is able to generate fractional anisotropy (FA) and mean diffusivity\n(MD) maps, as well as fiber tractography, from as few as six DWIs. With results\nfrom 90 DWIs as the ground truth, the proposed method from six DWIs achieves a\nquantification error of less than 3% in all regions of interest in white matter\nstructures and 15% in gray matter structures. In addition, we also demonstrate\nthat the neural network trained using healthy volunteers can be directly\napplied/tested on stroke patients' DWIs data without compromising the lesion\ndetectability. Such a significant reduction in scan time will allow the\ninclusion of DTI into clinical routine for many potential applications.\n",
    "topics": "{'Word Embeddings': 0.4686385}",
    "score": 0.8359254612
  },
  {
    "id": "1807.03923",
    "title": "Generative Adversarial Networks with Decoder-Encoder Output Noise",
    "abstract": "  In recent years, research on image generation methods has been developing\nfast. The auto-encoding variational Bayes method (VAEs) was proposed in 2013,\nwhich uses variational inference to learn a latent space from the image\ndatabase and then generates images using the decoder. The generative\nadversarial networks (GANs) came out as a promising framework, which uses\nadversarial training to improve the generative ability of the generator.\nHowever, the generated pictures by GANs are generally blurry. The deep\nconvolutional generative adversarial networks (DCGANs) were then proposed to\nleverage the quality of generated images. Since the input noise vectors are\nrandomly sampled from a Gaussian distribution, the generator has to map from a\nwhole normal distribution to the images. This makes DCGANs unable to reflect\nthe inherent structure of the training data. In this paper, we propose a novel\ndeep model, called generative adversarial networks with decoder-encoder output\nnoise (DE-GANs), which takes advantage of both the adversarial training and the\nvariational Bayesain inference to improve the performance of image generation.\nDE-GANs use a pre-trained decoder-encoder architecture to map the random\nGaussian noise vectors to informative ones and pass them to the generator of\nthe adversarial networks. Since the decoder-encoder architecture is trained by\nthe same images as the generators, the output vectors could carry the intrinsic\ndistribution information of the original images. Moreover, the loss function of\nDE-GANs is different from GANs and DCGANs. A hidden-space loss function is\nadded to the adversarial loss function to enhance the robustness of the model.\nExtensive empirical results show that DE-GANs can accelerate the convergence of\nthe adversarial training process and improve the quality of the generated\nimages.\n",
    "topics": "{'Image Generation': 0.99999213, 'Variational Inference': 0.51164967}",
    "score": 0.8356319828
  },
  {
    "id": "1911.10608",
    "title": "AnoNet: Weakly Supervised Anomaly Detection in Textured Surfaces",
    "abstract": "  Humans can easily detect a defect (anomaly) because it is different or\nsalient when compared to the surface it resides on. Today, manual human visual\ninspection is still the norm because it is difficult to automate anomaly\ndetection. Neural networks are a useful tool that can teach a machine to find\ndefects. However, they require a lot of training examples to learn what a\ndefect is and it is tedious and expensive to get these samples. We tackle the\nproblem of teaching a network with a low number of training samples with a\nsystem we call AnoNet. AnoNet's architecture is similar to CompactCNN with the\nexceptions that (1) it is a fully convolutional network and does not use\nstrided convolution; (2) it is shallow and compact which minimizes over-fitting\nby design; (3) the compact design constrains the size of intermediate features\nwhich allows training to be done without image downsizing; (4) the model\nfootprint is low making it suitable for edge computation; and (5) the anomaly\ncan be detected and localized despite the weak labelling. AnoNet learns to\ndetect the underlying shape of the anomalies despite the weak annotation as\nwell as preserves the spatial localization of the anomaly. Pre-seeding AnoNet\nwith an engineered filter bank initialization technique reduces the total\nsamples required for training and also achieves state-of-the-art performance.\nCompared to the CompactCNN, AnoNet achieved a massive 94% reduction of network\nparameters from 1.13 million to 64 thousand parameters. Experiments were\nconducted on four data-sets and results were compared against CompactCNN and\nDeepLabv3. AnoNet improved the performance on an average across all data-sets\nby 106% to an F1 score of 0.98 and by 13% to an AUROC value of 0.942. AnoNet\ncan learn from a limited number of images. For one of the data-sets, AnoNet\nlearnt to detect anomalies after a single pass through just 53 training images.\n",
    "topics": "{'Anomaly Detection': 1.0}",
    "score": 0.8354916007
  },
  {
    "id": "2006.10552",
    "title": "XRayGAN: Consistency-preserving Generation of X-ray Images from\n  Radiology Reports",
    "abstract": "  To effectively train medical students to become qualified radiologists, a\nlarge number of X-ray images collected from patients with diverse medical\nconditions are needed. However, due to data privacy concerns, such images are\ntypically difficult to obtain. To address this problem, we develop methods to\ngenerate view-consistent, high-fidelity, and high-resolution X-ray images from\nradiology reports to facilitate radiology training of medical students. This\ntask is presented with several challenges. First, from a single report, images\nwith different views (e.g., frontal, lateral) need to be generated. How to\nensure consistency of these images (i.e., make sure they are about the same\npatient)? Second, X-ray images are required to have high resolution. Otherwise,\nmany details of diseases would be lost. How to generate high-resolutions\nimages? Third, radiology reports are long and have complicated structure. How\nto effectively understand their semantics to generate high-fidelity images that\naccurately reflect the contents of the reports? To address these three\nchallenges, we propose an XRayGAN composed of three modules: (1) a view\nconsistency network that maximizes the consistency between generated\nfrontal-view and lateral-view images; (2) a multi-scale conditional GAN that\nprogressively generates a cascade of images with increasing resolution; (3) a\nhierarchical attentional encoder that learns the latent semantics of a\nradiology report by capturing its hierarchical linguistic structure and various\nlevels of clinical importance of words and sentences. Experiments on two\nradiology datasets demonstrate the effectiveness of our methods. To our best\nknowledge, this work represents the first one generating consistent and\nhigh-resolution X-ray images from radiology reports. The code is available at\nhttps://github.com/UCSD-AI4H/XRayGAN.\n",
    "topics": "{}",
    "score": 0.8354178108
  },
  {
    "id": "2006.15693",
    "title": "Simulation of Brain Resection for Cavity Segmentation Using\n  Self-Supervised and Semi-Supervised Learning",
    "abstract": "  Resective surgery may be curative for drug-resistant focal epilepsy, but only\n40% to 70% of patients achieve seizure freedom after surgery. Retrospective\nquantitative analysis could elucidate patterns in resected structures and\npatient outcomes to improve resective surgery. However, the resection cavity\nmust first be segmented on the postoperative MR image. Convolutional neural\nnetworks (CNNs) are the state-of-the-art image segmentation technique, but\nrequire large amounts of annotated data for training. Annotation of medical\nimages is a time-consuming process requiring highly-trained raters, and often\nsuffering from high inter-rater variability. Self-supervised learning can be\nused to generate training instances from unlabeled data. We developed an\nalgorithm to simulate resections on preoperative MR images. We curated a new\ndataset, EPISURG, comprising 431 postoperative and 269 preoperative MR images\nfrom 431 patients who underwent resective surgery. In addition to EPISURG, we\nused three public datasets comprising 1813 preoperative MR images for training.\nWe trained a 3D CNN on artificially resected images created on the fly during\ntraining, using images from 1) EPISURG, 2) public datasets and 3) both. To\nevaluate trained models, we calculate Dice score (DSC) between model\nsegmentations and 200 manual annotations performed by three human raters. The\nmodel trained on data with manual annotations obtained a median (interquartile\nrange) DSC of 65.3 (30.6). The DSC of our best-performing model, trained with\nno manual annotations, is 81.7 (14.2). For comparison, inter-rater agreement\nbetween human annotators was 84.0 (9.9). We demonstrate a training method for\nCNNs using simulated resection cavities that can accurately segment real\nresection cavities, without manual annotations.\n",
    "topics": "{'Self-Supervised Learning': 0.99990845, 'Semantic Segmentation': 0.872164}",
    "score": 0.8352659852
  },
  {
    "id": "2002.08820",
    "title": "Deep Learning Estimation of Multi-Tissue Constrained Spherical\n  Deconvolution with Limited Single Shell DW-MRI",
    "abstract": "  Diffusion-weighted magnetic resonance imaging (DW-MRI) is the only\nnon-invasive approach for estimation of intra-voxel tissue microarchitecture\nand reconstruction of in vivo neural pathways for the human brain. With\nimprovement in accelerated MRI acquisition technologies, DW-MRI protocols that\nmake use of multiple levels of diffusion sensitization have gained popularity.\nA well-known advanced method for reconstruction of white matter microstructure\nthat uses multi-shell data is multi-tissue constrained spherical deconvolution\n(MT-CSD). MT-CSD substantially improves the resolution of intra-voxel structure\nover the traditional single shell version, constrained spherical deconvolution\n(CSD). Herein, we explore the possibility of using deep learning on single\nshell data (using the b=1000 s/mm2 from the Human Connectome Project (HCP)) to\nestimate the information content captured by 8th order MT-CSD using the full\nthree shell data (b=1000, 2000, and 3000 s/mm2 from HCP). Briefly, we examine\ntwo network architectures: 1.) Sequential network of fully connected dense\nlayers with a residual block in the middle (ResDNN), 2.) Patch based\nconvolutional neural network with a residual block (ResCNN). For both networks\nan additional output block for estimation of voxel fraction was used with a\nmodified loss function. Each approach was compared against the baseline of\nusing MT-CSD on all data on 15 subjects from the HCP divided into 5 training, 2\nvalidation, and 8 testing subjects with a total of 6.7 million voxels. The\nfiber orientation distribution function (fODF) can be recovered with high\ncorrelation (0.77 vs 0.74 and 0.65) as compared to the ground truth of MT-CST,\nwhich was derived from the multi-shell DW-MRI acquisitions. Source code and\nmodels have been made publicly available.\n",
    "topics": "{'Semantic Segmentation': 0.5845032}",
    "score": 0.8351991196
  },
  {
    "id": "1911.05113",
    "title": "Semi-Supervised Multi-Organ Segmentation through Quality Assurance\n  Supervision",
    "abstract": "  Human in-the-loop quality assurance (QA) is typically performed after medical\nimage segmentation to ensure that the systems are performing as intended, as\nwell as identifying and excluding outliers. By performing QA on large-scale,\npreviously unlabeled testing data, categorical QA scores can be generatedIn\nthis paper, we propose a semi-supervised multi-organ segmentation deep neural\nnetwork consisting of a traditional segmentation model generator and a QA\ninvolved discriminator. A large-scale dataset of 2027 volumes are used to train\nthe generator, whose 2-D montage images and segmentation mask with QA scores\nare used to train the discriminator. To generate the QA scores, the 2-D montage\nimages were reviewed manually and coded 0 (success), 1 (errors consistent with\npublished performance), and 2 (gross failure). Then, the ResNet-18 network was\ntrained with 1623 montage images in equal distribution of all three code labels\nand achieved an accuracy 94% for classification predictions with 404 montage\nimages withheld for the test cohort. To assess the performance of using the QA\nsupervision, the discriminator was used as a loss function in a multi-organ\nsegmentation pipeline. The inclusion of QA-loss function boosted performance on\nthe unlabeled test dataset from 714 patients to 951 patients over the baseline\nmodel. Additionally, the number of failures decreased from 606 (29.90%) to 402\n(19.83%). The contributions of the proposed method are threefold: We show that\n(1) the QA scores can be used as a loss function to perform semi-supervised\nlearning for unlabeled data, (2) the well trained discriminator is learnt by QA\nscore rather than traditional true/false, and (3) the performance of\nmulti-organ segmentation on unlabeled datasets can be fine-tuned with more\nrobust and higher accuracy than the original baseline method.\n",
    "topics": "{'Medical Image Segmentation': 0.9999486, 'Semantic Segmentation': 0.9657918}",
    "score": 0.8351933618
  },
  {
    "id": "1808.05896",
    "title": "Whole-Slide Mitosis Detection in H&E Breast Histology Using PHH3 as a\n  Reference to Train Distilled Stain-Invariant Convolutional Networks",
    "abstract": "  Manual counting of mitotic tumor cells in tissue sections constitutes one of\nthe strongest prognostic markers for breast cancer. This procedure, however, is\ntime-consuming and error-prone. We developed a method to automatically detect\nmitotic figures in breast cancer tissue sections based on convolutional neural\nnetworks (CNNs). Application of CNNs to hematoxylin and eosin (H&E) stained\nhistological tissue sections is hampered by: (1) noisy and expensive reference\nstandards established by pathologists, (2) lack of generalization due to\nstaining variation across laboratories, and (3) high computational requirements\nneeded to process gigapixel whole-slide images (WSIs). In this paper, we\npresent a method to train and evaluate CNNs to specifically solve these issues\nin the context of mitosis detection in breast cancer WSIs. First, by combining\nimage analysis of mitotic activity in phosphohistone-H3 (PHH3) restained slides\nand registration, we built a reference standard for mitosis detection in entire\nH&E WSIs requiring minimal manual annotation effort. Second, we designed a data\naugmentation strategy that creates diverse and realistic H&E stain variations\nby modifying the hematoxylin and eosin color channels directly. Using it during\ntraining combined with network ensembling resulted in a stain invariant mitosis\ndetector. Third, we applied knowledge distillation to reduce the computational\nrequirements of the mitosis detection ensemble with a negligible loss of\nperformance. The system was trained in a single-center cohort and evaluated in\nan independent multicenter cohort from The Cancer Genome Atlas on the three\ntasks of the Tumor Proliferation Assessment Challenge (TUPAC). We obtained a\nperformance within the top-3 best methods for most of the tasks of the\nchallenge.\n",
    "topics": "{'whole slide images': 0.9929396, 'Data Augmentation': 0.8452583}",
    "score": 0.8351704953
  },
  {
    "id": "1701.08816",
    "title": "Fully Convolutional Architectures for Multi-Class Segmentation in Chest\n  Radiographs",
    "abstract": "  The success of deep convolutional neural networks on image classification and\nrecognition tasks has led to new applications in very diversified contexts,\nincluding the field of medical imaging. In this paper we investigate and\npropose neural network architectures for automated multi-class segmentation of\nanatomical organs in chest radiographs, namely for lungs, clavicles and heart.\nWe address several open challenges including model overfitting, reducing number\nof parameters and handling of severely imbalanced data in CXR by fusing recent\nconcepts in convolutional networks and adapting them to the segmentation\nproblem task in CXR. We demonstrate that our architecture combining delayed\nsubsampling, exponential linear units, highly restrictive regularization and a\nlarge number of high resolution low level abstract features outperforms\nstate-of-the-art methods on all considered organs, as well as the human\nobserver on lungs and heart. The models use a multi-class configuration with\nthree target classes and are trained and tested on the publicly available JSRT\ndatabase, consisting of 247 X-ray images the ground-truth masks for which are\navailable in the SCR database. Our best performing model, trained with the loss\nfunction based on the Dice coefficient, reached mean Jaccard overlap scores of\n95.0\\% for lungs, 86.8\\% for clavicles and 88.2\\% for heart. This architecture\noutperformed the human observer results for lungs and heart.\n",
    "topics": "{'Image Classification': 0.9777826}",
    "score": 0.8351311332
  },
  {
    "id": "1811.08305",
    "title": "IVD-Net: Intervertebral disc localization and segmentation in MRI with a\n  multi-modal UNet",
    "abstract": "  Accurate localization and segmentation of intervertebral disc (IVD) is\ncrucial for the assessment of spine disease diagnosis. Despite the\ntechnological advances in medical imaging, IVD localization and segmentation\nare still manually performed, which is time-consuming and prone to errors. If,\nin addition, multi-modal imaging is considered, the burden imposed on disease\nassessments increases substantially. In this paper, we propose an architecture\nfor IVD localization and segmentation in multi-modal MRI, which extends the\nwell-known UNet. Compared to single images, multi-modal data brings\ncomplementary information, contributing to better data representation and\ndiscriminative power. Our contributions are three-fold. First, how to\neffectively integrate and fully leverage multi-modal data remains almost\nunexplored. In this work, each MRI modality is processed in a different path to\nbetter exploit their unique information. Second, inspired by HyperDenseNet, the\nnetwork is densely-connected both within each path and across different paths,\ngranting the model the freedom to learn where and how the different modalities\nshould be processed and combined. Third, we improved standard U-Net modules by\nextending inception modules with two dilated convolutions blocks of different\nscale, which helps handling multi-scale context. We report experiments over the\ndata set of the public MICCAI 2018 Challenge on Automatic Intervertebral Disc\nLocalization and Segmentation, with 13 multi-modal MRI images used for training\nand 3 for validation. We trained IVD-Net on an NVidia TITAN XP GPU with 16 GBs\nRAM, using ADAM as optimizer and a learning rate of 10e-5 during 200 epochs.\nTraining took about 5 hours, and segmentation of a whole volume about 2-3\nseconds, on average. Several baselines, with different multi-modal fusion\nstrategies, were used to demonstrate the effectiveness of the proposed\narchitecture.\n",
    "topics": "{'Visual Localization': 0.42827645}",
    "score": 0.8350614575
  },
  {
    "id": "2001.04189",
    "title": "Separating Content from Style Using Adversarial Learning for Recognizing\n  Text in the Wild",
    "abstract": "  We propose to improve text recognition from a new perspective by separating\nthe text content from complex backgrounds. As vanilla GANs are not sufficiently\nrobust to generate sequence-like characters in natural images, we propose an\nadversarial learning framework for the generation and recognition of multiple\ncharacters in an image. The proposed framework consists of an attention-based\nrecognizer and a generative adversarial architecture. Furthermore, to tackle\nthe issue of lacking paired training samples, we design an interactive joint\ntraining scheme, which shares attention masks from the recognizer to the\ndiscriminator, and enables the discriminator to extract the features of each\ncharacter for further adversarial training. Benefiting from the character-level\nadversarial training, our framework requires only unpaired simple data for\nstyle supervision. Each target style sample containing only one randomly chosen\ncharacter can be simply synthesized online during the training. This is\nsignificant as the training does not require costly paired samples or\ncharacter-level annotations. Thus, only the input images and corresponding text\nlabels are needed. In addition to the style normalization of the backgrounds,\nwe refine character patterns to ease the recognition task. A feedback mechanism\nis proposed to bridge the gap between the discriminator and the recognizer.\nTherefore, the discriminator can guide the generator according to the confusion\nof the recognizer, so that the generated patterns are clearer for recognition.\nExperiments on various benchmarks, including both regular and irregular text,\ndemonstrate that our method significantly reduces the difficulty of\nrecognition. Our framework can be integrated into recent recognition methods to\nachieve new state-of-the-art recognition accuracy.\n",
    "topics": "{'Style Transfer': 0.9854534}",
    "score": 0.8349538278
  },
  {
    "id": "1706.09569",
    "title": "Recurrent neural networks with specialized word embeddings for\n  health-domain named-entity recognition",
    "abstract": "  Background. Previous state-of-the-art systems on Drug Name Recognition (DNR)\nand Clinical Concept Extraction (CCE) have focused on a combination of text\n\"feature engineering\" and conventional machine learning algorithms such as\nconditional random fields and support vector machines. However, developing good\nfeatures is inherently heavily time-consuming. Conversely, more modern machine\nlearning approaches such as recurrent neural networks (RNNs) have proved\ncapable of automatically learning effective features from either random\nassignments or automated word \"embeddings\". Objectives. (i) To create a highly\naccurate DNR and CCE system that avoids conventional, time-consuming feature\nengineering. (ii) To create richer, more specialized word embeddings by using\nhealth domain datasets such as MIMIC-III. (iii) To evaluate our systems over\nthree contemporary datasets. Methods. Two deep learning methods, namely the\nBidirectional LSTM and the Bidirectional LSTM-CRF, are evaluated. A CRF model\nis set as the baseline to compare the deep learning systems to a traditional\nmachine learning approach. The same features are used for all the models.\nResults. We have obtained the best results with the Bidirectional LSTM-CRF\nmodel, which has outperformed all previously proposed systems. The specialized\nembeddings have helped to cover unusual words in DDI-DrugBank and DDI-MedLine,\nbut not in the 2010 i2b2/VA IRB Revision dataset. Conclusion. We present a\nstate-of-the-art system for DNR and CCE. Automated word embeddings has allowed\nus to avoid costly feature engineering and achieve higher accuracy.\nNevertheless, the embeddings need to be retrained over datasets that are\nadequate for the domain, in order to adequately cover the domain-specific\nvocabulary.\n",
    "topics": "{'Feature Engineering': 1.0, 'Word Embeddings': 0.9999876, 'Sentence Classification': 0.3658979}",
    "score": 0.8348743002
  },
  {
    "id": "1802.05914",
    "title": "3D Regression Neural Network for the Quantification of Enlarged\n  Perivascular Spaces in Brain MRI",
    "abstract": "  Enlarged perivascular spaces (EPVS) in the brain are an emerging imaging\nmarker for cerebral small vessel disease, and have been shown to be related to\nincreased risk of various neurological diseases, including stroke and dementia.\nAutomatic quantification of EPVS would greatly help to advance research into\nits etiology and its potential as a risk indicator of disease. We propose a\nconvolutional network regression method to quantify the extent of EPVS in the\nbasal ganglia from 3D brain MRI. We first segment the basal ganglia and\nsubsequently apply a 3D convolutional regression network designed for small\nobject detection within this region of interest. The network takes an image as\ninput, and outputs a quantification score of EPVS. The network has\nsignificantly more convolution operations than pooling ones and no final\nactivation, allowing it to span the space of real numbers. We validated our\napproach using a dataset of 2000 brain MRI scans scored visually. Experiments\nwith varying sizes of training and test sets showed that a good performance can\nbe achieved with a training set of only 200 scans. With a training set of 1000\nscans, the intraclass correlation coefficient (ICC) between our scoring method\nand the expert's visual score was 0.74. Our method outperforms by a large\nmargin - more than 0.10 - four more conventional automated approaches based on\nintensities, scale-invariant feature transform, and random forest. We show that\nthe network learns the structures of interest and investigate the influence of\nhyper-parameters on the performance. We also evaluate the reproducibility of\nour network using a set of 60 subjects scanned twice (scan-rescan\nreproducibility). On this set our network achieves an ICC of 0.93, while the\nintrarater agreement reaches 0.80. Furthermore, the automatic EPVS scoring\ncorrelates similarly to age as visual scoring.\n",
    "topics": "{'Object Detection': 0.7829909, 'Brain Segmentation': 0.46285897}",
    "score": 0.8348717538
  },
  {
    "id": "1803.00127",
    "title": "SalientDSO: Bringing Attention to Direct Sparse Odometry",
    "abstract": "  Although cluttered indoor scenes have a lot of useful high-level semantic\ninformation which can be used for mapping and localization, most Visual\nOdometry (VO) algorithms rely on the usage of geometric features such as\npoints, lines and planes. Lately, driven by this idea, the joint optimization\nof semantic labels and obtaining odometry has gained popularity in the robotics\ncommunity. The joint optimization is good for accurate results but is generally\nvery slow. At the same time, in the vision community, direct and sparse\napproaches for VO have stricken the right balance between speed and accuracy.\n  We merge the successes of these two communities and present a way to\nincorporate semantic information in the form of visual saliency to Direct\nSparse Odometry - a highly successful direct sparse VO algorithm. We also\npresent a framework to filter the visual saliency based on scene parsing. Our\nframework, SalientDSO, relies on the widely successful deep learning based\napproaches for visual saliency and scene parsing which drives the feature\nselection for obtaining highly-accurate and robust VO even in the presence of\nas few as 40 point features per frame. We provide extensive quantitative\nevaluation of SalientDSO on the ICL-NUIM and TUM monoVO datasets and show that\nwe outperform DSO and ORB-SLAM - two very popular state-of-the-art approaches\nin the literature. We also collect and publicly release a CVL-UMD dataset which\ncontains two indoor cluttered sequences on which we show qualitative\nevaluations. To our knowledge this is the first paper to use visual saliency\nand scene parsing to drive the feature selection in direct VO.\n",
    "topics": "{'Visual Odometry': 0.9999999, 'Scene Parsing': 0.9995103, 'Feature Selection': 0.99889934}",
    "score": 0.8347689117
  },
  {
    "id": "2006.00422",
    "title": "EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary\n  Neuromorphic Vision Sensors",
    "abstract": "  In this paper, we present a hybrid event-frame approach for detecting and\ntracking objects recorded by a stationary neuromorphic vision sensor (NVS) used\nin the application of traffic monitoring with a hardware efficient processing\npipeline that optimizes memory and computational needs. The usage of NVS gives\nthe advantage of rejecting background while it has a unique disadvantage of\nfragmented objects due to lack of events generated by smooth areas such as\nglass windows. To exploit the background removal, we propose an event based\nbinary image (EBBI) creation that signals presence or absence of events in a\nframe duration. This reduces memory requirement and enables usage of simple\nalgorithms like median filtering and connected component labeling (CCL) for\ndenoise and region proposal (RP) respectively. To overcome the fragmentation\nissue, a YOLO inspired neural network based detector and classifier (NNDC) to\nmerge fragmented region proposals has been proposed. Finally, a simplified\nversion of Kalman filter, termed overlap based tracker (OT), exploiting overlap\nbetween detections and tracks is proposed with heuristics to overcome\nocclusion.\n  The proposed pipeline is evaluated using more than 5 hours of traffic\nrecordings. Our proposed hybrid architecture outperformed (AUC = $0.45$) Deep\nlearning (DL) based tracker SiamMask (AUC = $0.33$) operating on simultaneously\nrecorded RGB frames while requiring $2200\\times$ less computations. Compared to\npure event based mean shift (AUC = $0.31$), our approach requires $68\\times$\nmore computations but provides much better performance. Finally, we also\nevaluated our performance on two different NVS: DAVIS and CeleX and\ndemonstrated similar gains. To the best of our knowledge, this is the first\nreport where an NVS based solution is directly compared to other simultaneously\nrecorded frame based method and shows tremendous promise.\n",
    "topics": "{'Region Proposal': 0.99998343}",
    "score": 0.8347685816
  },
  {
    "id": "1610.00731",
    "title": "Can Ground Truth Label Propagation from Video help Semantic\n  Segmentation?",
    "abstract": "  For state-of-the-art semantic segmentation task, training convolutional\nneural networks (CNNs) requires dense pixelwise ground truth (GT) labeling,\nwhich is expensive and involves extensive human effort. In this work, we study\nthe possibility of using auxiliary ground truth, so-called \\textit{pseudo\nground truth} (PGT) to improve the performance. The PGT is obtained by\npropagating the labels of a GT frame to its subsequent frames in the video\nusing a simple CRF-based, cue integration framework. Our main contribution is\nto demonstrate the use of noisy PGT along with GT to improve the performance of\na CNN. We perform a systematic analysis to find the right kind of PGT that\nneeds to be added along with the GT for training a CNN. In this regard, we\nexplore three aspects of PGT which influence the learning of a CNN: i) the PGT\nlabeling has to be of good quality; ii) the PGT images have to be different\ncompared to the GT images; iii) the PGT has to be trusted differently than GT.\nWe conclude that PGT which is diverse from GT images and has good quality of\nlabeling can indeed help improve the performance of a CNN. Also, when PGT is\nmultiple folds larger than GT, weighing down the trust on PGT helps in\nimproving the accuracy. Finally, We show that using PGT along with GT, the\nperformance of Fully Convolutional Network (FCN) on Camvid data is increased by\n$2.7\\%$ on IoU accuracy. We believe such an approach can be used to train CNNs\nfor semantic video segmentation where sequentially labeled image frames are\nneeded. To this end, we provide recommendations for using PGT strategically for\nsemantic segmentation and hence bypass the need for extensive human efforts in\nlabeling.\n",
    "topics": "{'Semantic Segmentation': 0.9999802, 'Video Segmentation': 0.97937703, 'Video Semantic Segmentation': 0.8294316}",
    "score": 0.8345048741
  },
  {
    "id": "2008.12413",
    "title": "W-Net: Dense Semantic Segmentation of Subcutaneous Tissue in Ultrasound\n  Images by Expanding U-Net to Incorporate Ultrasound RF Waveform Data",
    "abstract": "  We present W-Net, a novel Convolution Neural Network (CNN) framework that\nemploys raw ultrasound waveforms from each A-scan, typically referred to as\nultrasound Radio Frequency (RF) data, in addition to the gray ultrasound image\nto semantically segment and label tissues. Unlike prior work, we seek to label\nevery pixel in the image, without the use of a background class. To the best of\nour knowledge, this is also the first deep-learning or CNN approach for\nsegmentation that analyses ultrasound raw RF data along with the gray image.\nInternational patent(s) pending [PCT/US20/37519]. We chose subcutaneous tissue\n(SubQ) segmentation as our initial clinical goal since it has diverse\nintermixed tissues, is challenging to segment, and is an underrepresented\nresearch area. SubQ potential applications include plastic surgery, adipose\nstem-cell harvesting, lymphatic monitoring, and possibly detection/treatment of\ncertain types of tumors. A custom dataset consisting of hand-labeled images by\nan expert clinician and trainees are used for the experimentation, currently\nlabeled into the following categories: skin, fat, fat fascia/stroma, muscle and\nmuscle fascia. We compared our results with U-Net and Attention U-Net. Our\nnovel \\emph{W-Net}'s RF-Waveform input and architecture increased mIoU accuracy\n(averaged across all tissue classes) by 4.5\\% and 4.9\\% compared to regular\nU-Net and Attention U-Net, respectively. We present analysis as to why the\nMuscle fascia and Fat fascia/stroma are the most difficult tissues to label.\nMuscle fascia in particular, the most difficult anatomic class to recognize for\nboth humans and AI algorithms, saw mIoU improvements of 13\\% and 16\\% from our\nW-Net vs U-Net and Attention U-Net respectively.\n",
    "topics": "{'Semantic Segmentation': 0.96142656}",
    "score": 0.8344544465
  },
  {
    "id": "1606.07239",
    "title": "Non Local Spatial and Angular Matching : Enabling higher spatial\n  resolution diffusion MRI datasets through adaptive denoising",
    "abstract": "  Diffusion magnetic resonance imaging datasets suffer from low Signal-to-Noise\nRatio, especially at high b-values. Acquiring data at high b-values contains\nrelevant information and is now of great interest for microstructural and\nconnectomics studies. High noise levels bias the measurements due to the\nnon-Gaussian nature of the noise, which in turn can lead to a false and biased\nestimation of the diffusion parameters. Additionally, the usage of in-plane\nacceleration techniques during the acquisition leads to a spatially varying\nnoise distribution, which depends on the parallel acceleration method\nimplemented on the scanner. This paper proposes a novel diffusion MRI denoising\ntechnique that can be used on all existing data, without adding to the scanning\ntime. We first apply a statistical framework to convert the noise to Gaussian\ndistributed noise, effectively removing the bias. We then introduce a spatially\nand angular adaptive denoising technique, the Non Local Spatial and Angular\nMatching (NLSAM) algorithm. Each volume is first decomposed in small 4D\noverlapping patches to capture the structure of the diffusion data and a\ndictionary of atoms is learned on those patches. A local sparse decomposition\nis then found by bounding the reconstruction error with the local noise\nvariance. We compare against three other state-of-the-art denoising methods and\nshow quantitative local and connectivity results on a synthetic phantom and on\nan in-vivo high resolution dataset. Overall, our method restores perceptual\ninformation, removes the noise bias in common diffusion metrics, restores the\nextracted peaks coherence and improves reproducibility of tractography. Our\nwork paves the way for higher spatial resolution acquisition of diffusion MRI\ndatasets, which could in turn reveal new anatomical details that are not\ndiscernible at the spatial resolution currently used by the diffusion MRI\ncommunity.\n",
    "topics": "{'Denoising': 0.99999964}",
    "score": 0.8344438102
  },
  {
    "id": "1901.08954",
    "title": "Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder\n  Anomaly Detection",
    "abstract": "  Despite inherent ill-definition, anomaly detection is a research endeavor of\ngreat interest within machine learning and visual scene understanding alike.\nMost commonly, anomaly detection is considered as the detection of outliers\nwithin a given data distribution based on some measure of normality. The most\nsignificant challenge in real-world anomaly detection problems is that\navailable data is highly imbalanced towards normality (i.e. non-anomalous) and\ncontains a most a subset of all possible anomalous samples - hence limiting the\nuse of well-established supervised learning methods. By contrast, we introduce\nan unsupervised anomaly detection model, trained only on the normal\n(non-anomalous, plentiful) samples in order to learn the normality distribution\nof the domain and hence detect abnormality based on deviation from this model.\nOur proposed approach employs an encoder-decoder convolutional neural network\nwith skip connections to thoroughly capture the multi-scale distribution of the\nnormal data distribution in high-dimensional image space. Furthermore,\nutilizing an adversarial training scheme for this chosen architecture provides\nsuperior reconstruction both within high-dimensional image space and a\nlower-dimensional latent vector space encoding. Minimizing the reconstruction\nerror metric within both the image and hidden vector spaces during training\naids the model to learn the distribution of normality as required. Higher\nreconstruction metrics during subsequent test and deployment are thus\nindicative of a deviation from this normal distribution, hence indicative of an\nanomaly. Experimentation over established anomaly detection benchmarks and\nchallenging real-world datasets, within the context of X-ray security\nscreening, shows the unique promise of such a proposed approach.\n",
    "topics": "{'Anomaly Detection': 1.0, 'Scene Understanding': 0.99999654}",
    "score": 0.8343769537
  },
  {
    "id": "1803.11151",
    "title": "Fine-Grained Energy and Performance Profiling framework for Deep\n  Convolutional Neural Networks",
    "abstract": "  There is a huge demand for on-device execution of deep learning algorithms on\nmobile and embedded platforms. These devices present constraints on the\napplication due to limited resources and power. Hence, developing\nenergy-efficient solutions to address this issue will require innovation in\nalgorithmic design, software and hardware. Such innovation requires\nbenchmarking and characterization of Deep Neural Networks based on performance\nand energy-consumption alongside accuracy. However, current benchmarks studies\nin existing deep learning frameworks (for example, Caffe, Tensorflow, Torch and\nothers) are based on performance of these applications on high-end CPUs and\nGPUs. In this work, we introduce a benchmarking framework called \"SyNERGY\" to\nmeasure the energy and time of 11 representative Deep Convolutional Neural\nNetworks on embedded platforms such as NVidia Jetson TX1. We integrate ARM's\nStreamline Performance Analyser with standard deep learning frameworks such as\nCaffe and CuDNNv5, to study the execution behaviour of current deep learning\nmodels at a fine-grained level (or specific layers) on image processing tasks.\nIn addition, we build an initial multi-variable linear regression model to\npredict energy consumption of unseen neural network models based on the number\nof SIMD instructions executed and main memory accesses of the CPU cores of the\nTX1 with an average relative test error rate of 8.04 +/- 5.96 %. Surprisingly,\nwe find that it is possible to refine the model to predict the number of SIMD\ninstructions and main memory accesses solely from the application's\nMultiply-Accumulate (MAC) counts, hence, eliminating the need for actual\nmeasurements. Our predicted results demonstrate 7.08 +/- 6.0 % average relative\nerror over actual energy measurements of all 11 networks tested, except\nMobileNet. By including MobileNet the average relative test error increases to\n17.33 +/- 12.2 %.\n",
    "topics": "{}",
    "score": 0.8342041383
  },
  {
    "id": "1909.04169",
    "title": "OncoNetExplainer: Explainable Predictions of Cancer Types Based on Gene\n  Expression Data",
    "abstract": "  The discovery of important biomarkers is a significant step towards\nunderstanding the molecular mechanisms of carcinogenesis; enabling accurate\ndiagnosis for, and prognosis of, a certain cancer type. Before recommending any\ndiagnosis, genomics data such as gene expressions(GE) and clinical outcomes\nneed to be analyzed. However, complex nature, high dimensionality, and\nheterogeneity in genomics data make the overall analysis challenging.\nConvolutional neural networks(CNN) have shown tremendous success in solving\nsuch problems. However, neural network models are perceived mostly as `black\nbox' methods because of their not well-understood internal functioning.\nHowever, interpretability is important to provide insights on why a given\ncancer case has a certain type. Besides, finding the most important biomarkers\ncan help in recommending more accurate treatments and drug repositioning. In\nthis paper, we propose a new approach called OncoNetExplainer to make\nexplainable predictions of cancer types based on GE data. We used genomics data\nabout 9,074 cancer patients covering 33 different cancer types from the\nPan-Cancer Atlas on which we trained CNN and VGG16 networks using\nguided-gradient class activation maps++(GradCAM++). Further, we generate\nclass-specific heat maps to identify significant biomarkers and computed\nfeature importance in terms of mean absolute impact to rank top genes across\nall the cancer types. Quantitative and qualitative analyses show that both\nmodels exhibit high confidence at predicting the cancer types correctly giving\nan average precision of 96.25%. To provide comparisons with the baselines, we\nidentified top genes, and cancer-specific driver genes using gradient boosted\ntrees and SHapley Additive exPlanations(SHAP). Finally, our findings were\nvalidated with the annotations provided by the TumorPortal.\n",
    "topics": "{'Feature Importance': 0.94211096, 'Language Modelling': 0.48248342}",
    "score": 0.8341712211
  },
  {
    "id": "1811.01533",
    "title": "Transfer learning for time series classification",
    "abstract": "  Transfer learning for deep neural networks is the process of first training a\nbase network on a source dataset, and then transferring the learned features\n(the network's weights) to a second network to be trained on a target dataset.\nThis idea has been shown to improve deep neural network's generalization\ncapabilities in many computer vision tasks such as image recognition and object\nlocalization. Apart from these applications, deep Convolutional Neural Networks\n(CNNs) have also recently gained popularity in the Time Series Classification\n(TSC) community. However, unlike for image recognition problems, transfer\nlearning techniques have not yet been investigated thoroughly for the TSC task.\nThis is surprising as the accuracy of deep learning models for TSC could\npotentially be improved if the model is fine-tuned from a pre-trained neural\nnetwork instead of training it from scratch. In this paper, we fill this gap by\ninvestigating how to transfer deep CNNs for the TSC task. To evaluate the\npotential of transfer learning, we performed extensive experiments using the\nUCR archive which is the largest publicly available TSC benchmark containing 85\ndatasets. For each dataset in the archive, we pre-trained a model and then\nfine-tuned it on the other datasets resulting in 7140 different deep neural\nnetworks. These experiments revealed that transfer learning can improve or\ndegrade the model's predictions depending on the dataset used for transfer.\nTherefore, in an effort to predict the best source dataset for a given target\ndataset, we propose a new method relying on Dynamic Time Warping to measure\ninter-datasets similarities. We describe how our method can guide the transfer\nto choose the best source dataset leading to an improvement in accuracy on 71\nout of 85 datasets.\n",
    "topics": "{'Transfer Learning': 1.0, 'Time Series Classification': 1.0, 'Object Localization': 1.0, 'Time Series': 0.66107833}",
    "score": 0.834093017
  },
  {
    "id": "1610.07940",
    "title": "End-to-end Learning of Deep Visual Representations for Image Retrieval",
    "abstract": "  While deep learning has become a key ingredient in the top performing methods\nfor many computer vision tasks, it has failed so far to bring similar\nimprovements to instance-level image retrieval. In this article, we argue that\nreasons for the underwhelming results of deep methods on image retrieval are\nthreefold: i) noisy training data, ii) inappropriate deep architecture, and\niii) suboptimal training procedure. We address all three issues.\n  First, we leverage a large-scale but noisy landmark dataset and develop an\nautomatic cleaning method that produces a suitable training set for deep\nretrieval. Second, we build on the recent R-MAC descriptor, show that it can be\ninterpreted as a deep and differentiable architecture, and present improvements\nto enhance it. Last, we train this network with a siamese architecture that\ncombines three streams with a triplet loss. At the end of the training process,\nthe proposed architecture produces a global image representation in a single\nforward pass that is well suited for image retrieval. Extensive experiments\nshow that our approach significantly outperforms previous retrieval approaches,\nincluding state-of-the-art methods based on costly local descriptor indexing\nand spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively\nreport 94.7, 96.6, and 94.8 mean average precision. Our representations can\nalso be heavily compressed using product quantization with little loss in\naccuracy. For additional material, please see\nwww.xrce.xerox.com/Deep-Image-Retrieval.\n",
    "topics": "{'Image Retrieval': 1.0, 'Quantization': 0.81694686}",
    "score": 0.8340337504
  },
  {
    "id": "1506.02328",
    "title": "EventNet: A Large Scale Structured Concept Library for Complex Event\n  Detection in Video",
    "abstract": "  Event-specific concepts are the semantic concepts designed for the events of\ninterest, which can be used as a mid-level representation of complex events in\nvideos. Existing methods only focus on defining event-specific concepts for a\nsmall number of predefined events, but cannot handle novel unseen events. This\nmotivates us to build a large scale event-specific concept library that covers\nas many real-world events and their concepts as possible. Specifically, we\nchoose WikiHow, an online forum containing a large number of how-to articles on\nhuman daily life events. We perform a coarse-to-fine event discovery process\nand discover 500 events from WikiHow articles. Then we use each event name as\nquery to search YouTube and discover event-specific concepts from the tags of\nreturned videos. After an automatic filter process, we end up with 95,321\nvideos and 4,490 concepts. We train a Convolutional Neural Network (CNN) model\non the 95,321 videos over the 500 events, and use the model to extract deep\nlearning feature from video content. With the learned deep learning feature, we\ntrain 4,490 binary SVM classifiers as the event-specific concept library. The\nconcepts and events are further organized in a hierarchical structure defined\nby WikiHow, and the resultant concept library is called EventNet. Finally, the\nEventNet concept library is used to generate concept based representation of\nevent videos. To the best of our knowledge, EventNet represents the first video\nevent ontology that organizes events and their concepts into a semantic\nstructure. It offers great potential for event retrieval and browsing.\nExtensive experiments over the zero-shot event retrieval task when no training\nsamples are available show that the EventNet concept library consistently and\nsignificantly outperforms the state-of-the-art (such as the 20K ImageNet\nconcepts trained with CNN) by a large margin up to 207%.\n",
    "topics": "{}",
    "score": 0.8340263898
  },
  {
    "id": "1707.01976",
    "title": "A Generalised Seizure Prediction with Convolutional Neural Networks for\n  Intracranial and Scalp Electroencephalogram Data Analysis",
    "abstract": "  Seizure prediction has attracted a growing attention as one of the most\nchallenging predictive data analysis efforts in order to improve the life of\npatients living with drug-resistant epilepsy and tonic seizures. Many\noutstanding works have been reporting great results in providing a sensible\nindirect (warning systems) or direct (interactive neural-stimulation) control\nover refractory seizures, some of which achieved high performance. However,\nmany works put heavily handcraft feature extraction and/or carefully tailored\nfeature engineering to each patient to achieve very high sensitivity and low\nfalse prediction rate for a particular dataset. This limits the benefit of\ntheir approaches if a different dataset is used. In this paper we apply\nConvolutional Neural Networks (CNNs) on different intracranial and scalp\nelectroencephalogram (EEG) datasets and proposed a generalized retrospective\nand patient-specific seizure prediction method. We use Short-Time Fourier\nTransform (STFT) on 30-second EEG windows with 50% overlapping to extract\ninformation in both frequency and time domains. A standardization step is then\napplied on STFT components across the whole frequency range to prevent high\nfrequencies features being influenced by those at lower frequencies. A\nconvolutional neural network model is used for both feature extraction and\nclassification to separate preictal segments from interictal ones. The proposed\napproach achieves sensitivity of 81.4%, 81.2%, 82.3% and false prediction rate\n(FPR) of 0.06/h, 0.16/h, 0.22/h on Freiburg Hospital intracranial EEG (iEEG)\ndataset, Children's Hospital of Boston-MIT scalp EEG (sEEG) dataset, and Kaggle\nAmerican Epilepsy Society Seizure Prediction Challenge's dataset, respectively.\nOur prediction method is also statistically better than an unspecific random\npredictor for most of patients in all three datasets.\n",
    "topics": "{'EEG': 1.0, 'Feature Engineering': 0.9998317}",
    "score": 0.8339244803
  },
  {
    "id": "1908.00758",
    "title": "Clustering Wi-Fi Fingerprints for Indoor-Outdoor Detection",
    "abstract": "  This paper presents a method for continuous indoor-outdoor environment\ndetection on mobile devices based solely on WiFi fingerprints. Detection of\nindoor outdoor switching is an important part of identifying a user's context,\nand it provides important information for upper layer context aware mobile\napplications such as recommender systems, navigation tools, etc. Moreover,\nfuture indoor positioning systems are likely to use Wi-Fi fingerprints, and\ntherefore Wi-Fi receivers will be on most of the time. In contrast to existing\nresearch, we believe that these fingerprints should be leveraged, and they\nserve as the basis of the proposed method. Using various machine learning\nalgorithms, we train a supervised classifier based on features extracted from\nthe raw fingerprints, clusters, and cluster transition graph. The contribution\nof each of the features to the method is assessed. Our method assumes no prior\nknowledge of the environment, and a training set consisting of the data\ncollected for just a few hours on a single device is sufficient in order to\nprovide indoor-outdoor classification, even in an unknown location or when\nusing new devices. We evaluate our method in an experiment involving 12\nparticipants during their daily routine, with a total of 828 hours' worth of\ndata collected by the participants. We report a predictive performance of the\nAUC (area under the curve) of 0.94 using the gradient boosting machine ensemble\nlearning method. We show that our method can be used for other context\ndetection tasks such as learning and recognizing a given building or room.\n",
    "topics": "{'Recommendation Systems': 0.49701127}",
    "score": 0.8337102255
  },
  {
    "id": "2001.01330",
    "title": "Convolutional Neural Networks with Intermediate Loss for 3D\n  Super-Resolution of CT and MRI Scans",
    "abstract": "  CT scanners that are commonly-used in hospitals nowadays produce\nlow-resolution images, up to 512 pixels in size. One pixel in the image\ncorresponds to a one millimeter piece of tissue. In order to accurately segment\ntumors and make treatment plans, doctors need CT scans of higher resolution.\nThe same problem appears in MRI. In this paper, we propose an approach for the\nsingle-image super-resolution of 3D CT or MRI scans. Our method is based on\ndeep convolutional neural networks (CNNs) composed of 10 convolutional layers\nand an intermediate upscaling layer that is placed after the first 6\nconvolutional layers. Our first CNN, which increases the resolution on two axes\n(width and height), is followed by a second CNN, which increases the resolution\non the third axis (depth). Different from other methods, we compute the loss\nwith respect to the ground-truth high-resolution output right after the\nupscaling layer, in addition to computing the loss after the last convolutional\nlayer. The intermediate loss forces our network to produce a better output,\ncloser to the ground-truth. A widely-used approach to obtain sharp results is\nto add Gaussian blur using a fixed standard deviation. In order to avoid\noverfitting to a fixed standard deviation, we apply Gaussian smoothing with\nvarious standard deviations, unlike other approaches. We evaluate our method in\nthe context of 2D and 3D super-resolution of CT and MRI scans from two\ndatabases, comparing it to relevant related works from the literature and\nbaselines based on various interpolation schemes, using 2x and 4x scaling\nfactors. The empirical results show that our approach attains superior results\nto all other methods. Moreover, our human annotation study reveals that both\ndoctors and regular annotators chose our method in favor of Lanczos\ninterpolation in 97.55% cases for 2x upscaling factor and in 96.69% cases for\n4x upscaling factor.\n",
    "topics": "{'Super-Resolution': 0.99999356, 'Super Resolution': 0.99998164, 'Image Super-Resolution': 0.99975413}",
    "score": 0.8336886047
  },
  {
    "id": "1911.03558",
    "title": "Joint Demosaicing and Super-Resolution (JDSR): Network Design and\n  Perceptual Optimization",
    "abstract": "  Image demosaicing and super-resolution are two important tasks in color\nimaging pipeline. So far they have been mostly independently studied in the\nopen literature of deep learning; little is known about the potential benefit\nof formulating a joint demosaicing and super-resolution (JDSR) problem. In this\npaper, we propose an end-to-end optimization solution to the JDSR problem and\ndemonstrate its practical significance in computational imaging. Our technical\ncontributions are mainly two-fold. On network design, we have developed a\nResidual-Dense Squeeze-and-Excitation Networks (RDSEN) supported by a\npre-demosaicing network (PDNet) as the pre-processing step. We address the\nissue of spatio-spectral attention for color-filter-array (CFA) data and\ndiscuss how to achieve better information flow by concatenating Residue-Dense\nSqueeze-and-Excitation Blocks (RDSEBs) for JDSR. Experimental results have\nshown that significant PSNR/SSIM gain can be achieved by RDSEN over previous\nnetwork architectures including state-of-the-art RCAN. On perceptual\noptimization, we propose to leverage the latest ideas including relativistic\ndiscriminator and pre-excitation perceptual loss function to further improve\nthe visual quality of textured regions in reconstructed images. Our extensive\nexperiment results have shown that Texture-enhanced Relativistic average\nGenerative Adversarial Network (TRaGAN) can produce both subjectively more\npleasant images and objectively lower perceptual distortion scores than\nstandard GAN for JDSR. Finally, we have verified the benefit of JDSR to\nhigh-quality image reconstruction from real-world Bayer pattern data collected\nby NASA Mars Curiosity.\n",
    "topics": "{'Super Resolution': 0.99999905, 'Super-Resolution': 0.9999974, 'Image Reconstruction': 0.9999919, 'Demosaicking': 0.99954766, 'SSIM': 0.99911255}",
    "score": 0.8336516564
  },
  {
    "id": "1808.02455",
    "title": "Data augmentation using synthetic data for time series classification\n  with deep residual networks",
    "abstract": "  Data augmentation in deep neural networks is the process of generating\nartificial data in order to reduce the variance of the classifier with the goal\nto reduce the number of errors. This idea has been shown to improve deep neural\nnetwork's generalization capabilities in many computer vision tasks such as\nimage recognition and object localization. Apart from these applications, deep\nConvolutional Neural Networks (CNNs) have also recently gained popularity in\nthe Time Series Classification (TSC) community. However, unlike in image\nrecognition problems, data augmentation techniques have not yet been\ninvestigated thoroughly for the TSC task. This is surprising as the accuracy of\ndeep learning models for TSC could potentially be improved, especially for\nsmall datasets that exhibit overfitting, when a data augmentation method is\nadopted. In this paper, we fill this gap by investigating the application of a\nrecently proposed data augmentation technique based on the Dynamic Time Warping\ndistance, for a deep learning model for TSC. To evaluate the potential of\naugmenting the training set, we performed extensive experiments using the UCR\nTSC benchmark. Our preliminary experiments reveal that data augmentation can\ndrastically increase deep CNN's accuracy on some datasets and significantly\nimprove the deep model's accuracy when the method is used in an ensemble\napproach.\n",
    "topics": "{'Time Series Classification': 1.0, 'Data Augmentation': 1.0, 'Object Localization': 0.9999999, 'Time Series': 0.38866994}",
    "score": 0.8336507553
  },
  {
    "id": "1907.01162",
    "title": "Sample Adaptive Multiple Kernel Learning for Failure Prediction of\n  Railway Points",
    "abstract": "  Railway points are among the key components of railway infrastructure. As a\npart of signal equipment, points control the routes of trains at railway\njunctions, having a significant impact on the reliability, capacity, and\npunctuality of rail transport. Traditionally, maintenance of points is based on\na fixed time interval or raised after the equipment failures. Instead, it would\nbe of great value if we could forecast points' failures and take action\nbeforehand, minimising any negative effect. To date, most of the existing\nprediction methods are either lab-based or relying on specially installed\nsensors which makes them infeasible for large-scale implementation. Besides,\nthey often use data from only one source. We, therefore, explore a new way that\nintegrates multi-source data which are ready to hand to fulfil this task. We\nconducted our case study based on Sydney Trains rail network which is an\nextensive network of passenger and freight railways. Unfortunately, the\nreal-world data are usually incomplete due to various reasons, e.g., faults in\nthe database, operational errors or transmission faults. Besides, railway\npoints differ in their locations, types and some other properties, which means\nit is hard to use a unified model to predict their failures. Aiming at this\nchallenging task, we firstly constructed a dataset from multiple sources and\nselected key features with the help of domain experts. In this paper, we\nformulate our prediction task as a multiple kernel learning problem with\nmissing kernels. We present a robust multiple kernel learning algorithm for\npredicting points failures. Our model takes into account the missing pattern of\ndata as well as the inherent variance on different sets of railway points.\nExtensive experiments demonstrate the superiority of our algorithm compared\nwith other state-of-the-art methods.\n",
    "topics": "{}",
    "score": 0.8335046259
  },
  {
    "id": "1809.00665",
    "title": "Context-Patch Face Hallucination Based on Thresholding\n  Locality-constrained Representation and Reproducing Learning",
    "abstract": "  Face hallucination is a technique that reconstruct high-resolution (HR) faces\nfrom low-resolution (LR) faces, by using the prior knowledge learned from HR/LR\nface pairs. Most state-of-the-arts leverage position-patch prior knowledge of\nhuman face to estimate the optimal representation coefficients for each image\npatch. However, they focus only the position information and usually ignore the\ncontext information of image patch. In addition, when they are confronted with\nmisalignment or the Small Sample Size (SSS) problem, the hallucination\nperformance is very poor. To this end, this study incorporates the contextual\ninformation of image patch and proposes a powerful and efficient context-patch\nbased face hallucination approach, namely Thresholding Locality-constrained\nRepresentation and Reproducing learning (TLcR-RL). Under the context-patch\nbased framework, we advance a thresholding based representation method to\nenhance the reconstruction accuracy and reduce the computational complexity. To\nfurther improve the performance of the proposed algorithm, we propose a\npromotion strategy called reproducing learning. By adding the estimated HR face\nto the training set, which can simulates the case that the HR version of the\ninput LR face is present in the training set, thus iteratively enhancing the\nfinal hallucination result. Experiments demonstrate that the proposed TLcR-RL\nmethod achieves a substantial increase in the hallucinated results, both\nsubjectively and objectively. Additionally, the proposed framework is more\nrobust to face misalignment and the SSS problem, and its hallucinated HR face\nis still very good when the LR test face is from the real-world. The MATLAB\nsource code is available at https://github.com/junjun-jiang/TLcR-RL\n",
    "topics": "{'3D Face Reconstruction': 0.3583996}",
    "score": 0.8334916517
  },
  {
    "id": "1606.03788",
    "title": "Unsupervised Non Linear Dimensionality Reduction Machine Learning\n  methods applied to Multiparametric MRI in cerebral ischemia: Preliminary\n  Results",
    "abstract": "  The evaluation and treatment of acute cerebral ischemia requires a technique\nthat can determine the total area of tissue at risk for infarction using\ndiagnostic magnetic resonance imaging (MRI) sequences. Typical MRI data sets\nconsist of T1- and T2-weighted imaging (T1WI, T2WI) along with advanced MRI\nparameters of diffusion-weighted imaging (DWI) and perfusion weighted imaging\n(PWI) methods. Each of these parameters has distinct radiological-pathological\nmeaning. For example, DWI interrogates the movement of water in the tissue and\nPWI gives an estimate of the blood flow, both are critical measures during the\nevolution of stroke. In order to integrate these data and give an estimate of\nthe tissue at risk or damaged, we have developed advanced machine learning\nmethods based on unsupervised non-linear dimensionality reduction (NLDR)\ntechniques. NLDR methods are a class of algorithms that uses mathematically\ndefined manifolds for statistical sampling of multidimensional classes to\ngenerate a discrimination rule of guaranteed statistical accuracy and they can\ngenerate a two- or three-dimensional map, which represents the prominent\nstructures of the data and provides an embedded image of meaningful\nlow-dimensional structures hidden in their high-dimensional observations. In\nthis manuscript, we develop NLDR methods on high dimensional MRI data sets of\npreclinical animals and clinical patients with stroke. On analyzing the\nperformance of these methods, we observed that there was a high of similarity\nbetween multiparametric embedded images from NLDR methods and the ADC map and\nperfusion map. It was also observed that embedded scattergram of abnormal\n(infarcted or at risk) tissue can be visualized and provides a mechanism for\nautomatic methods to delineate potential stroke volumes and early tissue at\nrisk.\n",
    "topics": "{'Dimensionality Reduction': 0.9999064}",
    "score": 0.8334766253
  },
  {
    "id": "2007.07097",
    "title": "Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack",
    "abstract": "  Deep neural networks (DNNs) have achieved high accuracy on various tasks and\nare even robust to natural noise that widely exists in captured images due to\nlow quality imaging sensors, etc. However, the high performance DNNs also raise\ninevitable security problems, e.g., automatically recognizing a high-profile\nperson's face and switching with a maliciously generated fake one to influence\nthe outcomes of various critical events. This fact posts an important and\npractical problem, i.e., how to generate visually clean images while letting\nthem have the capability of misleading the state-of-the-art DNNs to avoid\npotential security issues. In this paper, we initiate the very first attempt to\naddress this very new problem from the perspective of adversarial attack and\npropose the adversarial denoise attack aiming to simultaneously denoise input\nimages while fooling DNNs. More specifically, our main contributions are\nthree-fold: First, we identify a totally new task that stealthily embeds\nattacks inside image denoising module widely deployed in multimedia devices as\nan image post-processing operation to simultaneously enhance the visual image\nquality and fool DNNs. Second, we formulate this new task as a kernel\nprediction problem for image filtering and propose the adversarial-denoising\nkernel prediction that can produce adversarial-noiseless kernels for effective\ndenoising and adversarial attacking simultaneously. Third, we implement an\nadaptive perceptual region localization to identify semantic-related\nvulnerability regions with which the attack can be more effective while not\ndoing too much harm to the denoising. We validate our method on the NeurIPS'17\nadversarial competition dataset. The comprehensive evaluation and analysis\ndemonstrate that our method not only realizes denoising but also achieves\nhigher success rate and transferability over the state-of-the-art attacks.\n",
    "topics": "{'Denoising': 1.0, 'Adversarial Attack': 0.9999808, 'Image Denoising': 0.99793255}",
    "score": 0.8334433332
  },
  {
    "id": "1711.08609",
    "title": "Improving the Accuracy of Pre-trained Word Embeddings for Sentiment\n  Analysis",
    "abstract": "  Sentiment analysis is one of the well-known tasks and fast growing research\nareas in natural language processing (NLP) and text classifications. This\ntechnique has become an essential part of a wide range of applications\nincluding politics, business, advertising and marketing. There are various\ntechniques for sentiment analysis, but recently word embeddings methods have\nbeen widely used in sentiment classification tasks. Word2Vec and GloVe are\ncurrently among the most accurate and usable word embedding methods which can\nconvert words into meaningful vectors. However, these methods ignore sentiment\ninformation of texts and need a huge corpus of texts for training and\ngenerating exact vectors which are used as inputs of deep learning models. As a\nresult, because of the small size of some corpuses, researcher often have to\nuse pre-trained word embeddings which were trained on other large text corpus\nsuch as Google News with about 100 billion words. The increasing accuracy of\npre-trained word embeddings has a great impact on sentiment analysis research.\nIn this paper we propose a novel method, Improved Word Vectors (IWV), which\nincreases the accuracy of pre-trained word embeddings in sentiment analysis.\nOur method is based on Part-of-Speech (POS) tagging techniques, lexicon-based\napproaches and Word2Vec/GloVe methods. We tested the accuracy of our method via\ndifferent deep learning models and sentiment datasets. Our experiment results\nshow that Improved Word Vectors (IWV) are very effective for sentiment\nanalysis.\n",
    "topics": "{'Word Embeddings': 1.0, 'Sentiment Analysis': 0.986439}",
    "score": 0.8333727232
  },
  {
    "id": "1808.08531",
    "title": "DeepTracker: Visualizing the Training Process of Convolutional Neural\n  Networks",
    "abstract": "  Deep convolutional neural networks (CNNs) have achieved remarkable success in\nvarious fields. However, training an excellent CNN is practically a\ntrial-and-error process that consumes a tremendous amount of time and computer\nresources. To accelerate the training process and reduce the number of trials,\nexperts need to understand what has occurred in the training process and why\nthe resulting CNN behaves as such. However, current popular training platforms,\nsuch as TensorFlow, only provide very little and general information, such as\ntraining/validation errors, which is far from enough to serve this purpose. To\nbridge this gap and help domain experts with their training tasks in a\npractical environment, we propose a visual analytics system, DeepTracker, to\nfacilitate the exploration of the rich dynamics of CNN training processes and\nto identify the unusual patterns that are hidden behind the huge amount of\ntraining log. Specifically,we combine a hierarchical index mechanism and a set\nof hierarchical small multiples to help experts explore the entire training log\nfrom different levels of detail. We also introduce a novel cube-style\nvisualization to reveal the complex correlations among multiple types of\nheterogeneous training data including neuron weights, validation images, and\ntraining iterations. Three case studies are conducted to demonstrate how\nDeepTracker provides its users with valuable knowledge in an industry-level CNN\ntraining process, namely in our case, training ResNet-50 on the ImageNet\ndataset. We show that our method can be easily applied to other\nstate-of-the-art \"very deep\" CNN models.\n",
    "topics": "{}",
    "score": 0.8332218262
  },
  {
    "id": "1902.05316",
    "title": "JND-SalCAR: A Novel JND-based Saliency-Channel Attention Residual\n  Network for Image Quality Prediction",
    "abstract": "  In image quality enhancement processing, it is the most important to predict\nhow humans perceive processed images since human observers are the ultimate\nreceivers of the images. Thus, objective image quality assessment (IQA) methods\nbased on human visual sensitivity from psychophysical experiments have been\nextensively studied. Thanks to the powerfulness of deep convolutional neural\nnetworks (CNN), many CNN based IQA models have been studied. However, previous\nCNN-based IQA models have not fully utilized the characteristics of human\nvisual systems (HVS) for IQA problems by simply entrusting everything to CNN\nwhere the CNN-based models are often trained as a regressor to predict the\nscores of subjective quality assessment obtained from IQA datasets. In this\npaper, we propose a novel JND-based saliency-channel attention residual network\nfor image quality assessment, called JND-SalCAR, where the human psychophysical\ncharacteristics such as visual saliency and just noticeable difference (JND)\nare effectively incorporated. We newly propose a SalCAR block so that\nperceptually important features can be extracted by using a saliency-based\nspatial attention and a channel attention. In addition, the visual saliency map\nis further used as a guideline for predicting the patch weight map in order to\nafford a stable training of end-to-end optimization for the JND-SalCAR. To our\nbest knowledge, our work is the first HVS-inspired trainable IQA network that\nconsiders both the visual saliency and JND characteristics of HVS. We evaluate\nthe proposed JND-SalCAR on large IQA datasets where it outperforms all the\nrecent state-of-the-art IQA methods.\n",
    "topics": "{'Image Quality Assessment': 1.0}",
    "score": 0.8330924497
  },
  {
    "id": "1810.01021",
    "title": "Large batch size training of neural networks with adversarial training\n  and second-order information",
    "abstract": "  The most straightforward method to accelerate Stochastic Gradient Descent\n(SGD) computation is to distribute the randomly selected batch of inputs over\nmultiple processors. To keep the distributed processors fully utilized requires\ncommensurately growing the batch size. However, large batch training often\nleads to poorer generalization. A recently proposed solution for this problem\nis to use adaptive batch sizes in SGD. In this case, one starts with a small\nnumber of processes and scales the processes as training progresses. Two major\nchallenges with this approach are (i) that dynamically resizing the cluster can\nadd non-trivial overhead, in part since it is currently not supported, and (ii)\nthat the overall speed up is limited by the initial phase with smaller batches.\nIn this work, we address both challenges by developing a new adaptive batch\nsize framework, with autoscaling based on the Ray framework. This allows very\nefficient elastic scaling with negligible resizing overhead (0.32\\% of time for\nResNet18 ImageNet training). Furthermore, we propose a new adaptive batch size\ntraining scheme using second order methods and adversarial training. These\nenable increasing batch sizes earlier during training, which leads to better\ntraining time. We extensively evaluate our method on Cifar-10/100, SVHN,\nTinyImageNet, and ImageNet datasets, using multiple neural networks, including\nResNets and smaller networks such as SqueezeNext. Our method exceeds the\nperformance of existing solutions in terms of both accuracy and the number of\nSGD iterations (up to 1\\% and $5\\times$, respectively). Importantly, this is\nachieved without any additional hyper-parameter tuning to tailor our method in\nany of these experiments.\n",
    "topics": "{}",
    "score": 0.8329778489
  },
  {
    "id": "1704.06544",
    "title": "A 3D fully convolutional neural network and a random walker to segment\n  the esophagus in CT",
    "abstract": "  Precise delineation of organs at risk (OAR) is a crucial task in radiotherapy\ntreatment planning, which aims at delivering high dose to the tumour while\nsparing healthy tissues. In recent years algorithms showed high performance and\nthe possibility to automate this task for many OAR. However, for some OAR\nprecise delineation remains challenging. The esophagus with a versatile shape\nand poor contrast is among these structures. To tackle these issues we propose\na 3D fully (convolutional neural network (CNN) driven random walk (RW) approach\nto automatically segment the esophagus on CT. First, a soft probability map is\ngenerated by the CNN. Then an active contour model (ACM) is fitted on the\nprobability map to get a first estimation of the center line. The outputs of\nthe CNN and ACM are then used in addition to CT Hounsfield values to drive the\nRW. Evaluation and training was done on 50 CTs with peer reviewed esophagus\ncontours. Results were assessed regarding spatial overlap and shape\nsimilarities.\n  The generated contours showed a mean Dice coefficient of 0.76, an average\nsymmetric square distance of 1.36 mm and an average Hausdorff distance of 11.68\ncompared to the reference. These figures translate into a very good agreement\nwith the reference contours and an increase in accuracy compared to other\nmethods.\n  We show that by employing a CNN accurate estimations of esophagus location\ncan be obtained and refined by a post processing RW step. One of the main\nadvantages compared to previous methods is that our network performs\nconvolutions in a 3D manner, fully exploiting the 3D spatial context and\nperforming an efficient and precise volume-wise prediction. The whole\nsegmentation process is fully automatic and yields esophagus delineations in\nvery good agreement with the used gold standard, showing that it can compete\nwith previously published methods.\n",
    "topics": "{}",
    "score": 0.8329307625
  },
  {
    "id": "1904.01308",
    "title": "CANU-ReID: A Conditional Adversarial Network for Unsupervised person\n  Re-IDentification",
    "abstract": "  Unsupervised person re-ID is the task of identifying people on a target data\nset for which the ID labels are unavailable during training. In this paper, we\npropose to unify two trends in unsupervised person re-ID: clustering &\nfine-tuning and adversarial learning. On one side, clustering groups training\nimages into pseudo-ID labels, and uses them to fine-tune the feature extractor.\nOn the other side, adversarial learning is used, inspired by domain adaptation,\nto match distributions from different domains. Since target data is distributed\nacross different camera viewpoints, we propose to model each camera as an\nindependent domain, and aim to learn domain-independent features.\nStraightforward adversarial learning yields negative transfer, we thus\nintroduce a conditioning vector to mitigate this undesirable effect. In our\nframework, the centroid of the cluster to which the visual sample belongs is\nused as conditioning vector of our conditional adversarial network, where the\nvector is permutation invariant (clusters ordering does not matter) and its\nsize is independent of the number of clusters. To our knowledge, we are the\nfirst to propose the use of conditional adversarial networks for unsupervised\nperson re-ID. We evaluate the proposed architecture on top of two\nstate-of-the-art clustering-based unsupervised person re-identification (re-ID)\nmethods on four different experimental settings with three different data sets\nand set the new state-of-the-art performance on all four of them. Our code and\nmodel will be made publicly available at\nhttps://team.inria.fr/perception/canu-reid/.\n",
    "topics": "{'Person Re-Identification': 1.0, 'Domain Adaptation': 0.87012994}",
    "score": 0.8326744123
  },
  {
    "id": "1811.00648",
    "title": "Prediction Error Meta Classification in Semantic Segmentation: Detection\n  via Aggregated Dispersion Measures of Softmax Probabilities",
    "abstract": "  We present a method that \"meta\" classifies whether seg-ments predicted by a\nsemantic segmentation neural networkintersect with the ground truth. For this\npurpose, we employ measures of dispersion for predicted pixel-wise class\nprobability distributions, like classification entropy, that yield heat maps of\nthe input scene's size. We aggregate these dispersion measures segment-wise and\nderive metrics that are well-correlated with the segment-wise IoU of prediction\nand ground truth. This procedure yields an almost plug and play post-processing\ntool to rate the prediction quality of semantic segmentation networks on\nsegment level. This is especially relevant for monitoring neural networks in\nonline applications like automated driving or medical imaging where reliability\nis of utmost importance. In our tests, we use publicly available\nstate-of-the-art networks trained on the Cityscapes dataset and the BraTS2017\ndataset and analyze the predictive power of different metrics as well as\ndifferent sets of metrics. To this end, we compute logistic LASSO regression\nfits for the task of classifying IoU=0 vs. IoU>0 per segment and obtain AUROC\nvalues of up to 91.55%. We complement these tests with linear regression fits\nto predict the segment-wise IoU and obtain prediction standard deviations of\ndown to 0.130 as well as $R^2$ values of up to 84.15%. We show that these\nresults clearly outperform standard approaches.\n",
    "topics": "{'Semantic Segmentation': 0.9999994}",
    "score": 0.8322509031
  },
  {
    "id": "2006.14147",
    "title": "FastSpec: Scalable Generation and Detection of Spectre Gadgets Using\n  Neural Embeddings",
    "abstract": "  Several techniques have been proposed to detect vulnerable Spectre gadgets in\nwidely deployed commercial software. Unfortunately, detection techniques\nproposed so far rely on hand-written rules which fall short in covering subtle\nvariations of known Spectre gadgets as well as demand a huge amount of time to\nanalyze each conditional branch in software. Since it requires arduous effort\nto craft new gadgets manually, the evaluations of detection mechanisms are\nbased only on a handful of these gadgets.\n  In this work, we employ deep learning techniques for automated generation and\ndetection of Spectre gadgets. We first create a diverse set of Spectre-V1\ngadgets by introducing perturbations to the known gadgets. Using mutational\nfuzzing, we produce a data set with more than 1 million Spectre-V1 gadgets\nwhich is the largest Spectre gadget data set built to date. Next, we conduct\nthe first empirical usability study of Generative Adversarial Networks (GANs)\nfor creating assembly code without any human interaction. We introduce\nSpectreGAN which leverages masking implementation of GANs for both learning the\ngadget structures and generating new gadgets. This provides the first scalable\nsolution to extend the variety of Spectre gadgets.\n  Finally, we propose FastSpec which builds a classifier with the generated\nSpectre gadgets based on the novel high dimensional Neural Embedding technique\nBERT. For case studies, we demonstrate that FastSpec discovers potential\ngadgets in OpenSSL libraries and Phoronix benchmarks. Further, FastSpec offers\nmuch greater flexibility and much faster classification compared to what is\noffered by the existing tools. Therefore FastSpec can be used for gadget\ndetection in large-scale projects.\n",
    "topics": "{}",
    "score": 0.8322254897
  },
  {
    "id": "1804.00248",
    "title": "SampleAhead: Online Classifier-Sampler Communication for Learning from\n  Synthesized Data",
    "abstract": "  State-of-the-art techniques of artificial intelligence, in particular deep\nlearning, are mostly data-driven. However, collecting and manually labeling a\nlarge scale dataset is both difficult and expensive. A promising alternative is\nto introduce synthesized training data, so that the dataset size can be\nsignificantly enlarged with little human labor. But, this raises an important\nproblem in active vision: given an {\\bf infinite} data space, how to\neffectively sample a {\\bf finite} subset to train a visual classifier? This\npaper presents an approach for learning from synthesized data effectively. The\nmotivation is straightforward -- increasing the probability of seeing difficult\ntraining data. We introduce a module named {\\bf SampleAhead} to formulate the\nlearning process into an online communication between a {\\em classifier} and a\n{\\em sampler}, and update them iteratively. In each round, we adjust the\nsampling distribution according to the classification results, and train the\nclassifier using the data sampled from the updated distribution. Experiments\nare performed by introducing synthesized images rendered from ShapeNet models\nto assist PASCAL3D+ classification. Our approach enjoys higher classification\naccuracy, especially in the scenario of a limited number of training samples.\nThis demonstrates its efficiency in exploring the infinite data space.\n",
    "topics": "{}",
    "score": 0.8322004797
  },
  {
    "id": "1609.04802",
    "title": "Photo-Realistic Single Image Super-Resolution Using a Generative\n  Adversarial Network",
    "abstract": "  Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.\n",
    "topics": "{'Image Super-Resolution': 1.0, 'Super-Resolution': 0.9999999, 'Super Resolution': 0.9999999}",
    "score": 0.8321643462
  },
  {
    "id": "1512.07506",
    "title": "Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd",
    "abstract": "  Object detection and 6D pose estimation in the crowd (scenes with multiple\nobject instances, severe foreground occlusions and background distractors), has\nbecome an important problem in many rapidly evolving technological areas such\nas robotics and augmented reality. Single shot-based 6D pose estimators with\nmanually designed features are still unable to tackle the above challenges,\nmotivating the research towards unsupervised feature learning and\nnext-best-view estimation. In this work, we present a complete framework for\nboth single shot-based 6D object pose estimation and next-best-view prediction\nbased on Hough Forests, the state of the art object pose estimator that\nperforms classification and regression jointly. Rather than using manually\ndesigned features we a) propose an unsupervised feature learnt from\ndepth-invariant patches using a Sparse Autoencoder and b) offer an extensive\nevaluation of various state of the art features. Furthermore, taking advantage\nof the clustering performed in the leaf nodes of Hough Forests, we learn to\nestimate the reduction of uncertainty in other views, formulating the problem\nof selecting the next-best-view. To further improve pose estimation, we propose\nan improved joint registration and hypotheses verification module as a final\nrefinement step to reject false detections. We provide two additional\nchallenging datasets inspired from realistic scenarios to extensively evaluate\nthe state of the art and our framework. One is related to domestic environments\nand the other depicts a bin-picking scenario mostly found in industrial\nsettings. We show that our framework significantly outperforms state of the art\nboth on public and on our datasets.\n",
    "topics": "{'Pose Estimation': 0.9999893, '6D Pose Estimation using RGB': 0.999964, '6D Pose Estimation': 0.9989071, 'Object Detection': 0.88209826}",
    "score": 0.8319440778
  },
  {
    "id": "2010.07849",
    "title": "A Hamiltonian Monte Carlo Method for Probabilistic Adversarial Attack\n  and Learning",
    "abstract": "  Although deep convolutional neural networks (CNNs) have demonstrated\nremarkable performance on multiple computer vision tasks, researches on\nadversarial learning have shown that deep models are vulnerable to adversarial\nexamples, which are crafted by adding visually imperceptible perturbations to\nthe input images. Most of the existing adversarial attack methods only create a\nsingle adversarial example for the input, which just gives a glimpse of the\nunderlying data manifold of adversarial examples. An attractive solution is to\nexplore the solution space of the adversarial examples and generate a diverse\nbunch of them, which could potentially improve the robustness of real-world\nsystems and help prevent severe security threats and vulnerabilities. In this\npaper, we present an effective method, called Hamiltonian Monte Carlo with\nAccumulated Momentum (HMCAM), aiming to generate a sequence of adversarial\nexamples. To improve the efficiency of HMC, we propose a new regime to\nautomatically control the length of trajectories, which allows the algorithm to\nmove with adaptive step sizes along the search direction at different\npositions. Moreover, we revisit the reason for high computational cost of\nadversarial training under the view of MCMC and design a new generative method\ncalled Contrastive Adversarial Training (CAT), which approaches equilibrium\ndistribution of adversarial examples with only few iterations by building from\nsmall modifications of the standard Contrastive Divergence (CD) and achieve a\ntrade-off between efficiency and accuracy. Both quantitative and qualitative\nanalysis on several natural image datasets and practical systems have confirmed\nthe superiority of the proposed algorithm.\n",
    "topics": "{'Adversarial Attack': 1.0}",
    "score": 0.8319121683
  },
  {
    "id": "1709.09075",
    "title": "Automated sub-cortical brain structure segmentation combining spatial\n  and deep convolutional features",
    "abstract": "  Sub-cortical brain structure segmentation in Magnetic Resonance Images (MRI)\nhas attracted the interest of the research community for a long time because\nmorphological changes in these structures are related to different\nneurodegenerative disorders. However, manual segmentation of these structures\ncan be tedious and prone to variability, highlighting the need for robust\nautomated segmentation methods. In this paper, we present a novel convolutional\nneural network based approach for accurate segmentation of the sub-cortical\nbrain structures that combines both convolutional and prior spatial features\nfor improving the segmentation accuracy. In order to increase the accuracy of\nthe automated segmentation, we propose to train the network using a restricted\nsample selection to force the network to learn the most difficult parts of the\nstructures. We evaluate the accuracy of the proposed method on the public\nMICCAI 2012 challenge and IBSR 18 datasets, comparing it with different\navailable state-of-the-art methods and other recently proposed deep learning\napproaches. On the MICCAI 2012 dataset, our method shows an excellent\nperformance comparable to the best challenge participant strategy, while\nperforming significantly better than state-of-the-art techniques such as\nFreeSurfer and FIRST. On the IBSR 18 dataset, our method also exhibits a\nsignificant increase in the performance with respect to not only FreeSurfer and\nFIRST, but also comparable or better results than other recent deep learning\napproaches. Moreover, our experiments show that both the addition of the\nspatial priors and the restricted sampling strategy have a significant effect\non the accuracy of the proposed method. In order to encourage the\nreproducibility and the use of the proposed method, a public version of our\napproach is available to download for the neuroimaging community.\n",
    "topics": "{}",
    "score": 0.8318703595
  },
  {
    "id": "1706.08126",
    "title": "ToolNet: Holistically-Nested Real-Time Segmentation of Robotic Surgical\n  Tools",
    "abstract": "  Real-time tool segmentation from endoscopic videos is an essential part of\nmany computer-assisted robotic surgical systems and of critical importance in\nrobotic surgical data science. We propose two novel deep learning architectures\nfor automatic segmentation of non-rigid surgical instruments. Both methods take\nadvantage of automated deep-learning-based multi-scale feature extraction while\ntrying to maintain an accurate segmentation quality at all resolutions. The two\nproposed methods encode the multi-scale constraint inside the network\narchitecture. The first proposed architecture enforces it by cascaded\naggregation of predictions and the second proposed network does it by means of\na holistically-nested architecture where the loss at each scale is taken into\naccount for the optimization process. As the proposed methods are for real-time\nsemantic labeling, both present a reduced number of parameters. We propose the\nuse of parametric rectified linear units for semantic labeling in these small\narchitectures to increase the regularization ability of the design and maintain\nthe segmentation accuracy without overfitting the training sets. We compare the\nproposed architectures against state-of-the-art fully convolutional networks.\nWe validate our methods using existing benchmark datasets, including ex vivo\ncases with phantom tissue and different robotic surgical instruments present in\nthe scene. Our results show a statistically significant improved Dice\nSimilarity Coefficient over previous instrument segmentation methods. We\nanalyze our design choices and discuss the key drivers for improving accuracy.\n",
    "topics": "{}",
    "score": 0.8317196031
  },
  {
    "id": "1909.02062",
    "title": "DCGANs for Realistic Breast Mass Augmentation in X-ray Mammography",
    "abstract": "  Early detection of breast cancer has a major contribution to curability, and\nusing mammographic images, this can be achieved non-invasively. Supervised deep\nlearning, the dominant CADe tool currently, has played a great role in object\ndetection in computer vision, but it suffers from a limiting property: the need\nof a large amount of labelled data. This becomes stricter when it comes to\nmedical datasets which require high-cost and time-consuming annotations.\nFurthermore, medical datasets are usually imbalanced, a condition that often\nhinders classifiers performance. The aim of this paper is to learn the\ndistribution of the minority class to synthesise new samples in order to\nimprove lesion detection in mammography. Deep Convolutional Generative\nAdversarial Networks (DCGANs) can efficiently generate breast masses. They are\ntrained on increasing-size subsets of one mammographic dataset and used to\ngenerate diverse and realistic breast masses. The effect of including the\ngenerated images and/or applying horizontal and vertical flipping is tested in\nan environment where a 1:10 imbalanced dataset of masses and normal tissue\npatches is classified by a fully-convolutional network. A maximum of ~ 0:09\nimprovement of F1 score is reported by using DCGANs along with flipping\naugmentation over using the original images. We show that DCGANs can be used\nfor synthesising photo-realistic breast mass patches with considerable\ndiversity. It is demonstrated that appending synthetic images in this\nenvironment, along with flipping, outperforms the traditional augmentation\nmethod of flipping solely, offering faster improvements as a function of the\ntraining set size.\n",
    "topics": "{'Object Detection': 0.93550354}",
    "score": 0.8316778481
  },
  {
    "id": "2005.02589",
    "title": "Unsupervised Pre-trained Models from Healthy ADLs Improve Parkinson's\n  Disease Classification of Gait Patterns",
    "abstract": "  Application and use of deep learning algorithms for different healthcare\napplications is gaining interest at a steady pace. However, use of such\nalgorithms can prove to be challenging as they require large amounts of\ntraining data that capture different possible variations. This makes it\ndifficult to use them in a clinical setting since in most health applications\nresearchers often have to work with limited data. Less data can cause the deep\nlearning model to over-fit. In this paper, we ask how can we use data from a\ndifferent environment, different use-case, with widely differing data\ndistributions. We exemplify this use case by using single-sensor accelerometer\ndata from healthy subjects performing activities of daily living - ADLs (source\ndataset), to extract features relevant to multi-sensor accelerometer gait data\n(target dataset) for Parkinson's disease classification. We train the\npre-trained model using the source dataset and use it as a feature extractor.\nWe show that the features extracted for the target dataset can be used to train\nan effective classification model. Our pre-trained source model consists of a\nconvolutional autoencoder, and the target classification model is a simple\nmulti-layer perceptron model. We explore two different pre-trained source\nmodels, trained using different activity groups, and analyze the influence the\nchoice of pre-trained model has over the task of Parkinson's disease\nclassification.\n",
    "topics": "{}",
    "score": 0.8314165618
  },
  {
    "id": "2003.08560",
    "title": "CPR-GCN: Conditional Partial-Residual Graph Convolutional Network in\n  Automated Anatomical Labeling of Coronary Arteries",
    "abstract": "  Automated anatomical labeling plays a vital role in coronary artery disease\ndiagnosing procedure. The main challenge in this problem is the large\nindividual variability inherited in human anatomy. Existing methods usually\nrely on the position information and the prior knowledge of the topology of the\ncoronary artery tree, which may lead to unsatisfactory performance when the\nmain branches are confusing. Motivated by the wide application of the graph\nneural network in structured data, in this paper, we propose a conditional\npartial-residual graph convolutional network (CPR-GCN), which takes both\nposition and CT image into consideration, since CT image contains abundant\ninformation such as branch size and spanning direction. Two majority parts, a\nPartial-Residual GCN and a conditions extractor, are included in CPR-GCN. The\nconditions extractor is a hybrid model containing the 3D CNN and the LSTM,\nwhich can extract 3D spatial image features along the branches. On the\ntechnical side, the Partial-Residual GCN takes the position features of the\nbranches, with the 3D spatial image features as conditions, to predict the\nlabel for each branches. While on the mathematical side, our approach twists\nthe partial differential equation (PDE) into the graph modeling. A dataset with\n511 subjects is collected from the clinic and annotated by two experts with a\ntwo-phase annotation process. According to the five-fold cross-validation, our\nCPR-GCN yields 95.8% meanRecall, 95.4% meanPrecision and 0.955 meanF1, which\noutperforms state-of-the-art approaches.\n",
    "topics": "{}",
    "score": 0.8313542772
  },
  {
    "id": "1808.00100",
    "title": "WeedMap: A large-scale semantic weed mapping framework using aerial\n  multispectral imaging and deep neural network for precision farming",
    "abstract": "  We present a novel weed segmentation and mapping framework that processes\nmultispectral images obtained from an unmanned aerial vehicle (UAV) using a\ndeep neural network (DNN). Most studies on crop/weed semantic segmentation only\nconsider single images for processing and classification. Images taken by UAVs\noften cover only a few hundred square meters with either color only or color\nand near-infrared (NIR) channels. Computing a single large and accurate\nvegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties\narising from: (1) limited ground sample distances (GSDs) in high-altitude\ndatasets, (2) sacrificed resolution resulting from downsampling high-fidelity\nimages, and (3) multispectral image alignment. To address these issues, we\nadopt a stand sliding window approach that operates on only small portions of\nmultispectral orthomosaic maps (tiles), which are channel-wise aligned and\ncalibrated radiometrically across the entire map. We define the tile size to be\nthe same as that of the DNN input to avoid resolution loss. Compared to our\nbaseline model (i.e., SegNet with 3 channel RGB inputs) yielding an area under\nthe curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed\nmodel with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we\nprovide an extensive analysis of 20 trained models, both qualitatively and\nquantitatively, in order to evaluate the effects of varying input channels and\ntunable network hyperparameters. Furthermore, we release a large sugar\nbeet/weed aerial dataset with expertly guided annotations for further research\nin the fields of remote sensing, precision agriculture, and agricultural\nrobotics.\n",
    "topics": "{'Semantic Segmentation': 0.7537554}",
    "score": 0.8313090185
  },
  {
    "id": "1805.08699",
    "title": "OFF-ApexNet on Micro-expression Recognition System",
    "abstract": "  When a person attempts to conceal an emotion, the genuine emotion is manifest\nas a micro-expression. Exploration of automatic facial micro-expression\nrecognition systems is relatively new in the computer vision domain. This is\ndue to the difficulty in implementing optimal feature extraction methods to\ncope with the subtlety and brief motion characteristics of the expression. Most\nof the existing approaches extract the subtle facial movements based on\nhand-crafted features. In this paper, we address the micro-expression\nrecognition task with a convolutional neural network (CNN) architecture, which\nwell integrates the features extracted from each video. A new feature\ndescriptor, Optical Flow Features from Apex frame Network (OFF-ApexNet) is\nintroduced. This feature descriptor combines the optical ow guided context with\nthe CNN. Firstly, we obtain the location of the apex frame from each video\nsequence as it portrays the highest intensity of facial motion among all\nframes. Then, the optical ow information are attained from the apex frame and a\nreference frame (i.e., onset frame). Finally, the optical flow features are fed\ninto a pre-designed CNN model for further feature enhancement as well as to\ncarry out the expression classification. To evaluate the effectiveness of\nOFF-ApexNet, comprehensive evaluations are conducted on three public\nspontaneous micro-expression datasets (i.e., SMIC, CASME II and SAMM). The\npromising recognition result suggests that the proposed method can optimally\ndescribe the significant micro-expression details. In particular, we report\nthat, in a multi-database with leave-one-subject-out cross-validation\nexperimental protocol, the recognition performance reaches 74.60% of\nrecognition accuracy and F-measure of 71.04%. We also note that this is the\nfirst work that performs cross-dataset validation on three databases in this\ndomain.\n",
    "topics": "{'Optical Flow Estimation': 1.0}",
    "score": 0.8313069361
  },
  {
    "id": "1709.02081",
    "title": "An unsupervised long short-term memory neural network for event\n  detection in cell videos",
    "abstract": "  We propose an automatic unsupervised cell event detection and classification\nmethod, which expands convolutional Long Short-Term Memory (LSTM) neural\nnetworks, for cellular events in cell video sequences. Cells in images that are\ncaptured from various biomedical applications usually have different shapes and\nmotility, which pose difficulties for the automated event detection in cell\nvideos. Current methods to detect cellular events are based on supervised\nmachine learning and rely on tedious manual annotation from investigators with\nspecific expertise. So that our LSTM network could be trained in an\nunsupervised manner, we designed it with a branched structure where one branch\nlearns the frequent, regular appearance and movements of objects and the second\nlearns the stochastic events, which occur rarely and without warning in a cell\nvideo sequence. We tested our network on a publicly available dataset of\ndensely packed stem cell phase-contrast microscopy images undergoing cell\ndivision. This dataset is considered to be more challenging that a dataset with\nsparse cells. We compared our method to several published supervised methods\nevaluated on the same dataset and to a supervised LSTM method with a similar\ndesign and configuration to our unsupervised method. We used an F1-score, which\nis a balanced measure for both precision and recall. Our results show that our\nunsupervised method has a higher or similar F1-score when compared to two fully\nsupervised methods that are based on Hidden Conditional Random Fields (HCRF),\nand has comparable accuracy with the current best supervised HCRF-based method.\nOur method was generalizable as after being trained on one video it could be\napplied to videos where the cells were in different conditions. The accuracy of\nour unsupervised method approached that of its supervised counterpart.\n",
    "topics": "{'Cell Segmentation': 0.8854972}",
    "score": 0.8312965195
  },
  {
    "id": "1905.13215",
    "title": "Image classification using quantum inference on the D-Wave 2X",
    "abstract": "  We use a quantum annealing D-Wave 2X computer to obtain solutions to NP-hard\nsparse coding problems. To reduce the dimensionality of the sparse coding\nproblem to fit on the quantum D-Wave 2X hardware, we passed downsampled MNIST\nimages through a bottleneck autoencoder. To establish a benchmark for\nclassification performance on this reduced dimensional data set, we used an\nAlexNet-like architecture implemented in TensorFlow, obtaining a classification\nscore of $94.54 \\pm 0.7 \\%$. As a control, we showed that the same AlexNet-like\narchitecture produced near-state-of-the-art classification performance $(\\sim\n99\\%)$ on the original MNIST images. To obtain a set of optimized features for\ninferring sparse representations of the reduced dimensional MNIST dataset, we\nimprinted on a random set of $47$ image patches followed by an off-line\nunsupervised learning algorithm using stochastic gradient descent to optimize\nfor sparse coding. Our single-layer of sparse coding matched the stride and\npatch size of the first convolutional layer of the AlexNet-like deep neural\nnetwork and contained $47$ fully-connected features, $47$ being the maximum\nnumber of dictionary elements that could be embedded onto the D-Wave $2$X\nhardware. Recent work suggests that the optimal level of sparsity corresponds\nto a critical value of the trade-off parameter associated with a putative\nsecond order phase transition, an observation supported by a free energy\nanalysis of D-Wave energy states. When the sparse representations inferred by\nthe D-Wave $2$X were passed to a linear support vector machine, we obtained a\nclassification score of $95.68\\%$. Thus, on this problem, we find that a\nsingle-layer of quantum inference is able to outperform a standard deep neural\nnetwork architecture.\n",
    "topics": "{'Image Classification': 0.98617375}",
    "score": 0.8312652395
  },
  {
    "id": "2007.12525",
    "title": "Study of Different Deep Learning Approach with Explainable AI for\n  Screening Patients with COVID-19 Symptoms: Using CT Scan and Chest X-ray\n  Image Dataset",
    "abstract": "  The outbreak of COVID-19 disease caused more than 100,000 deaths so far in\nthe USA alone. It is necessary to conduct an initial screening of patients with\nthe symptoms of COVID-19 disease to control the spread of the disease. However,\nit is becoming laborious to conduct the tests with the available testing kits\ndue to the growing number of patients. Some studies proposed CT scan or chest\nX-ray images as an alternative solution. Therefore, it is essential to use\nevery available resource, instead of either a CT scan or chest X-ray to conduct\na large number of tests simultaneously. As a result, this study aims to develop\na deep learning-based model that can detect COVID-19 patients with better\naccuracy both on CT scan and chest X-ray image dataset. In this work, eight\ndifferent deep learning approaches such as VGG16, InceptionResNetV2, ResNet50,\nDenseNet201, VGG19, MobilenetV2, NasNetMobile, and ResNet15V2 have been tested\non two dataset-one dataset includes 400 CT scan images, and another dataset\nincludes 400 chest X-ray images studied. Besides, Local Interpretable\nModel-agnostic Explanations (LIME) is used to explain the model's\ninterpretability. Using LIME, test results demonstrate that it is conceivable\nto interpret top features that should have worked to build a trust AI framework\nto distinguish between patients with COVID-19 symptoms with other patients.\n",
    "topics": "{'Test results': 0.9999994}",
    "score": 0.8312276576
  },
  {
    "id": "2010.06879",
    "title": "Semantic Segmentation for Partially Occluded Apple Trees Based on Deep\n  Learning",
    "abstract": "  Fruit tree pruning and fruit thinning require a powerful vision system that\ncan provide high resolution segmentation of the fruit trees and their branches.\nHowever, recent works only consider the dormant season, where there are minimal\nocclusions on the branches or fit a polynomial curve to reconstruct branch\nshape and hence, losing information about branch thickness. In this work, we\napply two state-of-the-art supervised learning models U-Net and DeepLabv3, and\na conditional Generative Adversarial Network Pix2Pix (with and without the\ndiscriminator) to segment partially occluded 2D-open-V apple trees. Binary\naccuracy, Mean IoU, Boundary F1 score and Occluded branch recall were used to\nevaluate the performances of the models. DeepLabv3 outperforms the other models\nat Binary accuracy, Mean IoU and Boundary F1 score, but is surpassed by Pix2Pix\n(without discriminator) and U-Net in Occluded branch recall. We define two\ndifficulty indices to quantify the difficulty of the task: (1) Occlusion\nDifficulty Index and (2) Depth Difficulty Index. We analyze the worst 10 images\nin both difficulty indices by means of Branch Recall and Occluded Branch\nRecall. U-Net outperforms the other two models in the current metrics. On the\nother hand, Pix2Pix (without discriminator) provides more information on branch\npaths, which are not reflected by the metrics. This highlights the need for\nmore specific metrics on recovering occluded information. Furthermore, this\nshows the usefulness of image-transfer networks for hallucination behind\nocclusions. Future work is required to further enhance the models to recover\nmore information from occlusions such that this technology can be applied to\nautomating agricultural tasks in a commercial environment.\n",
    "topics": "{'Semantic Segmentation': 0.9737352}",
    "score": 0.8312101889
  },
  {
    "id": "1709.04518",
    "title": "Recurrent Saliency Transformation Network: Incorporating Multi-Stage\n  Visual Cues for Small Organ Segmentation",
    "abstract": "  We aim at segmenting small organs (e.g., the pancreas) from abdominal CT\nscans. As the target often occupies a relatively small region in the input\nimage, deep neural networks can be easily confused by the complex and variable\nbackground. To alleviate this, researchers proposed a coarse-to-fine approach,\nwhich used prediction from the first (coarse) stage to indicate a smaller input\nregion for the second (fine) stage. Despite its effectiveness, this algorithm\ndealt with two stages individually, which lacked optimizing a global energy\nfunction, and limited its ability to incorporate multi-stage visual cues.\nMissing contextual information led to unsatisfying convergence in iterations,\nand that the fine stage sometimes produced even lower segmentation accuracy\nthan the coarse stage.\n  This paper presents a Recurrent Saliency Transformation Network. The key\ninnovation is a saliency transformation module, which repeatedly converts the\nsegmentation probability map from the previous iteration as spatial weights and\napplies these weights to the current iteration. This brings us two-fold\nbenefits. In training, it allows joint optimization over the deep networks\ndealing with different input scales. In testing, it propagates multi-stage\nvisual information throughout iterations to improve segmentation accuracy.\nExperiments in the NIH pancreas segmentation dataset demonstrate the\nstate-of-the-art accuracy, which outperforms the previous best by an average of\nover 2%. Much higher accuracies are also reported on several small organs in a\nlarger dataset collected by ourselves. In addition, our approach enjoys better\nconvergence properties, making it more efficient and reliable in practice.\n",
    "topics": "{}",
    "score": 0.8311272516
  },
  {
    "id": "2003.03808",
    "title": "PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of\n  Generative Models",
    "abstract": "  The primary aim of single-image super-resolution is to construct\nhigh-resolution (HR) images from corresponding low-resolution (LR) inputs. In\nprevious approaches, which have generally been supervised, the training\nobjective typically measures a pixel-wise average distance between the\nsuper-resolved (SR) and HR images. Optimizing such metrics often leads to\nblurring, especially in high variance (detailed) regions. We propose an\nalternative formulation of the super-resolution problem based on creating\nrealistic SR images that downscale correctly. We present an algorithm\naddressing this problem, PULSE (Photo Upsampling via Latent Space Exploration),\nwhich generates high-resolution, realistic images at resolutions previously\nunseen in the literature. It accomplishes this in an entirely self-supervised\nfashion and is not confined to a specific degradation operator used during\ntraining, unlike previous methods (which require supervised training on\ndatabases of LR-HR image pairs). Instead of starting with the LR image and\nslowly adding detail, PULSE traverses the high-resolution natural image\nmanifold, searching for images that downscale to the original LR image. This is\nformalized through the \"downscaling loss,\" which guides exploration through the\nlatent space of a generative model. By leveraging properties of\nhigh-dimensional Gaussians, we restrict the search space to guarantee realistic\noutputs. PULSE thereby generates super-resolved images that both are realistic\nand downscale correctly. We show proof of concept of our approach in the domain\nof face super-resolution (i.e., face hallucination). We also present a\ndiscussion of the limitations and biases of the method as currently implemented\nwith an accompanying model card with relevant metrics. Our method outperforms\nstate-of-the-art methods in perceptual quality at higher resolutions and scale\nfactors than previously possible.\n",
    "topics": "{'Super-Resolution': 0.9999962, 'Super Resolution': 0.99999, 'Image Super-Resolution': 0.9999112}",
    "score": 0.8310808853
  },
  {
    "id": "1910.06302",
    "title": "Finding New Diagnostic Information for Detecting Glaucoma using Neural\n  Networks",
    "abstract": "  We describe a new approach to automated Glaucoma detection in 3D Spectral\nDomain Optical Coherence Tomography (OCT) optic nerve scans. First, we gathered\na unique and diverse multi-ethnic dataset of OCT scans consisting of glaucoma\nand non-glaucomatous cases obtained from four tertiary care eye hospitals\nlocated in four different countries. Using this longitudinal data, we achieved\nstate-of-the-art results for automatically detecting Glaucoma from a single raw\nOCT using a 3D Deep Learning system. These results are close to human doctors\nin a variety of settings across heterogeneous datasets and scanning\nenvironments. To verify correctness and interpretability of the automated\ncategorization, we used saliency maps to find areas of focus for the model.\nMatching human doctor behavior, the model predictions indeed correlated with\nthe conventional diagnostic parameters in the OCT printouts, such as the\nretinal nerve fiber layer. We further used our model to find new areas in the\n3D data that are presently not being identified as a diagnostic parameter to\ndetect glaucoma by human doctors. Namely, we found that the Lamina Cribrosa\n(LC) region can be a valuable source of helpful diagnostic information\npreviously unavailable to doctors during routine clinical care because it lacks\na quantitative printout. Our model provides such volumetric quantification of\nthis region. We found that even when a majority of the RNFL is removed, the LC\nregion can distinguish glaucoma. This is clinically relevant in high myopes,\nwhen the RNFL is already reduced, and thus the LC region may help differentiate\nglaucoma in this confounding situation. We further generalize this approach to\ncreate a new algorithm called DiagFind that provides a recipe for finding new\ndiagnostic information in medical imagery that may have been previously\nunusable by doctors.\n",
    "topics": "{'Test results': 0.9997683}",
    "score": 0.831045073
  },
  {
    "id": "1906.02826",
    "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier\n  Parameter Estimation",
    "abstract": "  Many important classification problems, such as object classification, speech\nrecognition, and machine translation, have been tackled by the supervised\nlearning paradigm in the past, where training corpora of parallel input-output\npairs are required with high cost. To remove the need for the parallel training\ncorpora has practical significance for real-world applications, and it is one\nof the main goals of unsupervised learning. Recently, encouraging progress in\nunsupervised learning for solving such classification problems has been made\nand the nature of the challenges has been clarified. In this article, we review\nthis progress and disseminate a class of promising new methods to facilitate\nunderstanding the methods for machine learning researchers. In particular, we\nemphasize the key information that enables the success of unsupervised learning\n- the sequential statistics as the distributional prior in the labels.\nExploitation of such sequential statistics makes it possible to estimate\nparameters of classifiers without the need of paired input-output data.\n  In this paper, we first introduce the concept of Caesar Cipher and its\ndecryption, which motivated the construction of the novel loss function for\nunsupervised learning we use throughout the paper. Then we use a simple but\nrepresentative binary classification task as an example to derive and describe\nthe unsupervised learning algorithm in a step-by-step, easy-to-understand\nfashion. We include two cases, one with Bigram language model as the sequential\nstatistics for use in unsupervised parameter estimation, and another with a\nsimpler Unigram language model. For both cases, detailed derivation steps for\nthe learning algorithm are included. Further, a summary table compares\ncomputational steps of the two cases in executing the unsupervised learning\nalgorithm for learning binary classifiers.\n",
    "topics": "{'Object Classification': 0.9997141, 'Language Modelling': 0.99836785, 'Machine Translation': 0.76732874, 'Speech Recognition': 0.6178019}",
    "score": 0.8310002829
  },
  {
    "id": "2004.00698",
    "title": "Adversarial Learning for Personalized Tag Recommendation",
    "abstract": "  We have recently seen great progress in image classification due to the\nsuccess of deep convolutional neural networks and the availability of\nlarge-scale datasets. Most of the existing work focuses on single-label image\nclassification. However, there are usually multiple tags associated with an\nimage. The existing works on multi-label classification are mainly based on lab\ncurated labels. Humans assign tags to their images differently, which is mainly\nbased on their interests and personal tagging behavior. In this paper, we\naddress the problem of personalized tag recommendation and propose an\nend-to-end deep network which can be trained on large-scale datasets. The\nuser-preference is learned within the network in an unsupervised way where the\nnetwork performs joint optimization for user-preference and visual encoding. A\njoint training of user-preference and visual encoding allows the network to\nefficiently integrate the visual preference with tagging behavior for a better\nuser recommendation. In addition, we propose the use of adversarial learning,\nwhich enforces the network to predict tags resembling user-generated tags. We\ndemonstrate the effectiveness of the proposed model on two different\nlarge-scale and publicly available datasets, YFCC100M and NUS-WIDE. The\nproposed method achieves significantly better performance on both the datasets\nwhen compared to the baselines and other state-of-the-art methods. The code is\npublicly available at https://github.com/vyzuer/ALTReco.\n",
    "topics": "{'Multi-Label Classification': 0.999451, 'Image Classification': 0.9975937}",
    "score": 0.8309457537
  },
  {
    "id": "1812.04118",
    "title": "Montage based 3D Medical Image Retrieval from Traumatic Brain Injury\n  Cohort using Deep Convolutional Neural Network",
    "abstract": "  Brain imaging analysis on clinically acquired computed tomography (CT) is\nessential for the diagnosis, risk prediction of progression, and treatment of\nthe structural phenotypes of traumatic brain injury (TBI). However, in real\nclinical imaging scenarios, entire body CT images (e.g., neck, abdomen, chest,\npelvis) are typically captured along with whole brain CT scans. For instance,\nin a typical sample of clinical TBI imaging cohort, only ~15% of CT scans\nactually contain whole brain CT images suitable for volumetric brain analyses;\nthe remaining are partial brain or non-brain images. Therefore, a manual image\nretrieval process is typically required to isolate the whole brain CT scans\nfrom the entire cohort. However, the manual image retrieval is time and\nresource consuming and even more difficult for the larger cohorts. To alleviate\nthe manual efforts, in this paper we propose an automated 3D medical image\nretrieval pipeline, called deep montage-based image retrieval (dMIR), which\nperforms classification on 2D montage images via a deep convolutional neural\nnetwork. The novelty of the proposed method for image processing is to\ncharacterize the medical image retrieval task based on the montage images. In a\ncohort of 2000 clinically acquired TBI scans, 794 scans were used as training\ndata, 206 scans were used as validation data, and the remaining 1000 scans were\nused as testing data. The proposed achieved accuracy=1.0, recall=1.0,\nprecision=1.0, f1=1.0 for validation data, while achieved accuracy=0.988,\nrecall=0.962, precision=0.962, f1=0.962 for testing data. Thus, the proposed\ndMIR is able to perform accurate CT whole brain image retrieval from\nlarge-scale clinical cohorts.\n",
    "topics": "{'Image Retrieval': 1.0, 'Computed Tomography (CT)': 0.99944764}",
    "score": 0.8309331304
  },
  {
    "id": "2007.10879",
    "title": "A temporal-to-spatial deep convolutional neural network for\n  classification of hand movements from multichannel electromyography data",
    "abstract": "  Deep convolutional neural networks (CNNs) are appealing for the purpose of\nclassification of hand movements from surface electromyography (sEMG) data\nbecause they have the ability to perform automated person-specific feature\nextraction from raw data. In this paper, we make the novel contribution of\nproposing and evaluating a design for the early processing layers in the deep\nCNN for multichannel sEMG. Specifically, we propose a novel temporal-to-spatial\n(TtS) CNN architecture, where the first layer performs convolution separately\non each sEMG channel to extract temporal features. This is motivated by the\nidea that sEMG signals in each channel are mediated by one or a small subset of\nmuscles, whose temporal activation patterns are associated with the signature\nfeatures of a gesture. The temporal layer captures these signature features for\neach channel separately, which are then spatially mixed in successive layers to\nrecognise a specific gesture. A practical advantage is that this approach also\nmakes the CNN simple to design for different sample rates. We use NinaPro\ndatabase 1 (27 subjects and 52 movements + rest), sampled at 100 Hz, and\ndatabase 2 (40 subjects and 40 movements + rest), sampled at 2 kHz, to evaluate\nour proposed CNN design. We benchmark against a feature-based support vector\nmachine (SVM) classifier, two CNNs from the literature, and an additional\nstandard design of CNN. We find that our novel TtS CNN design achieves 66.6%\nper-class accuracy on database 1, and 67.8% on database 2, and that the TtS CNN\noutperforms all other compared classifiers using a statistical hypothesis test\nat the 2% significance level.\n",
    "topics": "{}",
    "score": 0.8307338879
  },
  {
    "id": "1511.06233",
    "title": "Towards Open Set Deep Networks",
    "abstract": "  Deep networks have produced significant gains for various visual recognition\nproblems, leading to high impact academic and commercial applications. Recent\nwork in deep networks highlighted that it is easy to generate images that\nhumans would never classify as a particular object class, yet networks classify\nsuch images high confidence as that given class - deep network are easily\nfooled with images humans do not consider meaningful. The closed set nature of\ndeep networks forces them to choose from one of the known classes leading to\nsuch artifacts. Recognition in the real world is open set, i.e. the recognition\nsystem should reject unknown/unseen classes at test time. We present a\nmethodology to adapt deep networks for open set recognition, by introducing a\nnew model layer, OpenMax, which estimates the probability of an input being\nfrom an unknown class. A key element of estimating the unknown probability is\nadapting Meta-Recognition concepts to the activation patterns in the\npenultimate layer of the network. OpenMax allows rejection of \"fooling\" and\nunrelated open set images presented to the system; OpenMax greatly reduces the\nnumber of obvious errors made by a deep network. We prove that the OpenMax\nconcept provides bounded open space risk, thereby formally providing an open\nset recognition solution. We evaluate the resulting open set deep networks\nusing pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation\ndata, and thousands of fooling and open set images. The proposed OpenMax model\nsignificantly outperforms open set recognition accuracy of basic deep networks\nas well as deep networks with thresholding of SoftMax probabilities.\n",
    "topics": "{'Open Set Learning': 0.8796513, 'Open Information Extraction': 0.61292154}",
    "score": 0.830694279
  },
  {
    "id": "1804.10002",
    "title": "Force Estimation from OCT Volumes using 3D CNNs",
    "abstract": "  \\textit{Purpose} Estimating the interaction forces of instruments and tissue\nis of interest, particularly to provide haptic feedback during robot assisted\nminimally invasive interventions. Different approaches based on external and\nintegrated force sensors have been proposed. These are hampered by friction,\nsensor size, and sterilizability. We investigate a novel approach to estimate\nthe force vector directly from optical coherence tomography image volumes.\n  \\textit{Methods} We introduce a novel Siamese 3D CNN architecture. The\nnetwork takes an undeformed reference volume and a deformed sample volume as an\ninput and outputs the three components of the force vector. We employ a deep\nresidual architecture with bottlenecks for increased efficiency. We compare the\nSiamese approach to methods using difference volumes and two-dimensional\nprojections. Data was generated using a robotic setup to obtain ground truth\nforce vectors for silicon tissue phantoms as well as porcine tissue.\n  \\textit{Results} Our method achieves a mean average error of 7.7 +- 4.3 mN\nwhen estimating the force vector. Our novel Siamese 3D CNN architecture\noutperforms single-path methods that achieve a mean average error of 11.59 +-\n6.7 mN. Moreover, the use of volume data leads to significantly higher\nperformance compared to processing only surface information which achieves a\nmean average error of 24.38 +- 22.0 mN. Based on the tissue dataset, our\nmethods shows good generalization in between different subjects.\n  \\textit{Conclusions} We propose a novel image-based force estimation method\nusing optical coherence tomography. We illustrate that capturing the\ndeformation of subsurface structures substantially improves force estimation.\nOur approach can provide accurate force estimates in surgical setups when using\nintraoperative optical coherence tomography.\n",
    "topics": "{}",
    "score": 0.8306769356
  },
  {
    "id": "1901.08969",
    "title": "A Zero-Shot Learning application in Deep Drawing process using\n  Hyper-Process Model",
    "abstract": "  One of the consequences of passing from mass production to mass customization\nparadigm in the nowadays industrialized world is the need to increase\nflexibility and responsiveness of manufacturing companies. The high-mix /\nlow-volume production forces constant accommodations of unknown product\nvariants, which ultimately leads to high periods of machine calibration. The\ndifficulty related with machine calibration is that experience is required\ntogether with a set of experiments to meet the final product quality.\nUnfortunately, all possible combinations of machine parameters is so high that\nis difficult to build empirical knowledge. Due to this fact, normally trial and\nerror approaches are taken making one-of-a-kind products not viable. Therefore,\na Zero-Shot Learning (ZSL) based approach called hyper-process model (HPM) to\nlearn the relation among multiple tasks is used as a way to shorten the\ncalibration phase. Assuming each product variant is a task to solve, first, a\nshape analysis on data to learn common modes of deformation between tasks is\nmade, and secondly, a mapping between these modes and task descriptions is\nperformed. Ultimately, the present work has two main contributions: 1)\nFormulation of an industrial problem into a ZSL setting where new process\nmodels can be generated for process optimization and 2) the definition of a\nregression problem in the domain of ZSL. For that purpose, a 2-d deep drawing\nsimulated process was used based on data collected from the Abaqus simulator,\nwhere a significant number of process models were collected to test the\neffectiveness of the approach. The obtained results show that is possible to\nlearn new tasks without any available data (both labeled and unlabeled) by\nleveraging information about already existing tasks, allowing to speed up the\ncalibration phase and make a quicker integration of new products into\nmanufacturing systems.\n",
    "topics": "{'Zero-Shot Learning': 0.9999999}",
    "score": 0.8306330166
  },
  {
    "id": "1604.01545",
    "title": "Training Constrained Deconvolutional Networks for Road Scene Semantic\n  Segmentation",
    "abstract": "  In this work we investigate the problem of road scene semantic segmentation\nusing Deconvolutional Networks (DNs). Several constraints limit the practical\nperformance of DNs in this context: firstly, the paucity of existing pixel-wise\nlabelled training data, and secondly, the memory constraints of embedded\nhardware, which rule out the practical use of state-of-the-art DN architectures\nsuch as fully convolutional networks (FCN). To address the first constraint, we\nintroduce a Multi-Domain Road Scene Semantic Segmentation (MDRS3) dataset,\naggregating data from six existing densely and sparsely labelled datasets for\ntraining our models, and two existing, separate datasets for testing their\ngeneralisation performance. We show that, while MDRS3 offers a greater volume\nand variety of data, end-to-end training of a memory efficient DN does not\nyield satisfactory performance. We propose a new training strategy to overcome\nthis, based on (i) the creation of a best-possible source network (S-Net) from\nthe aggregated data, ignoring time and memory constraints; and (ii) the\ntransfer of knowledge from S-Net to the memory-efficient target network\n(T-Net). We evaluate different techniques for S-Net creation and T-Net\ntransferral, and demonstrate that training a constrained deconvolutional\nnetwork in this manner can unlock better performance than existing training\napproaches. Specifically, we show that a target network can be trained to\nachieve improved accuracy versus an FCN despite using less than 1\\% of the\nmemory. We believe that our approach can be useful beyond automotive scenarios\nwhere labelled data is similarly scarce or fragmented and where practical\nconstraints exist on the desired model size. We make available our network\nmodels and aggregated multi-domain dataset for reproducibility.\n",
    "topics": "{'Semantic Segmentation': 0.9997534}",
    "score": 0.8305001592
  },
  {
    "id": "2009.05847",
    "title": "Machine Learning Against Cancer: Accurate Diagnosis of Cancer by Machine\n  Learning Classification of the Whole Genome Sequencing Data",
    "abstract": "  Machine learning can precisely identify different cancer tumors at any stage\nby classifying cancerous and healthy samples based on their genomic profile. We\nhave developed novel methods of MLAC (Machine Learning Against Cancer)\nachieving perfect results with perfect precision, sensitivity, and specificity.\nWe have used the whole genome sequencing data acquired by next-generation RNA\nsequencing techniques in The Cancer Genome Atlas and Genotype-Tissue Expression\nprojects for cancerous and healthy tissues respectively. Moreover, we have\nshown that unsupervised machine learning clustering has great potential to be\nused for cancer diagnosis. Indeed, a creative way to work with data and general\nalgorithms has resulted in perfect classification i.e. all precision,\nsensitivity, and specificity are equal to 1 for most of the different tumor\ntypes even with a modest amount of data, and the same method works well on a\nseries of cancers and results in great clustering of cancerous and healthy\nsamples too. Our system can be used in practice because once the classifier is\ntrained, it can be used to classify any new sample of new potential patients.\nOne advantage of our work is that the aforementioned perfect precision and\nrecall are obtained on samples of all stages including very early stages of\ncancer; therefore, it is a promising tool for diagnosis of cancers in early\nstages. Another advantage of our novel model is that it works with normalized\nvalues of RNA sequencing data, hence people's private sensitive medical data\nwill remain hidden, protected, and safe. This type of analysis will be\nwidespread and economical in the future and people can even learn to receive\ntheir RNA sequencing data and do their own preliminary cancer studies\nthemselves which have the potential to help the healthcare systems. It is a\ngreat step forward toward good health that is the main base of sustainable\nsocieties.\n",
    "topics": "{}",
    "score": 0.8301958857
  },
  {
    "id": "1812.11163",
    "title": "Center Emphasized Visual Saliency and a Contrast-based Full Reference\n  Image Quality Index",
    "abstract": "  Objective image quality assessment (IQA) is imperative in the current\nmultimedia-intensive world, in order to assess the visual quality of an image\nat close to a human level of ability. Many~parameters such as color intensity,\nstructure, sharpness, contrast, presence of an object, etc., draw human\nattention to an image. Psychological vision research suggests that human vision\nis biased to the center area of an image and display screen. As a result, if\nthe center part contains any visually salient information, it draws human\nattention even more and any distortion in that part will be better perceived\nthan other parts. To the best of our knowledge, previous IQA methods have not\nconsidered this fact. In this paper, we propose a full reference image quality\nassessment (FR-IQA) approach using visual saliency and contrast; however, we\ngive extra attention to the center by increasing the sensitivity of the\nsimilarity maps in that region. We evaluated our method on three large-scale\npopular benchmark databases used by most of the current IQA researchers\n(TID2008, CSIQ~and LIVE), having a total of 3345 distorted images with\n28~different kinds of distortions. Our~method is compared with 13\nstate-of-the-art approaches. This comparison reveals the stronger correlation\nof our method with human-evaluated values. The prediction-of-quality score is\nconsistent for distortion specific as well as distortion independent cases.\nMoreover, faster processing makes it applicable to any real-time application.\nThe MATLAB code is publicly available to test the algorithm and can be found\nonline at http://layek.khu.ac.kr/CEQI.\n",
    "topics": "{'Image Quality Assessment': 1.0}",
    "score": 0.8301769965
  },
  {
    "id": "1803.08467",
    "title": "BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled\n  Representation Learning and Image Synthesis",
    "abstract": "  We introduce BSD-GAN, a novel multi-branch and scale-disentangled training\nmethod which enables unconditional Generative Adversarial Networks (GANs) to\nlearn image representations at multiple scales, benefiting a wide range of\ngeneration and editing tasks. The key feature of BSD-GAN is that it is trained\nin multiple branches, progressively covering both the breadth and depth of the\nnetwork, as resolutions of the training images increase to reveal finer-scale\nfeatures. Specifically, each noise vector, as input to the generator network of\nBSD-GAN, is deliberately split into several sub-vectors, each corresponding to,\nand is trained to learn, image representations at a particular scale. During\ntraining, we progressively \"de-freeze\" the sub-vectors, one at a time, as a new\nset of higher-resolution images is employed for training and more network\nlayers are added. A consequence of such an explicit sub-vector designation is\nthat we can directly manipulate and even combine latent (sub-vector) codes\nwhich model different feature scales.Extensive experiments demonstrate the\neffectiveness of our training method in scale-disentangled learning of image\nrepresentations and synthesis of novel image contents, without any extra labels\nand without compromising quality of the synthesized high-resolution images. We\nfurther demonstrate several image generation and manipulation applications\nenabled or improved by BSD-GAN. Source codes are available at\nhttps://github.com/duxingren14/BSD-GAN.\n",
    "topics": "{'Image Generation': 0.9999987, 'Representation Learning': 0.9865957}",
    "score": 0.8300245857
  },
  {
    "id": "1712.04415",
    "title": "Deception Detection in Videos",
    "abstract": "  We present a system for covert automated deception detection in real-life\ncourtroom trial videos. We study the importance of different modalities like\nvision, audio and text for this task. On the vision side, our system uses\nclassifiers trained on low level video features which predict human\nmicro-expressions. We show that predictions of high-level micro-expressions can\nbe used as features for deception prediction. Surprisingly, IDT (Improved Dense\nTrajectory) features which have been widely used for action recognition, are\nalso very good at predicting deception in videos. We fuse the score of\nclassifiers trained on IDT features and high-level micro-expressions to improve\nperformance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio\ndomain also provide a significant boost in performance, while information from\ntranscripts is not very beneficial for our system. Using various classifiers,\nour automated system obtains an AUC of 0.877 (10-fold cross-validation) when\nevaluated on subjects which were not part of the training set. Even though\nstate-of-the-art methods use human annotations of micro-expressions for\ndeception detection, our fully automated approach outperforms them by 5%. When\ncombined with human annotations of micro-expressions, our AUC improves to\n0.922. We also present results of a user-study to analyze how well do average\nhumans perform on this task, what modalities they use for deception detection\nand how they perform if only one modality is accessible. Our project page can\nbe found at \\url{https://doubaibai.github.io/DARE/}.\n",
    "topics": "{'Action Recognition': 0.9997129, 'Deception Detection': 0.99924916, 'Temporal Action Localization': 0.99904233}",
    "score": 0.8300114841
  },
  {
    "id": "1809.00396",
    "title": "Learning to Navigate Autonomously in Outdoor Environments : MAVNet",
    "abstract": "  In the modern era of automation and robotics, autonomous vehicles are\ncurrently the focus of academic and industrial research. With the ever\nincreasing number of unmanned aerial vehicles getting involved in activities in\nthe civilian and commercial domain, there is an increased need for autonomy in\nthese systems too. Due to guidelines set by the governments regarding the\noperation ceiling of civil drones, road-tracking based navigation is garnering\ninterest . In an attempt to achieve the above mentioned tasks, we propose an\nimitation learning based, data-driven solution to UAV autonomy for navigating\nthrough city streets by learning to fly by imitating an expert pilot. Derived\nfrom the classic image classification algorithms, our classifier has been\nconstructed in the form of a fast 39-layered Inception model, that evaluates\nthe presence of roads using the tomographic reconstructions of the input\nframes. Based on the Inception-v3 architecture, our system performs better in\nterms of processing complexity and accuracy than many existing models for\nimitation learning. The data used for training the system has been captured\nfrom the drone, by flying it in and around urban and semi-urban streets, by\nexperts having at least 6-8 years of flying experience. Permissions were taken\nfrom required authorities who made sure that minimal risk (to pedestrians) is\ninvolved in the data collection process. With the extensive amount of drone\ndata that we collected, we have been able to navigate successfully through\nroads without crashing or overshooting, with an accuracy of 98.44%. The\ncomputational efficiency of MAVNet enables the drone to fly at high speeds of\nupto 6m/sec. We present the same results in this research and compare them with\nother state-of-the-art methods of vision and learning based navigation.\n",
    "topics": "{'Imitation Learning': 0.9999944, 'Autonomous Vehicles': 0.99467516}",
    "score": 0.8299622382
  },
  {
    "id": "1901.06551",
    "title": "Synthesizing facial photometries and corresponding geometries using\n  generative adversarial networks",
    "abstract": "  Artificial data synthesis is currently a well studied topic with useful\napplications in data science, computer vision, graphics and many other fields.\nGenerating realistic data is especially challenging since human perception is\nhighly sensitive to non realistic appearance. In recent times, new levels of\nrealism have been achieved by advances in GAN training procedures and\narchitectures. These successful models, however, are tuned mostly for use with\nregularly sampled data such as images, audio and video. Despite the successful\napplication of the architecture on these types of media, applying the same\ntools to geometric data poses a far greater challenge. The study of geometric\ndeep learning is still a debated issue within the academic community as the\nlack of intrinsic parametrization inherent to geometric objects prohibits the\ndirect use of convolutional filters, a main building block of today's machine\nlearning systems. In this paper we propose a new method for generating\nrealistic human facial geometries coupled with overlayed textures. We\ncircumvent the parametrization issue by imposing a global mapping from our data\nto the unit rectangle. We further discuss how to design such a mapping to\ncontrol the mapping distortion and conserve area within the mapped image. By\nrepresenting geometric textures and geometries as images, we are able to use\nadvanced GAN methodologies to generate new geometries. We address the often\nneglected topic of relation between texture and geometry and propose to use\nthis correlation to match between generated textures and their corresponding\ngeometries. We offer a new method for training GAN models on partially\ncorrupted data. Finally, we provide empirical evidence demonstrating our\ngenerative model's ability to produce examples of new identities independent\nfrom the training data while maintaining a high level of realism, two traits\nthat are often at odds.\n",
    "topics": "{'Face Generation': 0.58219475, 'Texture Synthesis': 0.47008687}",
    "score": 0.8299211921
  },
  {
    "id": "1606.04801",
    "title": "A Powerful Generative Model Using Random Weights for the Deep Image\n  Representation",
    "abstract": "  To what extent is the success of deep visualization due to the training?\nCould we do deep visualization using untrained, random weight networks? To\naddress this issue, we explore new and powerful generative models for three\npopular deep visualization tasks using untrained, random weight convolutional\nneural networks. First we invert representations in feature spaces and\nreconstruct images from white noise inputs. The reconstruction quality is\nstatistically higher than that of the same method applied on well trained\nnetworks with the same architecture. Next we synthesize textures using scaled\ncorrelations of representations in multiple layers and our results are almost\nindistinguishable with the original natural texture and the synthesized\ntextures based on the trained network. Third, by recasting the content of an\nimage in the style of various artworks, we create artistic images with high\nperceptual quality, highly competitive to the prior work of Gatys et al. on\npretrained networks. To our knowledge this is the first demonstration of image\nrepresentations using untrained deep neural networks. Our work provides a new\nand fascinating tool to study the representation of deep network architecture\nand sheds light on new understandings on deep visualization.\n",
    "topics": "{'Data Visualization': 0.80196494, 'Texture Synthesis': 0.60394675}",
    "score": 0.8298944876
  },
  {
    "id": "1812.06181",
    "title": "Efficient Interpretation of Deep Learning Models Using Graph Structure\n  and Cooperative Game Theory: Application to ASD Biomarker Discovery",
    "abstract": "  Discovering imaging biomarkers for autism spectrum disorder (ASD) is critical\nto help explain ASD and predict or monitor treatment outcomes. Toward this end,\ndeep learning classifiers have recently been used for identifying ASD from\nfunctional magnetic resonance imaging (fMRI) with higher accuracy than\ntraditional learning strategies. However, a key challenge with deep learning\nmodels is understanding just what image features the network is using, which\ncan in turn be used to define the biomarkers. Current methods extract\nbiomarkers, i.e., important features, by looking at how the prediction changes\nif \"ignoring\" one feature at a time. In this work, we go beyond looking at only\nindividual features by using Shapley value explanation (SVE) from cooperative\ngame theory. Cooperative game theory is advantageous here because it directly\nconsiders the interaction between features and can be applied to any machine\nlearning method, making it a novel, more accurate way of determining\ninstance-wise biomarker importance from deep learning models. A barrier to\nusing SVE is its computational complexity: $2^N$ given $N$ features. We\nexplicitly reduce the complexity of SVE computation by two approaches based on\nthe underlying graph structure of the input data: 1) only consider the\ncentralized coalition of each feature; 2) a hierarchical pipeline which first\nclusters features into small communities, then applies SVE in each community.\nMonte Carlo approximation can be used for large permutation sets. We first\nvalidate our methods on the MNIST dataset and compare to human perception.\nNext, to insure plausibility of our biomarker results, we train a Random Forest\n(RF) to classify ASD/control subjects from fMRI and compare SVE results to\nstandard RF-based feature importance. Finally, we show initial results on\nranked fMRI biomarkers using SVE on a deep learning classifier for the\nASD/control dataset.\n",
    "topics": "{'Feature Importance': 0.9927987}",
    "score": 0.8298823123
  },
  {
    "id": "2010.06285",
    "title": "Land Cover Semantic Segmentation Using ResUNet",
    "abstract": "  In this paper we present our work on developing an automated system for land\ncover classification. This system takes a multiband satellite image of an area\nas input and outputs the land cover map of the area at the same resolution as\nthe input. For this purpose convolutional machine learning models were trained\nin the task of predicting the land cover semantic segmentation of satellite\nimages. This is a case of supervised learning. The land cover label data were\ntaken from the CORINE Land Cover inventory and the satellite images were taken\nfrom the Copernicus hub. As for the model, U-Net architecture variations were\napplied. Our area of interest are the Ionian islands (Greece). We created a\ndataset from scratch covering this particular area. In addition, transfer\nlearning from the BigEarthNet dataset [1] was performed. In [1] simple\nclassification of satellite images into the classes of CLC is performed but not\nsegmentation as we do. However, their models have been trained into a dataset\nmuch bigger than ours, so we applied transfer learning using their pretrained\nmodels as the first part of out network, utilizing the ability these networks\nhave developed to extract useful features from the satellite images (we\ntransferred a pretrained ResNet50 into a U-Res-Net). Apart from transfer\nlearning other techniques were applied in order to overcome the limitations set\nby the small size of our area of interest. We used data augmentation (cutting\nimages into overlapping patches, applying random transformations such as\nrotations and flips) and cross validation. The results are tested on the 3 CLC\nclass hierarchy levels and a comparative study is made on the results of\ndifferent approaches.\n",
    "topics": "{'Transfer Learning': 0.99793845, 'Semantic Segmentation': 0.99660146, 'Data Augmentation': 0.98056465}",
    "score": 0.8298308494
  },
  {
    "id": "1902.04713",
    "title": "Automated Segmentation of the Optic Disk and Cup using Dual-Stage Fully\n  Convolutional Networks",
    "abstract": "  Automated segmentation of the optic cup and disk on retinal fundus images is\nfundamental for the automated detection / analysis of glaucoma. Traditional\nsegmentation approaches depend heavily upon hand-crafted features and a priori\nknowledge of the user. As such, these methods are difficult to be adapt to the\nclinical environment. Recently, deep learning methods based on fully\nconvolutional networks (FCNs) have been successful in resolving segmentation\nproblems. However, the reliance on large annotated training data is problematic\nwhen dealing with medical images. If a sufficient amount of annotated training\ndata to cover all possible variations is not available, FCNs do not provide\naccurate segmentation. In addition, FCNs have a large receptive field in the\nconvolutional layers, and hence produce coarse outputs of boundaries. Hence, we\npropose a new fully automated method that we refer to as a dual-stage fully\nconvolutional networks (DSFCN). Our approach leverages deep residual\narchitectures and FCNs and learns and infers the location of the optic cup and\ndisk in a step-wise manner with fine-grained details. During training, our\napproach learns from the training data and the estimated results derived from\nthe previous iteration. The ability to learn from the previous iteration\noptimizes the learning of the optic cup and the disk boundaries. During testing\n(prediction), DSFCN uses test (input) images and the estimated probability map\nderived from previous iterations to gradually improve the segmentation\naccuracy. Our method achieved an average Dice co-efficient of 0.8488 and 0.9441\nfor optic cup and disk segmentation and an area under curve (AUC) of 0.9513 for\nglaucoma detection.\n",
    "topics": "{}",
    "score": 0.8298178408
  },
  {
    "id": "1807.02552",
    "title": "M-ADDA: Unsupervised Domain Adaptation with Deep Metric Learning",
    "abstract": "  Unsupervised domain adaptation techniques have been successful for a wide\nrange of problems where supervised labels are limited. The task is to classify\nan unlabeled `target' dataset by leveraging a labeled `source' dataset that\ncomes from a slightly similar distribution. We propose metric-based adversarial\ndiscriminative domain adaptation (M-ADDA) which performs two main steps. First,\nit uses a metric learning approach to train the source model on the source\ndataset by optimizing the triplet loss function. This results in clusters where\nembeddings of the same label are close to each other and those with different\nlabels are far from one another. Next, it uses the adversarial approach (as\nthat used in ADDA \\cite{2017arXiv170205464T}) to make the extracted features\nfrom the source and target datasets indistinguishable. Simultaneously, we\noptimize a novel loss function that encourages the target dataset's embeddings\nto form clusters. While ADDA and M-ADDA use similar architectures, we show that\nM-ADDA performs significantly better on the digits adaptation datasets of MNIST\nand USPS. This suggests that using metric-learning for domain adaptation can\nlead to large improvements in classification accuracy for the domain adaptation\ntask. The code is available at \\url{https://github.com/IssamLaradji/M-ADDA}.\n",
    "topics": "{'Unsupervised Domain Adaptation': 0.9999995, 'Domain Adaptation': 0.9999989, 'Metric Learning': 0.99996924}",
    "score": 0.8298158351
  },
  {
    "id": "1807.11091",
    "title": "StructADMM: A Systematic, High-Efficiency Framework of Structured Weight\n  Pruning for DNNs",
    "abstract": "  Weight pruning methods of DNNs have been demonstrated to achieve a good model\npruning rate without loss of accuracy, thereby alleviating the significant\ncomputation/storage requirements of large-scale DNNs. Structured weight pruning\nmethods have been proposed to overcome the limitation of irregular network\nstructure and demonstrated actual GPU acceleration. However, in prior work the\npruning rate (degree of sparsity) and GPU acceleration are limited (to less\nthan 50%) when accuracy needs to be maintained. In this work,we overcome these\nlimitations by proposing a unified, systematic framework of structured weight\npruning for DNNs. It is a framework that can be used to induce different types\nof structured sparsity, such as filter-wise, channel-wise, and shape-wise\nsparsity, as well non-structured sparsity. The proposed framework incorporates\nstochastic gradient descent with ADMM, and can be understood as a dynamic\nregularization method in which the regularization target is analytically\nupdated in each iteration. Without loss of accuracy on the AlexNet model, we\nachieve 2.58X and 3.65X average measured speedup on two GPUs, clearly\noutperforming the prior work. The average speedups reach 3.15X and 8.52X when\nallowing a moderate ac-curacy loss of 2%. In this case the model compression\nfor convolutional layers is 15.0X, corresponding to 11.93X measured CPU\nspeedup. Our experiments on ResNet model and on other data sets like UCF101 and\nCIFAR-10 demonstrate the consistently higher performance of our framework.\n",
    "topics": "{'Model Compression': 0.9999968}",
    "score": 0.8297311622
  },
  {
    "id": "1712.04407",
    "title": "Logo Synthesis and Manipulation with Clustered Generative Adversarial\n  Networks",
    "abstract": "  Designing a logo for a new brand is a lengthy and tedious back-and-forth\nprocess between a designer and a client. In this paper we explore to what\nextent machine learning can solve the creative task of the designer. For this,\nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web.\nTraining Generative Adversarial Networks (GANs) for logo synthesis on such\nmulti-modal data is not straightforward and results in mode collapse for some\nstate-of-the-art methods. We propose the use of synthetic labels obtained\nthrough clustering to disentangle and stabilize GAN training. We are able to\ngenerate a high diversity of plausible logos and we demonstrate latent space\nexploration techniques to ease the logo design task in an interactive manner.\nMoreover, we validate the proposed clustered GAN training on CIFAR 10,\nachieving state-of-the-art Inception scores when using synthetic labels\nobtained via clustering the features of an ImageNet classifier. GANs can cope\nwith multi-modal data by means of synthetic labels achieved through clustering,\nand our results show the creative potential of such techniques for logo\nsynthesis and manipulation. Our dataset and models will be made publicly\navailable at https://data.vision.ee.ethz.ch/cvl/lld/.\n",
    "topics": "{'Face Generation': 0.7566558, 'Image Manipulation': 0.39864367}",
    "score": 0.8296850114
  },
  {
    "id": "1806.06650",
    "title": "Source Printer Classification using Printer Specific Local Texture\n  Descriptor",
    "abstract": "  The knowledge of source printer can help in printed text document\nauthentication, copyright ownership, and provide important clues about the\nauthor of a fraudulent document along with his/her potential means and motives.\nDevelopment of automated systems for classifying printed documents based on\ntheir source printer, using image processing techniques, is gaining a lot of\nattention in multimedia forensics. Currently, state-of-the-art systems require\nthat the font of letters present in test documents of unknown origin must be\navailable in those used for training the classifier. In this work, we attempt\nto take the first step towards overcoming this limitation. Specifically, we\nintroduce a novel printer specific local texture descriptor. The highlight of\nour technique is the use of encoding and regrouping strategy based on small\nlinear-shaped structures composed of pixels having similar intensity and\ngradient. The results of experiments performed on two separate datasets show\nthat: 1) on a publicly available dataset, the proposed method outperforms\nstate-of-the-art algorithms for characters printed in the same font, and 2) on\nanother dataset\\footnote{Code and dataset will be made publicly available with\npublished version of this paper.} having documents printed in four different\nfonts, the proposed method correctly classifies all test samples when\nsufficient training data is available in same font setup. In addition, it\noutperforms state-of-the-art methods for cross font experiments. Moreover, it\nreduces the confusion between the printers of same brand and model.\n",
    "topics": "{}",
    "score": 0.8295827159
  },
  {
    "id": "2009.13008",
    "title": "Visual Steering for One-Shot Deep Neural Network Synthesis",
    "abstract": "  Recent advancements in the area of deep learning have shown the effectiveness\nof very large neural networks in several applications. However, as these deep\nneural networks continue to grow in size, it becomes more and more difficult to\nconfigure their many parameters to obtain good results. Presently, analysts\nmust experiment with many different configurations and parameter settings,\nwhich is labor-intensive and time-consuming. On the other hand, the capacity of\nfully automated techniques for neural network architecture search is limited\nwithout the domain knowledge of human experts. To deal with the problem, we\nformulate the task of neural network architecture optimization as a graph space\nexploration, based on the one-shot architecture search technique. In this\napproach, a super-graph of all candidate architectures is trained in one-shot\nand the optimal neural network is identified as a sub-graph. In this paper, we\npresent a framework that allows analysts to effectively build the solution\nsub-graph space and guide the network search by injecting their domain\nknowledge. Starting with the network architecture space composed of basic\nneural network components, analysts are empowered to effectively select the\nmost promising components via our one-shot search scheme. Applying this\ntechnique in an iterative manner allows analysts to converge to the best\nperforming neural network architecture for a given application. During the\nexploration, analysts can use their domain knowledge aided by cues provided\nfrom a scatterplot visualization of the search space to edit different\ncomponents and guide the search for faster convergence. We designed our\ninterface in collaboration with several deep learning researchers and its final\neffectiveness is evaluated with a user study and two case studies.\n",
    "topics": "{'Neural Architecture Search': 0.6807065}",
    "score": 0.8295131489
  },
  {
    "id": "1611.08195",
    "title": "Domain Adaptation by Mixture of Alignments of Second- or Higher-Order\n  Scatter Tensors",
    "abstract": "  In this paper, we propose an approach to the domain adaptation, dubbed\nSecond- or Higher-order Transfer of Knowledge (So-HoT), based on the mixture of\nalignments of second- or higher-order scatter statistics between the source and\ntarget domains. The human ability to learn from few labeled samples is a\nrecurring motivation in the literature for domain adaptation. Towards this end,\nwe investigate the supervised target scenario for which few labeled target\ntraining samples per category exist. Specifically, we utilize two CNN streams:\nthe source and target networks fused at the classifier level. Features from the\nfully connected layers fc7 of each network are used to compute second- or even\nhigher-order scatter tensors; one per network stream per class. As the source\nand target distributions are somewhat different despite being related, we align\nthe scatters of the two network streams of the same class (within-class\nscatters) to a desired degree with our bespoke loss while maintaining good\nseparation of the between-class scatters. We train the entire network in\nend-to-end fashion. We provide evaluations on the standard Office benchmark\n(visual domains), RGB-D combined with Caltech256 (depth-to-rgb transfer) and\nPascal VOC2007 combined with the TU Berlin dataset (image-to-sketch transfer).\nWe attain state-of-the-art results.\n",
    "topics": "{'Domain Adaptation': 0.99935573}",
    "score": 0.8293787808
  },
  {
    "id": "2008.11560",
    "title": "Performance Optimization for Federated Person Re-identification via\n  Benchmark Analysis",
    "abstract": "  Federated learning is a privacy-preserving machine learning technique that\nlearns a shared model across decentralized clients. It can alleviate privacy\nconcerns of personal re-identification, an important computer vision task. In\nthis work, we implement federated learning to person re-identification\n(FedReID) and optimize its performance affected by statistical heterogeneity in\nthe real-world scenario. We first construct a new benchmark to investigate the\nperformance of FedReID. This benchmark consists of (1) nine datasets with\ndifferent volumes sourced from different domains to simulate the heterogeneous\nsituation in reality, (2) two federated scenarios, and (3) an enhanced\nfederated algorithm for FedReID. The benchmark analysis shows that the\nclient-edge-cloud architecture, represented by the federated-by-dataset\nscenario, has better performance than client-server architecture in FedReID. It\nalso reveals the bottlenecks of FedReID under the real-world scenario,\nincluding poor performance of large datasets caused by unbalanced weights in\nmodel aggregation and challenges in convergence. Then we propose two\noptimization methods: (1) To address the unbalanced weight problem, we propose\na new method to dynamically change the weights according to the scale of model\nchanges in clients in each training round; (2) To facilitate convergence, we\nadopt knowledge distillation to refine the server model with knowledge\ngenerated from client models on a public dataset. Experiment results\ndemonstrate that our strategies can achieve much better convergence with\nsuperior performance on all datasets. We believe that our work will inspire the\ncommunity to further explore the implementation of federated learning on more\ncomputer vision tasks in real-world scenarios.\n",
    "topics": "{'Federated Learning': 0.9999999, 'Person Re-Identification': 0.9999989}",
    "score": 0.8293440389
  },
  {
    "id": "1708.00973",
    "title": "Attention Transfer from Web Images for Video Recognition",
    "abstract": "  Training deep learning based video classifiers for action recognition\nrequires a large amount of labeled videos. The labeling process is\nlabor-intensive and time-consuming. On the other hand, large amount of\nweakly-labeled images are uploaded to the Internet by users everyday. To\nharness the rich and highly diverse set of Web images, a scalable approach is\nto crawl these images to train deep learning based classifier, such as\nConvolutional Neural Networks (CNN). However, due to the domain shift problem,\nthe performance of Web images trained deep classifiers tend to degrade when\ndirectly deployed to videos. One way to address this problem is to fine-tune\nthe trained models on videos, but sufficient amount of annotated videos are\nstill required. In this work, we propose a novel approach to transfer knowledge\nfrom image domain to video domain. The proposed method can adapt to the target\ndomain (i.e. video data) with limited amount of training data. Our method maps\nthe video frames into a low-dimensional feature space using the\nclass-discriminative spatial attention map for CNNs. We design a novel Siamese\nEnergyNet structure to learn energy functions on the attention maps by jointly\noptimizing two loss functions, such that the attention map corresponding to a\nground truth concept would have higher energy. We conduct extensive experiments\non two challenging video recognition datasets (i.e. TVHI and UCF101), and\ndemonstrate the efficacy of our proposed method.\n",
    "topics": "{'Video Recognition': 0.99992466, 'Action Recognition': 0.99906605, 'Temporal Action Localization': 0.9976121}",
    "score": 0.8293401031
  },
  {
    "id": "2008.09824",
    "title": "Self-Competitive Neural Networks",
    "abstract": "  Deep Neural Networks (DNNs) have improved the accuracy of classification\nproblems in lots of applications. One of the challenges in training a DNN is\nits need to be fed by an enriched dataset to increase its accuracy and avoid it\nsuffering from overfitting. One way to improve the generalization of DNNs is to\naugment the training data with new synthesized adversarial samples. Recently,\nresearchers have worked extensively to propose methods for data augmentation.\nIn this paper, we generate adversarial samples to refine the Domains of\nAttraction (DoAs) of each class. In this approach, at each stage, we use the\nmodel learned by the primary and generated adversarial data (up to that stage)\nto manipulate the primary data in a way that look complicated to the DNN. The\nDNN is then retrained using the augmented data and then it again generates\nadversarial data that are hard to predict for itself. As the DNN tries to\nimprove its accuracy by competing with itself (generating hard samples and then\nlearning them), the technique is called Self-Competitive Neural Network (SCNN).\nTo generate such samples, we pose the problem as an optimization task, where\nthe network weights are fixed and use a gradient descent based method to\nsynthesize adversarial samples that are on the boundary of their true labels\nand the nearest wrong labels. Our experimental results show that data\naugmentation using SCNNs can significantly increase the accuracy of the\noriginal network. As an example, we can mention improving the accuracy of a CNN\ntrained with 1000 limited training data of MNIST dataset from 94.26% to 98.25%.\n",
    "topics": "{'Data Augmentation': 0.99993}",
    "score": 0.8292073853
  },
  {
    "id": "1804.07814",
    "title": "A Deep Representation Empowered Distant Supervision Paradigm for\n  Clinical Information Extraction",
    "abstract": "  Objective: To automatically create large labeled training datasets and reduce\nthe efforts of feature engineering for training accurate machine learning\nmodels for clinical information extraction. Materials and Methods: We propose a\ndistant supervision paradigm empowered by deep representation for extracting\ninformation from clinical text. In this paradigm, the rule-based NLP algorithms\nare utilized to generate weak labels and create large training datasets\nautomatically. Additionally, we use pre-trained word embeddings as deep\nrepresentation to eliminate the need of task-specific feature engineering for\nmachine learning. We evaluated the effectiveness of the proposed paradigm on\ntwo clinical information extraction tasks: smoking status extraction and\nproximal femur (hip) fracture extraction. We tested three prevalent machine\nlearning models, namely, Convolutional Neural Networks (CNN), Support Vector\nMachine (SVM), and Random Forrest (RF). Results: The results indicate that CNN\nis the best fit to the proposed distant supervision paradigm. It outperforms\nthe rule-based NLP algorithms given large datasets by capturing additional\nextraction patterns. We also verified the advantage of word embedding feature\nrepresentation in the paradigm over term frequency-inverse document frequency\n(tf-idf) and topic modeling representations. Discussion: In the clinical\ndomain, the limited amount of labeled data is always a bottleneck for applying\nmachine learning. Additionally, the performance of machine learning approaches\nhighly depends on task-specific feature engineering. The proposed paradigm\ncould alleviate those problems by leveraging rule-based NLP algorithms to\nautomatically assign weak labels and eliminating the need of task-specific\nfeature engineering using word embedding feature representation.\n",
    "topics": "{'Feature Engineering': 1.0, 'Word Embeddings': 0.8511241}",
    "score": 0.829151589
  },
  {
    "id": "1903.02306",
    "title": "Video-based surgical skill assessment using 3D convolutional neural\n  networks",
    "abstract": "  Purpose: A profound education of novice surgeons is crucial to ensure that\nsurgical interventions are effective and safe. One important aspect is the\nteaching of technical skills for minimally invasive or robot-assisted\nprocedures. This includes the objective and preferably automatic assessment of\nsurgical skill. Recent studies presented good results for automatic, objective\nskill evaluation by collecting and analyzing motion data such as trajectories\nof surgical instruments. However, obtaining the motion data generally requires\nadditional equipment for instrument tracking or the availability of a robotic\nsurgery system to capture kinematic data. In contrast, we investigate a method\nfor automatic, objective skill assessment that requires video data only. This\nhas the advantage that video can be collected effortlessly during minimally\ninvasive and robot-assisted training scenarios.\n  Methods: Our method builds on recent advances in deep learning-based video\nclassification. Specifically, we propose to use an inflated 3D ConvNet to\nclassify snippets, i.e., stacks of a few consecutive frames, extracted from\nsurgical video. The network is extended into a Temporal Segment Network during\ntraining.\n  Results: We evaluate the method on the publicly available JIGSAWS dataset,\nwhich consists of recordings of basic robot-assisted surgery tasks performed on\na dry lab bench-top model. Our approach achieves high skill classification\naccuracies ranging from 95.1% to 100.0%.\n  Conclusions: Our results demonstrate the feasibility of deep learning-based\nassessment of technical skill from surgical video. Notably, the 3D ConvNet is\nable to learn meaningful patterns directly from the data, alleviating the need\nfor manual feature engineering. Further evaluation will require more annotated\ndata for training and testing.\n",
    "topics": "{'Video Classification': 0.9980191, 'Feature Engineering': 0.9781352}",
    "score": 0.829031008
  },
  {
    "id": "1901.02757",
    "title": "How Compact?: Assessing Compactness of Representations through\n  Layer-Wise Pruning",
    "abstract": "  Various forms of representations may arise in the many layers embedded in\ndeep neural networks (DNNs). Of these, where can we find the most compact\nrepresentation? We propose to use a pruning framework to answer this question:\nHow compact can each layer be compressed, without losing performance? Most of\nthe existing DNN compression methods do not consider the relative\ncompressibility of the individual layers. They uniformly apply a single target\nsparsity to all layers or adapt layer sparsity using heuristics and additional\ntraining. We propose a principled method that automatically determines the\nsparsity of individual layers derived from the importance of each layer. To do\nthis, we consider a metric to measure the importance of each layer based on the\nlayer-wise capacity. Given the trained model and the total target sparsity, we\nfirst evaluate the importance of each layer from the model. From the evaluated\nimportance, we compute the layer-wise sparsity of each layer. The proposed\nmethod can be applied to any DNN architecture and can be combined with any\npruning method that takes the total target sparsity as a parameter. To validate\nthe proposed method, we carried out an image classification task with two types\nof DNN architectures on two benchmark datasets and used three pruning methods\nfor compression. In case of VGG-16 model with weight pruning on the ImageNet\ndataset, we achieved up to 75% (17.5% on average) better top-5 accuracy than\nthe baseline under the same total target sparsity. Furthermore, we analyzed\nwhere the maximum compression can occur in the network. This kind of analysis\ncan help us identify the most compact representation within a deep neural\nnetwork.\n",
    "topics": "{'Image Classification': 0.9885041}",
    "score": 0.8288766649
  },
  {
    "id": "1906.01792",
    "title": "PAC-GAN: An Effective Pose Augmentation Scheme for Unsupervised\n  Cross-View Person Re-identification",
    "abstract": "  Person re-identification (person Re-Id) aims to retrieve the pedestrian\nimages of a same person that captured by disjoint and non-overlapping cameras.\nLots of researchers recently focuse on this hot issue and propose deep learning\nbased methods to enhance the recognition rate in a supervised or unsupervised\nmanner. However, two limitations that cannot be ignored: firstly, compared with\nother image retrieval benchmarks, the size of existing person Re-Id datasets\nare far from meeting the requirement, which cannot provide sufficient\npedestrian samples for the training of deep model; secondly, the samples in\nexisting datasets do not have sufficient human motions or postures coverage to\nprovide more priori knowledges for learning. In this paper, we introduce a\nnovel unsupervised pose augmentation cross-view person Re-Id scheme called\nPAC-GAN to overcome these limitations. We firstly present the formal definition\nof cross-view pose augmentation and then propose the framework of PAC-GAN that\nis a novel conditional generative adversarial network (CGAN) based approach to\nimprove the performance of unsupervised corss-view person Re-Id. Specifically,\nThe pose generation model in PAC-GAN called CPG-Net is to generate enough\nquantity of pose-rich samples from original image and skeleton samples. The\npose augmentation dataset is produced by combining the synthesized pose-rich\nsamples with the original samples, which is fed into the corss-view person\nRe-Id model named Cross-GAN. Besides, we use weight-sharing strategy in the\nCPG-Net to improve the quality of new generated samples. To the best of our\nknowledge, we are the first try to enhance the unsupervised cross-view person\nRe-Id by pose augmentation, and the results of extensive experiments show that\nthe proposed scheme can combat the state-of-the-arts.\n",
    "topics": "{'Person Re-Identification': 0.9999994}",
    "score": 0.8287488212
  },
  {
    "id": "1904.01740",
    "title": "FaceQnet: Quality Assessment for Face Recognition based on Deep Learning",
    "abstract": "  In this paper we develop a Quality Assessment approach for face recognition\nbased on deep learning. The method consists of a Convolutional Neural Network,\nFaceQnet, that is used to predict the suitability of a specific input image for\nface recognition purposes. The training of FaceQnet is done using the VGGFace2\ndatabase. We employ the BioLab-ICAO framework for labeling the VGGFace2 images\nwith quality information related to their ICAO compliance level. The\ngroundtruth quality labels are obtained using FaceNet to generate comparison\nscores. We employ the groundtruth data to fine-tune a ResNet-based CNN, making\nit capable of returning a numerical quality measure for each input image.\nFinally, we verify if the FaceQnet scores are suitable to predict the expected\nperformance when employing a specific image for face recognition with a COTS\nface recognition system. Several conclusions can be drawn from this work, most\nnotably: 1) we managed to employ an existing ICAO compliance framework and a\npretrained CNN to automatically label data with quality information, 2) we\ntrained FaceQnet for quality estimation by fine-tuning a pre-trained face\nrecognition network (ResNet-50), and 3) we have shown that the predictions from\nFaceQnet are highly correlated with the face recognition accuracy of a\nstate-of-the-art commercial system not used during development. FaceQnet is\npublicly available in GitHub.\n",
    "topics": "{'Face Recognition': 1.0}",
    "score": 0.8287340146
  },
  {
    "id": "1909.07689",
    "title": "Prediction of rare feature combinations in population synthesis:\n  Application of deep generative modelling",
    "abstract": "  In population synthesis applications, when considering populations with many\nattributes, a fundamental problem is the estimation of rare combinations of\nfeature attributes. Unsurprisingly, it is notably more difficult to reliably\nrepresentthe sparser regions of such multivariate distributions and in\nparticular combinations of attributes which are absent from the original\nsample. In the literature this is commonly known as sampling zeros for which no\nsystematic solution has been proposed so far. In this paper, two machine\nlearning algorithms, from the family of deep generative models,are proposed for\nthe problem of population synthesis and with particular attention to the\nproblem of sampling zeros. Specifically, we introduce the Wasserstein\nGenerative Adversarial Network (WGAN) and the Variational Autoencoder(VAE), and\nadapt these algorithms for a large-scale population synthesis application. The\nmodels are implemented on a Danish travel survey with a feature-space of more\nthan 60 variables. The models are validated in a cross-validation scheme and a\nset of new metrics for the evaluation of the sampling-zero problem is proposed.\nResults show how these models are able to recover sampling zeros while keeping\nthe estimation of truly impossible combinations, the structural zeros, at a\ncomparatively low level. Particularly, for a low dimensional experiment, the\nVAE, the marginal sampler and the fully random sampler generate 5%, 21% and\n26%, respectively, more structural zeros per sampling zero generated by the\nWGAN, while for a high dimensional case, these figures escalate to 44%, 2217%\nand 170440%, respectively. This research directly supports the development of\nagent-based systems and in particular cases where detailed socio-economic or\ngeographical representations are required.\n",
    "topics": "{}",
    "score": 0.8287122183
  },
  {
    "id": "2006.15940",
    "title": "Adversarial Multi-Source Transfer Learning in Healthcare: Application to\n  Glucose Prediction for Diabetic People",
    "abstract": "  Deep learning has yet to revolutionize general practices in healthcare,\ndespite promising results for some specific tasks. This is partly due to data\nbeing in insufficient quantities hurting the training of the models. To address\nthis issue, data from multiple health actors or patients could be combined by\ncapitalizing on their heterogeneity through the use of transfer learning.\n  To improve the quality of the transfer between multiple sources of data, we\npropose a multi-source adversarial transfer learning framework that enables the\nlearning of a feature representation that is similar across the sources, and\nthus more general and more easily transferable. We apply this idea to glucose\nforecasting for diabetic people using a fully convolutional neural network. The\nevaluation is done by exploring various transfer scenarios with three datasets\ncharacterized by their high inter and intra variability.\n  While transferring knowledge is beneficial in general, we show that the\nstatistical and clinical accuracies can be further improved by using of the\nadversarial training methodology, surpassing the current state-of-the-art\nresults. In particular, it shines when using data from different datasets, or\nwhen there is too little data in an intra-dataset situation. To understand the\nbehavior of the models, we analyze the learnt feature representations and\npropose a new metric in this regard. Contrary to a standard transfer, the\nadversarial transfer does not discriminate the patients and datasets, helping\nthe learning of a more general feature representation.\n  The adversarial training framework improves the learning of a general feature\nrepresentation in a multi-source environment, enhancing the knowledge transfer\nto an unseen target.\n  The proposed method can help improve the efficiency of data shared by\ndifferent health actors in the training of deep models.\n",
    "topics": "{'Transfer Learning': 0.99999976}",
    "score": 0.8284576771
  },
  {
    "id": "2006.16633",
    "title": "Primary Tumor Origin Classification of Lung Nodules in Spectral CT using\n  Transfer Learning",
    "abstract": "  Early detection of lung cancer has been proven to decrease mortality\nsignificantly. A recent development in computed tomography (CT), spectral CT,\ncan potentially improve diagnostic accuracy, as it yields more information per\nscan than regular CT. However, the shear workload involved with analyzing a\nlarge number of scans drives the need for automated diagnosis methods.\nTherefore, we propose a detection and classification system for lung nodules in\nCT scans. Furthermore, we want to observe whether spectral images can increase\nclassifier performance. For the detection of nodules we trained a VGG-like 3D\nconvolutional neural net (CNN). To obtain a primary tumor classifier for our\ndataset we pre-trained a 3D CNN with similar architecture on nodule\nmalignancies of a large publicly available dataset, the LIDC-IDRI dataset.\nSubsequently we used this pre-trained network as feature extractor for the\nnodules in our dataset. The resulting feature vectors were classified into two\n(benign/malignant) and three (benign/primary lung cancer/metastases) classes\nusing support vector machine (SVM). This classification was performed both on\nnodule- and scan-level. We obtained state-of-the art performance for detection\nand malignancy regression on the LIDC-IDRI database. Classification performance\non our own dataset was higher for scan- than for nodule-level predictions. For\nthe three-class scan-level classification we obtained an accuracy of 78\\%.\nSpectral features did increase classifier performance, but not significantly.\nOur work suggests that a pre-trained feature extractor can be used as primary\ntumor origin classifier for lung nodules, eliminating the need for elaborate\nfine-tuning of a new network and large datasets. Code is available at\n\\url{https://github.com/tueimage/lung-nodule-msc-2018}.\n",
    "topics": "{'Computed Tomography (CT)': 0.99950457, 'Transfer Learning': 0.9324879}",
    "score": 0.8284201161
  },
  {
    "id": "1803.09861",
    "title": "A Classification Model for Sensing Human Trust in Machines Using EEG and\n  GSR",
    "abstract": "  Today, intelligent machines \\emph{interact and collaborate} with humans in a\nway that demands a greater level of trust between human and machine. A first\nstep towards building intelligent machines that are capable of building and\nmaintaining trust with humans is the design of a sensor that will enable\nmachines to estimate human trust level in real-time. In this paper, two\napproaches for developing classifier-based empirical trust sensor models are\npresented that specifically use electroencephalography (EEG) and galvanic skin\nresponse (GSR) measurements. Human subject data collected from 45 participants\nis used for feature extraction, feature selection, classifier training, and\nmodel validation. The first approach considers a general set of\npsychophysiological features across all participants as the input variables and\ntrains a classifier-based model for each participant, resulting in a trust\nsensor model based on the general feature set (i.e., a \"general trust sensor\nmodel\"). The second approach considers a customized feature set for each\nindividual and trains a classifier-based model using that feature set,\nresulting in improved mean accuracy but at the expense of an increase in\ntraining time. This work represents the first use of real-time\npsychophysiological measurements for the development of a human trust sensor.\nImplications of the work, in the context of trust management algorithm design\nfor intelligent machines, are also discussed.\n",
    "topics": "{'EEG': 0.99889344, 'Feature Selection': 0.7415836}",
    "score": 0.8284090645
  },
  {
    "id": "1910.06748",
    "title": "Language Identification on Massive Datasets of Short Message using an\n  Attention Mechanism CNN",
    "abstract": "  Language Identification (LID) is a challenging task, especially when the\ninput texts are short and noisy such as posts and statuses on social media or\nchat logs on gaming forums. The task has been tackled by either designing a\nfeature set for a traditional classifier (e.g. Naive Bayes) or applying a deep\nneural network classifier (e.g. Bi-directional Gated Recurrent Unit,\nEncoder-Decoder). These methods are usually trained and tested on a huge amount\nof private data, then used and evaluated as off-the-shelf packages by other\nresearchers using their own datasets, and consequently the various results\npublished are not directly comparable. In this paper, we first create a new\nmassive labelled dataset based on one year of Twitter data. We use this dataset\nto test several existing language identification systems, in order to obtain a\nset of coherent benchmarks, and we make our dataset publicly available so that\nothers can add to this set of benchmarks. Finally, we propose a shallow but\nefficient neural LID system, which is a ngram-regional convolution neural\nnetwork enhanced with an attention mechanism. Experimental results show that\nour architecture is able to predict tens of thousands of samples per second and\nsurpasses all state-of-the-art systems with an improvement of 5%.\n",
    "topics": "{'Language Identification': 1.0}",
    "score": 0.8283582204
  },
  {
    "id": "2009.00675",
    "title": "Applying a random projection algorithm to optimize machine learning\n  model for predicting peritoneal metastasis in gastric cancer patients using\n  CT images",
    "abstract": "  Background and Objective: Non-invasively predicting the risk of cancer\nmetastasis before surgery plays an essential role in determining optimal\ntreatment methods for cancer patients (including who can benefit from\nneoadjuvant chemotherapy). Although developing radiomics based machine learning\n(ML) models has attracted broad research interest for this purpose, it often\nfaces a challenge of how to build a highly performed and robust ML model using\nsmall and imbalanced image datasets. Methods: In this study, we explore a new\napproach to build an optimal ML model. A retrospective dataset involving\nabdominal computed tomography (CT) images acquired from 159 patients diagnosed\nwith gastric cancer is assembled. Among them, 121 cases have peritoneal\nmetastasis (PM), while 38 cases do not have PM. A computer-aided detection\n(CAD) scheme is first applied to segment primary gastric tumor volumes and\ninitially computes 315 image features. Then, two Gradient Boosting Machine\n(GBM) models embedded with two different feature dimensionality reduction\nmethods, namely, the principal component analysis (PCA) and a random projection\nalgorithm (RPA) and a synthetic minority oversampling technique, are built to\npredict the risk of the patients having PM. All GBM models are trained and\ntested using a leave-one-case-out cross-validation method. Results: Results\nshow that the GBM embedded with RPA yielded a significantly higher prediction\naccuracy (71.2%) than using PCA (65.2%) (p<0.05). Conclusions: The study\ndemonstrated that CT images of the primary gastric tumors contain\ndiscriminatory information to predict the risk of PM, and RPA is a promising\nmethod to generate optimal feature vector, improving the performance of ML\nmodels of medical images.\n",
    "topics": "{'Computed Tomography (CT)': 0.9999839, 'Dimensionality Reduction': 0.996283}",
    "score": 0.8282990218
  },
  {
    "id": "1611.06474",
    "title": "Nazr-CNN: Fine-Grained Classification of UAV Imagery for Damage\n  Assessment",
    "abstract": "  We propose Nazr-CNN1, a deep learning pipeline for object detection and\nfine-grained classification in images acquired from Unmanned Aerial Vehicles\n(UAVs) for damage assessment and monitoring. Nazr-CNN consists of two\ncomponents. The function of the first component is to localize objects (e.g.\nhouses or infrastructure) in an image by carrying out a pixel-level\nclassification. In the second component, a hidden layer of a Convolutional\nNeural Network (CNN) is used to encode Fisher Vectors (FV) of the segments\ngenerated from the first component in order to help discriminate between\ndifferent levels of damage. To showcase our approach we use data from UAVs that\nwere deployed to assess the level of damage in the aftermath of a devastating\ncyclone that hit the island of Vanuatu in 2015. The collected images were\nlabeled by a crowdsourcing effort and the labeling categories consisted of\nfine-grained levels of damage to built structures. Since our data set is\nrelatively small, a pre- trained network for pixel-level classification and FV\nencoding was used. Nazr-CNN attains promising results both for object detection\nand damage assessment suggesting that the integrated pipeline is robust in the\nface of small data sets and labeling errors by annotators. While the focus of\nNazr-CNN is on assessment of UAV images in a post-disaster scenario, our\nsolution is general and can be applied in many diverse settings. We show one\nsuch case of transfer learning to assess the level of damage in aerial images\ncollected after a typhoon in Philippines.\n",
    "topics": "{'Small Data Image Classification': 0.9999732, 'Object Detection': 0.999204, 'Transfer Learning': 0.65075445}",
    "score": 0.8282930886
  },
  {
    "id": "1909.08118",
    "title": "Physics-guided Convolutional Neural Network (PhyCNN) for Data-driven\n  Seismic Response Modeling",
    "abstract": "  Seismic events, among many other natural hazards, reduce due functionality\nand exacerbate vulnerability of in-service buildings. Accurate modeling and\nprediction of building's response subjected to earthquakes makes possible to\nevaluate building performance. To this end, we leverage the recent advances in\ndeep learning and develop a physics-guided convolutional neural network\n(PhyCNN) framework for data-driven seismic response modeling and serviceability\nassessment of buildings. The proposed PhyCNN approach is capable of accurately\npredicting building's seismic response in a data-driven fashion without the\nneed of a physics-based analytical/numerical model. The basic concept is to\ntrain a deep PhyCNN model based on available seismic input-output datasets\n(e.g., from simulation or sensing) and physics constraints. The trained PhyCNN\ncan then used as a surrogate model for structural seismic response prediction.\nAvailable physics (e.g., the law of dynamics) can provide constraints to the\nnetwork outputs, alleviate overfitting issues, reduce the need of big training\ndatasets, and thus improve the robustness of the trained model for more\nreliable prediction. The trained surrogate model is then utilized for fragility\nanalysis given certain limit state criteria (e.g., the serviceability state).\nIn addition, an unsupervised learning algorithm based on K-means clustering is\nalso proposed to partition the limited number of datasets to training,\nvalidation and prediction categories, so as to maximize the use of limited\ndatasets. The performance of the proposed approach is demonstrated through\nthree case studies including both numerical and experimental examples.\nConvincing results illustrate that the proposed PhyCNN paradigm outperforms\nconventional pure data-based neural networks.\n",
    "topics": "{'Language Modelling': 0.33505863}",
    "score": 0.8282664316
  },
  {
    "id": "1904.12546",
    "title": "ConvTimeNet: A Pre-trained Deep Convolutional Neural Network for Time\n  Series Classification",
    "abstract": "  Training deep neural networks often requires careful hyper-parameter tuning\nand significant computational resources. In this paper, we propose ConvTimeNet\n(CTN): an off-the-shelf deep convolutional neural network (CNN) trained on\ndiverse univariate time series classification (TSC) source tasks. Once trained,\nCTN can be easily adapted to new TSC target tasks via a small amount of\nfine-tuning using labeled instances from the target tasks. We note that the\nlength of convolutional filters is a key aspect when building a pre-trained\nmodel that can generalize to time series of different lengths across datasets.\nTo achieve this, we incorporate filters of multiple lengths in all\nconvolutional layers of CTN to capture temporal features at multiple time\nscales. We consider all 65 datasets with time series of lengths up to 512\npoints from the UCR TSC Benchmark for training and testing transferability of\nCTN: We train CTN on a randomly chosen subset of 24 datasets using a multi-head\napproach with a different softmax layer for each training dataset, and study\ngeneralizability and transferability of the learned filters on the remaining 41\nTSC datasets. We observe significant gains in classification accuracy as well\nas computational efficiency when using pre-trained CTN as a starting point for\nsubsequent task-specific fine-tuning compared to existing state-of-the-art TSC\napproaches. We also provide qualitative insights into the working of CTN by: i)\nanalyzing the activations and filters of first convolution layer suggesting the\nfilters in CTN are generically useful, ii) analyzing the impact of the design\ndecision to incorporate multiple length decisions, and iii) finding regions of\ntime series that affect the final classification decision via occlusion\nsensitivity analysis.\n",
    "topics": "{'Time Series Classification': 1.0, 'Time Series': 0.9995222}",
    "score": 0.8282498525
  },
  {
    "id": "1610.01119",
    "title": "Knowledge Guided Disambiguation for Large-Scale Scene Classification\n  with Multi-Resolution CNNs",
    "abstract": "  Convolutional Neural Networks (CNNs) have made remarkable progress on scene\nrecognition, partially due to these recent large-scale scene datasets, such as\nthe Places and Places2. Scene categories are often defined by multi-level\ninformation, including local objects, global layout, and background\nenvironment, thus leading to large intra-class variations. In addition, with\nthe increasing number of scene categories, label ambiguity has become another\ncrucial issue in large-scale classification. This paper focuses on large-scale\nscene recognition and makes two major contributions to tackle these issues.\nFirst, we propose a multi-resolution CNN architecture that captures visual\ncontent and structure at multiple levels. The multi-resolution CNNs are\ncomposed of coarse resolution CNNs and fine resolution CNNs, which are\ncomplementary to each other. Second, we design two knowledge guided\ndisambiguation techniques to deal with the problem of label ambiguity. (i) We\nexploit the knowledge from the confusion matrix computed on validation data to\nmerge ambiguous classes into a super category. (ii) We utilize the knowledge of\nextra networks to produce a soft label for each image. Then the super\ncategories or soft labels are employed to guide CNN training on the Places2. We\nconduct extensive experiments on three large-scale image datasets (ImageNet,\nPlaces, and Places2), demonstrating the effectiveness of our approach.\nFurthermore, our method takes part in two major scene recognition challenges,\nand achieves the second place at the Places2 challenge in ILSVRC 2015, and the\nfirst place at the LSUN challenge in CVPR 2016. Finally, we directly test the\nlearned representations on other scene benchmarks, and obtain the new\nstate-of-the-art results on the MIT Indoor67 (86.7\\%) and SUN397 (72.0\\%). We\nrelease the code and models\nat~\\url{https://github.com/wanglimin/MRCNN-Scene-Recognition}.\n",
    "topics": "{'Scene Recognition': 1.0, 'Scene Classification': 0.9999976}",
    "score": 0.8282196093
  },
  {
    "id": "1907.01102",
    "title": "Nature Inspired Dimensional Reduction Technique for Fast and Invariant\n  Visual Feature Extraction",
    "abstract": "  Fast and invariant feature extraction is crucial in certain computer vision\napplications where the computation time is constrained in both training and\ntesting phases of the classifier. In this paper, we propose a nature-inspired\ndimensionality reduction technique for fast and invariant visual feature\nextraction. The human brain can exchange the spatial and spectral resolution to\nreconstruct missing colors in visual perception. The phenomenon is widely used\nin the printing industry to reduce the number of colors used to print, through\na technique, called color dithering. In this work, we adopt a fast\nerror-diffusion color dithering algorithm to reduce the spectral resolution and\nextract salient features by employing novel Hessian matrix analysis technique,\nwhich is then described by a spatial-chromatic histogram. The computation time,\ndescriptor dimensionality and classification performance of the proposed\nfeature are assessed under drastic variances in orientation, viewing angle and\nillumination of objects comparing with several different state-of-the-art\nhandcrafted and deep-learned features. Extensive experiments on two publicly\navailable object datasets, coil-100 and ALOI carried on both a desktop PC and a\nRaspberry Pi device show multiple advantages of using the proposed approach,\nsuch as the lower computation time, high robustness, and comparable\nclassification accuracy under weakly supervised environment. Further, it showed\nthe capability of operating solely inside a conventional SoC device utilizing a\nsmall fraction of the available hardware resources.\n",
    "topics": "{'Dimensionality Reduction': 0.99986005}",
    "score": 0.8281527127
  },
  {
    "id": "1909.09945",
    "title": "To What Extent Does Downsampling, Compression, and Data Scarcity Impact\n  Renal Image Analysis?",
    "abstract": "  The condition of the Glomeruli, or filter sacks, in renal Direct\nImmunofluorescence (DIF) specimens is a critical indicator for diagnosing\nkidney diseases. A digital pathology system which digitizes a glass histology\nslide into a Whole Slide Image (WSI) and then automatically detects and zooms\nin on the glomeruli with a higher magnification objective will be extremely\nhelpful for pathologists. In this paper, using glomerulus detection as the\nstudy case, we provide analysis and observations on several important issues to\nhelp with the development of Computer Aided Diagnostic (CAD) systems to process\nWSIs. Large image resolution, large file size, and data scarcity are always\nchallenging to deal with. To this end, we first examine image downsampling\nrates in terms of their effect on detection accuracy. Second, we examine the\nimpact of image compression. Third, we examine the relationship between the\nsize of the training set and detection accuracy. To understand the above\nissues, experiments are performed on the state-of-the-art detectors: Faster\nR-CNN, R-FCN, Mask R-CNN and SSD. Critical findings are observed: (1) The best\nbalance between detection accuracy, detection speed and file size is achieved\nat 8 times downsampling captured with a $40\\times$ objective; (2) compression\nwhich reduces the file size dramatically, does not necessarily have an adverse\neffect on overall accuracy; (3) reducing the amount of training data to some\nextents causes a drop in precision but has a negligible impact on the recall;\n(4) in most cases, Faster R-CNN achieves the best accuracy in the glomerulus\ndetection task. We show that the image file size of $40\\times$ WSI images can\nbe reduced by a factor of over 6000 with negligible loss of glomerulus\ndetection accuracy.\n",
    "topics": "{'Image Compression': 0.99999976}",
    "score": 0.8279653511
  },
  {
    "id": "1709.06429",
    "title": "Neural Networks for Text Correction and Completion in Keyboard Decoding",
    "abstract": "  Despite the ubiquity of mobile and wearable text messaging applications, the\nproblem of keyboard text decoding is not tackled sufficiently in the light of\nthe enormous success of the deep learning Recurrent Neural Network (RNN) and\nConvolutional Neural Networks (CNN) for natural language understanding. In\nparticular, considering that the keyboard decoders should operate on devices\nwith memory and processor resource constraints, makes it challenging to deploy\nindustrial scale deep neural network (DNN) models. This paper proposes a\nsequence-to-sequence neural attention network system for automatic text\ncorrection and completion. Given an erroneous sequence, our model encodes\ncharacter level hidden representations and then decodes the revised sequence\nthus enabling auto-correction and completion. We achieve this by a combination\nof character level CNN and gated recurrent unit (GRU) encoder along with and a\nword level gated recurrent unit (GRU) attention decoder. Unlike traditional\nlanguage models that learn from billions of words, our corpus size is only 12\nmillion words; an order of magnitude smaller. The memory footprint of our\nlearnt model for inference and prediction is also an order of magnitude smaller\nthan the conventional language model based text decoders. We report baseline\nperformance for neural keyboard decoders in such limited domain. Our models\nachieve a word level accuracy of $90\\%$ and a character error rate CER of\n$2.4\\%$ over the Twitter typo dataset. We present a novel dataset of noisy to\ncorrected mappings by inducing the noise distribution from the Twitter data\nover the OpenSubtitles 2009 dataset; on which our model predicts with a word\nlevel accuracy of $98\\%$ and sequence accuracy of $68.9\\%$. In our user study,\nour model achieved an average CER of $2.6\\%$ with the state-of-the-art\nnon-neural touch-screen keyboard decoder at CER of $1.6\\%$.\n",
    "topics": "{'Natural Language Understanding': 0.9999528, 'Language Modelling': 0.956772}",
    "score": 0.827890926
  },
  {
    "id": "1807.06081",
    "title": "A Dataset of Laryngeal Endoscopic Images with Comparative Study on\n  Convolution Neural Network Based Semantic Segmentation",
    "abstract": "  Purpose Automated segmentation of anatomical structures in medical image\nanalysis is a prerequisite for autonomous diagnosis as well as various computer\nand robot aided interventions. Recent methods based on deep convolutional\nneural networks (CNN) have outperformed former heuristic methods. However,\nthose methods were primarily evaluated on rigid, real-world environments. In\nthis study, existing segmentation methods were evaluated for their use on a new\ndataset of transoral endoscopic exploration. Methods Four machine learning\nbased methods SegNet, UNet, ENet and ErfNet were trained with supervision on a\nnovel 7-class dataset of the human larynx. The dataset contains 536 manually\nsegmented images from two patients during laser incisions. The\nIntersection-over-Union (IoU) evaluation metric was used to measure the\naccuracy of each method. Data augmentation and network ensembling were employed\nto increase segmentation accuracy. Stochastic inference was used to show\nuncertainties of the individual models. Patient-to-patient transfer was\ninvestigated using patient-specific fine-tuning. Results In this study, a\nweighted average ensemble network of UNet and ErfNet was best suited for the\nsegmentation of laryngeal soft tissue with a mean IoU of 84.7 %. The highest\nefficiency was achieved by ENet with a mean inference time of 9.22 ms per\nimage. It is shown that 10 additional images from a new patient are sufficient\nfor patient-specific fine-tuning. Conclusion CNN-based methods for semantic\nsegmentation are applicable to endoscopic images of laryngeal soft tissue. The\nsegmentation can be used for active constraints or to monitor morphological\nchanges and autonomously detect pathologies. Further improvements could be\nachieved by using a larger dataset or training the models in a self-supervised\nmanner on additional unlabeled data.\n",
    "topics": "{'Semantic Segmentation': 0.9990664, 'Data Augmentation': 0.99379104, 'Medical Image Segmentation': 0.96666294}",
    "score": 0.8278569934
  },
  {
    "id": "1802.03584",
    "title": "Joint Learning for Pulmonary Nodule Segmentation, Attributes and\n  Malignancy Prediction",
    "abstract": "  Refer to the literature of lung nodule classification, many studies adopt\nConvolutional Neural Networks (CNN) to directly predict the malignancy of lung\nnodules with original thoracic Computed Tomography (CT) and nodule location.\nHowever, these studies cannot tell how the CNN works in terms of predicting the\nmalignancy of the given nodule, e.g., it's hard to conclude that whether the\nregion within the nodule or the contextual information matters according to the\noutput of the CNN. In this paper, we propose an interpretable and multi-task\nlearning CNN -- Joint learning for \\textbf{P}ulmonary \\textbf{N}odule\n\\textbf{S}egmentation \\textbf{A}ttributes and \\textbf{M}alignancy\n\\textbf{P}rediction (PN-SAMP). It is able to not only accurately predict the\nmalignancy of lung nodules, but also provide semantic high-level attributes as\nwell as the areas of detected nodules. Moreover, the combination of nodule\nsegmentation, attributes and malignancy prediction is helpful to improve the\nperformance of each single task. In addition, inspired by the fact that\nradiologists often change window widths and window centers to help to make\ndecision on uncertain nodules, PN-SAMP mixes multiple WW/WC together to gain\ninformation for the raw CT input images. To verify the effectiveness of the\nproposed method, the evaluation is implemented on the public LIDC-IDRI dataset,\nwhich is one of the largest dataset for lung nodule malignancy prediction.\nExperiments indicate that the proposed PN-SAMP achieves significant improvement\nwith respect to lung nodule classification, and promising performance on lung\nnodule segmentation and attribute learning, compared with the-state-of-the-art\nmethods.\n",
    "topics": "{'Computed Tomography (CT)': 0.9999682, 'Multi-Task Learning': 0.9981933}",
    "score": 0.8277271326
  },
  {
    "id": "2004.12084",
    "title": "POCOVID-Net: Automatic Detection of COVID-19 From a New Lung Ultrasound\n  Imaging Dataset (POCUS)",
    "abstract": "  With the rapid development of COVID-19 into a global pandemic, there is an\never more urgent need for cheap, fast and reliable tools that can assist\nphysicians in diagnosing COVID-19. Medical imaging such as CT can take a key\nrole in complementing conventional diagnostic tools from molecular biology,\nand, using deep learning techniques, several automatic systems were\ndemonstrated promising performances using CT or X-ray data. Here, we advocate a\nmore prominent role of point-of-care ultrasound imaging to guide COVID-19\ndetection. Ultrasound is non-invasive and ubiquitous in medical facilities\naround the globe. Our contribution is threefold. First, we gather a lung\nultrasound (POCUS) dataset consisting of (currently) 1103 images (654 COVID-19,\n277 bacterial pneumonia and 172 healthy controls), sampled from 64 videos.\nWhile this dataset was assembled from various online sources and is by no means\nexhaustive, it was processed specifically to feed deep learning models and is\nintended to serve as a starting point for an open-access initiative. Second, we\ntrain a deep convolutional neural network (POCOVID-Net) on this 3-class dataset\nand achieve an accuracy of 89% and, by a majority vote, a video accuracy of 92%\n. For detecting COVID-19 in particular, the model performs with a sensitivity\nof 0.96, a specificity of 0.79 and F1-score of 0.92 in a 5-fold cross\nvalidation. Third, we provide an open-access web service (POCOVIDScreen) that\nis available at: https://pocovidscreen.org. The website deploys the predictive\nmodel, allowing to perform predictions on ultrasound lung images. In addition,\nit grants medical staff the option to (bulk) upload their own screenings in\norder to contribute to the growing public database of pathological lung\nultrasound images.\n  Dataset and code are available from:\nhttps://github.com/jannisborn/covid19_pocus_ultrasound\n",
    "topics": "{}",
    "score": 0.8275641147
  },
  {
    "id": "1701.03555",
    "title": "Active Self-Paced Learning for Cost-Effective and Progressive Face\n  Identification",
    "abstract": "  This paper aims to develop a novel cost-effective framework for face\nidentification, which progressively maintains a batch of classifiers with the\nincreasing face images of different individuals. By naturally combining two\nrecently rising techniques: active learning (AL) and self-paced learning (SPL),\nour framework is capable of automatically annotating new instances and\nincorporating them into training under weak expert re-certification. We first\ninitialize the classifier using a few annotated samples for each individual,\nand extract image features using the convolutional neural nets. Then, a number\nof candidates are selected from the unannotated samples for classifier\nupdating, in which we apply the current classifiers ranking the samples by the\nprediction confidence. In particular, our approach utilizes the high-confidence\nand low-confidence samples in the self-paced and the active user-query way,\nrespectively. The neural nets are later fine-tuned based on the updated\nclassifiers. Such heuristic implementation is formulated as solving a concise\nactive SPL optimization problem, which also advances the SPL development by\nsupplementing a rational dynamic curriculum constraint. The new model finely\naccords with the \"instructor-student-collaborative\" learning mode in human\neducation. The advantages of this proposed framework are two-folds: i) The\nrequired number of annotated samples is significantly decreased while the\ncomparable performance is guaranteed. A dramatic reduction of user effort is\nalso achieved over other state-of-the-art active learning techniques. ii) The\nmixture of SPL and AL effectively improves not only the classifier accuracy\ncompared to existing AL/SPL methods but also the robustness against noisy data.\nWe evaluate our framework on two challenging datasets, and demonstrate very\npromising results. (http://hcp.sysu.edu.cn/projects/aspl/)\n",
    "topics": "{'Active Learning': 0.99994934, 'Face Identification': 0.99976414}",
    "score": 0.8272888234
  },
  {
    "id": "1908.05489",
    "title": "Deep learning for Plankton and Coral Classification",
    "abstract": "  Oceans are the essential lifeblood of the Earth: they provide over 70% of the\noxygen and over 97% of the water. Plankton and corals are two of the most\nfundamental components of ocean ecosystems, the former due to their function at\nmany levels of the oceans food chain, the latter because they provide spawning\nand nursery grounds to many fish populations. Studying and monitoring plankton\ndistribution and coral reefs is vital for environment protection. In the last\nyears there has been a massive proliferation of digital imagery for the\nmonitoring of underwater ecosystems and much research is concentrated on the\nautomated recognition of plankton and corals. In this paper, we present a study\nabout an automated system for monitoring of underwater ecosystems. The system\nhere proposed is based on the fusion of different deep learning methods. We\nstudy how to create an ensemble based of different CNN models, fine tuned on\nseveral datasets with the aim of exploiting their diversity. The aim of our\nstudy is to experiment the possibility of fine-tuning pretrained CNN for\nunderwater imagery analysis, the opportunity of using different datasets for\npretraining models, the possibility to design an ensemble using the same\narchitecture with small variations in the training procedure. The experimental\nresults are very encouraging, our experiments performed on 5 well-knowns\ndatasets (3 plankton and 2 coral datasets) show that the fusion of such\ndifferent CNN models in a heterogeneous ensemble grants a substantial\nperformance improvement with respect to other state-of-the-art approaches in\nall the tested problems. One of the main contributions of this work is a wide\nexperimental evaluation of famous CNN architectures to report performance of\nboth single CNN and ensemble of CNNs in different problems. Moreover, we show\nhow to create an ensemble which improves the performance of the best single\nmodel.\n",
    "topics": "{}",
    "score": 0.8270945994
  },
  {
    "id": "1812.01278",
    "title": "Singing Voice Separation Using a Deep Convolutional Neural Network\n  Trained by Ideal Binary Mask and Cross Entropy",
    "abstract": "  Separating a singing voice from its music accompaniment remains an important\nchallenge in the field of music information retrieval. We present a unique\nneural network approach inspired by a technique that has revolutionized the\nfield of vision: pixel-wise image classification, which we combine with cross\nentropy loss and pretraining of the CNN as an autoencoder on singing voice\nspectrograms. The pixel-wise classification technique directly estimates the\nsound source label for each time-frequency (T-F) bin in our spectrogram image,\nthus eliminating common pre- and postprocessing tasks. The proposed network is\ntrained by using the Ideal Binary Mask (IBM) as the target output label. The\nIBM identifies the dominant sound source in each T-F bin of the magnitude\nspectrogram of a mixture signal, by considering each T-F bin as a pixel with a\nmulti-label (for each sound source). Cross entropy is used as the training\nobjective, so as to minimize the average probability error between the target\nand predicted label for each pixel. By treating the singing voice separation\nproblem as a pixel-wise classification task, we additionally eliminate one of\nthe commonly used, yet not easy to comprehend, postprocessing steps: the Wiener\nfilter postprocessing.\n  The proposed CNN outperforms the first runner up in the Music Information\nRetrieval Evaluation eXchange (MIREX) 2016 and the winner of MIREX 2014 with a\ngain of 2.2702 ~ 5.9563 dB global normalized source to distortion ratio (GNSDR)\nwhen applied to the iKala dataset. An experiment with the DSD100 dataset on the\nfull-tracks song evaluation task also shows that our model is able to compete\nwith cutting-edge singing voice separation systems which use multi-channel\nmodeling, data augmentation, and model blending.\n",
    "topics": "{'Music Information Retrieval': 0.9999778, 'Information Retrieval': 0.985872, 'Data Augmentation': 0.94285846, 'Image Classification': 0.9354879}",
    "score": 0.8270579149
  },
  {
    "id": "1904.13234",
    "title": "Automatic Emotion Recognition (AER) System based on Two-Level Ensemble\n  of Lightweight Deep CNN Models",
    "abstract": "  Emotions play a crucial role in human interaction, health care and security\ninvestigations and monitoring. Automatic emotion recognition (AER) using\nelectroencephalogram (EEG) signals is an effective method for decoding the real\nemotions, which are independent of body gestures, but it is a challenging\nproblem. Several automatic emotion recognition systems have been proposed,\nwhich are based on traditional hand-engineered approaches and their\nperformances are very poor. Motivated by the outstanding performance of deep\nlearning (DL) in many recognition tasks, we introduce an AER system (Deep-AER)\nbased on EEG brain signals using DL. A DL model involves a large number of\nlearnable parameters, and its training needs a large dataset of EEG signals,\nwhich is difficult to acquire for AER problem. To overcome this problem, we\nproposed a lightweight pyramidal one-dimensional convolutional neural network\n(LP-1D-CNN) model, which involves a small number of learnable parameters. Using\nLP-1D-CNN, we build a two level ensemble model. In the first level of the\nensemble, each channel is scanned incrementally by LP-1D-CNN to generate\npredictions, which are fused using majority vote. The second level of the\nensemble combines the predictions of all channels of an EEG signal using\nmajority vote for detecting the emotion state. We validated the effectiveness\nand robustness of Deep-AER using DEAP, a benchmark dataset for emotion\nrecognition research. The results indicate that FRONT plays dominant role in\nAER and over this region, Deep-AER achieved the accuracies of 98.43% and 97.65%\nfor two AER problems, i.e., high valence vs low valence (HV vs LV) and high\narousal vs low arousal (HA vs LA), respectively. The comparison reveals that\nDeep-AER outperforms the state-of-the-art systems with large margin. The\nDeep-AER system will be helpful in monitoring for health care and security\ninvestigations.\n",
    "topics": "{'EEG': 1.0, 'Emotion Recognition': 0.9999988}",
    "score": 0.8270243377
  },
  {
    "id": "1805.09738",
    "title": "Detecting Homoglyph Attacks with a Siamese Neural Network",
    "abstract": "  A homoglyph (name spoofing) attack is a common technique used by adversaries\nto obfuscate file and domain names. This technique creates process or domain\nnames that are visually similar to legitimate and recognized names. For\ninstance, an attacker may create malware with the name svch0st.exe so that in a\nvisual inspection of running processes or a directory listing, the process or\nfile name might be mistaken as the Windows system process svchost.exe. There\nhas been limited published research on detecting homoglyph attacks. Current\napproaches rely on string comparison algorithms (such as Levenshtein distance)\nthat result in computationally heavy solutions with a high number of false\npositives. In addition, there is a deficiency in the number of publicly\navailable datasets for reproducible research, with most datasets focused on\nphishing attacks, in which homoglyphs are not always used. This paper presents\na fundamentally different solution to this problem using a Siamese\nconvolutional neural network (CNN). Rather than leveraging similarity based on\ncharacter swaps and deletions, this technique uses a learned metric on strings\nrendered as images: a CNN learns features that are optimized to detect visual\nsimilarity of the rendered strings. The trained model is used to convert\nthousands of potentially targeted process or domain names to feature vectors.\nThese feature vectors are indexed using randomized KD-Trees to make similarity\nsearches extremely fast with minimal computational processing. This technique\nshows a considerable 13% to 45% improvement over baseline techniques in terms\nof area under the receiver operating characteristic curve (ROC AUC). In\naddition, we provide both code and data to further future research.\n",
    "topics": "{}",
    "score": 0.8269020023
  },
  {
    "id": "2006.15559",
    "title": "SAR Image Despeckling by Deep Neural Networks: from a pre-trained model\n  to an end-to-end training strategy",
    "abstract": "  Speckle reduction is a longstanding topic in synthetic aperture radar (SAR)\nimages. Many different schemes have been proposed for the restoration of\nintensity SAR images. Among the different possible approaches, methods based on\nconvolutional neural networks (CNNs) have recently shown to reach\nstate-of-the-art performance for SAR image restoration. CNN training requires\ngood training data: many pairs of speckle-free / speckle-corrupted images. This\nis an issue in SAR applications, given the inherent scarcity of speckle-free\nimages. To handle this problem, this paper analyzes different strategies one\ncan adopt, depending on the speckle removal task one wishes to perform and the\navailability of multitemporal stacks of SAR data. The first strategy applies a\nCNN model, trained to remove additive white Gaussian noise from natural images,\nto a recently proposed SAR speckle removal framework: MuLoG (MUlti-channel\nLOgarithm with Gaussian denoising). No training on SAR images is performed, the\nnetwork is readily applied to speckle reduction tasks. The second strategy\nconsiders a novel approach to construct a reliable dataset of speckle-free SAR\nimages necessary to train a CNN model. Finally, a hybrid approach is also\nanalyzed: the CNN used to remove additive white Gaussian noise is trained on\nspeckle-free SAR images. The proposed methods are compared to other\nstate-of-the-art speckle removal filters, to evaluate the quality of denoising\nand to discuss the pros and cons of the different strategies. Along with the\npaper, we make available the weights of the trained network to allow its usage\nby other researchers.\n",
    "topics": "{'Image Restoration': 0.99990475, 'Denoising': 0.9944588}",
    "score": 0.8268493548
  },
  {
    "id": "1908.04682",
    "title": "CMB-GAN: Fast Simulations of Cosmic Microwave background anisotropy maps\n  using Deep Learning",
    "abstract": "  Cosmic Microwave Background (CMB) has been a cornerstone in many cosmology\nexperiments and studies since it was discovered back in 1964. Traditional\ncomputational models like CAMB that are used for generating CMB temperature\nanisotropy maps are extremely resource intensive and act as a bottleneck in\ncosmology experiments that require a large amount of CMB data for analysis. In\nthis paper, we present a new approach to the generation of CMB temperature maps\nusing a specific class of neural networks called Generative Adversarial Network\n(GAN). We train our deep generative model to learn the complex distribution of\nCMB maps and efficiently generate new sets of CMB data in the form of 2D\npatches of anisotropy maps without losing much accuracy. We limit our\nexperiment to the generation of 56$^{\\circ}$ and 112$^{\\circ}$ square patches\nof CMB maps. We have also trained a Multilayer perceptron model for estimation\nof baryon density from a CMB map, we will be using this model for the\nperformance evaluation of our generative model using diagnostic measures like\nHistogram of pixel intensities, the standard deviation of pixel intensity\ndistribution, Power Spectrum, Cross power spectrum, Correlation matrix of the\npower spectrum and Peak count. We show that the GAN model is able to\nefficiently generate CMB samples of multiple sizes and is sensitive to the\ncosmological parameters corresponding to the underlying distribution of the\ndata. The primiary advantage of this method is the exponential reduction in the\ncomputational time needed to generate the CMB data, the GAN model is able to\ngenerate the samples within seconds as opposed to hours required by the CAMB\npackage with an acceptable value to error and loss of information. We hope that\nfuture iterations of this methodology will replace traditional statistical\nmethods of CMB data generation and help in large scale cosmological\nexperiments.\n",
    "topics": "{}",
    "score": 0.8268392595
  },
  {
    "id": "2005.08752",
    "title": "Learning Spatial-Spectral Prior for Super-Resolution of Hyperspectral\n  Imagery",
    "abstract": "  Recently, single gray/RGB image super-resolution reconstruction task has been\nextensively studied and made significant progress by leveraging the advanced\nmachine learning techniques based on deep convolutional neural networks\n(DCNNs). However, there has been limited technical development focusing on\nsingle hyperspectral image super-resolution due to the high-dimensional and\ncomplex spectral patterns in hyperspectral image. In this paper, we make a step\nforward by investigating how to adapt state-of-the-art residual learning based\nsingle gray/RGB image super-resolution approaches for computationally efficient\nsingle hyperspectral image super-resolution, referred as SSPSR. Specifically,\nwe introduce a spatial-spectral prior network (SSPN) to fully exploit the\nspatial information and the correlation between the spectra of the\nhyperspectral data. Considering that the hyperspectral training samples are\nscarce and the spectral dimension of hyperspectral image data is very high, it\nis nontrivial to train a stable and effective deep network. Therefore, a group\nconvolution (with shared network parameters) and progressive upsampling\nframework is proposed. This will not only alleviate the difficulty in feature\nextraction due to high-dimension of the hyperspectral data, but also make the\ntraining process more stable. To exploit the spatial and spectral prior, we\ndesign a spatial-spectral block (SSB), which consists of a spatial residual\nmodule and a spectral attention residual module. Experimental results on some\nhyperspectral images demonstrate that the proposed SSPSR method enhances the\ndetails of the recovered high-resolution hyperspectral images, and outperforms\nstate-of-the-arts. The source code is available at\n\\url{https://github.com/junjun-jiang/SSPSR\n",
    "topics": "{'Super-Resolution': 1.0, 'Super Resolution': 1.0, 'Image Super-Resolution': 1.0}",
    "score": 0.826819136
  },
  {
    "id": "1801.01275",
    "title": "DeepTriage: Exploring the Effectiveness of Deep Learning for Bug\n  Triaging",
    "abstract": "  For a given software bug report, identifying an appropriate developer who\ncould potentially fix the bug is the primary task of a bug triaging process. A\nbug title (summary) and a detailed description is present in most of the bug\ntracking systems. Automatic bug triaging algorithm can be formulated as a\nclassification problem, with the bug title and description as the input,\nmapping it to one of the available developers (classes). The major challenge is\nthat the bug description usually contains a combination of free unstructured\ntext, code snippets, and stack trace making the input data noisy. The existing\nbag-of-words (BOW) feature models do not consider the syntactical and\nsequential word information available in the unstructured text. We propose a\nnovel bug report representation algorithm using an attention based deep\nbidirectional recurrent neural network (DBRNN-A) model that learns a syntactic\nand semantic feature from long word sequences in an unsupervised manner.\nInstead of BOW features, the DBRNN-A based bug representation is then used for\ntraining the classifier. Using an attention mechanism enables the model to\nlearn the context representation over a long word sequence, as in a bug report.\nTo provide a large amount of data to learn the feature learning model, the\nunfixed bug reports (~70% bugs in an open source bug tracking system) are\nleveraged, which were completely ignored in the previous studies. Another\ncontribution is to make this research reproducible by making the source code\navailable and creating a public benchmark dataset of bug reports from three\nopen source bug tracking system: Google Chromium (383,104 bug reports), Mozilla\nCore (314,388 bug reports), and Mozilla Firefox (162,307 bug reports).\nExperimentally we compare our approach with BOW model and machine learning\napproaches and observe that DBRNN-A provides a higher rank-10 average accuracy.\n",
    "topics": "{}",
    "score": 0.8267087745
  },
  {
    "id": "1910.13222",
    "title": "Adversarial Example in Remote Sensing Image Recognition",
    "abstract": "  With the wide application of remote sensing technology in various fields, the\naccuracy and security requirements for remote sensing images (RSIs) recognition\nare also increasing. In recent years, due to the rapid development of deep\nlearning in the field of image recognition, RSI recognition models based on\ndeep convolution neural networks (CNNs) outperform traditional hand-craft\nfeature techniques. However, CNNs also pose security issues when they show\ntheir capability of accurate classification. By adding a very small variation\nof the adversarial perturbation to the input image, the CNN model can be caused\nto produce erroneous results with extremely high confidence, and the\nmodification of the image is not perceived by the human eye. This added\nadversarial perturbation image is called an adversarial example, which poses a\nserious security problem for systems based on CNN model recognition results.\nThis paper, for the first time, analyzes adversarial example problem of RSI\nrecognition under CNN models. In the experiments, we used different attack\nalgorithms to fool multiple high-accuracy RSI recognition models trained on\nmultiple RSI datasets. The results show that RSI recognition models are also\nvulnerable to adversarial examples, and the models with different structures\ntrained on the same RSI dataset also have different vulnerabilities. For each\nRSI dataset, the number of features also affects the vulnerability of the\nmodel. Many features are good for defensive adversarial examples. Further, we\nfind that the attacked class of RSI has an attack selectivity property. The\nmisclassification of adversarial examples of the RSIs are related to the\nsimilarity of the original classes in the CNN feature space. In addition,\nadversarial examples in RSI recognition are of great significance for the\nsecurity of remote sensing applications, showing a huge potential for future\nresearch.\n",
    "topics": "{}",
    "score": 0.8266739548
  },
  {
    "id": "1904.10044",
    "title": "UDFNet: Unsupervised Disparity Fusion with Adversarial Networks",
    "abstract": "  Existing disparity fusion methods based on deep learning achieve\nstate-of-the-art performance, but they require ground truth disparity data to\ntrain. As far as I know, this is the first time an unsupervised disparity\nfusion not using ground truth disparity data has been proposed. In this paper,\na mathematical model for disparity fusion is proposed to guide an adversarial\nnetwork to train effectively without ground truth disparity data. The initial\ndisparity maps are inputted from the left view along with auxiliary information\n(gradient, left & right intensity image) into the refiner and the refiner is\ntrained to output the refined disparity map registered on the left view. The\nrefined left disparity map and left intensity image are used to reconstruct a\nfake right intensity image. Finally, the fake and real right intensity images\n(from the right stereo vision camera) are fed into the discriminator. In the\nmodel, the refiner is trained to output a refined disparity value close to the\nweighted sum of the disparity inputs for global initialisation. Then, three\nrefinement principles are adopted to refine the results further. (1) The\nreconstructed intensity error between the fake and real right intensity image\nis minimised. (2) The similarities between the fake and real right image in\ndifferent receptive fields are maximised. (3) The refined disparity map is\nsmoothed based on the corresponding intensity image. The adversarial networks'\narchitectures are effective for the fusion task. The fusion time using the\nproposed network is small. The network can achieve 90 fps using Nvidia Geforce\nGTX 1080Ti on the Kitti2015 dataset when the input resolution is 1242 * 375\n(Width * Height) without downsampling and cropping. The accuracy of this work\nis equal to (or better than) the state-of-the-art supervised methods.\n",
    "topics": "{'Disparity Estimation': 0.9999764}",
    "score": 0.8266538364
  },
  {
    "id": "1905.10887",
    "title": "Classification Accuracy Score for Conditional Generative Models",
    "abstract": "  Deep generative models (DGMs) of images are now sufficiently mature that they\nproduce nearly photorealistic samples and obtain scores similar to the data\ndistribution on heuristics such as Frechet Inception Distance (FID). These\nresults, especially on large-scale datasets such as ImageNet, suggest that DGMs\nare learning the data distribution in a perceptually meaningful space and can\nbe used in downstream tasks. To test this latter hypothesis, we use\nclass-conditional generative models from a number of model\nclasses---variational autoencoders, autoregressive models, and generative\nadversarial networks (GANs)---to infer the class labels of real data. We\nperform this inference by training an image classifier using only synthetic\ndata and using the classifier to predict labels on real data. The performance\non this task, which we call Classification Accuracy Score (CAS), reveals some\nsurprising results not identified by traditional metrics and constitute our\ncontributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1\nand Top-5 accuracy decrease by 27.9\\% and 41.6\\%, respectively, compared to the\noriginal data; and conditional generative models from other model classes, such\nas Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical\nAutoregressive Models (HAMs), substantially outperform GANs on this benchmark.\nSecond, CAS automatically surfaces particular classes for which generative\nmodels failed to capture the data distribution, and were previously unknown in\nthe literature. Third, we find traditional GAN metrics such as Inception Score\n(IS) and FID neither predictive of CAS nor useful when evaluating non-GAN\nmodels. Furthermore, in order to facilitate better diagnoses of generative\nmodels, we open-source the proposed metric.\n",
    "topics": "{}",
    "score": 0.8266306131
  },
  {
    "id": "2009.12597",
    "title": "Potential Features of ICU Admission in X-ray Images of COVID-19 Patients",
    "abstract": "  X-ray images may present non-trivial features with predictive information of\npatients that develop severe symptoms of COVID-19. If true, this hypothesis may\nhave practical value in allocating resources to particular patients while using\na relatively inexpensive imaging technique. The difficulty of testing such a\nhypothesis comes from the need for large sets of labelled data, which not only\nneed to be well-annotated but also should contemplate the post-imaging severity\noutcome. On this account, this paper presents a methodology for extracting\nfeatures from a limited data set with outcome label (patient required ICU\nadmission or not) and correlating its significance to an additional, larger\ndata set with hundreds of images. The methodology employs a neural network\ntrained to recognise lung pathologies to extract the semantic features, which\nare then analysed with a shallow decision tree to limit overfitting while\nincreasing interpretability. This analysis points out that only a few features\nexplain most of the variance between patients that developed severe symptoms.\nWhen applied to an unrelated, larger data set with labels extracted from\nclinical notes, the method classified distinct sets of samples where there was\na much higher frequency of labels such as `Consolidation', `Effusion', and\n`alveolar'. A further brief analysis on the locations of such labels also\nshowed a significant increase in the frequency of words like `bilateral',\n`middle', and `lower' in patients classified as with higher chances of going\nsevere. The methodology for dealing with the lack of specific ICU label data\nwhile attesting correlations with a data set containing text notes is novel;\nits results suggest that some pathologies should receive higher weights when\nassessing disease severity.\n",
    "topics": "{}",
    "score": 0.8266279339
  },
  {
    "id": "1609.06118",
    "title": "Adaptive Decontamination of the Training Set: A Unified Formulation for\n  Discriminative Visual Tracking",
    "abstract": "  Tracking-by-detection methods have demonstrated competitive performance in\nrecent years. In these approaches, the tracking model heavily relies on the\nquality of the training set. Due to the limited amount of labeled training\ndata, additional samples need to be extracted and labeled by the tracker\nitself. This often leads to the inclusion of corrupted training samples, due to\nocclusions, misalignments and other perturbations. Existing\ntracking-by-detection methods either ignore this problem, or employ a separate\ncomponent for managing the training set.\n  We propose a novel generic approach for alleviating the problem of corrupted\ntraining samples in tracking-by-detection frameworks. Our approach dynamically\nmanages the training set by estimating the quality of the samples. Contrary to\nexisting approaches, we propose a unified formulation by minimizing a single\nloss over both the target appearance model and the sample quality weights. The\njoint formulation enables corrupted samples to be down-weighted while\nincreasing the impact of correct ones. Experiments are performed on three\nbenchmarks: OTB-2015 with 100 videos, VOT-2015 with 60 videos, and Temple-Color\nwith 128 videos. On the OTB-2015, our unified formulation significantly\nimproves the baseline, with a gain of 3.8% in mean overlap precision. Finally,\nour method achieves state-of-the-art results on all three datasets. Code and\nsupplementary material are available at\nhttp://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html .\n",
    "topics": "{'Visual Tracking': 0.9999684}",
    "score": 0.8265546038
  },
  {
    "id": "2008.00932",
    "title": "AUTSL: A Large Scale Multi-modal Turkish Sign Language Dataset and\n  Baseline Methods",
    "abstract": "  Sign language recognition is a challenging problem where signs are identified\nby simultaneous local and global articulations of multiple sources, i.e. hand\nshape and orientation, hand movements, body posture and facial expressions.\nSolving this problem computationally for a large vocabulary of signs in real\nlife settings is still a challenge, even with the state-of-the-art models. In\nthis study, we present a new large-scale multi-modal Turkish Sign Language\ndataset (AUTSL) with a benchmark and provide baseline models for performance\nevaluations. Our dataset consists of 226 signs performed by 43 different\nsigners and 38,336 isolated sign video samples in total. Samples contain a wide\nvariety of backgrounds recorded in indoor and outdoor environments. Moreover,\nspatial positions and the postures of signers also vary in the recordings. Each\nsample is recorded with Microsoft Kinect v2 and contains color image (RGB),\ndepth and skeleton data modalities.\n  We prepared benchmark training and test sets for user independent assessments\nof the models. We trained several deep learning based models and provide\nempirical evaluations using the benchmark; we used Convolutional Neural\nNetworks (CNNs) to extract features, unidirectional and bidirectional Long\nShort-Term Memory (LSTM) models to characterize temporal information. We also\nincorporated feature pooling modules and temporal attention to our models to\nimprove the performances. Using the benchmark test set, we obtained 62.02%\naccuracy with RGB+Depth data and 47.62% accuracy with RGB only data with the\nCNN+FPM+BLSTM+Attention model. Our dataset will be made publicly available at\nhttps://cvml.ankara.edu.tr.\n",
    "topics": "{'Sign Language Recognition': 0.9999995}",
    "score": 0.8262938323
  },
  {
    "id": "2008.05676",
    "title": "Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance\n  Segmentation",
    "abstract": "  Despite the previous success of object analysis, detecting and segmenting a\nlarge number of object categories with a long-tailed data distribution remains\na challenging problem and is less investigated. For a large-vocabulary\nclassifier, the chance of obtaining noisy logits is much higher, which can\neasily lead to a wrong recognition. In this paper, we exploit prior knowledge\nof the relations among object categories to cluster fine-grained classes into\ncoarser parent classes, and construct a classification tree that is responsible\nfor parsing an object instance into a fine-grained category via its parent\nclass. In the classification tree, as the number of parent class nodes are\nsignificantly less, their logits are less noisy and can be utilized to suppress\nthe wrong/noisy logits existed in the fine-grained class nodes. As the way to\nconstruct the parent class is not unique, we further build multiple trees to\nform a classification forest where each tree contributes its vote to the\nfine-grained classification. To alleviate the imbalanced learning caused by the\nlong-tail phenomena, we propose a simple yet effective resampling method, NMS\nResampling, to re-balance the data distribution. Our method, termed as Forest\nR-CNN, can serve as a plug-and-play module being applied to most object\nrecognition models for recognizing more than 1000 categories. Extensive\nexperiments are performed on the large vocabulary dataset LVIS. Compared with\nthe Mask R-CNN baseline, the Forest R-CNN significantly boosts the performance\nwith 11.5% and 3.9% AP improvements on the rare categories and overall\ncategories, respectively. Moreover, we achieve state-of-the-art results on the\nLVIS dataset. Code is available at https://github.com/JialianW/Forest_RCNN.\n",
    "topics": "{'Instance Segmentation': 0.9998591, 'Object Recognition': 0.87576646, 'Object Detection': 0.828068}",
    "score": 0.8262759337
  },
  {
    "id": "1909.04126",
    "title": "DeepObfuscator: Adversarial Training Framework for Privacy-Preserving\n  Image Classification",
    "abstract": "  Deep learning has been widely utilized in many computer vision applications\nand achieved remarkable commercial success. However, running deep learning\nmodels on mobile devices is generally challenging due to limitation of the\navailable computing resources. It is common to let the users send their service\nrequests to cloud servers that run the large-scale deep learning models to\nprocess. Sending the data associated with the service requests to the cloud,\nhowever, impose risks on the user data privacy. Some prior arts proposed\nsending the features extracted from raw data (e.g., images) to the cloud.\nUnfortunately, these extracted features can still be exploited by attackers to\nrecover raw images and to infer embedded private attributes (e.g., age, gender,\netc.). In this paper, we propose an adversarial training framework\nDeepObfuscator that can prevent extracted features from being utilized to\nreconstruct raw images and infer private attributes, while retaining the useful\ninformation for the intended cloud service (i.e., image classification).\nDeepObfuscator includes a learnable encoder, namely, obfuscator that is\ndesigned to hide privacy-related sensitive information from the features by\nperformingour proposed adversarial training algorithm. Our experiments on\nCelebAdataset show that the quality of the reconstructed images fromthe\nobfuscated features of the raw image is dramatically decreased from 0.9458 to\n0.3175 in terms of multi-scale structural similarity (MS-SSIM). The person in\nthe reconstructed image, hence, becomes hardly to be re-identified. The\nclassification accuracy of the inferred private attributes that can be achieved\nby the attacker drops down to a random-guessing level, e.g., the accuracy of\ngender is reduced from 97.36% to 58.85%. As a comparison, the accuracy of the\nintended classification tasks performed via the cloud service drops by only 2%\n",
    "topics": "{'Image Classification': 0.996342}",
    "score": 0.8260828405
  },
  {
    "id": "1907.09194",
    "title": "FD-FCN: 3D Fully Dense and Fully Convolutional Network for Semantic\n  Segmentation of Brain Anatomy",
    "abstract": "  In this paper, a 3D patch-based fully dense and fully convolutional network\n(FD-FCN) is proposed for fast and accurate segmentation of subcortical\nstructures in T1-weighted magnetic resonance images. Developed from the seminal\nFCN with an end-to-end learning-based approach and constructed by newly\ndesigned dense blocks including a dense fully-connected layer, the proposed\nFD-FCN is different from other FCN-based methods and leads to an outperformance\nin the perspective of both efficiency and accuracy. Compared with the U-shaped\narchitecture, FD-FCN discards the upsampling path for model fitness. To\nalleviate the problem of parameter explosion, the inputs of dense blocks are no\nlonger directly passed to subsequent layers. This architecture of FD-FCN brings\na great reduction on both memory and time consumption in training process.\nAlthough FD-FCN is slimmed down, in model competence it gains better capability\nof dense inference than other conventional networks. This benefits from the\nconstruction of network architecture and the incorporation of redesigned dense\nblocks. The multi-scale FD-FCN models both local and global context by\nembedding intermediate-layer outputs in the final prediction, which encourages\nconsistency between features extracted at different scales and embeds\nfine-grained information directly in the segmentation process. In addition,\ndense blocks are rebuilt to enlarge the receptive fields without significantly\nincreasing parameters, and spectral coordinates are exploited for spatial\ncontext of the original input patch. The experiments were performed over the\nIBSR dataset, and FD-FCN produced an accurate segmentation result of overall\nDice overlap value of 89.81% for 11 brain structures in 53 seconds, with at\nleast 3.66% absolute improvement of dice accuracy than state-of-the-art 3D\nFCN-based methods.\n",
    "topics": "{'Semantic Segmentation': 0.9504038}",
    "score": 0.8260827502
  },
  {
    "id": "1801.06288",
    "title": "An End-to-End Deep Learning Histochemical Scoring System for Breast\n  Cancer Tissue Microarray",
    "abstract": "  One of the methods for stratifying different molecular classes of breast\ncancer is the Nottingham Prognostic Index Plus (NPI+) which uses breast cancer\nrelevant biomarkers to stain tumour tissues prepared on tissue microarray\n(TMA). To determine the molecular class of the tumour, pathologists will have\nto manually mark the nuclei activity biomarkers through a microscope and use a\nsemi-quantitative assessment method to assign a histochemical score (H-Score)\nto each TMA core. Manually marking positively stained nuclei is a time\nconsuming, imprecise and subjective process which will lead to inter-observer\nand intra-observer discrepancies. In this paper, we present an end-to-end deep\nlearning system which directly predicts the H-Score automatically. Our system\nimitates the pathologists' decision process and uses one fully convolutional\nnetwork (FCN) to extract all nuclei region (tumour and non-tumour), a second\nFCN to extract tumour nuclei region, and a multi-column convolutional neural\nnetwork which takes the outputs of the first two FCNs and the stain intensity\ndescription image as input and acts as the high-level decision making mechanism\nto directly output the H-Score of the input TMA image. To the best of our\nknowledge, this is the first end-to-end system that takes a TMA image as input\nand directly outputs a clinical score. We will present experimental results\nwhich demonstrate that the H-Scores predicted by our model have very high and\nstatistically significant correlation with experienced pathologists' scores and\nthat the H-Score discrepancy between our algorithm and the pathologists is on\npar with the inter-subject discrepancy between the pathologists.\n",
    "topics": "{'Decision Making': 0.9199247}",
    "score": 0.8260822339
  },
  {
    "id": "1911.08718",
    "title": "Towards Ghost-free Shadow Removal via Dual Hierarchical Aggregation\n  Network and Shadow Matting GAN",
    "abstract": "  Shadow removal is an essential task for scene understanding. Many studies\nconsider only matching the image contents, which often causes two types of\nghosts: color in-consistencies in shadow regions or artifacts on shadow\nboundaries. In this paper, we tackle these issues in two ways. First, to\ncarefully learn the border artifacts-free image, we propose a novel network\nstructure named the dual hierarchically aggregation network~(DHAN). It contains\na series of growth dilated convolutions as the backbone without any\ndown-samplings, and we hierarchically aggregate multi-context features for\nattention and prediction, respectively. Second, we argue that training on a\nlimited dataset restricts the textural understanding of the network, which\nleads to the shadow region color in-consistencies. Currently, the largest\ndataset contains 2k+ shadow/shadow-free image pairs. However, it has only 0.1k+\nunique scenes since many samples share exactly the same background with\ndifferent shadow positions. Thus, we design a shadow matting generative\nadversarial network~(SMGAN) to synthesize realistic shadow mattings from a\ngiven shadow mask and shadow-free image. With the help of novel masks or\nscenes, we enhance the current datasets using synthesized shadow images.\nExperiments show that our DHAN can erase the shadows and produce high-quality\nghost-free images. After training on the synthesized and real datasets, our\nnetwork outperforms other state-of-the-art methods by a large margin. The code\nis available: http://github.com/vinthony/ghost-free-shadow-removal/\n",
    "topics": "{'Scene Understanding': 1.0}",
    "score": 0.8259525878
  },
  {
    "id": "1905.10142",
    "title": "FasTrCaps: An Integrated Framework for Fast yet Accurate Training of\n  Capsule Networks",
    "abstract": "  Recently, Capsule Networks (CapsNets) have shown improved performance\ncompared to the traditional Convolutional Neural Networks (CNNs), by encoding\nand preserving spatial relationships between the detected features in a better\nway. This is achieved through the so-called Capsules (i.e., groups of neurons)\nthat encode both the instantiation probability and the spatial information.\nHowever, one of the major hurdles in the wide adoption of CapsNets is their\ngigantic training time, which is primarily due to the relatively higher\ncomplexity of their new constituting elements that are different from CNNs. In\nthis paper, we implement different optimizations in the training loop of the\nCapsNets, and investigate how these optimizations affect their training speed\nand the accuracy. Towards this, we propose a novel framework FasTrCaps that\nintegrates multiple lightweight optimizations and a novel learning rate policy\ncalled WarmAdaBatch (that jointly performs warm restarts and adaptive batch\nsize), and steers them in an appropriate way to provide high training-loop\nspeedup at minimal accuracy loss. We also propose weight sharing for capsule\nlayers. The goal is to reduce the hardware requirements of CapsNets by removing\nunused/redundant connections and capsules, while keeping high accuracy through\ntests of different learning rate policies and batch sizes. We demonstrate that\none of the solutions generated by the FasTrCaps framework can achieve 58.6%\nreduction in the training time, while preserving the accuracy (even 0.12%\naccuracy improvement for the MNIST dataset), compared to the CapsNet by Google\nBrain. The Pareto-optimal solutions generated by FasTrCaps can be leveraged to\nrealize trade-offs between training time and achieved accuracy. We have\nopen-sourced our framework on https://github.com/Alexei95/FasTrCaps.\n",
    "topics": "{}",
    "score": 0.8259155381
  },
  {
    "id": "1909.08148",
    "title": "AdaCompress: Adaptive Compression for Online Computer Vision Services",
    "abstract": "  With the growth of computer vision based applications and services, an\nexplosive amount of images have been uploaded to cloud servers which host such\ncomputer vision algorithms, usually in the form of deep learning models. JPEG\nhas been used as the {\\em de facto} compression and encapsulation method before\none uploads the images, due to its wide adaptation. However, standard JPEG\nconfiguration does not always perform well for compressing images that are to\nbe processed by a deep learning model, e.g., the standard quality level of JPEG\nleads to 50\\% of size overhead (compared with the best quality level selection)\non ImageNet under the same inference accuracy in popular computer vision models\nincluding InceptionNet, ResNet, etc. Knowing this, designing a better JPEG\nconfiguration for online computer vision services is still extremely\nchallenging: 1) Cloud-based computer vision models are usually a black box to\nend-users; thus it is difficult to design JPEG configuration without knowing\ntheir model structures. 2) JPEG configuration has to change when different\nusers use it. In this paper, we propose a reinforcement learning based JPEG\nconfiguration framework. In particular, we design an agent that adaptively\nchooses the compression level according to the input image's features and\nbackend deep learning models. Then we train the agent in a reinforcement\nlearning way to adapt it for different deep learning cloud services that act as\nthe {\\em interactive training environment} and feeding a reward with\ncomprehensive consideration of accuracy and data size. In our real-world\nevaluation on Amazon Rekognition, Face++ and Baidu Vision, our approach can\nreduce the size of images by 1/2 -- 1/3 while the overall classification\naccuracy only decreases slightly.\n",
    "topics": "{}",
    "score": 0.8253909449
  },
  {
    "id": "1207.0143",
    "title": "CDAS: A Crowdsourcing Data Analytics System",
    "abstract": "  Some complex problems, such as image tagging and natural language processing,\nare very challenging for computers, where even state-of-the-art technology is\nyet able to provide satisfactory accuracy. Therefore, rather than relying\nsolely on developing new and better algorithms to handle such tasks, we look to\nthe crowdsourcing solution -- employing human participation -- to make good the\nshortfall in current technology. Crowdsourcing is a good supplement to many\ncomputer tasks. A complex job may be divided into computer-oriented tasks and\nhuman-oriented tasks, which are then assigned to machines and humans\nrespectively. To leverage the power of crowdsourcing, we design and implement a\nCrowdsourcing Data Analytics System, CDAS. CDAS is a framework designed to\nsupport the deployment of various crowdsourcing applications. The core part of\nCDAS is a quality-sensitive answering model, which guides the crowdsourcing\nengine to process and monitor the human tasks. In this paper, we introduce the\nprinciples of our quality-sensitive model. To satisfy user required accuracy,\nthe model guides the crowdsourcing query engine for the design and processing\nof the corresponding crowdsourcing jobs. It provides an estimated accuracy for\neach generated result based on the human workers' historical performances. When\nverifying the quality of the result, the model employs an online strategy to\nreduce waiting time. To show the effectiveness of the model, we implement and\ndeploy two analytics jobs on CDAS, a twitter sentiment analytics job and an\nimage tagging job. We use real Twitter and Flickr data as our queries\nrespectively. We compare our approaches with state-of-the-art classification\nand image annotation techniques. The results show that the human-assisted\nmethods can indeed achieve a much higher accuracy. By embedding the\nquality-sensitive model into crowdsourcing query engine, we\neffectiv...[truncated].\n",
    "topics": "{}",
    "score": 0.825385167
  },
  {
    "id": "1906.02182",
    "title": "Two-Stream Region Convolutional 3D Network for Temporal Activity\n  Detection",
    "abstract": "  We address the problem of temporal activity detection in continuous,\nuntrimmed video streams. This is a difficult task that requires extracting\nmeaningful spatio-temporal features to capture activities, accurately\nlocalizing the start and end times of each activity. We introduce a new model,\nRegion Convolutional 3D Network (R-C3D), which encodes the video streams using\na three-dimensional fully convolutional network, then generates candidate\ntemporal regions containing activities and finally classifies selected regions\ninto specific activities. Computation is saved due to the sharing of\nconvolutional features between the proposal and the classification pipelines.\nWe further improve the detection performance by efficiently integrating an\noptical flow based motion stream with the original RGB stream. The two-stream\nnetwork is jointly optimized by fusing the flow and RGB feature maps at\ndifferent levels. Additionally, the training stage incorporates an online hard\nexample mining strategy to address the extreme foreground-background imbalance\ntypically observed in any detection pipeline. Instead of heuristically sampling\nthe candidate segments for the final activity classification stage, we rank\nthem according to their performance and only select the worst performers to\nupdate the model. This improves the model without heavy hyper-parameter tuning.\nExtensive experiments on three benchmark datasets are carried out to show\nsuperior performance over existing temporal activity detection methods. Our\nmodel achieves state-of-the-art results on the THUMOS'14 and Charades datasets.\nWe further demonstrate that our model is a general temporal activity detection\nframework that does not rely on assumptions about particular dataset properties\nby evaluating our approach on the ActivityNet dataset.\n",
    "topics": "{'Activity Detection': 0.99999714, 'Optical Flow Estimation': 0.99978966, 'Action Detection': 0.9940691, 'Action Recognition': 0.8728453}",
    "score": 0.8253441468
  },
  {
    "id": "2007.08637",
    "title": "COV-ELM classifier: An Extreme Learning Machine based identification of\n  COVID-19 using Chest X-Ray Images",
    "abstract": "  Coronaviruses constitute a family of virus that gives rise to respiratory\ndiseases. Coronavirus disease 2019 (COVID-19) is an infectious disease caused\nby a newly discovered coronavirus also termed as Severe acute respiratory\nsyndrome coronavirus 2 (SARS-CoV-2). Due to its rapid spread, WHO has declared\nCOVID-19 outbreak a pandemic on 11th March 2020. Reverse\ntranscription-polymerase chain reaction (RT-PCR) test is popularly used\nworldwide for the detection of COVID-19. However, due to the high\nfalse-negative rate of RT-PCR test, chest X-ray (CXR) imaging is emerging as a\nfeasible alternative for the detection of COVID-19. In this work, we propose a\nmulticlass classification model COV-ELM, based on the extreme learning machine\nwhich classifies the CXR images into one of the three classes, namely COVID-19,\nnormal, and pneumonia. The choice of ELM in this work has been motivated by its\nsignificantly short training time as compared to conventional gradient-based\nlearning algorithms. After some preprocessing, we extract a pool of features\nbased on texture and frequency. This pool of features serves as an input to the\nELM and a 10-fold cross-validation method is employed to evaluate the proposed\nmodel. For experimentation, we use chest X-ray (CXR) images from three publicly\navailable sources. The results of applying COV-ELM on test data are quite\npromising. The COV-ELM achieved a macro average F1-score of 0.95 and the\noverall sensitivity of ${0.94 \\pm 0.02}$ at 95% confidence interval. When\ncompared to state-of-the-art machine learning algorithms, the COV-ELM is found\nto outperform in a three-class classification scenario. The main advantage of\nCOV-ELM is that its training time being quite low, as bigger and diverse\ndatasets become available, it can be quickly retrained as compared to its\ngradient-based competitor models.\n",
    "topics": "{'Multi-class Classification': 0.78864324}",
    "score": 0.8252754334
  },
  {
    "id": "1903.00183",
    "title": "Lung CT Imaging Sign Classification through Deep Learning on Small Data",
    "abstract": "  The annotated medical images are usually expensive to be collected. This\npaper proposes a deep learning method on small data to classify Common Imaging\nSigns of Lung diseases (CISL) in computed tomography (CT) images. We explore\nboth the real data and the data generated by Generative Adversarial Network\n(GAN) to improve the reliability and the generalization of learning. First, we\nuse GAN to generate a large number of CISLs from small annotated data, which\nare difficult to be distinguished from real counterparts. These generated\nsamples are used to pre-train a Convolutional Neural Network (CNN) for\nclassifying CISLs. Second, we fine-tune the CNN classification model with real\ndata. Experiments were conducted on the LISS database of CISLs. We successfully\nconvinced radiologists that our generated CISLs samples were real for 56.7% of\nour experiments. The pre-trained CNN model achieves 88.4% of mean accuracy of\nbinary classification, and after fine-tuning, the mean accuracy is\nsignificantly increased to 95.0%. For multi-classification of all types of\nCISLs and normal tissues, through the two stages of training, the mean\naccuracy, sensitivity and specificity are up to about 91.83%, 92.73% and 99.0%,\nrespectively. To our knowledge, this is the best result achieved on the LISS\ndatabase, which demonstrates that the proposed method is effective and\npromising for fulfilling deep learning on small data.\n",
    "topics": "{'Small Data Image Classification': 1.0, 'Computed Tomography (CT)': 0.9999342}",
    "score": 0.8252384579
  },
  {
    "id": "2003.05684",
    "title": "Skeleton Based Action Recognition using a Stacked Denoising Autoencoder\n  with Constraints of Privileged Information",
    "abstract": "  Recently, with the availability of cost-effective depth cameras coupled with\nreal-time skeleton estimation, the interest in skeleton-based human action\nrecognition is renewed. Most of the existing skeletal representation approaches\nuse either the joint location or the dynamics model. Differing from the\nprevious studies, we propose a new method called Denoising Autoencoder with\nTemporal and Categorical Constraints (DAE_CTC)} to study the skeletal\nrepresentation in a view of skeleton reconstruction. Based on the concept of\nlearning under privileged information, we integrate action categories and\ntemporal coordinates into a stacked denoising autoencoder in the training\nphase, to preserve category and temporal feature, while learning the hidden\nrepresentation from a skeleton. Thus, we are able to improve the discriminative\nvalidity of the hidden representation. In order to mitigate the variation\nresulting from temporary misalignment, a new method of temporal registration,\ncalled Locally-Warped Sequence Registration (LWSR), is proposed for registering\nthe sequences of inter- and intra-class actions. We finally represent the\nsequences using a Fourier Temporal Pyramid (FTP) representation and perform\nclassification using a combination of LWSR registration, FTP representation,\nand a linear Support Vector Machine (SVM). The experimental results on three\naction data sets, namely MSR-Action3D, UTKinect-Action, and Florence3D-Action,\nshow that our proposal performs better than many existing methods and\ncomparably to the state of the art.\n",
    "topics": "{'Denoising': 1.0, 'Skeleton Based Action Recognition': 0.9999956, 'Action Recognition': 0.9999889, 'Temporal Action Localization': 0.9998172}",
    "score": 0.8252011173
  },
  {
    "id": "1907.06781",
    "title": "Rethinking RGB-D Salient Object Detection: Models, Data Sets, and\n  Large-Scale Benchmarks",
    "abstract": "  The use of RGB-D information for salient object detection has been\nextensively explored in recent years. However, relatively few efforts have been\nput towards modeling salient object detection in real-world human activity\nscenes with RGBD. In this work, we fill the gap by making the following\ncontributions to RGB-D salient object detection. (1) We carefully collect a new\nSIP (salient person) dataset, which consists of ~1K high-resolution images that\ncover diverse real-world scenes from various viewpoints, poses, occlusions,\nilluminations, and backgrounds. (2) We conduct a large-scale (and, so far, the\nmost comprehensive) benchmark comparing contemporary methods, which has long\nbeen missing in the field and can serve as a baseline for future research. We\nsystematically summarize 32 popular models and evaluate 18 parts of 32 models\non seven datasets containing a total of about 97K images. (3) We propose a\nsimple general architecture, called Deep Depth-Depurator Network (D3Net). It\nconsists of a depth depurator unit (DDU) and a three-stream feature learning\nmodule (FLM), which performs low-quality depth map filtering and cross-modal\nfeature learning respectively. These components form a nested structure and are\nelaborately designed to be learned jointly. D3Net exceeds the performance of\nany prior contenders across all five metrics under consideration, thus serving\nas a strong model to advance research in this field. We also demonstrate that\nD3Net can be used to efficiently extract salient object masks from real scenes,\nenabling effective background changing application with a speed of 65fps on a\nsingle GPU. All the saliency maps, our new SIP dataset, the D3Net model, and\nthe evaluation tools are publicly available at\nhttps://github.com/DengPingFan/D3NetBenchmark.\n",
    "topics": "{'RGB Salient Object Detection': 1.0, 'Object Detection': 0.9999993}",
    "score": 0.8247465496
  },
  {
    "id": "1909.03044",
    "title": "Deep learning with sentence embeddings pre-trained on biomedical corpora\n  improves the performance of finding similar sentences in electronic medical\n  records",
    "abstract": "  Capturing sentence semantics plays a vital role in a range of text mining\napplications. Despite continuous efforts on the development of related datasets\nand models in the general domain, both datasets and models are limited in\nbiomedical and clinical domains. The BioCreative/OHNLP organizers have made the\nfirst attempt to annotate 1,068 sentence pairs from clinical notes and have\ncalled for a community effort to tackle the Semantic Textual Similarity\n(BioCreative/OHNLP STS) challenge. We developed models using traditional\nmachine learning and deep learning approaches. For the post challenge, we focus\non two models: the Random Forest and the Encoder Network. We applied sentence\nembeddings pre-trained on PubMed abstracts and MIMIC-III clinical notes and\nupdated the Random Forest and the Encoder Network accordingly. The official\nresults demonstrated our best submission was the ensemble of eight models. It\nachieved a Person correlation coefficient of 0.8328, the highest performance\namong 13 submissions from 4 teams. For the post challenge, the performance of\nboth Random Forest and the Encoder Network was improved; in particular, the\ncorrelation of the Encoder Network was improved by ~13%. During the challenge\ntask, no end-to-end deep learning models had better performance than machine\nlearning models that take manually-crafted features. In contrast, with the\nsentence embeddings pre-trained on biomedical corpora, the Encoder Network now\nachieves a correlation of ~0.84, which is higher than the original best model.\nThe ensembled model taking the improved versions of the Random Forest and\nEncoder Network as inputs further increased performance to 0.8528. Deep\nlearning models with sentence embeddings pre-trained on biomedical corpora\nachieve the highest performance on the test set.\n",
    "topics": "{'Sentence Embeddings': 1.0, 'Semantic Textual Similarity': 0.79418087}",
    "score": 0.8246319392
  },
  {
    "id": "2003.06451",
    "title": "The GraphNet Zoo: An All-in-One Graph Based Deep Semi-Supervised\n  Framework for Medical Image Classification",
    "abstract": "  We consider the problem of classifying a medical image dataset when we have a\nlimited amounts of labels. This is very common yet challenging setting as\nlabelled data is expensive, time consuming to collect and may require expert\nknowledge. The current classification go-to of deep supervised learning is\nunable to cope with such a problem setup. However, using semi-supervised\nlearning, one can produce accurate classifications using a significantly\nreduced amount of labelled data. Therefore, semi-supervised learning is\nperfectly suited for medical image classification. However, there has almost\nbeen no uptake of semi-supervised methods in the medical domain. In this work,\nwe propose an all-in-one framework for deep semi-supervised classification\nfocusing on graph based approaches, which up to our knowledge it is the first\ntime that an approach with minimal labels has been shown to such an\nunprecedented scale with medical data. We introduce the concept of hybrid\nmodels by defining a classifier as a combination between an energy-based model\nand a deep net. Our energy functional is built on the Dirichlet energy based on\nthe graph p-Laplacian. Our framework includes energies based on the $\\ell_1$\nand $\\ell_2$ norms. We then connected this energy model to a deep net to\ngenerate a much richer feature space to construct a stronger graph. Our\nframework can be set to be adapted to any complex dataset. We demonstrate,\nthrough extensive numerical comparisons, that our approach readily compete with\nfully-supervised state-of-the-art techniques for the applications of Malaria\nCells, Mammograms and Chest X-ray classification whilst using only 20% of\nlabels.\n",
    "topics": "{'Image Classification': 0.99486333}",
    "score": 0.8246223673
  },
  {
    "id": "1506.07271",
    "title": "Natural Scene Recognition Based on Superpixels and Deep Boltzmann\n  Machines",
    "abstract": "  The Deep Boltzmann Machines (DBM) is a state-of-the-art unsupervised learning\nmodel, which has been successfully applied to handwritten digit recognition\nand, as well as object recognition. However, the DBM is limited in scene\nrecognition due to the fact that natural scene images are usually very large.\nIn this paper, an efficient scene recognition approach is proposed based on\nsuperpixels and the DBMs. First, a simple linear iterative clustering (SLIC)\nalgorithm is employed to generate superpixels of input images, where each\nsuperpixel is regarded as an input of a learning model. Then, a two-layer DBM\nmodel is constructed by stacking two restricted Boltzmann machines (RBMs), and\na greedy layer-wise algorithm is applied to train the DBM model. Finally, a\nsoftmax regression is utilized to categorize scene images. The proposed\ntechnique can effectively reduce the computational complexity and enhance the\nperformance for large natural image recognition. The approach is verified and\nevaluated by extensive experiments, including the fifteen-scene categories\ndataset the UIUC eight-sports dataset, and the SIFT flow dataset, are used to\nevaluate the proposed method. The experimental results show that the proposed\napproach outperforms other state-of-the-art methods in terms of recognition\nrate.\n",
    "topics": "{'Scene Recognition': 1.0, 'Handwritten Digit Recognition': 0.9996433, 'Object Recognition': 0.97404253}",
    "score": 0.8245960779
  },
  {
    "id": "2005.12838",
    "title": "Neuro4Neuro: A neural network approach for neural tract segmentation\n  using large-scale population-based diffusion imaging",
    "abstract": "  Subtle changes in white matter (WM) microstructure have been associated with\nnormal aging and neurodegeneration. To study these associations in more detail,\nit is highly important that the WM tracts can be accurately and reproducibly\ncharacterized from brain diffusion MRI. In addition, to enable analysis of WM\ntracts in large datasets and in clinical practice it is essential to have\nmethodology that is fast and easy to apply. This work therefore presents a new\napproach for WM tract segmentation: Neuro4Neuro, that is capable of direct\nextraction of WM tracts from diffusion tensor images using convolutional neural\nnetwork (CNN). This 3D end-to-end method is trained to segment 25 WM tracts in\naging individuals from a large population-based study (N=9752, 1.5T MRI). The\nproposed method showed good segmentation performance and high reproducibility,\ni.e., a high spatial agreement (Cohen's kappa, k = 0.72 ~ 0.83) and a low\nscan-rescan error in tract-specific diffusion measures (e.g., fractional\nanisotropy: error = 1% ~ 5%). The reproducibility of the proposed method was\nhigher than that of a tractography-based segmentation algorithm, while being\norders of magnitude faster (0.5s to segment one tract). In addition, we showed\nthat the method successfully generalizes to diffusion scans from an external\ndementia dataset (N=58, 3T MRI). In two proof-of-principle experiments, we\nassociated WM microstructure obtained using the proposed method with age in a\nnormal elderly population, and with disease subtypes in a dementia cohort. In\nconcordance with the literature, results showed a widespread reduction of\nmicrostructural organization with aging and substantial group-wise\nmicrostructure differences between dementia subtypes. In conclusion, we\npresented a highly reproducible and fast method for WM tract segmentation that\nhas the potential of being used in large-scale studies and clinical practice.\n",
    "topics": "{}",
    "score": 0.8245019801
  },
  {
    "id": "1811.12673",
    "title": "ComDefend: An Efficient Image Compression Model to Defend Adversarial\n  Examples",
    "abstract": "  Deep neural networks (DNNs) have been demonstrated to be vulnerable to\nadversarial examples. Specifically, adding imperceptible perturbations to clean\nimages can fool the well trained deep neural networks. In this paper, we\npropose an end-to-end image compression model to defend adversarial examples:\n\\textbf{ComDefend}. The proposed model consists of a compression convolutional\nneural network (ComCNN) and a reconstruction convolutional neural network\n(ResCNN). The ComCNN is used to maintain the structure information of the\noriginal image and purify adversarial perturbations. And the ResCNN is used to\nreconstruct the original image with high quality. In other words, ComDefend can\ntransform the adversarial image to its clean version, which is then fed to the\ntrained classifier. Our method is a pre-processing module, and does not modify\nthe classifier's structure during the whole process. Therefore, it can be\ncombined with other model-specific defense models to jointly improve the\nclassifier's robustness. A series of experiments conducted on MNIST, CIFAR10\nand ImageNet show that the proposed method outperforms the state-of-the-art\ndefense methods, and is consistently effective to protect classifiers against\nadversarial attacks.\n",
    "topics": "{'Image Compression': 1.0}",
    "score": 0.8244113448
  },
  {
    "id": "2005.14511",
    "title": "NuClick: A Deep Learning Framework for Interactive Segmentation of\n  Microscopy Images",
    "abstract": "  Object segmentation is an important step in the workflow of computational\npathology. Deep learning based models generally require large amount of labeled\ndata for precise and reliable prediction. However, collecting labeled data is\nexpensive because it often requires expert knowledge, particularly in medical\nimaging domain where labels are the result of a time-consuming analysis made by\none or more human experts. As nuclei, cells and glands are fundamental objects\nfor downstream analysis in computational pathology/cytology, in this paper we\npropose a simple CNN-based approach to speed up collecting annotations for\nthese objects which requires minimum interaction from the annotator. We show\nthat for nuclei and cells in histology and cytology images, one click inside\neach object is enough for NuClick to yield a precise annotation. For\nmulticellular structures such as glands, we propose a novel approach to provide\nthe NuClick with a squiggle as a guiding signal, enabling it to segment the\nglandular boundaries. These supervisory signals are fed to the network as\nauxiliary inputs along with RGB channels. With detailed experiments, we show\nthat NuClick is adaptable to the object scale, robust against variations in the\nuser input, adaptable to new domains, and delivers reliable annotations. An\ninstance segmentation model trained on masks generated by NuClick achieved the\nfirst rank in LYON19 challenge. As exemplar outputs of our framework, we are\nreleasing two datasets: 1) a dataset of lymphocyte annotations within IHC\nimages, and 2) a dataset of segmented WBCs in blood smear images.\n",
    "topics": "{'Instance Segmentation': 0.9997714, 'Semantic Segmentation': 0.99947184}",
    "score": 0.8243283839
  },
  {
    "id": "1902.08716",
    "title": "Spatio-Temporal Convolutional LSTMs for Tumor Growth Prediction by\n  Learning 4D Longitudinal Patient Data",
    "abstract": "  Prognostic tumor growth modeling via volumetric medical imaging observations\ncan potentially lead to better outcomes of tumor treatment and surgical\nplanning. Recent advances of convolutional networks have demonstrated higher\naccuracy than traditional mathematical models in predicting future tumor\nvolumes. This indicates that deep learning-based techniques may have great\npotentials on addressing such problem. However, current 2D patch-based modeling\napproaches cannot make full use of the spatio-temporal imaging context of the\ntumor's longitudinal 4D (3D + time) data. Moreover, they are incapable to\npredict clinically-relevant tumor properties, other than volumes. In this\npaper, we exploit to formulate the tumor growth process through convolutional\nLong Short-Term Memory (ConvLSTM) that extract tumor's static imaging\nappearances and capture its temporal dynamic changes within a single network.\nWe extend ConvLSTM into the spatio-temporal domain (ST-ConvLSTM) by jointly\nlearning the inter-slice 3D contexts and the longitudinal or temporal dynamics\nfrom multiple patient studies. Our approach can incorporate other non-imaging\npatient information in an end-to-end trainable manner. Experiments are\nconducted on the largest 4D longitudinal tumor dataset of 33 patients to date.\nResults validate that the ST-ConvLSTM produces a Dice score of 83.2%+-5.1% and\na RVD of 11.2%+-10.8%, both significantly outperforming (p<0.05) other compared\nmethods of linear model, ConvLSTM, and generative adversarial network (GAN)\nunder the metric of predicting future tumor volumes. Additionally, our new\nmethod enables the prediction of both cell density and CT intensity numbers.\nLast, we demonstrate the generalizability of ST-ConvLSTM by employing it in 4D\nmedical image segmentation task, which achieves an averaged Dice score of\n86.3+-1.2% for left-ventricle segmentation in 4D ultrasound with 3 seconds per\npatient.\n",
    "topics": "{'Medical Image Segmentation': 0.99977106, 'Semantic Segmentation': 0.91019005}",
    "score": 0.8242424433
  },
  {
    "id": "1706.03791",
    "title": "Significantly Improving Lossy Compression for Scientific Data Sets Based\n  on Multidimensional Prediction and Error-Controlled Quantization",
    "abstract": "  Today's HPC applications are producing extremely large amounts of data, such\nthat data storage and analysis are becoming more challenging for scientific\nresearch. In this work, we design a new error-controlled lossy compression\nalgorithm for large-scale scientific data. Our key contribution is\nsignificantly improving the prediction hitting rate (or prediction accuracy)\nfor each data point based on its nearby data values along multiple dimensions.\nWe derive a series of multilayer prediction formulas and their unified formula\nin the context of data compression. One serious challenge is that the data\nprediction has to be performed based on the preceding decompressed values\nduring the compression in order to guarantee the error bounds, which may\ndegrade the prediction accuracy in turn. We explore the best layer for the\nprediction by considering the impact of compression errors on the prediction\naccuracy. Moreover, we propose an adaptive error-controlled quantization\nencoder, which can further improve the prediction hitting rate considerably.\nThe data size can be reduced significantly after performing the variable-length\nencoding because of the uneven distribution produced by our quantization\nencoder. We evaluate the new compressor on production scientific data sets and\ncompare it with many other state-of-the-art compressors: GZIP, FPZIP, ZFP,\nSZ-1.1, and ISABELA. Experiments show that our compressor is the best in class,\nespecially with regard to compression factors (or bit-rates) and compression\nerrors (including RMSE, NRMSE, and PSNR). Our solution is better than the\nsecond-best solution by more than a 2x increase in the compression factor and\n3.8x reduction in the normalized root mean squared error on average, with\nreasonable error bounds and user-desired bit-rates.\n",
    "topics": "{'Quantization': 0.99999595}",
    "score": 0.824152892
  },
  {
    "id": "2009.11166",
    "title": "Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial\n  Network Normalizer",
    "abstract": "  Foreseeing the brain evolution as a complex highly inter-connected system,\nwidely modeled as a graph, is crucial for mapping dynamic interactions between\ndifferent anatomical regions of interest (ROIs) in health and disease.\nInterestingly, brain graph evolution models remain almost absent in the\nliterature. Here we design an adversarial brain network normalizer for\nrepresenting each brain network as a transformation of a fixed centered\npopulation-driven connectional template. Such graph normalization with respect\nto a fixed reference paves the way for reliably identifying the most similar\ntraining samples (i.e., brain graphs) to the testing sample at baseline\ntimepoint. The testing evolution trajectory will be then spanned by the\nselected training graphs and their corresponding evolution trajectories. We\nbase our prediction framework on geometric deep learning which naturally\noperates on graphs and nicely preserves their topological properties.\nSpecifically, we propose the first graph-based Generative Adversarial Network\n(gGAN) that not only learns how to normalize brain graphs with respect to a\nfixed connectional brain template (CBT) (i.e., a brain template that\nselectively captures the most common features across a brain population) but\nalso learns a high-order representation of the brain graphs also called\nembeddings. We use these embeddings to compute the similarity between training\nand testing subjects which allows us to pick the closest training subjects at\nbaseline timepoint to predict the evolution of the testing brain graph over\ntime. A series of benchmarks against several comparison methods showed that our\nproposed method achieved the lowest brain disease evolution prediction error\nusing a single baseline timepoint. Our gGAN code is available at\nhttp://github.com/basiralab/gGAN.\n",
    "topics": "{'Brain Segmentation': 0.9443408, 'Template Matching': 0.37295544}",
    "score": 0.8241460657
  },
  {
    "id": "1506.04912",
    "title": "Subsampled terahertz data reconstruction based on spatio-temporal\n  dictionary learning",
    "abstract": "  In this paper, the problem of terahertz pulsed imaging and reconstruction is\naddressed. It is assumed that an incomplete (subsampled) three dimensional THz\ndata set has been acquired and the aim is to recover all missing samples. A\nsparsity-inducing approach is proposed for this purpose. First, a simple\ninterpolation is applied to incomplete noisy data. Then, we propose a\nspatio-temporal dictionary learning method to obtain an appropriate sparse\nrepresentation of data based on a joint sparse recovery algorithm. Then, using\nthe sparse coefficients and the learned dictionary, the 3D data is effectively\ndenoised by minimizing a simple cost function. We consider two types of\nterahertz data to evaluate the performance of the proposed approach; THz data\nacquired for a model sample with clear layered structures (e.g., a T-shape\nplastic sheet buried in a polythene pellet), and pharmaceutical tablet data\n(with low spatial resolution). The achieved signal-to-noise-ratio for\nreconstruction of T-shape data, from only 5% observation was 19 dB. Moreover,\nthe accuracies of obtained thickness and depth measurements for pharmaceutical\ntablet data after reconstruction from 10% observation were 98.8%, and 99.9%,\nrespectively. These results, along with chemical mapping analysis, presented at\nthe end of this paper, confirm the accuracy of the proposed method.\n",
    "topics": "{'Dictionary Learning': 1.0, '3D Reconstruction': 0.93720835}",
    "score": 0.8237830409
  },
  {
    "id": "2009.05383",
    "title": "COVIDNet-CT: A Tailored Deep Convolutional Neural Network Design for\n  Detection of COVID-19 Cases from Chest CT Images",
    "abstract": "  The coronavirus disease 2019 (COVID-19) pandemic continues to have a\ntremendous impact on patients and healthcare systems around the world. In the\nfight against this novel disease, there is a pressing need for rapid and\neffective screening tools to identify patients infected with COVID-19, and to\nthis end CT imaging has been proposed as one of the key screening methods which\nmay be used as a complement to RT-PCR testing, particularly in situations where\npatients undergo routine CT scans for non-COVID-19 related reasons, patients\nwith worsening respiratory status or developing complications that require\nexpedited care, and patients suspected to be COVID-19-positive but have\nnegative RT-PCR test results. Motivated by this, in this study we introduce\nCOVIDNet-CT, a deep convolutional neural network architecture that is tailored\nfor detection of COVID-19 cases from chest CT images via a machine-driven\ndesign exploration approach. Additionally, we introduce COVIDx-CT, a benchmark\nCT image dataset derived from CT imaging data collected by the China National\nCenter for Bioinformation comprising 104,009 images across 1,489 patient cases.\nFurthermore, in the interest of reliability and transparency, we leverage an\nexplainability-driven performance validation strategy to investigate the\ndecision-making behaviour of COVIDNet-CT, and in doing so ensure that\nCOVIDNet-CT makes predictions based on relevant indicators in CT images. Both\nCOVIDNet-CT and the COVIDx-CT dataset are available to the general public in an\nopen-source and open access manner as part of the COVID-Net initiative. While\nCOVIDNet-CT is not yet a production-ready screening solution, we hope that\nreleasing the model and dataset will encourage researchers, clinicians, and\ncitizen data scientists alike to leverage and build upon them.\n",
    "topics": "{'Test results': 1.0, 'Decision Making': 0.8586856}",
    "score": 0.8234768895
  },
  {
    "id": "1709.03485",
    "title": "NiftyNet: a deep-learning platform for medical imaging",
    "abstract": "  Medical image analysis and computer-assisted intervention problems are\nincreasingly being addressed with deep-learning-based solutions. Established\ndeep-learning platforms are flexible but do not provide specific functionality\nfor medical image analysis and adapting them for this application requires\nsubstantial implementation effort. Thus, there has been substantial duplication\nof effort and incompatible infrastructure developed across many research\ngroups. This work presents the open-source NiftyNet platform for deep learning\nin medical imaging. The ambition of NiftyNet is to accelerate and simplify the\ndevelopment of these solutions, and to provide a common mechanism for\ndisseminating research outputs for the community to use, adapt and build upon.\n  NiftyNet provides a modular deep-learning pipeline for a range of medical\nimaging applications including segmentation, regression, image generation and\nrepresentation learning applications. Components of the NiftyNet pipeline\nincluding data loading, data augmentation, network architectures, loss\nfunctions and evaluation metrics are tailored to, and take advantage of, the\nidiosyncracies of medical image analysis and computer-assisted intervention.\nNiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D\nand 3D images and computational graphs by default.\n  We present 3 illustrative medical image analysis applications built using\nNiftyNet: (1) segmentation of multiple abdominal organs from computed\ntomography; (2) image regression to predict computed tomography attenuation\nmaps from brain magnetic resonance images; and (3) generation of simulated\nultrasound images for specified anatomical poses.\n  NiftyNet enables researchers to rapidly develop and distribute deep learning\nsolutions for segmentation, regression, image generation and representation\nlearning applications, or extend the platform to new applications.\n",
    "topics": "{'Image Generation': 0.9983814, 'Data Augmentation': 0.9944964, 'Representation Learning': 0.9937431}",
    "score": 0.8233981046
  },
  {
    "id": "2009.00320",
    "title": "Active Deep Densely Connected Convolutional Network for Hyperspectral\n  Image Classification",
    "abstract": "  Deep learning based methods have seen a massive rise in popularity for\nhyperspectral image classification over the past few years. However, the\nsuccess of deep learning is attributed greatly to numerous labeled samples. It\nis still very challenging to use only a few labeled samples to train deep\nlearning models to reach a high classification accuracy. An active\ndeep-learning framework trained by an end-to-end manner is, therefore, proposed\nby this paper in order to minimize the hyperspectral image classification\ncosts. First, a deep densely connected convolutional network is considered for\nhyperspectral image classification. Different from the traditional active\nlearning methods, an additional network is added to the designed deep densely\nconnected convolutional network to predict the loss of input samples. Then, the\nadditional network could be used to suggest unlabeled samples that the deep\ndensely connected convolutional network is more likely to produce a wrong\nlabel. Note that the additional network uses the intermediate features of the\ndeep densely connected convolutional network as input. Therefore, the proposed\nmethod is an end-to-end framework. Subsequently, a few of the selected samples\nare labelled manually and added to the training samples. The deep densely\nconnected convolutional network is therefore trained using the new training\nset. Finally, the steps above are repeated to train the whole framework\niteratively. Extensive experiments illustrates that the method proposed could\nreach a high accuracy in classification after selecting just a few samples.\n",
    "topics": "{'Hyperspectral Image Classification': 0.99999976, 'Image Classification': 0.9999968, 'Active Learning': 0.8814206}",
    "score": 0.8230907542
  },
  {
    "id": "2002.10257",
    "title": "Using Wavelets to Analyze Similarities in Image-Classification Datasets",
    "abstract": "  Deep learning image classifiers usually rely on huge training sets and their\ntraining process can be described as learning the similarities and differences\namong training images. But, images in large training sets are not usually\nstudied from this perspective and fine-level similarities and differences among\nimages is usually overlooked. This is due to lack of fast and efficient\ncomputational methods to analyze the contents of these datasets. Some studies\naim to identify the influential and redundant training images, but such methods\nrequire a model that is already trained on the entire training set. Here, using\nimage processing and numerical analysis tools we develop a practical and fast\nmethod to analyze the similarities in image classification datasets. We show\nthat such analysis can provide valuable insights about the datasets and the\nclassification task at hand, prior to training a model. Our method uses wavelet\ndecomposition of images and other numerical analysis tools, with no need for a\npre-trained model. Interestingly, the results we obtain corroborate the\nprevious results in the literature that analyzed the similarities using\npre-trained CNNs. We show that similar images in standard datasets (such as\nCIFAR) can be identified in a few seconds, a significant speed-up compared to\nalternative methods in the literature. By removing the computational speed\nobstacle, it becomes practical to gain new insights about the contents of\ndatasets and the models trained on them. We show that similarities between\ntraining and testing images may provide insights about the generalization of\nmodels. Finally, we investigate the similarities between images in relation to\ndecision boundaries of a trained model.\n",
    "topics": "{'Image Classification': 0.9885916}",
    "score": 0.8230762361
  },
  {
    "id": "1812.00155",
    "title": "Learning RoI Transformer for Detecting Oriented Objects in Aerial Images",
    "abstract": "  Object detection in aerial images is an active yet challenging task in\ncomputer vision because of the birdview perspective, the highly complex\nbackgrounds, and the variant appearances of objects. Especially when detecting\ndensely packed objects in aerial images, methods relying on horizontal\nproposals for common object detection often introduce mismatches between the\nRegion of Interests (RoIs) and objects. This leads to the common misalignment\nbetween the final object classification confidence and localization accuracy.\nAlthough rotated anchors have been used to tackle this problem, the design of\nthem always multiplies the number of anchors and dramatically increases the\ncomputational complexity. In this paper, we propose a RoI Transformer to\naddress these problems. More precisely, to improve the quality of region\nproposals, we first designed a Rotated RoI (RRoI) learner to transform a\nHorizontal Region of Interest (HRoI) into a Rotated Region of Interest (RRoI).\nBased on the RRoIs, we then proposed a Rotated Position Sensitive RoI Align\n(RPS-RoI-Align) module to extract rotation-invariant features from them for\nboosting subsequent classification and regression. Our RoI Transformer is with\nlight weight and can be easily embedded into detectors for oriented object\ndetection. A simple implementation of the RoI Transformer has achieved\nstate-of-the-art performances on two common and challenging aerial datasets,\ni.e., DOTA and HRSC2016, with a neglectable reduction to detection speed. Our\nRoI Transformer exceeds the deformable Position Sensitive RoI pooling when\noriented bounding-box annotations are available. Extensive experiments have\nalso validated the flexibility and effectiveness of our RoI Transformer. The\nresults demonstrate that it can be easily integrated with other detector\narchitectures and significantly improve the performances.\n",
    "topics": "{'Object Detection': 0.99998844, 'Object Classification': 0.9967642}",
    "score": 0.8229226793
  },
  {
    "id": "1907.09728",
    "title": "Interpretable and Steerable Sequence Learning via Prototypes",
    "abstract": "  One of the major challenges in machine learning nowadays is to provide\npredictions with not only high accuracy but also user-friendly explanations.\nAlthough in recent years we have witnessed increasingly popular use of deep\nneural networks for sequence modeling, it is still challenging to explain the\nrationales behind the model outputs, which is essential for building trust and\nsupporting the domain experts to validate, critique and refine the model. We\npropose ProSeNet, an interpretable and steerable deep sequence model with\nnatural explanations derived from case-based reasoning. The prediction is\nobtained by comparing the inputs to a few prototypes, which are exemplar cases\nin the problem domain. For better interpretability, we define several criteria\nfor constructing the prototypes, including simplicity, diversity, and sparsity\nand propose the learning objective and the optimization procedure. ProSeNet\nalso provides a user-friendly approach to model steering: domain experts\nwithout any knowledge on the underlying model or parameters can easily\nincorporate their intuition and experience by manually refining the prototypes.\nWe conduct experiments on a wide range of real-world applications, including\npredictive diagnostics for automobiles, ECG, and protein sequence\nclassification and sentiment analysis on texts. The result shows that ProSeNet\ncan achieve accuracy on par with state-of-the-art deep learning models. We also\nevaluate the interpretability of the results with concrete case studies.\nFinally, through user study on Amazon Mechanical Turk (MTurk), we demonstrate\nthat the model selects high-quality prototypes which align well with human\nknowledge and can be interactively refined for better interpretability without\nloss of performance.\n",
    "topics": "{'Sentiment Analysis': 0.8747642}",
    "score": 0.8228745861
  },
  {
    "id": "1803.10567",
    "title": "Image Generation and Translation with Disentangled Representations",
    "abstract": "  Generative models have made significant progress in the tasks of modeling\ncomplex data distributions such as natural images. The introduction of\nGenerative Adversarial Networks (GANs) and auto-encoders lead to the\npossibility of training on big data sets in an unsupervised manner. However,\nfor many generative models it is not possible to specify what kind of image\nshould be generated and it is not possible to translate existing images into\nnew images of similar domains. Furthermore, models that can perform\nimage-to-image translation often need distinct models for each domain, making\nit hard to scale these systems to multiple domain image-to-image translation.\nWe introduce a model that can do both, controllable image generation and\nimage-to-image translation between multiple domains. We split our image\nrepresentation into two parts encoding unstructured and structured information\nrespectively. The latter is designed in a disentangled manner, so that\ndifferent parts encode different image characteristics. We train an encoder to\nencode images into these representations and use a small amount of labeled data\nto specify what kind of information should be encoded in the disentangled part.\nA generator is trained to generate images from these representations using the\ncharacteristics provided by the disentangled part of the representation.\nThrough this we can control what kind of images the generator generates,\ntranslate images between different domains, and even learn unknown\ndata-generating factors while only using one single model.\n",
    "topics": "{'Image-to-Image Translation': 1.0, 'Image Generation': 0.99999845}",
    "score": 0.8228469617
  },
  {
    "id": "2007.06676",
    "title": "UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a\n  Generic Framework for Handling Common Camera Distortion Models",
    "abstract": "  In classical computer vision, rectification is an integral part of multi-view\ndepth estimation. It typically includes epipolar rectification and lens\ndistortion correction. This process simplifies the depth estimation\nsignificantly, and thus it has been adopted in CNN approaches. However,\nrectification has several side effects, including a reduced field of view\n(FOV), resampling distortion, and sensitivity to calibration errors. The\neffects are particularly pronounced in case of significant distortion (e.g.,\nwide-angle fisheye cameras). In this paper, we propose a generic scale-aware\nself-supervised pipeline for estimating depth, euclidean distance, and visual\nodometry from unrectified monocular videos. We demonstrate a similar level of\nprecision on the unrectified KITTI dataset with barrel distortion comparable to\nthe rectified KITTI dataset. The intuition being that the rectification step\ncan be implicitly absorbed within the CNN model, which learns the distortion\nmodel without increasing complexity. Our approach does not suffer from a\nreduced field of view and avoids computational costs for rectification at\ninference time. To further illustrate the general applicability of the proposed\nframework, we apply it to wide-angle fisheye cameras with 190$^\\circ$\nhorizontal field of view. The training framework UnRectDepthNet takes in the\ncamera distortion model as an argument and adapts projection and unprojection\nfunctions accordingly. The proposed algorithm is evaluated further on the KITTI\nrectified dataset, and we achieve state-of-the-art results that improve upon\nour previous work FisheyeDistanceNet. Qualitative results on a distorted test\nscene video sequence indicate excellent performance\nhttps://youtu.be/K6pbx3bU4Ss.\n",
    "topics": "{'Depth Estimation': 0.9999999, 'Monocular Depth Estimation': 0.99999523, 'Visual Odometry': 0.99923587}",
    "score": 0.82281033
  },
  {
    "id": "1810.13304",
    "title": "Acute and sub-acute stroke lesion segmentation from multimodal MRI",
    "abstract": "  Acute stroke lesion segmentation tasks are of great clinical interest as they\ncan help doctors make better informed treatment decisions. Magnetic resonance\nimaging (MRI) is time demanding but can provide images that are considered gold\nstandard for diagnosis. Automated stroke lesion segmentation can provide with\nan estimate of the location and volume of the lesioned tissue, which can help\nin the clinical practice to better assess and evaluate the risks of each\ntreatment. We propose a deep learning methodology for acute and sub-acute\nstroke lesion segmentation using multimodal MR imaging. The proposed method is\nevaluated using two public datasets from the 2015 Ischemic Stroke Lesion\nSegmentation challenge (ISLES 2015). These involve the tasks of sub-acute\nstroke lesion segmentation (SISS) and acute stroke penumbra estimation (SPES)\nfrom diffusion, perfusion and anatomical MRI modalities. The performance is\ncompared against state-of-the-art methods with a blind online testing set\nevaluation on each of the challenges. At the time of submitting this\nmanuscript, our approach is the first method in the online rankings for the\nSISS (DSC=0.59$\\pm$0.31) and SPES sub-tasks (DSC=0.84$\\pm$0.10). When compared\nwith the rest of submitted strategies, we achieve top rank performance with a\nlower Hausdorff distance. Better segmentation results are obtained by\nleveraging the anatomy and pathophysiology of acute stroke lesions and using a\ncombined approach to minimize the effects of class imbalance. The same training\nprocedure is used for both tasks, showing the proposed methodology can\ngeneralize well enough to deal with different unrelated tasks and imaging\nmodalities without training hyper-parameter tuning. A public version of the\nproposed method has been released to the scientific community at\nhttps://github.com/NIC-VICOROB/stroke-mri-segmentation.\n",
    "topics": "{'Lesion Segmentation': 1.0}",
    "score": 0.8227877987
  },
  {
    "id": "2002.02200",
    "title": "RGB-based Semantic Segmentation Using Self-Supervised Depth Pre-Training",
    "abstract": "  Although well-known large-scale datasets, such as ImageNet, have driven image\nunderstanding forward, most of these datasets require extensive manual\nannotation and are thus not easily scalable. This limits the advancement of\nimage understanding techniques. The impact of these large-scale datasets can be\nobserved in almost every vision task and technique in the form of pre-training\nfor initialization. In this work, we propose an easily scalable and\nself-supervised technique that can be used to pre-train any semantic RGB\nsegmentation method. In particular, our pre-training approach makes use of\nautomatically generated labels that can be obtained using depth sensors. These\nlabels, denoted by HN-labels, represent different height and normal patches,\nwhich allow mining of local semantic information that is useful in the task of\nsemantic RGB segmentation. We show how our proposed self-supervised\npre-training with HN-labels can be used to replace ImageNet pre-training, while\nusing 25x less images and without requiring any manual labeling. We pre-train a\nsemantic segmentation network with our HN-labels, which resembles our final\ntask more than pre-training on a less related task, e.g. classification with\nImageNet. We evaluate on two datasets (NYUv2 and CamVid), and we show how the\nsimilarity in tasks is advantageous not only in speeding up the pre-training\nprocess, but also in achieving better final semantic segmentation accuracy than\nImageNet pre-training\n",
    "topics": "{'Semantic Segmentation': 0.9998598}",
    "score": 0.8226963662
  },
  {
    "id": "1803.08264",
    "title": "IMHOTEP - Virtual Reality Framework for Surgical Applications",
    "abstract": "  Purpose: The data which is available to surgeons before, during and after\nsurgery is steadily increasing in quantity as well as diversity. When planning\na patient's treatment, this large amount of information can be difficult to\ninterpret. To aid in processing the information, new methods need to be found\nto present multi-modal patient data, ideally combining textual, imagery,\ntemporal and 3D data in a holistic and context-aware system. Methods: We\npresent an open-source framework which allows handling of patient data in a\nvirtual reality (VR) environment. By using VR technology, the workspace\navailable to the surgeon is maximized and 3D patient data is rendered in\nstereo, which increases depth perception. The framework organizes the data into\nworkspaces and contains tools which allow users to control, manipulate and\nenhance the data. Due to the framework's modular design, it can easily be\nadapted and extended for various clinical applications. Results: The framework\nwas evaluated by clinical personnel (77 participants). The majority of the\ngroup stated that a complex surgical situation is easier to comprehend by using\nthe framework, and that it is very well suited for education. Furthermore, the\napplication to various clinical scenarios - including the simulation of\nexcitation-propagation in the human atrium - demonstrated the framework's\nadaptability. As a feasibility study, the framework was used during the\nplanning phase of the surgical removal of a large central carcinoma from a\npatient's liver. Conclusion: The clinical evaluation showed a large potential\nand high acceptance for the VR environment in a medical context. The various\napplications confirmed that the framework is easily extended and can be used in\nreal-time simulation as well as for the manipulation of complex anatomical\nstructures.\n",
    "topics": "{}",
    "score": 0.8226079399
  },
  {
    "id": "2003.05731",
    "title": "SUOD: Accelerating Large-scare Unsupervised Heterogeneous Outlier\n  Detection",
    "abstract": "  Outlier detection (OD) is a key data mining task for identifying abnormal\nobjects from general samples with numerous high-stake applications including\nfraud detection and intrusion detection. Due to the lack of ground truth\nlabels, practitioners often have to build a large number of unsupervised models\nthat are heterogeneous (i.e., different algorithms and hyperparameters) for\nfurther combination and analysis with ensemble learning, rather than relying on\na single model. However, this yields severe scalability issues on\nhigh-dimensional, large datasets.\n  How to accelerate the training and predicting with a large number of\nheterogeneous unsupervised OD models? How to ensure the acceleration does not\ndeteriorate detection models' accuracy? How to accommodate the acceleration\nneed for both a single worker setting and a distributed system with multiple\nworkers? In this study, we propose a three-module acceleration system called\nSUOD (scalable unsupervised outlier detection) to address these questions. It\nfocuses on three complementary aspects to accelerate (dimensionality reduction\nfor high-dimensional data, model approximation for complex models, and\nexecution efficiency improvement for taskload imbalance within distributed\nsystems), while controlling detection performance degradation. Extensive\nexperiments on more than 20 benchmark datasets demonstrate SUOD's effectiveness\nin heterogeneous OD acceleration. By the submission time, the released\nopen-source system has been widely used with more than 700,000 times downloads.\nA real-world deployment case on fraudulent claim analysis at IQVIA, a leading\nhealthcare firm, is also provided.\n",
    "topics": "{'Outlier Detection': 1.0, 'Intrusion Detection': 0.9999975, 'Fraud Detection': 0.9999931, 'Dimensionality Reduction': 0.32214394}",
    "score": 0.8225643713
  },
  {
    "id": "2006.08509",
    "title": "APQ: Joint Search for Network Architecture, Pruning and Quantization\n  Policy",
    "abstract": "  We present APQ for efficient deep learning inference on resource-constrained\nhardware. Unlike previous methods that separately search the neural\narchitecture, pruning policy, and quantization policy, we optimize them in a\njoint manner. To deal with the larger design space it brings, a promising\napproach is to train a quantization-aware accuracy predictor to quickly get the\naccuracy of the quantized model and feed it to the search engine to select the\nbest fit. However, training this quantization-aware accuracy predictor requires\ncollecting a large number of quantized <model, accuracy> pairs, which involves\nquantization-aware finetuning and thus is highly time-consuming. To tackle this\nchallenge, we propose to transfer the knowledge from a full-precision (i.e.,\nfp32) accuracy predictor to the quantization-aware (i.e., int8) accuracy\npredictor, which greatly improves the sample efficiency. Besides, collecting\nthe dataset for the fp32 accuracy predictor only requires to evaluate neural\nnetworks without any training cost by sampling from a pretrained once-for-all\nnetwork, which is highly efficient. Extensive experiments on ImageNet\ndemonstrate the benefits of our joint optimization approach. With the same\naccuracy, APQ reduces the latency/energy by 2x/1.3x over MobileNetV2+HAQ.\nCompared to the separate optimization approach (ProxylessNAS+AMC+HAQ), APQ\nachieves 2.3% higher ImageNet accuracy while reducing orders of magnitude GPU\nhours and CO2 emission, pushing the frontier for green AI that is\nenvironmental-friendly. The code and video are publicly available.\n",
    "topics": "{'Quantization': 0.9999043}",
    "score": 0.8223158692
  },
  {
    "id": "2007.04934",
    "title": "Anyone here? Smart embedded low-resolution omnidirectional video sensor\n  to measure room occupancy",
    "abstract": "  In this paper, we present a room occupancy sensing solution with unique\nproperties: (i) It is based on an omnidirectional vision camera, capturing rich\nscene info over a wide angle, enabling to count the number of people in a room\nand even their position. (ii) Although it uses a camera-input, no privacy\nissues arise because its extremely low image resolution, rendering people\nunrecognisable. (iii) The neural network inference is running entirely on a\nlow-cost processing platform embedded in the sensor, reducing the privacy risk\neven further. (iv) Limited manual data annotation is needed, because of the\nself-training scheme we propose. Such a smart room occupancy rate sensor can be\nused in e.g. meeting rooms and flex-desks. Indeed, by encouraging flex-desking,\nthe required office space can be reduced significantly. In some cases, however,\na flex-desk that has been reserved remains unoccupied without an update in the\nreservation system. A similar problem occurs with meeting rooms, which are\noften under-occupied. By optimising the occupancy rate a huge reduction in\ncosts can be achieved. Therefore, in this paper, we develop such system which\ndetermines the number of people present in office flex-desks and meeting rooms.\nUsing an omnidirectional camera mounted in the ceiling, combined with a person\ndetector, the company can intelligently update the reservation system based on\nthe measured occupancy. Next to the optimisation and embedded implementation of\nsuch a self-training omnidirectional people detection algorithm, in this work\nwe propose a novel approach that combines spatial and temporal image data,\nimproving performance of our system on extreme low-resolution images.\n",
    "topics": "{'Human Detection': 0.4907924}",
    "score": 0.8223096926
  },
  {
    "id": "1910.07763",
    "title": "Mixture-of-Experts Variational Autoencoder for clustering and generating\n  from similarity-based representations",
    "abstract": "  Clustering high-dimensional data, such as images or biological measurements,\nis a long-standing problem and has been studied extensively. Recently, Deep\nClustering gained popularity due to its flexibility in fitting the specific\npeculiarities of complex data. Here we introduce the Mixture-of-Experts\nSimilarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering\nmodel. The model can learn multi-modal distributions of high-dimensional data\nand use these to generate realistic data with high efficacy and efficiency.\nMoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder\nconsists of a Mixture-of-Experts (MoE) architecture. This specific architecture\nallows for various modes of the data to be automatically learned by means of\nthe experts. Additionally, we encourage the lower dimensional latent\nrepresentation of our model to follow a Gaussian mixture distribution and to\naccurately represent the similarities between the data points. We assess the\nperformance of our model on the MNIST benchmark data set and a challenging\nreal-world task of defining cell subpopulations from mass cytometry (CyTOF)\nmeasurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior\nclustering performance on all these tasks in comparison to the baselines as\nwell as competitor methods and we show that the MoE architecture in the decoder\nreduces the computational cost of sampling specific data modes with high\nfidelity.\n",
    "topics": "{'Deep Clustering': 0.99998116}",
    "score": 0.8222165381
  },
  {
    "id": "2009.12591",
    "title": "ProDOMA: improve PROtein DOMAin classification for third-generation\n  sequencing reads using deep learning",
    "abstract": "  Motivation: With the development of third-generation sequencing technologies,\npeople are able to obtain DNA sequences with lengths from 10s to 100s of kb.\nThese long reads allow protein domain annotation without assembly, thus can\nproduce important insights into the biological functions of the underlying\ndata. However, the high error rate in third-generation sequencing data raises a\nnew challenge to established domain analysis pipelines. The state-of-the-art\nmethods are not optimized for noisy reads and have shown unsatisfactory\naccuracy of domain classification in third-generation sequencing data. New\ncomputational methods are still needed to improve the performance of domain\nprediction in long noisy reads. Results: In this work, we introduce ProDOMA, a\ndeep learning model that conducts domain classification for third-generation\nsequencing reads. It uses deep neural networks with 3-frame translation\nencoding to learn conserved features from partially correct translations. In\naddition, we formulate our problem as an open-set problem and thus our model\ncan reject unrelated DNA reads such as those from noncoding regions. In the\nexperiments on simulated reads of protein coding sequences and real reads from\nthe human genome, our model outperforms HMMER and DeepFam on protein domain\nclassification. In summary, ProDOMA is a useful end-to-end protein domain\nanalysis tool for long noisy reads without relying on error correction.\nAvailability: The source code and the trained model are freely available at\nhttps://github.com/strideradu/ProDOMA. Contact: yannisun@cityu.edu.hk\n",
    "topics": "{}",
    "score": 0.8221834308
  },
  {
    "id": "2003.06356",
    "title": "Advanced Deep Learning Methodologies for Skin Cancer Classification in\n  Prodromal Stages",
    "abstract": "  Technology-assisted platforms provide reliable solutions in almost every\nfield these days. One such important application in the medical field is the\nskin cancer classification in preliminary stages that need sensitive and\nprecise data analysis. For the proposed study the Kaggle skin cancer dataset is\nutilized. The proposed study consists of two main phases. In the first phase,\nthe images are preprocessed to remove the clutters thus producing a refined\nversion of training images. To achieve that, a sharpening filter is applied\nfollowed by a hair removal algorithm. Different image quality measurement\nmetrics including Peak Signal to Noise (PSNR), Mean Square Error (MSE), Maximum\nAbsolute Squared Deviation (MXERR) and Energy Ratio/ Ratio of Squared Norms\n(L2RAT) are used to compare the overall image quality before and after applying\npreprocessing operations. The results from the aforementioned image quality\nmetrics prove that image quality is not compromised however it is upgraded by\napplying the preprocessing operations. The second phase of the proposed\nresearch work incorporates deep learning methodologies that play an imperative\nrole in accurate, precise and robust classification of the lesion mole. This\nhas been reflected by using two state of the art deep learning models:\nInception-v3 and MobileNet. The experimental results demonstrate notable\nimprovement in train and validation accuracy by using the refined version of\nimages of both the networks, however, the Inception-v3 network was able to\nachieve better validation accuracy thus it was finally selected to evaluate it\non test data. The final test accuracy using state of art Inception-v3 network\nwas 86%.\n",
    "topics": "{}",
    "score": 0.8221433653
  },
  {
    "id": "1506.07251",
    "title": "Benchmark of structured machine learning methods for microbial\n  identification from mass-spectrometry data",
    "abstract": "  Microbial identification is a central issue in microbiology, in particular in\nthe fields of infectious diseases diagnosis and industrial quality control. The\nconcept of species is tightly linked to the concept of biological and clinical\nclassification where the proximity between species is generally measured in\nterms of evolutionary distances and/or clinical phenotypes. Surprisingly, the\ninformation provided by this well-known hierarchical structure is rarely used\nby machine learning-based automatic microbial identification systems.\nStructured machine learning methods were recently proposed for taking into\naccount the structure embedded in a hierarchy and using it as additional a\npriori information, and could therefore allow to improve microbial\nidentification systems. We test and compare several state-of-the-art machine\nlearning methods for microbial identification on a new Matrix-Assisted Laser\nDesorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset.\nWe include in the benchmark standard and structured methods, that leverage the\nknowledge of the underlying hierarchical structure in the learning process. Our\nresults show that although some methods perform better than others, structured\nmethods do not consistently perform better than their \"flat\" counterparts. We\npostulate that this is partly due to the fact that standard methods already\nreach a high level of accuracy in this context, and that they mainly confuse\nspecies close to each other in the tree, a case where using the known hierarchy\nis not helpful.\n",
    "topics": "{}",
    "score": 0.8220967289
  },
  {
    "id": "1901.09124",
    "title": "DeepSZ: A Novel Framework to Compress Deep Neural Networks by Using\n  Error-Bounded Lossy Compression",
    "abstract": "  DNNs have been quickly and broadly exploited to improve the data analysis\nquality in many complex science and engineering applications. Today's DNNs are\nbecoming deeper and wider because of increasing demand on the analysis quality\nand more and more complex applications to resolve. The wide and deep DNNs,\nhowever, require large amounts of resources, significantly restricting their\nutilization on resource-constrained systems. Although some network\nsimplification methods have been proposed to address this issue, they suffer\nfrom either low compression ratios or high compression errors, which may\nintroduce a costly retraining process for the target accuracy. In this paper,\nwe propose DeepSZ: an accuracy-loss bounded neural network compression\nframework, which involves four key steps: network pruning, error bound\nassessment, optimization for error bound configuration, and compressed model\ngeneration, featuring a high compression ratio and low encoding time. The\ncontribution is three-fold. (1) We develop an adaptive approach to select the\nfeasible error bounds for each layer. (2) We build a model to estimate the\noverall loss of accuracy based on the accuracy degradation caused by individual\ndecompressed layers. (3) We develop an efficient optimization algorithm to\ndetermine the best-fit configuration of error bounds in order to maximize the\ncompression ratio under the user-set accuracy constraint. Experiments show that\nDeepSZ can compress AlexNet and VGG-16 on the ImageNet by a compression ratio\nof 46X and 116X, respectively, and compress LeNet-300-100 and LeNet-5 on the\nMNIST by a compression ratio of 57X and 56X, respectively, with only up to 0.3%\nloss of accuracy. Compared with other state-of-the-art methods, DeepSZ can\nimprove the compression ratio by up to 1.43X, the DNN encoding performance by\nup to 4.0X (with four Nvidia Tesla V100 GPUs), and the decoding performance by\nup to 6.2X.\n",
    "topics": "{'Network Pruning': 0.999998, 'Neural Network Compression': 0.99991405}",
    "score": 0.822040939
  },
  {
    "id": "1912.05170",
    "title": "Image Classification with Deep Learning in the Presence of Noisy Labels:\n  A Survey",
    "abstract": "  Image classification systems recently made a big leap with the advancement of\ndeep neural networks. However, these systems require an excessive amount of\nlabeled data in order to be trained properly. This is not always feasible due\nto several factors, such as expensiveness of labeling process or difficulty of\ncorrectly classifying data even for the experts. Because of these practical\nchallenges, label noise is a common problem in datasets and numerous methods to\ntrain deep networks with label noise are proposed in the literature. Although\ndeep networks are known to be relatively robust to label noise, their tendency\nto overfit data makes them vulnerable to memorizing even total random noise.\nTherefore, it is crucial to consider the existence of label noise and develop\ncounter algorithms to fade away its negative effects to train deep neural\nnetworks efficiently. Even though an extensive survey of machine learning\ntechniques under label noise exists, literature lacks a comprehensive survey of\nmethodologies centered explicitly around deep learning in the presence of noisy\nlabels. This paper aims to present these algorithms while categorizing them\ninto one of the two subgroups: noise model based and noise model free methods.\nAlgorithms in the first group aim to estimate the structure of the noise and\nuse this information to avoid the negative effects of noisy labels during\ntraining. On the other hand, methods in the second group try to come up with\nalgorithms that are inherently noise robust by using approaches like robust\nlosses, regularizers or other learning paradigms.\n",
    "topics": "{'Image Classification': 0.99969494}",
    "score": 0.8220393104
  },
  {
    "id": "2007.12229",
    "title": "SeismoFlow -- Data augmentation for the class imbalance problem",
    "abstract": "  In several application areas, such as medical diagnosis, spam filtering,\nfraud detection, and seismic data analysis, it is very usual to find relevant\nclassification tasks where some class occurrences are rare. This is the so\ncalled class imbalance problem, which is a challenge in machine learning. In\nthis work, we propose the SeismoFlow a flow-based generative model to create\nsynthetic samples, aiming to address the class imbalance. Inspired by the Glow\nmodel, it uses interpolation on the learned latent space to produce synthetic\nsamples for one rare class. We apply our approach to the development of a\nseismogram signal quality classifier. We introduce a dataset composed\nof5.223seismograms that are distributed between the good, medium, and bad\nclasses and with their respective frequencies of 66.68%,31.54%, and 1.76%. Our\nmethodology is evaluated on a stratified 10-fold cross-validation setting,\nusing the Miniceptionmodel as a baseline, and assessing the effects of adding\nthe generated samples on the training set of each iteration. In our\nexperiments, we achieve an improvement of 13.9% on the rare class F1-score,\nwhile not hurting the metric value for the other classes and thus observing the\noverall accuracy improvement. Our empirical findings indicate that our method\ncan generate high-quality synthetic seismograms with realistic looking and\nsufficient plurality to help the Miniception model to overcome the class\nimbalance problem. We believe that our results are a step forward in solving\nboth the task of seismogram signal quality classification and class imbalance.\n",
    "topics": "{'Medical Diagnosis': 0.99999964, 'Fraud Detection': 0.99999785, 'Data Augmentation': 0.98789793}",
    "score": 0.8219188233
  },
  {
    "id": "1710.09671",
    "title": "Improved Workflow for Unsupervised Multiphase Image Segmentation",
    "abstract": "  Quantitative image analysis often depends on accurate classification of\npixels through a segmentation process. However, imaging artifacts such as the\npartial volume effect and sensor noise complicate the classification process.\nThese effects increase the pixel intensity variance of each constituent class,\ncausing intensities from one class to overlap with another. This increased\nvariance makes threshold based segmentation methods insufficient due to\nambiguous overlap regions in the pixel intensity distributions. The class\nambiguity becomes even more complex for systems with more than two\nconstituents, such as unsaturated moist granular media. In this paper, we\npropose an image processing workflow that improves segmentation accuracy for\nmultiphase systems. First, the ambiguous transition regions between classes are\nidentified and removed, which allows for global thresholding of single-class\nregions. Then the transition regions are classified using a distance function,\nand finally both segmentations are combined into one classified image. This\nworkflow includes three methodologies for identifying transition pixels and we\ndemonstrate on a variety of synthetic images that these approaches are able to\naccurately separate the ambiguous transition pixels from the single-class\nregions. For situations with typical amounts of image noise, misclassification\nerrors and area differences calculated between each class of the synthetic\nimages and the resultant segmented images range from 0.69-1.48% and 0.01-0.74%,\nrespectively, showing the segmentation accuracy of this approach. We\ndemonstrate that we are able to accurately segment x-ray microtomography images\nof moist granular media using these computationally efficient methodologies.\n",
    "topics": "{'Semantic Segmentation': 0.9923585}",
    "score": 0.8218776634
  },
  {
    "id": "2006.13873",
    "title": "COVIDLite: A depth-wise separable deep neural network with white balance\n  and CLAHE for detection of COVID-19",
    "abstract": "  Background and Objective:Currently, the whole world is facing a pandemic\ndisease, novel Coronavirus also known as COVID-19, which spread in more than\n200 countries with around 3.3 million active cases and 4.4 lakh deaths\napproximately. Due to rapid increase in number of cases and limited supply of\ntesting kits, availability of alternative diagnostic method is necessary for\ncontaining the spread of COVID-19 cases at an early stage and reducing the\ndeath count. For making available an alternative diagnostic method, we proposed\na deep neural network based diagnostic method which can be easily integrated\nwith mobile devices for detection of COVID-19 and viral pneumonia using Chest\nX-rays (CXR) images. Methods:In this study, we have proposed a method named\nCOVIDLite, which is a combination of white balance followed by Contrast Limited\nAdaptive Histogram Equalization (CLAHE) and depth-wise separable convolutional\nneural network (DSCNN). In this method, white balance followed by CLAHE is used\nas an image preprocessing step for enhancing the visibility of CXR images and\nDSCNN trained using sparse cross entropy is used for image classification with\nlesser parameters and significantly lighter in size, i.e., 8.4 MB without\nquantization. Results:The proposed COVIDLite method resulted in improved\nperformance in comparison to vanilla DSCNN with no pre-processing. The proposed\nmethod achieved higher accuracy of 99.58% for binary classification, whereas\n96.43% for multiclass classification and out-performed various state-of-the-art\nmethods. Conclusion:Our proposed method, COVIDLite achieved exceptional results\non various performance metrics. With detailed model interpretations, COVIDLite\ncan assist radiologists in detecting COVID-19 patients from CXR images and can\nreduce the diagnosis time significantly.\n",
    "topics": "{'Image Classification': 0.90969867, 'Quantization': 0.8128582}",
    "score": 0.8218381781
  },
  {
    "id": "1910.09716",
    "title": "A deep active learning system for species identification and counting in\n  camera trap images",
    "abstract": "  Biodiversity conservation depends on accurate, up-to-date information about\nwildlife population distributions. Motion-activated cameras, also known as\ncamera traps, are a critical tool for population surveys, as they are cheap and\nnon-intrusive. However, extracting useful information from camera trap images\nis a cumbersome process: a typical camera trap survey may produce millions of\nimages that require slow, expensive manual review. Consequently, critical\ninformation is often lost due to resource limitations, and critical\nconservation questions may be answered too slowly to support decision-making.\nComputer vision is poised to dramatically increase efficiency in image-based\nbiodiversity surveys, and recent studies have harnessed deep learning\ntechniques for automatic information extraction from camera trap images.\nHowever, the accuracy of results depends on the amount, quality, and diversity\nof the data available to train models, and the literature has focused on\nprojects with millions of relevant, labeled training images. Many camera trap\nprojects do not have a large set of labeled images and hence cannot benefit\nfrom existing machine learning techniques. Furthermore, even projects that do\nhave labeled data from similar ecosystems have struggled to adopt deep learning\nmethods because image classification models overfit to specific image\nbackgrounds (i.e., camera locations). In this paper, we focus not on automating\nthe labeling of camera trap images, but on accelerating this process. We\ncombine the power of machine intelligence and human intelligence to build a\nscalable, fast, and accurate active learning system to minimize the manual work\nrequired to identify and count animals in camera trap images. Our proposed\nscheme can match the state of the art accuracy on a 3.2 million image dataset\nwith as few as 14,100 manual labels, which means decreasing manual labeling\neffort by over 99.5%.\n",
    "topics": "{'Active Learning': 0.9950558, 'Image Classification': 0.8932045, 'Decision Making': 0.81735593}",
    "score": 0.8218338916
  },
  {
    "id": "1701.07213",
    "title": "Learning from Label Proportions in Brain-Computer Interfaces: Online\n  Unsupervised Learning with Guarantees",
    "abstract": "  Objective: Using traditional approaches, a Brain-Computer Interface (BCI)\nrequires the collection of calibration data for new subjects prior to online\nuse. Calibration time can be reduced or eliminated e.g.~by transfer of a\npre-trained classifier or unsupervised adaptive classification methods which\nlearn from scratch and adapt over time. While such heuristics work well in\npractice, none of them can provide theoretical guarantees. Our objective is to\nmodify an event-related potential (ERP) paradigm to work in unison with the\nmachine learning decoder to achieve a reliable calibration-less decoding with a\nguarantee to recover the true class means.\n  Method: We introduce learning from label proportions (LLP) to the BCI\ncommunity as a new unsupervised, and easy-to-implement classification approach\nfor ERP-based BCIs. The LLP estimates the mean target and non-target responses\nbased on known proportions of these two classes in different groups of the\ndata. We modified a visual ERP speller to meet the requirements of the LLP. For\nevaluation, we ran simulations on artificially created data sets and conducted\nan online BCI study with N=13 subjects performing a copy-spelling task.\n  Results: Theoretical considerations show that LLP is guaranteed to minimize\nthe loss function similarly to a corresponding supervised classifier. It\nperformed well in simulations and in the online application, where 84.5% of\ncharacters were spelled correctly on average without prior calibration.\n  Significance: The continuously adapting LLP classifier is the first\nunsupervised decoder for ERP BCIs guaranteed to find the true class means. This\nmakes it an ideal solution to avoid a tedious calibration and to tackle\nnon-stationarities in the data. Additionally, LLP works on complementary\nprinciples compared to existing unsupervised methods, allowing for their\nfurther enhancement when combined with LLP.\n",
    "topics": "{}",
    "score": 0.8218201272
  },
  {
    "id": "2007.05008",
    "title": "StyPath: Style-Transfer Data Augmentation For Robust Histology Image\n  Classification",
    "abstract": "  The classification of Antibody Mediated Rejection (AMR) in kidney transplant\nremains challenging even for experienced nephropathologists; this is partly\nbecause histological tissue stain analysis is often characterized by low\ninter-observer agreement and poor reproducibility. One of the implicated causes\nfor inter-observer disagreement is the variability of tissue stain quality\nbetween (and within) pathology labs, coupled with the gradual fading of\narchival sections. Variations in stain colors and intensities can make tissue\nevaluation difficult for pathologists, ultimately affecting their ability to\ndescribe relevant morphological features. Being able to accurately predict the\nAMR status based on kidney histology images is crucial for improving patient\ntreatment and care. We propose a novel pipeline to build robust deep neural\nnetworks for AMR classification based on StyPath, a histological data\naugmentation technique that leverages a light weight style-transfer algorithm\nas a means to reduce sample-specific bias. Each image was generated in 1.84 +-\n0.03 seconds using a single GTX TITAN V gpu and pytorch, making it faster than\nother popular histological data augmentation techniques. We evaluated our model\nusing a Monte Carlo (MC) estimate of Bayesian performance and generate an\nepistemic measure of uncertainty to compare both the baseline and StyPath\naugmented models. We also generated Grad-CAM representations of the results\nwhich were assessed by an experienced nephropathologist; we used this\nqualitative analysis to elucidate on the assumptions being made by each model.\nOur results imply that our style-transfer augmentation technique improves\nhistological classification performance (reducing error from 14.8% to 11.5%)\nand generalization ability.\n",
    "topics": "{'Data Augmentation': 1.0, 'Image Classification': 0.97189575, 'Style Transfer': 0.93285984}",
    "score": 0.8217890283
  },
  {
    "id": "2006.03259",
    "title": "Real-time Human Activity Recognition Using Conditionally Parametrized\n  Convolutions on Mobile and Wearable Devices",
    "abstract": "  Recently, deep learning has represented an important research trend in human\nactivity recognition (HAR). In particular, deep convolutional neural networks\n(CNNs) have achieved state-of-the-art performance on various HAR datasets. For\ndeep learning, improvements in performance have to heavily rely on increasing\nmodel size or capacity to scale to larger and larger datasets, which inevitably\nleads to the increase of operations. A high number of operations in deep\nleaning increases computational cost and is not suitable for real-time HAR\nusing mobile and wearable sensors. Though shallow learning techniques often are\nlightweight, they could not achieve good performance. Therefore, deep learning\nmethods that can balance the trade-off between accuracy and computation cost is\nhighly needed, which to our knowledge has seldom been researched. In this\npaper, we for the first time propose a computation efficient CNN using\nconditionally parametrized convolution for real-time HAR on mobile and wearable\ndevices. We evaluate the proposed method on four public benchmark HAR datasets\nconsisting of WISDM dataset, PAMAP2 dataset, UNIMIB-SHAR dataset, and\nOPPORTUNITY dataset, achieving state-of-the-art accuracy without compromising\ncomputation cost. Various ablation experiments are performed to show how such a\nnetwork with large capacity is clearly preferable to baseline while requiring a\nsimilar amount of operations. The method can be used as a drop-in replacement\nfor the existing deep HAR architectures and easily deployed onto mobile and\nwearable devices for real-time HAR applications.\n",
    "topics": "{'Activity Recognition': 1.0}",
    "score": 0.8217816039
  },
  {
    "id": "1908.03093",
    "title": "ExtremeC3Net: Extreme Lightweight Portrait Segmentation Networks using\n  Advanced C3-modules",
    "abstract": "  Designing a lightweight and robust portrait segmentation algorithm is an\nimportant task for a wide range of face applications. However, the problem has\nbeen considered as a subset of the object segmentation problem. bviously,\nportrait segmentation has its unique requirements. First, because the portrait\nsegmentation is performed in the middle of a whole process of many realworld\napplications, it requires extremely lightweight models. Second, there has not\nbeen any public datasets in this domain that contain a sufficient number of\nimages with unbiased statistics. To solve the problems, we introduce a new\nextremely lightweight portrait segmentation model consisting of a two-branched\narchitecture based on the concentrated-comprehensive convolutions block. Our\nmethod reduces the number of parameters from 2.1M to 37.7K (around 98.2%\nreduction), while maintaining the accuracy within a 1% margin from the\nstate-of-the-art portrait segmentation method. In our qualitative and\nquantitative analysis on the EG1800 dataset, we show that our method\noutperforms various existing lightweight segmentation models. Second, we\npropose a simple method to create additional portrait segmentation data which\ncan improve accuracy on the EG1800 dataset. Also, we analyze the bias in public\ndatasets by additionally annotating race, gender, and age on our own. The\naugmented dataset, the additional annotations and code are available in\nhttps://github.com/HYOJINPARK/ExtPortraitSeg .\n",
    "topics": "{'Semantic Segmentation': 0.9335836}",
    "score": 0.8217361062
  },
  {
    "id": "1710.01408",
    "title": "A Fully Convolutional Network for Semantic Labeling of 3D Point Clouds",
    "abstract": "  When classifying point clouds, a large amount of time is devoted to the\nprocess of engineering a reliable set of features which are then passed to a\nclassifier of choice. Generally, such features - usually derived from the\n3D-covariance matrix - are computed using the surrounding neighborhood of\npoints. While these features capture local information, the process is usually\ntime-consuming, and requires the application at multiple scales combined with\ncontextual methods in order to adequately describe the diversity of objects\nwithin a scene. In this paper we present a 1D-fully convolutional network that\nconsumes terrain-normalized points directly with the corresponding spectral\ndata,if available, to generate point-wise labeling while implicitly learning\ncontextual features in an end-to-end fashion. Our method uses only the\n3D-coordinates and three corresponding spectral features for each point.\nSpectral features may either be extracted from 2D-georeferenced images, as\nshown here for Light Detection and Ranging (LiDAR) point clouds, or extracted\ndirectly for passive-derived point clouds,i.e. from muliple-view imagery. We\ntrain our network by splitting the data into square regions, and use a pooling\nlayer that respects the permutation-invariance of the input points. Evaluated\nusing the ISPRS 3D Semantic Labeling Contest, our method scored second place\nwith an overall accuracy of 81.6%. We ranked third place with a mean F1-score\nof 63.32%, surpassing the F1-score of the method with highest accuracy by\n1.69%. In addition to labeling 3D-point clouds, we also show that our method\ncan be easily extended to 2D-semantic segmentation tasks, with promising\ninitial results.\n",
    "topics": "{'Semantic Segmentation': 0.9549728}",
    "score": 0.8215939468
  },
  {
    "id": "2010.01251",
    "title": "UCP: Uniform Channel Pruning for Deep Convolutional Neural Networks\n  Compression and Acceleration",
    "abstract": "  To apply deep CNNs to mobile terminals and portable devices, many scholars\nhave recently worked on the compressing and accelerating deep convolutional\nneural networks. Based on this, we propose a novel uniform channel pruning\n(UCP) method to prune deep CNN, and the modified squeeze-and-excitation blocks\n(MSEB) is used to measure the importance of the channels in the convolutional\nlayers. The unimportant channels, including convolutional kernels related to\nthem, are pruned directly, which greatly reduces the storage cost and the\nnumber of calculations. There are two types of residual blocks in ResNet. For\nResNet with bottlenecks, we use the pruning method with traditional CNN to trim\nthe 3x3 convolutional layer in the middle of the blocks. For ResNet with basic\nresidual blocks, we propose an approach to consistently prune all residual\nblocks in the same stage to ensure that the compact network structure is\ndimensionally correct. Considering that the network loses considerable\ninformation after pruning and that the larger the pruning amplitude is, the\nmore information that will be lost, we do not choose fine-tuning but retrain\nfrom scratch to restore the accuracy of the network after pruning. Finally, we\nverified our method on CIFAR-10, CIFAR-100 and ILSVRC-2012 for image\nclassification. The results indicate that the performance of the compact\nnetwork after retraining from scratch, when the pruning rate is small, is\nbetter than the original network. Even when the pruning amplitude is large, the\naccuracy can be maintained or decreased slightly. On the CIFAR-100, when\nreducing the parameters and FLOPs up to 82% and 62% respectively, the accuracy\nof VGG-19 even improved by 0.54% after retraining.\n",
    "topics": "{'Network Pruning': 1.0, 'Image Classification': 0.99376595}",
    "score": 0.8213944468
  },
  {
    "id": "2009.13627",
    "title": "Fully Automated Left Atrium Segmentation from Anatomical Cine Long-axis\n  MRI Sequences using Deep Convolutional Neural Network with Unscented Kalman\n  Filter",
    "abstract": "  This study proposes a fully automated approach for the left atrial\nsegmentation from routine cine long-axis cardiac magnetic resonance image\nsequences using deep convolutional neural networks and Bayesian filtering. The\nproposed approach consists of a classification network that automatically\ndetects the type of long-axis sequence and three different convolutional neural\nnetwork models followed by unscented Kalman filtering (UKF) that delineates the\nleft atrium. Instead of training and predicting all long-axis sequence types\ntogether, the proposed approach first identifies the image sequence type as to\n2, 3 and 4 chamber views, and then performs prediction based on neural nets\ntrained for that particular sequence type. The datasets were acquired\nretrospectively and ground truth manual segmentation was provided by an expert\nradiologist. In addition to neural net based classification and segmentation,\nanother neural net is trained and utilized to select image sequences for\nfurther processing using UKF to impose temporal consistency over cardiac cycle.\nA cyclic dynamic model with time-varying angular frequency is introduced in UKF\nto characterize the variations in cardiac motion during image scanning. The\nproposed approach was trained and evaluated separately with varying amount of\ntraining data with images acquired from 20, 40, 60 and 80 patients. Evaluations\nover 1515 images with equal number of images from each chamber group acquired\nfrom an additional 20 patients demonstrated that the proposed model\noutperformed state-of-the-art and yielded a mean Dice coefficient value of\n94.1%, 93.7% and 90.1% for 2, 3 and 4-chamber sequences, respectively, when\ntrained with datasets from 80 patients.\n",
    "topics": "{}",
    "score": 0.8212589554
  },
  {
    "id": "2003.09150",
    "title": "Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual\n  Categorization",
    "abstract": "  ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is one of the most\nauthoritative academic competitions in the field of Computer Vision (CV) in\nrecent years. But applying ILSVRC's annual champion directly to fine-grained\nvisual categorization (FGVC) tasks does not achieve good performance. To FGVC\ntasks, the small inter-class variations and the large intra-class variations\nmake it a challenging problem. Our attention object location module (AOLM) can\npredict the position of the object and attention part proposal module (APPM)\ncan propose informative part regions without the need of bounding-box or part\nannotations. The obtained object images not only contain almost the entire\nstructure of the object, but also contains more details, part images have many\ndifferent scales and more fine-grained features, and the raw images contain the\ncomplete object. The three kinds of training images are supervised by our\nmulti-branch network. Therefore, our multi-branch and multi-scale learning\nnetwork(MMAL-Net) has good classification ability and robustness for images of\ndifferent scales. Our approach can be trained end-to-end, while provides short\ninference time. Through the comprehensive experiments demonstrate that our\napproach can achieves state-of-the-art results on CUB-200-2011, FGVC-Aircraft\nand Stanford Cars datasets. Our code will be available at\nhttps://github.com/ZF1044404254/MMAL-Net\n",
    "topics": "{'Fine-Grained Image Classification': 0.9999887, 'Object Recognition': 0.5524247}",
    "score": 0.8212016645
  },
  {
    "id": "2001.00258",
    "title": "A Generalized Deep Learning Framework for Whole-Slide Image Segmentation\n  and Analysis",
    "abstract": "  Histopathology tissue analysis is considered the gold standard in cancer\ndiagnosis and prognosis. Given the large size of these images and the increase\nin the number of potential cancer cases, an automated solution as an aid to\nhistopathologists is highly desirable. In the recent past, deep learning-based\ntechniques have provided state of the art results in a wide variety of image\nanalysis tasks, including analysis of digitized slides. However, the size of\nimages and variability in histopathology tasks makes it a challenge to develop\nan integrated framework for histopathology image analysis. We propose a deep\nlearning-based framework for histopathology tissue analysis. We demonstrate the\ngeneralizability of our framework, including training and inference, on several\nopen-source datasets, which include CAMELYON (breast cancer metastases),\nDigestPath (colon cancer), and PAIP (liver cancer) datasets. We discuss\nmultiple types of uncertainties pertaining to data and model, namely aleatoric\nand epistemic, respectively. Simultaneously, we demonstrate our model\ngeneralization across different data distribution by evaluating some samples on\nTCGA data. On CAMELYON16 test data (n=139) for the task of lesion detection,\nthe FROC score achieved was 0.86 and in the CAMELYON17 test-data (n=500) for\nthe task of pN-staging the Cohen's kappa score achieved was 0.9090 (third in\nthe open leaderboard). On DigestPath test data (n=212) for the task of tumor\nsegmentation, a Dice score of 0.782 was achieved (fourth in the challenge). On\nPAIP test data (n=40) for the task of viable tumor segmentation, a Jaccard\nIndex of 0.75 (third in the challenge) was achieved, and for viable tumor\nburden, a score of 0.633 was achieved (second in the challenge). Our entire\nframework and related documentation are freely available at GitHub and PyPi.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'whole slide images': 0.9860151, 'Semantic Segmentation': 0.71745497}",
    "score": 0.8210762578
  },
  {
    "id": "1911.10417",
    "title": "Atlas Based Segmentations via Semi-Supervised Diffeomorphic\n  Registrations",
    "abstract": "  Purpose: Segmentation of organs-at-risk (OARs) is a bottleneck in current\nradiation oncology pipelines and is often time consuming and labor intensive.\nIn this paper, we propose an atlas-based semi-supervised registration algorithm\nto generate accurate segmentations of OARs for which there are ground truth\ncontours and rough segmentations of all other OARs in the atlas. To the best of\nour knowledge, this is the first study to use learning-based registration\nmethods for the segmentation of head and neck patients and demonstrate its\nutility in clinical applications. Methods: Our algorithm cascades rigid and\ndeformable deformation blocks, and takes on an atlas image (M), set of\natlas-space segmentations (S_A), and a patient image (F) as inputs, while\noutputting patient-space segmentations of all OARs defined on the atlas. We\ntrain our model on 475 CT images taken from public archives and Stanford RadOnc\nClinic (SROC), validate on 5 CT images from SROC, and test our model on 20 CT\nimages from SROC. Results: Our method outperforms current state of the art\nlearning-based registration algorithms and achieves an overall dice score of\n0.789 on our test set. Moreover, our method yields a performance comparable to\nmanual segmentation and supervised segmentation, while solving a much more\ncomplex registration problem. Whereas supervised segmentation methods only\nautomate the segmentation process for a select few number of OARs, we\ndemonstrate that our methods can achieve similar performance for OARs of\ninterest, while also providing segmentations for every other OAR on the\nprovided atlas. Conclusions: Our proposed algorithm has significant clinical\napplications and could help reduce the bottleneck for segmentation of head and\nneck OARs. Further, our results demonstrate that semi-supervised diffeomorphic\nregistration can be accurately applied to both registration and segmentation\nproblems.\n",
    "topics": "{'Image Registration': 0.7920735, 'Point Cloud Registration': 0.60649943}",
    "score": 0.8209689921
  },
  {
    "id": "1903.11210",
    "title": "Colorectal cancer diagnosis from histology images: A comparative study",
    "abstract": "  Computer-aided diagnosis (CAD) based on histopathological imaging has\nprogressed rapidly in recent years with the rise of machine learning based\nmethodologies. Traditional approaches consist of training a classification\nmodel using features extracted from the images, based on textures or\nmorphological properties. Recently, deep-learning based methods have been\napplied directly to the raw (unprocessed) data. However, their usability is\nimpacted by the paucity of annotated data in the biomedical sector. In order to\nleverage the learning capabilities of deep Convolutional Neural Nets (CNNs)\nwithin the confines of limited labelled data, in this study we shall\ninvestigate the transfer learning approaches that aim to apply the knowledge\ngained from solving a source (e.g., non-medical) problem, to learn better\npredictive models for the target (e.g., biomedical) task. As an alternative, we\nshall further propose a new adaptive and compact CNN based architecture that\ncan be trained from scratch even on scarce and low-resolution data. Moreover,\nwe conduct quantitative comparative evaluations among the traditional methods,\ntransfer learning-based methods and the proposed adaptive approach for the\nparticular task of cancer detection and identification from scarce and\nlow-resolution histology images. Over the largest benchmark dataset formed for\nthis purpose, the proposed adaptive approach achieved a higher cancer detection\naccuracy with a significant gap, whereas the deep CNNs with transfer learning\nachieved a superior cancer identification.\n",
    "topics": "{'Transfer Learning': 0.9999354}",
    "score": 0.820907727
  },
  {
    "id": "1802.00565",
    "title": "Detecting Zones and Threat on 3D Body for Security in Airports using\n  Deep Machine Learning",
    "abstract": "  In this research, it was used a segmentation and classification method to\nidentify threat recognition in human scanner images of airport security. The\nDepartment of Homeland Security's (DHS) in USA has a higher false alarm,\nproduced from theirs algorithms using today's scanners at the airports. To\nrepair this problem they started a new competition at Kaggle site asking the\nscience community to improve their detection with new algorithms. The dataset\nused in this research comes from DHS at\nhttps://www.kaggle.com/c/passenger-screening-algorithm-challenge/data According\nto DHS: \"This dataset contains a large number of body scans acquired by a new\ngeneration of millimeter wave scanner called the High Definition-Advanced\nImaging Technology (HD-AIT) system. They are comprised of volunteers wearing\ndifferent clothing types (from light summer clothes to heavy winter clothes),\ndifferent body mass indices, different genders, different numbers of threats,\nand different types of threats\". Using Python as a principal language, the\npreprocessed of the dataset images extracted features from 200 bodies using:\nintensity, intensity differences and local neighbourhood to detect, to produce\nsegmentation regions and label those regions to be used as a truth in a\ntraining and test dataset. The regions are subsequently give to a CNN deep\nlearning classifier to predict 17 classes (that represents the body zones):\nzone1, zone2, ... zone17 and zones with threat in a total of 34 zones. The\nanalysis showed the results of the classifier an accuracy of 98.2863% and a\nloss of 0.091319, as well as an average of 100% for recall and precision.\n",
    "topics": "{}",
    "score": 0.8207800937
  },
  {
    "id": "1810.04260",
    "title": "Inter-Scanner Harmonization of High Angular Resolution DW-MRI using Null\n  Space Deep Learning",
    "abstract": "  Diffusion-weighted magnetic resonance imaging (DW-MRI) allows for\nnon-invasive imaging of the local fiber architecture of the human brain at a\nmillimetric scale. Multiple classical approaches have been proposed to detect\nboth single (e.g., tensors) and multiple (e.g., constrained spherical\ndeconvolution, CSD) fiber population orientations per voxel. However, existing\ntechniques generally exhibit low reproducibility across MRI scanners. Herein,\nwe propose a data-driven tech-nique using a neural network design which\nexploits two categories of data. First, training data were acquired on three\nsquirrel monkey brains using ex-vivo DW-MRI and histology of the brain. Second,\nrepeated scans of human subjects were acquired on two different scanners to\naugment the learning of the network pro-posed. To use these data, we propose a\nnew network architecture, the null space deep network (NSDN), to simultaneously\nlearn on traditional observed/truth pairs (e.g., MRI-histology voxels) along\nwith repeated observations without a known truth (e.g., scan-rescan MRI). The\nNSDN was tested on twenty percent of the histology voxels that were kept\ncompletely blind to the network. NSDN significantly improved absolute\nperformance relative to histology by 3.87% over CSD and 1.42% over a recently\nproposed deep neural network approach. More-over, it improved reproducibility\non the paired data by 21.19% over CSD and 10.09% over a recently proposed deep\napproach. Finally, NSDN improved gen-eralizability of the model to a third in\nvivo human scanner (which was not used in training) by 16.08% over CSD and\n10.41% over a recently proposed deep learn-ing approach. This work suggests\nthat data-driven approaches for local fiber re-construction are more\nreproducible, informative and precise and offers a novel, practical method for\ndetermining these models.\n",
    "topics": "{}",
    "score": 0.820570616
  },
  {
    "id": "1908.00375",
    "title": "Quality Assessment of In-the-Wild Videos",
    "abstract": "  Quality assessment of in-the-wild videos is a challenging problem because of\nthe absence of reference videos and shooting distortions. Knowledge of the\nhuman visual system can help establish methods for objective quality assessment\nof in-the-wild videos. In this work, we show two eminent effects of the human\nvisual system, namely, content-dependency and temporal-memory effects, could be\nused for this purpose. We propose an objective no-reference video quality\nassessment method by integrating both effects into a deep neural network. For\ncontent-dependency, we extract features from a pre-trained image classification\nneural network for its inherent content-aware property. For temporal-memory\neffects, long-term dependencies, especially the temporal hysteresis, are\nintegrated into the network with a gated recurrent unit and a\nsubjectively-inspired temporal pooling layer. To validate the performance of\nour method, experiments are conducted on three publicly available in-the-wild\nvideo quality assessment databases: KoNViD-1k, CVD2014, and LIVE-Qualcomm,\nrespectively. Experimental results demonstrate that our proposed method\noutperforms five state-of-the-art methods by a large margin, specifically,\n12.39%, 15.71%, 15.45%, and 18.09% overall performance improvements over the\nsecond-best method VBLIINDS, in terms of SROCC, KROCC, PLCC and RMSE,\nrespectively. Moreover, the ablation study verifies the crucial role of both\nthe content-aware features and the modeling of temporal-memory effects. The\nPyTorch implementation of our method is released at\nhttps://github.com/lidq92/VSFA.\n",
    "topics": "{'Image Classification': 0.86322117}",
    "score": 0.8205157623
  },
  {
    "id": "1301.4083",
    "title": "Knowledge Matters: Importance of Prior Information for Optimization",
    "abstract": "  We explore the effect of introducing prior information into the intermediate\nlevel of neural networks for a learning task on which all the state-of-the-art\nmachine learning algorithms tested failed to learn. We motivate our work from\nthe hypothesis that humans learn such intermediate concepts from other\nindividuals via a form of supervision or guidance using a curriculum. The\nexperiments we have conducted provide positive evidence in favor of this\nhypothesis. In our experiments, a two-tiered MLP architecture is trained on a\ndataset with 64x64 binary inputs images, each image with three sprites. The\nfinal task is to decide whether all the sprites are the same or one of them is\ndifferent. Sprites are pentomino tetris shapes and they are placed in an image\nwith different locations using scaling and rotation transformations. The first\npart of the two-tiered MLP is pre-trained with intermediate-level targets being\nthe presence of sprites at each location, while the second part takes the\noutput of the first part as input and predicts the final task's target binary\nevent. The two-tiered MLP architecture, with a few tens of thousand examples,\nwas able to learn the task perfectly, whereas all other algorithms (include\nunsupervised pre-training, but also traditional algorithms like SVMs, decision\ntrees and boosting) all perform no better than chance. We hypothesize that the\noptimization difficulty involved when the intermediate pre-training is not\nperformed is due to the {\\em composition} of two highly non-linear tasks. Our\nfindings are also consistent with hypotheses on cultural learning inspired by\nthe observations of optimization problems with deep learning, presumably\nbecause of effective local minima.\n",
    "topics": "{'Unsupervised Pre-training': 0.99987006}",
    "score": 0.8203989018
  },
  {
    "id": "1810.08881",
    "title": "Automated identification of hookahs (waterpipes) on Instagram: an\n  application in feature extraction using Convolutional Neural Network and\n  Support Vector Machine classification",
    "abstract": "  Background: Instagram, with millions of posts per day, can be used to inform\npublic health surveillance targets and policies. However, current research\nrelying on image-based data often relies on hand coding of images which is time\nconsuming and costly, ultimately limiting the scope of the study. Current best\npractices in automated image classification (e.g., support vector machine\n(SVM), Backpropagation (BP) neural network, and artificial neural network) are\nlimited in their capacity to accurately distinguish between objects within\nimages. Objective: This study demonstrates how convolutional neural network\n(CNN) can be used to extract unique features within an image and how SVM can\nthen be used to classify the image. Methods: Images of waterpipes or hookah (an\nemerging tobacco product possessing similar harms to that of cigarettes) were\ncollected from Instagram and used in analyses (n=840). CNN was used to extract\nunique features from images identified to contain waterpipes. A SVM classifier\nwas built to distinguish between images with and without waterpipes. Methods\nfor image classification were then compared to show how a CNN + SVM classifier\ncould improve accuracy. Results: As the number of the validated training images\nincreased, the total number of extracted features increased. Additionally, as\nthe number of features learned by the SVM classifier increased, the average\nlevel of accuracy increased. Overall, 99.5% of the 420 images classified were\ncorrectly identified as either hookah or non-hookah images. This level of\naccuracy was an improvement over earlier methods that used SVM, CNN or Bag of\nFeatures (BOF) alone. Conclusions: CNN extracts more features of the images\nallowing a SVM classifier to be better informed, resulting in higher accuracy\ncompared with methods that extract fewer features. Future research can use this\nmethod to grow the scope of image-based studies.\n",
    "topics": "{'Image Classification': 0.96933424}",
    "score": 0.82007961
  },
  {
    "id": "2002.12079",
    "title": "Two-stage breast mass detection and segmentation system towards\n  automated high-resolution full mammogram analysis",
    "abstract": "  Mammography is the primary imaging modality used for early detection and\ndiagnosis of breast cancer. Mammography analysis mainly refers to the\nextraction of regions of interest around tumors, followed by a segmentation\nstep, which is essential to further classification of benign or malignant\ntumors. Breast masses are the most important findings among breast\nabnormalities. However, manual delineation of masses from native mammogram is a\ntime consuming and error-prone task. An integrated computer-aided diagnosis\nsystem to assist radiologists in automatically detecting and segmenting breast\nmasses is therefore in urgent need. We propose a fully-automated approach that\nguides accurate mass segmentation from full mammograms at high resolution\nthrough a detection stage. First, mass detection is performed by an efficient\ndeep learning approach, You-Only-Look-Once, extended by integrating multi-scale\npredictions to improve automatic candidate selection. Second, a convolutional\nencoder-decoder network using nested and dense skip connections is employed to\nfine-delineate candidate masses. Unlike most previous studies based on\nsegmentation from regions, our framework handles mass segmentation from native\nfull mammograms without user intervention. Trained on INbreast and DDSM-CBIS\npublic datasets, the pipeline achieves an overall average Dice of 80.44% on\nhigh-resolution INbreast test images, outperforming state-of-the-art methods.\nOur system shows promising accuracy as an automatic full-image mass\nsegmentation system. The comprehensive evaluation provided for both detection\nand segmentation stages reveals strong robustness to the diversity of size,\nshape and appearance of breast masses, towards better computer-aided diagnosis.\n",
    "topics": "{'Semantic Segmentation': 0.44339174}",
    "score": 0.8200514966
  },
  {
    "id": "1909.05926",
    "title": "Encoding Visual Attributes in Capsules for Explainable Medical Diagnoses",
    "abstract": "  Convolutional neural network based systems have largely failed to be adopted\nin many high-risk application areas, including healthcare, military, security,\ntransportation, finance, and legal, due to their highly uninterpretable\n\"black-box\" nature. Towards solving this deficiency, we teach a novel\nmulti-task capsule network to improve the explainability of predictions by\nembodying the same high-level language used by human-experts. Our explainable\ncapsule network, X-Caps, encodes high-level visual object attributes within the\nvectors of its capsules, then forms predictions based solely on these\nhuman-interpretable features. To encode attributes, X-Caps utilizes a new\nrouting sigmoid function to independently route information from child capsules\nto parents. Further, to provide radiologists with an estimate of model\nconfidence, we train our network on a distribution of expert labels, modeling\ninter-observer agreement and punishing over/under confidence during training,\nsupervised by human-experts' agreement. X-Caps simultaneously learns attribute\nand malignancy scores from a multi-center dataset of over 1000 CT scans of lung\ncancer screening patients. We demonstrate a simple 2D capsule network can\noutperform a state-of-the-art deep dense dual-path 3D CNN at capturing\nvisually-interpretable high-level attributes and malignancy prediction, while\nproviding malignancy prediction scores approaching that of non-explainable 3D\nCNNs. To the best of our knowledge, this is the first study to investigate\ncapsule networks for making predictions based on radiologist-level\ninterpretable attributes and its applications to medical image diagnosis. Code\nis publicly available at https://github.com/lalonderodney/X-Caps .\n",
    "topics": "{'Multi-Task Learning': 0.7472829}",
    "score": 0.8198489781
  },
  {
    "id": "1906.10486",
    "title": "A Novel Deep Learning Based Approach for Left Ventricle Segmentation in\n  Echocardiography: MFP-Unet",
    "abstract": "  Segmentation of the Left ventricle (LV) is a crucial step for quantitative\nmeasurements such as area, volume, and ejection fraction. However, the\nautomatic LV segmentation in 2D echocardiographic images is a challenging task\ndue to ill-defined borders, and operator dependence issues (insufficient\nreproducibility). U-net, which is a well-known architecture in medical image\nsegmentation, addressed this problem through an encoder-decoder path. Despite\noutstanding overall performance, U-net ignores the contribution of all semantic\nstrengths in the segmentation procedure. In the present study, we have proposed\na novel architecture to tackle this drawback. Feature maps in all levels of the\ndecoder path of U-net are concatenated, their depths are equalized, and\nup-sampled to a fixed dimension. This stack of feature maps would be the input\nof the semantic segmentation layer. The proposed network yielded\nstate-of-the-art results when comparing with results from U-net, dilated U-net,\nand deeplabv3, using the same dataset. An average Dice Metric (DM) of 0.945,\nHausdorff Distance (HD) of 1.62, Jaccard Coefficient (JC) of 0.97, and Mean\nAbsolute Distance (MAD) of 1.32 are achieved. The correlation graph,\nbland-altman analysis, and box plot showed a great agreement between automatic\nand manually calculated volume, area, and length.\n",
    "topics": "{'Medical Image Segmentation': 0.999982, 'Semantic Segmentation': 0.9963225}",
    "score": 0.8197569134
  },
  {
    "id": "2007.11222",
    "title": "Greenhouse Segmentation on High-Resolution Optical Satellite Imagery\n  using Deep Learning Techniques",
    "abstract": "  Greenhouse segmentation has pivotal importance for climate-smart agricultural\nland-use planning. Deep learning-based approaches provide state-of-the-art\nperformance in natural image segmentation. However, semantic segmentation on\nhigh-resolution optical satellite imagery is a challenging task because of the\ncomplex environment. In this paper, a sound methodology is proposed for\npixel-wise classification on images acquired by the Azersky (SPOT-7) optical\nsatellite. In particular, customized variations of U-Net-like architectures are\nemployed to identify greenhouses. Two models are proposed which uniquely\nincorporate dilated convolutions and skip connections, and the results are\ncompared to that of the baseline U-Net model. The dataset used consists of\npan-sharpened orthorectified Azersky images (red, green, blue,and near infrared\nchannels) with 1.5-meter resolution and annotation masks, collected from 15\nregions in Azerbaijan where the greenhouses are densely congested. The images\ncover the cumulative area of 1008 $km^2$ and annotation masks contain 47559\npolygons in total. The $F_1, Kappa, AUC$, and $IOU$ scores are used for\nperformance evaluation. It is observed that the use of the deconvolutional\nlayers alone throughout the expansive path does not yield satisfactory results;\ntherefore, they are either replaced or coupled with bilinear interpolation. All\nmodels benefit from the hard example mining (HEM) strategy. It is also reported\nthat the best accuracy of $93.29\\%$ ($F_1\\,score$) is recorded when the\nweighted binary cross-entropy loss is coupled with the dice loss. Experimental\nresults showed that both of the proposed models outperformed the baseline U-Net\narchitecture such that the best model proposed scored $4.48\\%$ higher in\ncomparison to the baseline architecture.\n",
    "topics": "{'Semantic Segmentation': 0.9977532}",
    "score": 0.8196631187
  },
  {
    "id": "1807.11598",
    "title": "Pulse Sequence Resilient Fast Brain Segmentation",
    "abstract": "  Accurate automatic segmentation of brain anatomy from\n$T_1$-weighted~($T_1$-w) magnetic resonance images~(MRI) has been a\ncomputationally intensive bottleneck in neuroimaging pipelines, with\nstate-of-the-art results obtained by unsupervised intensity modeling-based\nmethods and multi-atlas registration and label fusion. With the advent of\npowerful supervised convolutional neural networks~(CNN)-based learning\nalgorithms, it is now possible to produce a high quality brain segmentation\nwithin seconds. However, the very supervised nature of these methods makes it\ndifficult to generalize them on data different from what they have been trained\non. Modern neuroimaging studies are necessarily multi-center initiatives with a\nwide variety of acquisition protocols. Despite stringent protocol harmonization\npractices, it is not possible to standardize the whole gamut of MRI imaging\nparameters across scanners, field strengths, receive coils etc., that affect\nimage contrast. In this paper we propose a CNN-based segmentation algorithm\nthat, in addition to being highly accurate and fast, is also resilient to\nvariation in the input $T_1$-w acquisition. Our approach relies on building\napproximate forward models of $T_1$-w pulse sequences that produce a typical\ntest image. We use the forward models to augment the training data with test\ndata specific training examples. These augmented data can be used to update\nand/or build a more robust segmentation model that is more attuned to the test\ndata imaging properties. Our method generates highly accurate, state-of-the-art\nsegmentation results~(overall Dice overlap=0.94), within seconds and is\nconsistent across a wide-range of protocols.\n",
    "topics": "{'Brain Segmentation': 0.9999989}",
    "score": 0.8195854427
  },
  {
    "id": "2004.14584",
    "title": "Out-of-the-box channel pruned networks",
    "abstract": "  In the last decade convolutional neural networks have become gargantuan.\nPre-trained models, when used as initializers are able to fine-tune ever larger\nnetworks on small datasets. Consequently, not all the convolutional features\nthat these fine-tuned models detect are requisite for the end-task. Several\nworks of channel pruning have been proposed to prune away compute and memory\nfrom models that were trained already. Typically, these involve policies that\ndecide which and how many channels to remove from each layer leading to\nchannel-wise and/or layer-wise pruning profiles, respectively. In this paper,\nwe conduct several baseline experiments and establish that profiles from random\nchannel-wise pruning policies are as good as metric-based ones. We also\nestablish that there may exist profiles from some layer-wise pruning policies\nthat are measurably better than common baselines. We then demonstrate that the\ntop layer-wise pruning profiles found using an exhaustive random search from\none datatset are also among the top profiles for other datasets. This implies\nthat we could identify out-of-the-box layer-wise pruning profiles using\nbenchmark datasets and use these directly for new datasets. Furthermore, we\ndevelop a Reinforcement Learning (RL) policy-based search algorithm with a\ndirect objective of finding transferable layer-wise pruning profiles using many\nmodels for the same architecture. We use a novel reward formulation that drives\nthis RL search towards an expected compression while maximizing accuracy. Our\nresults show that our transferred RL-based profiles are as good or better than\nbest profiles found on the original dataset via exhaustive search. We then\ndemonstrate that if we found the profiles using a mid-sized dataset such as\nCifar10/100, we are able to transfer them to even a large dataset such as\nImagenet.\n",
    "topics": "{}",
    "score": 0.8195224032
  },
  {
    "id": "1903.04433",
    "title": "ETNLP: a visual-aided systematic approach to select pre-trained\n  embeddings for a downstream task",
    "abstract": "  Given many recent advanced embedding models, selecting pre-trained word\nembedding (a.k.a., word representation) models best fit for a specific\ndownstream task is non-trivial. In this paper, we propose a systematic\napproach, called ETNLP, for extracting, evaluating, and visualizing multiple\nsets of pre-trained word embeddings to determine which embeddings should be\nused in a downstream task. For extraction, we provide a method to extract\nsubsets of the embeddings to be used in the downstream task. For evaluation, we\nanalyse the quality of pre-trained embeddings using an input word analogy list.\nFinally, we visualize the word representations in the embedding space to\nexplore the embedded words interactively.\n  We demonstrate the effectiveness of the proposed approach on our pre-trained\nword embedding models in Vietnamese to select which models are suitable for a\nnamed entity recognition (NER) task. Specifically, we create a large Vietnamese\nword analogy list to evaluate and select the pre-trained embedding models for\nthe task. We then utilize the selected embeddings for the NER task and achieve\nthe new state-of-the-art results on the task benchmark dataset. We also apply\nthe approach to another downstream task of privacy-guaranteed embedding\nselection, and show that it helps users quickly select the most suitable\nembeddings. In addition, we create an open-source system using the proposed\nsystematic approach to facilitate similar studies on other NLP tasks. The\nsource code and data are available at https://github.com/vietnlp/etnlp.\n",
    "topics": "{'Word Embeddings': 0.9576207, 'Named Entity Recognition': 0.94459265}",
    "score": 0.8194705651
  },
  {
    "id": "1412.6596",
    "title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping",
    "abstract": "  Current state-of-the-art deep learning systems for visual object recognition\nand detection use purely supervised training with regularization such as\ndropout to avoid overfitting. The performance depends critically on the amount\nof labeled examples, and in current practice the labels are assumed to be\nunambiguous and accurate. However, this assumption often does not hold; e.g. in\nrecognition, class labels may be missing; in detection, objects in the image\nmay not be localized; and in general, the labeling may be subjective. In this\nwork we propose a generic way to handle noisy and incomplete labeling by\naugmenting the prediction objective with a notion of consistency. We consider a\nprediction consistent if the same prediction is made given similar percepts,\nwhere the notion of similarity is between deep network features computed from\nthe input data. In experiments we demonstrate that our approach yields\nsubstantial robustness to label noise on several datasets. On MNIST handwritten\ndigits, we show that our model is robust to label corruption. On the Toronto\nFace Database, we show that our model handles well the case of subjective\nlabels in emotion recognition, achieving state-of-the- art results, and can\nalso benefit from unlabeled face images with no modification to our method. On\nthe ILSVRC2014 detection challenge data, we show that our approach extends to\nvery deep networks, high resolution images and structured outputs, and results\nin improved scalable detection.\n",
    "topics": "{'Object Recognition': 0.99014366, 'Emotion Recognition': 0.97697884}",
    "score": 0.8193781338
  },
  {
    "id": "2009.02062",
    "title": "Looking for change? Roll the Dice and demand Attention",
    "abstract": "  Change detection, i.e. identification per pixel of changes for some classes\nof interest from a set of bi-temporal co-registered images, is a fundamental\ntask in the field of remote sensing. It remains challenging due to unrelated\nforms of change that appear at different times in input images. These are\nchanges due to to different environmental conditions or simply changes of\nobjects that are not of interest. Here, we propose a reliable deep learning\nframework for the task of semantic change detection in very high-resolution\naerial images. Our framework consists of a new loss function, new attention\nmodules, new feature extraction building blocks, and a new backbone\narchitecture that is tailored for the task of semantic change detection.\nSpecifically, we define a new form of set similarity, that is based on an\niterative evaluation of a variant of the Dice coefficient. We use this\nsimilarity metric to define a new loss function as well as a new spatial and\nchannel convolution Attention layer (the FracTAL). The new attention layer,\ndesigned specifically for vision tasks, is memory efficient, thus suitable for\nuse in all levels of deep convolutional networks. Based on these, we introduce\ntwo new efficient self-contained feature extraction convolution units. We term\nthese units CEECNet and FracTAL ResNet units. We validate the performance of\nthese feature extraction building blocks on the CIFAR10 reference data and\ncompare the results with standard ResNet modules. Further, we introduce a new\nencoder/decoder scheme, a network macro-topology, that is tailored for the task\nof change detection. We validate our approach by showing excellent performance\nand achieving state of the art score (F1 and Intersection over Union -\nhereafter IoU) on two building change detection datasets, namely, the LEVIRCD\n(F1: 0.918, IoU: 0.848) and the WHU (F1: 0.938, IoU: 0.882) datasets.\n",
    "topics": "{}",
    "score": 0.8192520453
  },
  {
    "id": "1711.02666",
    "title": "Tensor-Generative Adversarial Network with Two-dimensional Sparse\n  Coding: Application to Real-time Indoor Localization",
    "abstract": "  Localization technology is important for the development of indoor\nlocation-based services (LBS). Global Positioning System (GPS) becomes invalid\nin indoor environments due to the non-line-of-sight issue, so it is urgent to\ndevelop a real-time high-accuracy localization approach for smartphones.\nHowever, accurate localization is challenging due to issues such as real-time\nresponse requirements, limited fingerprint samples and mobile device storage.\nTo address these problems, we propose a novel deep learning architecture:\nTensor-Generative Adversarial Network (TGAN).\n  We first introduce a transform-based 3D tensor to model fingerprint samples.\nInstead of those passive methods that construct a fingerprint database as a\nprior, our model applies artificial neural network with deep learning to train\nnetwork classifiers and then gives out estimations. Then we propose a novel\ntensor-based super-resolution scheme using the generative adversarial network\n(GAN) that adopts sparse coding as the generator network and a residual\nlearning network as the discriminator. Further, we analyze the performance of\ntensor-GAN and implement a trace-based localization experiment, which achieves\nbetter performance. Compared to existing methods for smartphones indoor\npositioning, that are energy-consuming and high demands on devices, TGAN can\ngive out an improved solution in localization accuracy, response time and\nimplementation complexity.\n",
    "topics": "{'Super-Resolution': 0.9374326, 'Super Resolution': 0.83189857}",
    "score": 0.8192293009
  },
  {
    "id": "1911.07916",
    "title": "Face shape classification using Inception v3",
    "abstract": "  In this paper, we present experimental results obtained from retraining the\nlast layer of the Inception v3 model in classifying images of human faces into\none of five basic face shapes. The accuracy of the retrained Inception v3 model\nwas compared with that of the following classification methods that uses facial\nlandmark distance ratios and angles as features: linear discriminant analysis\n(LDA), support vector machines with linear kernel (SVM-LIN), support vector\nmachines with radial basis function kernel (SVM-RBF), artificial neural\nnetworks or multilayer perceptron (MLP), and k-nearest neighbors (KNN). All\nclassifiers were trained and tested using a total of 500 images of female\ncelebrities with known face shapes collected from the Internet. Results show\nthat training accuracy and overall accuracy ranges from 98.0% to 100% and from\n84.4% to 84.8% for Inception v3 and from 50.6% to 73.0% and from 36.4% to 64.6%\nfor the other classifiers depending on the training set size used. This result\nshows that the retrained Inception v3 model was able to fit the training data\nwell and outperform the other classifiers without the need to handpick specific\nfeatures to include in model training. Future work should consider expanding\nthe labeled dataset, preferably one that can also be freely distributed to the\nresearch community, so that proper model cross-validation can be performed. As\nfar as we know, this is the first in the literature to use convolutional neural\nnetworks in face-shape classification. The scripts are available at\nhttps://github.com/adonistio/inception-face-shape-classifier.\n",
    "topics": "{}",
    "score": 0.8192134366
  },
  {
    "id": "1905.02793",
    "title": "Skin Lesion Classification Using CNNs with Patch-Based Attention and\n  Diagnosis-Guided Loss Weighting",
    "abstract": "  Objective: This work addresses two key problems of skin lesion\nclassification. The first problem is the effective use of high-resolution\nimages with pretrained standard architectures for image classification. The\nsecond problem is the high class imbalance encountered in real-world\nmulti-class datasets. Methods: To use high-resolution images, we propose a\nnovel patch-based attention architecture that provides global context between\nsmall, high-resolution patches. We modify three pretrained architectures and\nstudy the performance of patch-based attention. To counter class imbalance\nproblems, we compare oversampling, balanced batch sampling, and class-specific\nloss weighting. Additionally, we propose a novel diagnosis-guided loss\nweighting method which takes the method used for ground-truth annotation into\naccount. Results: Our patch-based attention mechanism outperforms previous\nmethods and improves the mean sensitivity by 7%. Class balancing significantly\nimproves the mean sensitivity and we show that our diagnosis-guided loss\nweighting method improves the mean sensitivity by 3% over normal loss\nbalancing. Conclusion: The novel patch-based attention mechanism can be\nintegrated into pretrained architectures and provides global context between\nlocal patches while outperforming other patch-based methods. Hence, pretrained\narchitectures can be readily used with high-resolution images without\ndownsampling. The new diagnosis-guided loss weighting method outperforms other\nmethods and allows for effective training when facing class imbalance.\nSignificance: The proposed methods improve automatic skin lesion\nclassification. They can be extended to other clinical applications where\nhigh-resolution image data and class imbalance are relevant.\n",
    "topics": "{'Lesion Classification': 0.9999776, 'Image Classification': 0.9011033}",
    "score": 0.8190688141
  },
  {
    "id": "2003.14052",
    "title": "3D Sketch-aware Semantic Scene Completion via Semi-supervised Structure\n  Prior",
    "abstract": "  The goal of the Semantic Scene Completion (SSC) task is to simultaneously\npredict a completed 3D voxel representation of volumetric occupancy and\nsemantic labels of objects in the scene from a single-view observation. Since\nthe computational cost generally increases explosively along with the growth of\nvoxel resolution, most current state-of-the-arts have to tailor their framework\ninto a low-resolution representation with the sacrifice of detail prediction.\nThus, voxel resolution becomes one of the crucial difficulties that lead to the\nperformance bottleneck.\n  In this paper, we propose to devise a new geometry-based strategy to embed\ndepth information with low-resolution voxel representation, which could still\nbe able to encode sufficient geometric information, e.g., room layout, object's\nsizes and shapes, to infer the invisible areas of the scene with well\nstructure-preserving details. To this end, we first propose a novel 3D\nsketch-aware feature embedding to explicitly encode geometric information\neffectively and efficiently. With the 3D sketch in hand, we further devise a\nsimple yet effective semantic scene completion framework that incorporates a\nlight-weight 3D Sketch Hallucination module to guide the inference of occupancy\nand the semantic labels via a semi-supervised structure prior learning\nstrategy. We demonstrate that our proposed geometric embedding works better\nthan the depth feature learning from habitual SSC frameworks. Our final model\nsurpasses state-of-the-arts consistently on three public benchmarks, which only\nrequires 3D volumes of 60 x 36 x 60 resolution for both input and output. The\ncode and the supplementary material will be available at\nhttps://charlesCXK.github.io.\n",
    "topics": "{}",
    "score": 0.8190310469
  },
  {
    "id": "2005.05236",
    "title": "ECG-DelNet: Delineation of Ambulatory Electrocardiograms with Mixed\n  Quality Labeling Using Neural Networks",
    "abstract": "  Electrocardiogram (ECG) detection and delineation are key steps for numerous\ntasks in clinical practice, as ECG is the most performed non-invasive test for\nassessing cardiac condition. State-of-the-art algorithms employ digital signal\nprocessing (DSP), which require laborious rule adaptation to new morphologies.\nIn contrast, deep learning (DL) algorithms, especially for classification, are\ngaining weight in academic and industrial settings. However, the lack of model\nexplainability and small databases hinder their applicability. We demonstrate\nDL can be successfully applied to low interpretative tasks by embedding ECG\ndetection and delineation onto a segmentation framework. For this purpose, we\nadapted and validated the most used neural network architecture for image\nsegmentation, the U-Net, to one-dimensional data. The model was trained using\nPhysioNet's QT database, comprised of 105 ambulatory ECG recordings, for\nsingle- and multi-lead scenarios. To alleviate data scarcity, data\nregularization techniques such as pre-training with low-quality data labels,\nperforming ECG-based data augmentation and applying strong model regularizers\nto the architecture were attempted. Other variations in the model's capacity\n(U-Net's depth and width), alongside the application of state-of-the-art\nadditions, were evaluated. These variations were exhaustively validated in a\n5-fold cross-validation manner. The best performing configuration reached\nprecisions of 90.12%, 99.14% and 98.25% and recalls of 98.73%, 99.94% and\n99.88% for the P, QRS and T waves, respectively, on par with DSP-based\napproaches. Despite being a data-hungry technique trained on a small dataset,\nDL-based approaches demonstrate to be a viable alternative to traditional\nDSP-based ECG processing techniques.\n",
    "topics": "{'Data Augmentation': 0.99912375, 'Semantic Segmentation': 0.9891182}",
    "score": 0.8189861008
  },
  {
    "id": "1605.01189",
    "title": "A Generic Method for Automatic Ground Truth Generation of\n  Camera-captured Documents",
    "abstract": "  The contribution of this paper is fourfold. The first contribution is a\nnovel, generic method for automatic ground truth generation of camera-captured\ndocument images (books, magazines, articles, invoices, etc.). It enables us to\nbuild large-scale (i.e., millions of images) labeled camera-captured/scanned\ndocuments datasets, without any human intervention. The method is generic,\nlanguage independent and can be used for generation of labeled documents\ndatasets (both scanned and cameracaptured) in any cursive and non-cursive\nlanguage, e.g., English, Russian, Arabic, Urdu, etc. To assess the\neffectiveness of the presented method, two different datasets in English and\nRussian are generated using the presented method. Evaluation of samples from\nthe two datasets shows that 99:98% of the images were correctly labeled. The\nsecond contribution is a large dataset (called C3Wi) of camera-captured\ncharacters and words images, comprising 1 million word images (10 million\ncharacter images), captured in a real camera-based acquisition. This dataset\ncan be used for training as well as testing of character recognition systems on\ncamera-captured documents. The third contribution is a novel method for the\nrecognition of cameracaptured document images. The proposed method is based on\nLong Short-Term Memory and outperforms the state-of-the-art methods for camera\nbased OCRs. As a fourth contribution, various benchmark tests are performed to\nuncover the behavior of commercial (ABBYY), open source (Tesseract), and the\npresented camera-based OCR using the presented C3Wi dataset. Evaluation results\nreveal that the existing OCRs, which already get very high accuracies on\nscanned documents, have limited performance on camera-captured document images;\nwhere ABBYY has an accuracy of 75%, Tesseract an accuracy of 50.22%, while the\npresented character recognition system has an accuracy of 95.10%.\n",
    "topics": "{'Optical Character Recognition': 0.9921463}",
    "score": 0.8189398322
  },
  {
    "id": "1812.04718",
    "title": "Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs",
    "abstract": "  In practice, it is common to find oneself with far too little text data to\ntrain a deep neural network. This \"Big Data Wall\" represents a challenge for\nminority language communities on the Internet, organizations, laboratories and\ncompanies that compete the GAFAM (Google, Amazon, Facebook, Apple, Microsoft).\nWhile most of the research effort in text data augmentation aims on the\nlong-term goal of finding end-to-end learning solutions, which is equivalent to\n\"using neural networks to feed neural networks\", this engineering work focuses\non the use of practical, robust, scalable and easy-to-implement data\naugmentation pre-processing techniques similar to those that are successful in\ncomputer vision. Several text augmentation techniques have been experimented.\nSome existing ones have been tested for comparison purposes such as noise\ninjection or the use of regular expressions. Others are modified or improved\ntechniques like lexical replacement. Finally more innovative ones, such as the\ngeneration of paraphrases using back-translation or by the transformation of\nsyntactic trees, are based on robust, scalable, and easy-to-use NLP Cloud APIs.\nAll the text augmentation techniques studied, with an amplification factor of\nonly 5, increased the accuracy of the results in a range of 4.3% to 21.6%, with\nsignificant statistical fluctuations, on a standardized task of text polarity\nprediction. Some standard deep neural network architectures were tested: the\nmultilayer perceptron (MLP), the long short-term memory recurrent network\n(LSTM) and the bidirectional LSTM (biLSTM). Classical XGBoost algorithm has\nbeen tested with up to 2.5% improvements.\n",
    "topics": "{'Data Augmentation': 1.0}",
    "score": 0.8189269195
  },
  {
    "id": "1810.09699",
    "title": "Semi-supervised acoustic model training for speech with code-switching",
    "abstract": "  In the FAME! project, we aim to develop an automatic speech recognition (ASR)\nsystem for Frisian-Dutch code-switching (CS) speech extracted from the archives\nof a local broadcaster with the ultimate goal of building a spoken document\nretrieval system. Unlike Dutch, Frisian is a low-resourced language with a very\nlimited amount of manually annotated speech data. In this paper, we describe\nseveral automatic annotation approaches to enable using of a large amount of\nraw bilingual broadcast data for acoustic model training in a semi-supervised\nsetting. Previously, it has been shown that the best-performing ASR system is\nobtained by two-stage multilingual deep neural network (DNN) training using 11\nhours of manually annotated CS speech (reference) data together with speech\ndata from other high-resourced languages. We compare the quality of\ntranscriptions provided by this bilingual ASR system with several other\napproaches that use a language recognition system for assigning language labels\nto raw speech segments at the front-end and using monolingual ASR resources for\ntranscription. We further investigate automatic annotation of the speakers\nappearing in the raw broadcast data by first labeling with (pseudo) speaker\ntags using a speaker diarization system and then linking to the known speakers\nappearing in the reference data using a speaker recognition system. These\nspeaker labels are essential for speaker-adaptive training in the proposed\nsetting. We train acoustic models using the manually and automatically\nannotated data and run recognition experiments on the development and test data\nof the FAME! speech corpus to quantify the quality of the automatic\nannotations. The ASR and CS detection results demonstrate the potential of\nusing automatic language and speaker tagging in semi-supervised bilingual\nacoustic model training.\n",
    "topics": "{'Speaker Recognition': 0.9999304, 'Speaker Diarization': 0.9994624, 'Speech Recognition': 0.9784022}",
    "score": 0.8188780149
  },
  {
    "id": "1802.03996",
    "title": "Know Your Mind: Adaptive Brain Signal Classification with Reinforced\n  Attentive Convolutional Neural Networks",
    "abstract": "  Electroencephalography (EEG) signals reflect activities on certain brain\nareas. Effective classification of time-varying EEG signals is still\nchallenging. First, EEG signal processing and feature engineering are\ntime-consuming and highly rely on expert knowledge. In addition, most existing\nstudies focus on domain-specific classification algorithms which may not be\napplicable to other domains. Moreover, the EEG signal usually has a low\nsignal-to-noise ratio and can be easily corrupted. In this regard, we propose a\ngeneric EEG signal classification framework that accommodates a wide range of\napplications to address the aforementioned issues. The proposed framework\ndevelops a reinforced selective attention model to automatically choose the\ndistinctive information among the raw EEG signals. A convolutional mapping\noperation is employed to dynamically transform the selected information to an\nover-complete feature space, wherein implicit spatial dependency of EEG samples\ndistribution is able to be uncovered. We demonstrate the effectiveness of the\nproposed framework using three representative scenarios: intention recognition\nwith motor imagery EEG, person identification, and neurological diagnosis.\nThree widely used public datasets and a local dataset are used for our\nevaluation. The experiments show that our framework outperforms the\nstate-of-the-art baselines and achieves the accuracy of more than 97% on all\nthe datasets with low latency and good resilience of handling complex EEG\nsignals across various domains. These results confirm the suitability of the\nproposed generic approach for a range of problems in the realm of\nBrain-Computer Interface applications.\n",
    "topics": "{'EEG': 1.0, 'Feature Engineering': 0.39444003}",
    "score": 0.8188456918
  },
  {
    "id": "1901.09822",
    "title": "Virtual Conditional Generative Adversarial Networks",
    "abstract": "  When trained on multimodal image datasets, normal Generative Adversarial\nNetworks (GANs) are usually outperformed by class-conditional GANs and ensemble\nGANs, but conditional GANs is restricted to labeled datasets and ensemble GANs\nlack efficiency. We propose a novel GAN variant called virtual conditional GAN\n(vcGAN) which is not only an ensemble GAN with multiple generative paths while\nadding almost zero network parameters, but also a conditional GAN that can be\ntrained on unlabeled datasets without explicit clustering steps or objectives\nother than the adversary loss. Inside the vcGAN's generator, a learnable\n``analog-to-digital converter (ADC)\" module maps a slice of the inputted\nmultivariate Gaussian noise to discrete/digital noise (virtual label),\naccording to which a selector selects the corresponding generative path to\nproduce the sample. All the generative paths share the same decoder network\nwhile in each path the decoder network is fed with a concatenation of a\ndifferent pre-computed amplified one-hot vector and the inputted Gaussian\nnoise. We conducted a lot of experiments on several balanced/imbalanced image\ndatasets to demonstrate that vcGAN converges faster and achieves improved\nFrech\\'et Inception Distance (FID). In addition, we show the training byproduct\nthat the ADC in vcGAN learned the categorical probability of each mode and that\neach generative path generates samples of specific mode, which enables\nclass-conditional sampling. Codes are available at\n\\url{https://github.com/annonnymmouss/vcgan}\n",
    "topics": "{'Conditional Image Generation': 0.99991953, 'Image Generation': 0.92566943}",
    "score": 0.8187206802
  },
  {
    "id": "1812.04697",
    "title": "Anomaly Generation using Generative Adversarial Networks in Host Based\n  Intrusion Detection",
    "abstract": "  Generative adversarial networks have been able to generate striking results\nin various domains. This generation capability can be general while the\nnetworks gain deep understanding regarding the data distribution. In many\ndomains, this data distribution consists of anomalies and normal data, with the\nanomalies commonly occurring relatively less, creating datasets that are\nimbalanced. The capabilities that generative adversarial networks offer can be\nleveraged to examine these anomalies and help alleviate the challenge that\nimbalanced datasets propose via creating synthetic anomalies. This anomaly\ngeneration can be specifically beneficial in domains that have costly data\ncreation processes as well as inherently imbalanced datasets. One of the\ndomains that fits this description is the host-based intrusion detection\ndomain. In this work, ADFA-LD dataset is chosen as the dataset of interest\ncontaining system calls of small foot-print next generation attacks. The data\nis first converted into images, and then a Cycle-GAN is used to create images\nof anomalous data from images of normal data. The generated data is combined\nwith the original dataset and is used to train a model to detect anomalies. By\ndoing so, it is shown that the classification results are improved, with the\nAUC rising from 0.55 to 0.71, and the anomaly detection rate rising from 17.07%\nto 80.49%. The results are also compared to SMOTE, showing the potential\npresented by generative adversarial networks in anomaly generation.\n",
    "topics": "{'Intrusion Detection': 1.0, 'Anomaly Detection': 0.9999728}",
    "score": 0.8187108374
  },
  {
    "id": "2006.16177",
    "title": "Unsupervised Learning Consensus Model for Dynamic Texture Videos\n  Segmentation",
    "abstract": "  Dynamic texture (DT) segmentation, and video processing in general, is\ncurrently widely dominated by methods based on deep neural networks that\nrequire the deployment of a large number of layers. Although this parametric\napproach has shown superior performances for the dynamic texture segmentation,\nall current deep learning methods suffer from a significant main weakness\nrelated to the lack of a sufficient reference annotation to train models and to\nmake them functional. This study explores the unsupervised segmentation\napproach that can be used in the absence of training data to segment new\nvideos. We present an effective unsupervised learning consensus model for the\nsegmentation of dynamic texture (ULCM). This model is designed to merge\ndifferent segmentation maps that contain multiple and weak quality regions in\norder to achieve a more accurate final result of segmentation. The diverse\nlabeling fields required for the combination process are obtained by a\nsimplified grouping scheme applied to an input video (on the basis of a three\northogonal planes: xy, yt and xt). In the proposed model, the set of values of\nthe requantized local binary patterns (LBP) histogram around the pixel to be\nclassified are used as features which represent both the spatial and temporal\ninformation replicated in the video. Experiments conducted on the challenging\nSynthDB dataset show that, contrary to current dynamic texture segmentation\napproaches that either require parameter estimation or a training step, ULCM is\nsignificantly faster, easier to code, simple and has limited parameters.\nFurther qualitative experiments based on the YUP++ dataset prove the\nefficiently and competitively of the ULCM.\n",
    "topics": "{'Texture Classification': 0.99426115}",
    "score": 0.8187061436
  },
  {
    "id": "1909.07330",
    "title": "Knowledge Discovery In Nanophotonics Using Geometric Deep Learning",
    "abstract": "  We present here a new approach for using the intelligence aspects of\nartificial intelligence for knowledge discovery rather than device optimization\nin electromagnetic (EM) nanostructures. This approach uses training data\nobtained through full-wave EM simulations of a series of nanostructures to\ntrain geometric deep learning algorithms to assess the range of feasible\nresponses as well as the feasibility of a desired response from a class of EM\nnanostructures. To facilitate the knowledge discovery and reduce the\ncomputation complexity, our approach combines the dimensionality reduction\ntechnique (using an autoencoder) with convex-hull and one-class\nsupport-vector-machine (SVM) algorithms to find the range of the feasible\nresponses in the latent (or the reduced) response space of the EM\nnanostructure. We show that by using a small set of training instances\n(compared to all possible structures), our approach can provide better than 95%\naccuracy in assessing the feasibility of a given response. More importantly,\nthe one-class SVM algorithm can be trained to provide the degree of feasibility\n(or unfeasibility) of a response from a given nanostructure. This important\ninformation can be used to modify the initial structure to an alternative one\nthat can enable an initially unfeasible response. To show the applicability of\nour approach, we apply it to two important classes of binary metasurfaces\n(MSs), formed by array of plasmonic nanostructures, and periodic MSs formed by\nan array of dielectric nanopillars. In addition to theoretical results, we show\nthe experimental results obtained by fabricating several MSs of the second\nclass. Our theoretical and experimental results confirm the unique features of\nthis approach for knowledge discovery in EM nanostructures.\n",
    "topics": "{'Dimensionality Reduction': 0.9966647}",
    "score": 0.8185815503
  },
  {
    "id": "2001.04253",
    "title": "Parameter-Efficient Transfer from Sequential Behaviors for User Modeling\n  and Recommendation",
    "abstract": "  Inductive transfer learning has had a big impact on computer vision and NLP\ndomains but has not been used in the area of recommender systems. Even though\nthere has been a large body of research on generating recommendations based on\nmodeling user-item interaction sequences, few of them attempt to represent and\ntransfer these models for serving downstream tasks where only limited data\nexists.\n  In this paper, we delve on the task of effectively learning a single user\nrepresentation that can be applied to a diversity of tasks, from cross-domain\nrecommendations to user profile predictions. Fine-tuning a large pre-trained\nnetwork and adapting it to downstream tasks is an effective way to solve such\ntasks. However, fine-tuning is parameter inefficient considering that an entire\nmodel needs to be re-trained for every new task. To overcome this issue, we\ndevelop a parameter efficient transfer learning architecture, termed as\nPeterRec, which can be configured on-the-fly to various downstream tasks.\nSpecifically, PeterRec allows the pre-trained parameters to remain unaltered\nduring fine-tuning by injecting a series of re-learned neural networks, which\nare small but as expressive as learning the entire network. We perform\nextensive experimental ablation to show the effectiveness of the learned user\nrepresentation in five downstream tasks. Moreover, we show that PeterRec\nperforms efficient transfer learning in multiple domains, where it achieves\ncomparable or sometimes better performance relative to fine-tuning the entire\nmodel parameters. Codes and datasets are available at\nhttps://github.com/fajieyuan/sigir2020_peterrec.\n",
    "topics": "{'Transfer Learning': 0.9999957, 'Recommendation Systems': 0.9863848}",
    "score": 0.8185519119
  },
  {
    "id": "1802.01021",
    "title": "DeepType: Multilingual Entity Linking by Neural Type System Evolution",
    "abstract": "  The wealth of structured (e.g. Wikidata) and unstructured data about the\nworld available today presents an incredible opportunity for tomorrow's\nArtificial Intelligence. So far, integration of these two different modalities\nis a difficult process, involving many decisions concerning how best to\nrepresent the information so that it will be captured or useful, and\nhand-labeling large amounts of data. DeepType overcomes this challenge by\nexplicitly integrating symbolic information into the reasoning process of a\nneural network with a type system. First we construct a type system, and\nsecond, we use it to constrain the outputs of a neural network to respect the\nsymbolic structure. We achieve this by reformulating the design problem into a\nmixed integer problem: create a type system and subsequently train a neural\nnetwork with it. In this reformulation discrete variables select which\nparent-child relations from an ontology are types within the type system, while\ncontinuous variables control a classifier fit to the type system. The original\nproblem cannot be solved exactly, so we propose a 2-step algorithm: 1)\nheuristic search or stochastic optimization over discrete variables that define\na type system informed by an Oracle and a Learnability heuristic, 2) gradient\ndescent to fit classifier parameters. We apply DeepType to the problem of\nEntity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC\nKBP 2010) and find that it outperforms all existing solutions by a wide margin,\nincluding approaches that rely on a human-designed type system or recent deep\nlearning-based entity embeddings, while explicitly using symbolic information\nlets it integrate new entities without retraining.\n",
    "topics": "{'Entity Linking': 1.0, 'Entity Embeddings': 0.995535, 'Stochastic Optimization': 0.99099284}",
    "score": 0.8184325092
  },
  {
    "id": "1909.03354",
    "title": "Deep weakly-supervised learning methods for classification and\n  localization in histology images: a survey",
    "abstract": "  Using state-of-the-art deep learning models for the computer-assisted\ndiagnosis of diseases like cancer raises several challenges related to the\nnature and availability of labeled histology images. In particular, cancer\ngrading and localization in these images normally relies on both image- and\npixel-level labels, the latter requiring a costly annotation process. In this\nsurvey, deep weakly-supervised learning (WSL) architectures are investigated to\nidentify and locate diseases in histology image, without the need for\npixel-level annotations. Given a training dataset with globally-annotated\nimages, these models allow to simultaneously classify histology images, while\nlocalizing the corresponding regions of interest. These models are organized\ninto two main approaches -- (1) bottom-up approaches (based on forward-pass\ninformation through a network, either by spatial pooling of\nrepresentations/scores, or by detecting class regions), and (2) top-down\napproaches (based on backward-pass information within a network, inspired by\nhuman visual attention). Since relevant WSL models have mainly been developed\nin the computer vision community, and validated on natural scene images, we\nassess the extent to which they apply to histology images which have\nchallenging properties, e.g., large size, non-salient and highly unstructured\nregions, stain heterogeneity, and coarse/ambiguous labels. The most relevant\ndeep WSL models (e.g., CAM, WILDCAT and Deep MIL) are compared experimentally\nin terms of accuracy (classification and pixel-level localization) on several\npublic benchmark histology datasets for breast and colon cancer (BACH ICIAR\n2018, BreakHis, CAMELYON16, and GlaS). Results indicate that several deep\nlearning models, and in particular WILDCAT and deep MIL can provide a high\nlevel of classification accuracy, although pixel-wise localization of cancer\nregions remains an issue for such images.\n",
    "topics": "{}",
    "score": 0.8184098751
  },
  {
    "id": "2010.02814",
    "title": "COVIDomaly: A Deep Convolutional Autoencoder Approach for Detecting\n  Early Cases of COVID-19",
    "abstract": "  As of September 2020, the COVID-19 pandemic continues to devastate the health\nand well-being of the global population. With more than 33 million confirmed\ncases and over a million deaths, global health organizations are still a long\nway from fully containing the pandemic. This pandemic has raised serious\nquestions about the emergency preparedness of health agencies, not only in\nterms of treatment of an unseen disease, but also in identifying its early\nsymptoms. In the particular case of COVID-19, several studies have indicated\nthat chest radiography images of the infected patients show characteristic\nabnormalities. However, at the onset of a given pandemic, such as COVID-19,\nthere may not be sufficient data for the affected cases to train models for\ntheir robust detection. Hence, supervised classification is ill-posed for this\nproblem because the time spent in collecting large amounts of infected peoples'\ndata could lead to the loss of human lives and delays in preventive\ninterventions. Therefore, we formulate this problem within a one-class\nclassification framework, in which the data for healthy patients is abundantly\navailable, whereas no training data is present for the class of interest\n(COVID-19 in our case). To solve this problem, we present COVIDomaly, a\nconvolutional autoencoder framework to detect unseen COVID-19 cases from the\nchest radiographs. We tested two settings on a publicly available dataset\n(COVIDx) by training the model on chest X-rays from (i) only healthy adults,\nand (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an\nanomaly. After performing 3-fold cross validation, we obtain a pooled ROC-AUC\nof 0.7652 and 0.6902 in the two settings respectively. These results are very\nencouraging and pave the way towards research for ensuring emergency\npreparedness in future pandemics, especially the ones that could be detected\nfrom chest X-rays.\n",
    "topics": "{'Anomaly Detection': 0.87535274}",
    "score": 0.8183497085
  },
  {
    "id": "1405.4308",
    "title": "Coarse-to-Fine Classification via Parametric and Nonparametric Models\n  for Computer-Aided Diagnosis",
    "abstract": "  Classification is one of the core problems in Computer-Aided Diagnosis (CAD),\ntargeting for early cancer detection using 3D medical imaging interpretation.\nHigh detection sensitivity with desirably low false positive (FP) rate is\ncritical for a CAD system to be accepted as a valuable or even indispensable\ntool in radiologists' workflow. Given various spurious imagery noises which\ncause observation uncertainties, this remains a very challenging task. In this\npaper, we propose a novel, two-tiered coarse-to-fine (CTF) classification\ncascade framework to tackle this problem. We first obtain\nclassification-critical data samples (e.g., samples on the decision boundary)\nextracted from the holistic data distributions using a robust parametric model\n(e.g., \\cite{Raykar08}); then we build a graph-embedding based nonparametric\nclassifier on sampled data, which can more accurately preserve or formulate the\ncomplex classification boundary. These two steps can also be considered as\neffective \"sample pruning\" and \"feature pursuing + $k$NN/template matching\",\nrespectively. Our approach is validated comprehensively in colorectal polyp\ndetection and lung nodule detection CAD systems, as the top two deadly cancers,\nusing hospital scale, multi-site clinical datasets. The results show that our\nmethod achieves overall better classification/detection performance than\nexisting state-of-the-art algorithms using single-layer classifiers, such as\nthe support vector machine variants \\cite{Wang08}, boosting \\cite{Slabaugh10},\nlogistic regression \\cite{Ravesteijn10}, relevance vector machine\n\\cite{Raykar08}, $k$-nearest neighbor \\cite{Murphy09} or spectral projections\non graph \\cite{Cai08}.\n",
    "topics": "{'Template Matching': 0.9999571, 'Graph Embedding': 0.33306277}",
    "score": 0.8183435782
  },
  {
    "id": "1504.03967",
    "title": "Deep convolutional networks for pancreas segmentation in CT imaging",
    "abstract": "  Automatic organ segmentation is an important prerequisite for many\ncomputer-aided diagnosis systems. The high anatomical variability of organs in\nthe abdomen, such as the pancreas, prevents many segmentation methods from\nachieving high accuracies when compared to other segmentation of organs like\nthe liver, heart or kidneys. Recently, the availability of large annotated\ntraining sets and the accessibility of affordable parallel computing resources\nvia GPUs have made it feasible for \"deep learning\" methods such as\nconvolutional networks (ConvNets) to succeed in image classification tasks.\nThese methods have the advantage that used classification features are trained\ndirectly from the imaging data. We present a fully-automated bottom-up method\nfor pancreas segmentation in computed tomography (CT) images of the abdomen.\nThe method is based on hierarchical coarse-to-fine classification of local\nimage regions (superpixels). Superpixels are extracted from the abdominal\nregion using Simple Linear Iterative Clustering (SLIC). An initial probability\nresponse map is generated, using patch-level confidences and a two-level\ncascade of random forest classifiers, from which superpixel regions with\nprobabilities larger 0.5 are retained. These retained superpixels serve as a\nhighly sensitive initial input of the pancreas and its surroundings to a\nConvNet that samples a bounding box around each superpixel at different scales\n(and random non-rigid deformations at training time) in order to assign a more\ndistinct probability of each superpixel region being pancreas or not. We\nevaluate our method on CT images of 82 patients (60 for training, 2 for\nvalidation, and 20 for testing). Using ConvNets we achieve average Dice scores\nof 68%+-10% (range, 43-80%) in testing. This shows promise for accurate\npancreas segmentation, using a deep learning approach and compares favorably to\nstate-of-the-art methods.\n",
    "topics": "{'Computed Tomography (CT)': 0.9999033, 'Image Classification': 0.9877708}",
    "score": 0.8183181913
  },
  {
    "id": "1911.13259",
    "title": "Flatsomatic: A Method for Compression of Somatic Mutation Profiles in\n  Cancer",
    "abstract": "  In this study, we present Flatsomatic - a Variational Auto Encoder (VAE)\noptimized to compress somatic mutations that allow for unbiased data\ncompression whilst maintaining the signal. We compared two different neural\nnetwork architectures for the VAE: Multilayer Perceptron (MLP) and\nbidirectional LSTM. The somatic profiles we used to train our models consisted\nof 8,062 Pan-Cancer patients from The Cancer Genome Atlas and 989 cell lines\nfrom the COSMIC cell line project. The profiles for each patient were\nrepresented by the genomic loci where somatic mutations occurred and, to reduce\nsparsity, the locations with a frequency <5 were removed. We enhanced the VAE\nperformance by changing its evidence lower bound, and devised an F1-score based\nloss showing that it helps the VAE learn better than with binary cross-entropy.\nWe also employed beta-VAE to weight the variational regularisation term in the\nloss function and showed the best performance through a preliminary function to\nincrease the weight of the regularisation term with each epoch. We assessed the\nreconstruction ability of the VAE using the micro F1-score metric and showed\nthat our best performing model was a 2-layer deep MLP VAE. Our analysis also\nshowed that the size of the latent space did not have a significant effect on\nthe VAE learning ability. We compared the Flatsomatic embeddings created to a\nlower dimension version of the data from principal component analysis, showing\nsuperior performance of Flatsomatic, and performed K-means clustering on both\ndatasets to draw comparisons to known cancer types of each profile. Finally, we\npresent results that confirm that the Flatsomatic representations of 64\ndimensions maintain the same predictive power as the original 8,298 dimensions\nvector, through prediction of drug response.\n",
    "topics": "{}",
    "score": 0.8182026433
  },
  {
    "id": "1711.00441",
    "title": "Data, Depth, and Design: Learning Reliable Models for Skin Lesion\n  Analysis",
    "abstract": "  Deep learning fostered a leap ahead in automated skin lesion analysis in the\nlast two years. Those models are expensive to train and difficult to\nparameterize. Objective: We investigate methodological issues for designing and\nevaluating deep learning models for skin lesion analysis. We explore 10 choices\nfaced by researchers: use of transfer learning, model architecture, train\ndataset, image resolution, type of data augmentation, input normalization, use\nof segmentation, duration of training, additional use of SVMs, and test data\naugmentation. Methods: We perform two full factorial experiments, for five\ndifferent test datasets, resulting in 2560 exhaustive trials in our main\nexperiment, and 1280 trials in our assessment of transfer learning. We analyze\nboth with multi-way ANOVA. We use the exhaustive trials to simulate sequential\ndecisions and ensembles, with and without the use of privileged information\nfrom the test set. Results -- main experiment: Amount of train data has\ndisproportionate influence, explaining almost half the variation in\nperformance. Of the other factors, test data augmentation and input resolution\nare the most influential. Deeper models, when combined, with extra data, also\nhelp. -- transfer experiment: Transfer learning is critical, its absence brings\nhuge performance penalties. -- simulations: Ensembles of models are the best\noption to provide reliable results with limited resources, without using\nprivileged information and sacrificing methodological rigor. Conclusions and\nSignificance: Advancing research on automated skin lesion analysis requires\ncurating larger public datasets. Indirect use of privileged information from\nthe test set to design the models is a subtle, but frequent methodological\nmistake that leads to overoptimistic results. Ensembles of models are a\ncost-effective alternative to the expensive full-factorial and to the unstable\nsequential designs.\n",
    "topics": "{'Data Augmentation': 0.99999976, 'Transfer Learning': 0.9993911}",
    "score": 0.8180828887
  },
  {
    "id": "1907.13550",
    "title": "Towards Automated Infographic Design: Deep Learning-based\n  Auto-Extraction of Extensible Timeline",
    "abstract": "  Designers need to consider not only perceptual effectiveness but also visual\nstyles when creating an infographic. This process can be difficult and time\nconsuming for professional designers, not to mention non-expert users, leading\nto the demand for automated infographics design. As a first step, we focus on\ntimeline infographics, which have been widely used for centuries. We contribute\nan end-to-end approach that automatically extracts an extensible timeline\ntemplate from a bitmap image. Our approach adopts a deconstruction and\nreconstruction paradigm. At the deconstruction stage, we propose a multi-task\ndeep neural network that simultaneously parses two kinds of information from a\nbitmap timeline: 1) the global information, i.e., the representation, scale,\nlayout, and orientation of the timeline, and 2) the local information, i.e.,\nthe location, category, and pixels of each visual element on the timeline. At\nthe reconstruction stage, we propose a pipeline with three techniques, i.e.,\nNon-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an\nextensible template from the infographic, by utilizing the deconstruction\nresults. To evaluate the effectiveness of our approach, we synthesize a\ntimeline dataset (4296 images) and collect a real-world timeline dataset (393\nimages) from the Internet. We first report quantitative evaluation results of\nour approach over the two datasets. Then, we present examples of automatically\nextracted templates and timelines automatically generated based on these\ntemplates to qualitatively demonstrate the performance. The results confirm\nthat our approach can effectively extract extensible templates from real-world\ntimeline infographics.\n",
    "topics": "{}",
    "score": 0.8179438814
  },
  {
    "id": "1801.06742",
    "title": "Multi-pseudo Regularized Label for Generated Data in Person\n  Re-Identification",
    "abstract": "  Sufficient training data normally is required to train deeply learned models.\nHowever, due to the expensive manual process for labelling large number of\nimages, the amount of available training data is always limited. To produce\nmore data for training a deep network, Generative Adversarial Network (GAN) can\nbe used to generate artificial sample data. However, the generated data usually\ndoes not have annotation labels. To solve this problem, in this paper, we\npropose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign\nit to the generated data. With MpRL, the generated data will be used as the\nsupplementary of real training data to train a deep neural network in a\nsemi-supervised learning fashion. To build the corresponding relationship\nbetween the real data and generated data, MpRL assigns each generated data a\nproper virtual label which reflects the likelihood of the affiliation of the\ngenerated data to pre-defined training classes in the real data domain. Unlike\nthe traditional label which usually is a single integral number, the virtual\nlabel proposed in this work is a set of weight-based values each individual of\nwhich is a number in (0,1] called multi-pseudo label and reflects the degree of\nrelation between each generated data to every pre-defined class of real data. A\ncomprehensive evaluation is carried out by adopting two state-of-the-art\nconvolutional neural networks (CNNs) in our experiments to verify the\neffectiveness of MpRL. Experiments demonstrate that by assigning MpRL to\ngenerated data, we can further improve the person re-ID performance on five\nre-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01.\nThe proposed method obtains +6.29%, +6.30%, +5.58%, +5.84%, and +3.48%\nimprovements in rank-1 accuracy over a strong CNN baseline on the five datasets\nrespectively, and outperforms state-of-the-art methods.\n",
    "topics": "{'Person Re-Identification': 0.99903667}",
    "score": 0.8176611683
  },
  {
    "id": "2001.04663",
    "title": "Effects of annotation granularity in deep learning models for\n  histopathological images",
    "abstract": "  Pathological is crucial to cancer diagnosis. Usually, Pathologists draw their\nconclusion based on observed cell and tissue structure on histology slides.\nRapid development in machine learning, especially deep learning have\nestablished robust and accurate classifiers. They are being used to analyze\nhistopathological slides and assist pathologists in diagnosis. Most machine\nlearning systems rely heavily on annotated data sets to gain experiences and\nknowledge to correctly and accurately perform various tasks such as\nclassification and segmentation. This work investigates different granularity\nof annotations in histopathological data set including image-wise, bounding\nbox, ellipse-wise, and pixel-wise to verify the influence of annotation in\npathological slide on deep learning models. We design corresponding experiments\nto test classification and segmentation performance of deep learning models\nbased on annotations with different annotation granularity. In classification,\nstate-of-the-art deep learning-based classifiers perform better when trained by\npixel-wise annotation dataset. On average, precision, recall and F1-score\nimproves by 7.87%, 8.83% and 7.85% respectively. Thus, it is suggested that\nfiner granularity annotations are better utilized by deep learning algorithms\nin classification tasks. Similarly, semantic segmentation algorithms can\nachieve 8.33% better segmentation accuracy when trained by pixel-wise\nannotations. Our study shows not only that finer-grained annotation can improve\nthe performance of deep learning models, but also help extracts more accurate\nphenotypic information from histopathological slides. Intelligence systems\ntrained on granular annotations may help pathologists inspecting certain\nregions for better diagnosis. The compartmentalized prediction approach similar\nto this work may contribute to phenotype and genotype association studies.\n",
    "topics": "{'Semantic Segmentation': 0.9696126}",
    "score": 0.81765943
  },
  {
    "id": "1906.07307",
    "title": "Towards Transfer Learning for End-to-End Speech Synthesis from Deep\n  Pre-Trained Language Models",
    "abstract": "  Modern text-to-speech (TTS) systems are able to generate audio that sounds\nalmost as natural as human speech. However, the bar of developing high-quality\nTTS systems remains high since a sizable set of studio-quality <text, audio>\npairs is usually required. Compared to commercial data used to develop\nstate-of-the-art systems, publicly available data are usually worse in terms of\nboth quality and size. Audio generated by TTS systems trained on publicly\navailable data tends to not only sound less natural, but also exhibits more\nbackground noise. In this work, we aim to lower TTS systems' reliance on\nhigh-quality data by providing them the textual knowledge extracted by deep\npre-trained language models during training. In particular, we investigate the\nuse of BERT to assist the training of Tacotron-2, a state of the art TTS\nconsisting of an encoder and an attention-based decoder. BERT representations\nlearned from large amounts of unlabeled text data are shown to contain very\nrich semantic and syntactic information about the input text, and have\npotential to be leveraged by a TTS system to compensate the lack of\nhigh-quality data. We incorporate BERT as a parallel branch to the Tacotron-2\nencoder with its own attention head. For an input text, it is simultaneously\npassed into BERT and the Tacotron-2 encoder. The representations extracted by\nthe two branches are concatenated and then fed to the decoder. As a preliminary\nstudy, although we have not found incorporating BERT into Tacotron-2 generates\nmore natural or cleaner speech at a human-perceivable level, we observe\nimprovements in other aspects such as the model is being significantly better\nat knowing when to stop decoding such that there is much less babbling at the\nend of the synthesized audio and faster convergence during training.\n",
    "topics": "{'Speech Synthesis': 0.99999356, 'Transfer Learning': 0.8614775}",
    "score": 0.8175728018
  },
  {
    "id": "1809.00193",
    "title": "Data Dropout: Optimizing Training Data for Convolutional Neural Networks",
    "abstract": "  Deep learning models learn to fit training data while they are highly\nexpected to generalize well to testing data. Most works aim at finding such\nmodels by creatively designing architectures and fine-tuning parameters. To\nadapt to particular tasks, hand-crafted information such as image prior has\nalso been incorporated into end-to-end learning. However, very little progress\nhas been made on investigating how an individual training sample will influence\nthe generalization ability of a model. In other words, to achieve high\ngeneralization accuracy, do we really need all the samples in a training\ndataset? In this paper, we demonstrate that deep learning models such as\nconvolutional neural networks may not favor all training samples, and\ngeneralization accuracy can be further improved by dropping those unfavorable\nsamples. Specifically, the influence of removing a training sample is\nquantifiable, and we propose a Two-Round Training approach, aiming to achieve\nhigher generalization accuracy. We locate unfavorable samples after the first\nround of training, and then retrain the model from scratch with the reduced\ntraining dataset in the second round. Since our approach is essentially\ndifferent from fine-tuning or further training, the computational cost should\nnot be a concern. Our extensive experimental results indicate that, with\nidentical settings, the proposed approach can boost performance of the\nwell-known networks on both high-level computer vision problems such as image\nclassification, and low-level vision problems such as image denoising.\n",
    "topics": "{'Image Denoising': 0.99939525, 'Denoising': 0.9899742, 'Image Classification': 0.93971914}",
    "score": 0.8175333256
  },
  {
    "id": "1606.07695",
    "title": "Fully DNN-based Multi-label regression for audio tagging",
    "abstract": "  Acoustic event detection for content analysis in most cases relies on lots of\nlabeled data. However, manually annotating data is a time-consuming task, which\nthus makes few annotated resources available so far. Unlike audio event\ndetection, automatic audio tagging, a multi-label acoustic event classification\ntask, only relies on weakly labeled data. This is highly desirable to some\npractical applications using audio analysis. In this paper we propose to use a\nfully deep neural network (DNN) framework to handle the multi-label\nclassification task in a regression way. Considering that only chunk-level\nrather than frame-level labels are available, the whole or almost whole frames\nof the chunk were fed into the DNN to perform a multi-label regression for the\nexpected tags. The fully DNN, which is regarded as an encoding function, can\nwell map the audio features sequence to a multi-tag vector. A deep pyramid\nstructure was also designed to extract more robust high-level features related\nto the target tags. Further improved methods were adopted, such as the Dropout\nand background noise aware training, to enhance its generalization capability\nfor new audio recordings in mismatched environments. Compared with the\nconventional Gaussian Mixture Model (GMM) and support vector machine (SVM)\nmethods, the proposed fully DNN-based method could well utilize the long-term\ntemporal information with the whole chunk as the input. The results show that\nour approach obtained a 15% relative improvement compared with the official\nGMM-based method of DCASE 2016 challenge.\n",
    "topics": "{'Multi-Label Classification': 0.9999974}",
    "score": 0.8174706233
  },
  {
    "id": "1808.04256",
    "title": "CT Super-resolution GAN Constrained by the Identical, Residual, and\n  Cycle Learning Ensemble(GAN-CIRCLE)",
    "abstract": "  Computed tomography (CT) is widely used in screening, diagnosis, and\nimage-guided therapy for both clinical and research purposes. Since CT involves\nionizing radiation, an overarching thrust of related technical research is\ndevelopment of novel methods enabling ultrahigh quality imaging with fine\nstructural details while reducing the X-ray radiation. In this paper, we\npresent a semi-supervised deep learning approach to accurately recover\nhigh-resolution (HR) CT images from low-resolution (LR) counterparts.\nSpecifically, with the generative adversarial network (GAN) as the building\nblock, we enforce the cycle-consistency in terms of the Wasserstein distance to\nestablish a nonlinear end-to-end mapping from noisy LR input images to denoised\nand deblurred HR outputs. We also include the joint constraints in the loss\nfunction to facilitate structural preservation. In this deep imaging process,\nwe incorporate deep convolutional neural network (CNN), residual learning, and\nnetwork in network techniques for feature extraction and restoration. In\ncontrast to the current trend of increasing network depth and complexity to\nboost the CT imaging performance, which limit its real-world applications by\nimposing considerable computational and memory overheads, we apply a parallel\n$1\\times1$ CNN to compress the output of the hidden layer and optimize the\nnumber of layers and the number of filters for each convolutional layer.\nQuantitative and qualitative evaluations demonstrate that our proposed model is\naccurate, efficient and robust for super-resolution (SR) image restoration from\nnoisy LR input images. In particular, we validate our composite SR networks on\nthree large-scale CT datasets, and obtain promising results as compared to the\nother state-of-the-art methods.\n",
    "topics": "{'Computed Tomography (CT)': 0.9999827, 'Image Restoration': 0.9999244, 'Super-Resolution': 0.999468, 'Super Resolution': 0.9984693}",
    "score": 0.8174167927
  },
  {
    "id": "1802.02601",
    "title": "Digital Watermarking for Deep Neural Networks",
    "abstract": "  Although deep neural networks have made tremendous progress in the area of\nmultimedia representation, training neural models requires a large amount of\ndata and time. It is well-known that utilizing trained models as initial\nweights often achieves lower training error than neural networks that are not\npre-trained. A fine-tuning step helps to reduce both the computational cost and\nimprove performance. Therefore, sharing trained models has been very important\nfor the rapid progress of research and development. In addition, trained models\ncould be important assets for the owner(s) who trained them, hence we regard\ntrained models as intellectual property. In this paper, we propose a digital\nwatermarking technology for ownership authorization of deep neural networks.\nFirst, we formulate a new problem: embedding watermarks into deep neural\nnetworks. We also define requirements, embedding situations, and attack types\non watermarking in deep neural networks. Second, we propose a general framework\nfor embedding a watermark in model parameters, using a parameter regularizer.\nOur approach does not impair the performance of networks into which a watermark\nis placed because the watermark is embedded while training the host network.\nFinally, we perform comprehensive experiments to reveal the potential of\nwatermarking deep neural networks as the basis of this new research effort. We\nshow that our framework can embed a watermark during the training of a deep\nneural network from scratch, and during fine-tuning and distilling, without\nimpairing its performance. The embedded watermark does not disappear even after\nfine-tuning or parameter pruning; the watermark remains complete even after 65%\nof parameters are pruned.\n",
    "topics": "{}",
    "score": 0.8171834412
  },
  {
    "id": "2003.08736",
    "title": "Real-Time High-Performance Semantic Image Segmentation of Urban Street\n  Scenes",
    "abstract": "  Deep Convolutional Neural Networks (DCNNs) have recently shown outstanding\nperformance in semantic image segmentation. However, state-of-the-art\nDCNN-based semantic segmentation methods usually suffer from high computational\ncomplexity due to the use of complex network architectures. This greatly limits\ntheir applications in the real-world scenarios that require real-time\nprocessing. In this paper, we propose a real-time high-performance DCNN-based\nmethod for robust semantic segmentation of urban street scenes, which achieves\na good trade-off between accuracy and speed. Specifically, a Lightweight\nBaseline Network with Atrous convolution and Attention (LBN-AA) is firstly used\nas our baseline network to efficiently obtain dense feature maps. Then, the\nDistinctive Atrous Spatial Pyramid Pooling (DASPP), which exploits the\ndifferent sizes of pooling operations to encode the rich and distinctive\nsemantic information, is developed to detect objects at multiple scales.\nMeanwhile, a Spatial detail-Preserving Network (SPN) with shallow convolutional\nlayers is designed to generate high-resolution feature maps preserving the\ndetailed spatial information. Finally, a simple but practical Feature Fusion\nNetwork (FFN) is used to effectively combine both shallow and deep features\nfrom the semantic branch (DASPP) and the spatial branch (SPN), respectively.\nExtensive experimental results show that the proposed method respectively\nachieves the accuracy of 73.6% and 68.0% mean Intersection over Union (mIoU)\nwith the inference speed of 51.0 fps and 39.3 fps on the challenging Cityscapes\nand CamVid test datasets (by only using a single NVIDIA TITAN X card). This\ndemonstrates that the proposed method offers excellent performance at the\nreal-time speed for semantic segmentation of urban street scenes.\n",
    "topics": "{'Semantic Segmentation': 0.9999999, 'Real-Time Semantic Segmentation': 0.8721252}",
    "score": 0.8171329886
  },
  {
    "id": "1804.02386",
    "title": "Inferring transportation modes from GPS trajectories using a\n  convolutional neural network",
    "abstract": "  Identifying the distribution of users' transportation modes is an essential\npart of travel demand analysis and transportation planning. With the advent of\nubiquitous GPS-enabled devices (e.g., a smartphone), a cost-effective approach\nfor inferring commuters' mobility mode(s) is to leverage their GPS\ntrajectories. A majority of studies have proposed mode inference models based\non hand-crafted features and traditional machine learning algorithms. However,\nmanual features engender some major drawbacks including vulnerability to\ntraffic and environmental conditions as well as possessing human's bias in\ncreating efficient features. One way to overcome these issues is by utilizing\nConvolutional Neural Network (CNN) schemes that are capable of automatically\ndriving high-level features from the raw input. Accordingly, in this paper, we\ntake advantage of CNN architectures so as to predict travel modes based on only\nraw GPS trajectories, where the modes are labeled as walk, bike, bus, driving,\nand train. Our key contribution is designing the layout of the CNN's input\nlayer in such a way that not only is adaptable with the CNN schemes but\nrepresents fundamental motion characteristics of a moving object including\nspeed, acceleration, jerk, and bearing rate. Furthermore, we ameliorate the\nquality of GPS logs through several data preprocessing steps. Using the clean\ninput layer, a variety of CNN configurations are evaluated to achieve the best\nCNN architecture. The highest accuracy of 84.8% has been achieved through the\nensemble of the best CNN configuration. In this research, we contrast our\nmethodology with traditional machine learning algorithms as well as the seminal\nand most related studies to demonstrate the superiority of our framework.\n",
    "topics": "{}",
    "score": 0.817050821
  },
  {
    "id": "1803.03396",
    "title": "Cross-View Image Synthesis using Conditional GANs",
    "abstract": "  Learning to generate natural scenes has always been a challenging task in\ncomputer vision. It is even more painstaking when the generation is conditioned\non images with drastically different views. This is mainly because\nunderstanding, corresponding, and transforming appearance and semantic\ninformation across the views is not trivial. In this paper, we attempt to solve\nthe novel problem of cross-view image synthesis, aerial to street-view and vice\nversa, using conditional generative adversarial networks (cGAN). Two new\narchitectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq)\nare proposed to generate scenes with resolutions of 64x64 and 256x256 pixels.\nX-Fork architecture has a single discriminator and a single generator. The\ngenerator hallucinates both the image and its semantic segmentation in the\ntarget view. X-Seq architecture utilizes two cGANs. The first one generates the\ntarget image which is subsequently fed to the second cGAN for generating its\ncorresponding semantic segmentation map. The feedback from the second cGAN\nhelps the first cGAN generate sharper images. Both of our proposed\narchitectures learn to generate natural images as well as their semantic\nsegmentation maps. The proposed methods show that they are able to capture and\nmaintain the true semantics of objects in source and target views better than\nthe traditional image-to-image translation method which considers only the\nvisual appearance of the scene. Extensive qualitative and quantitative\nevaluations support the effectiveness of our frameworks, compared to two state\nof the art methods, for natural scene generation across drastically different\nviews.\n",
    "topics": "{'Image Generation': 0.9999732, 'Semantic Segmentation': 0.9999696, 'Image-to-Image Translation': 0.7736162}",
    "score": 0.81685322
  },
  {
    "id": "1912.05190",
    "title": "IoU-uniform R-CNN: Breaking Through the Limitations of RPN",
    "abstract": "  Region Proposal Network (RPN) is the cornerstone of two-stage object\ndetectors, it generates a sparse set of object proposals and alleviates the\nextrem foregroundbackground class imbalance problem during training. However,\nwe find that the potential of the detector has not been fully exploited due to\nthe IoU distribution imbalance and inadequate quantity of the training samples\ngenerated by RPN. With the increasing intersection over union (IoU), the\nexponentially smaller numbers of positive samples would lead to the\ndistribution skewed towards lower IoUs, which hinders the optimization of\ndetector at high IoU levels. In this paper, to break through the limitations of\nRPN, we propose IoU-Uniform R-CNN, a simple but effective method that directly\ngenerates training samples with uniform IoU distribution for the regression\nbranch as well as the IoU prediction branch. Besides, we improve the\nperformance of IoU prediction branch by eliminating the feature offsets of RoIs\nat inference, which helps the NMS procedure by preserving accurately localized\nbounding box. Extensive experiments on the PASCAL VOC and MS COCO dataset show\nthe effectiveness of our method, as well as its compatibility and adaptivity to\nmany object detection architectures. The code is made publicly available at\nhttps://github.com/zl1994/IoU-Uniform-R-CNN,\n",
    "topics": "{'Region Proposal': 1.0, 'Object Detection': 0.9795735, 'Language Modelling': 0.5406306}",
    "score": 0.8167893032
  },
  {
    "id": "1809.04356",
    "title": "Deep learning for time series classification: a review",
    "abstract": "  Time Series Classification (TSC) is an important and challenging problem in\ndata mining. With the increase of time series data availability, hundreds of\nTSC algorithms have been proposed. Among these methods, only a few have\nconsidered Deep Neural Networks (DNNs) to perform this task. This is surprising\nas deep learning has seen very successful applications in the last years. DNNs\nhave indeed revolutionized the field of computer vision especially with the\nadvent of novel deeper architectures such as Residual and Convolutional Neural\nNetworks. Apart from images, sequential data such as text and audio can also be\nprocessed with DNNs to reach state-of-the-art performance for document\nclassification and speech recognition. In this article, we study the current\nstate-of-the-art performance of deep learning algorithms for TSC by presenting\nan empirical study of the most recent DNN architectures for TSC. We give an\noverview of the most successful deep learning applications in various time\nseries domains under a unified taxonomy of DNNs for TSC. We also provide an\nopen source deep learning framework to the TSC community where we implemented\neach of the compared approaches and evaluated them on a univariate TSC\nbenchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By\ntraining 8,730 deep learning models on 97 time series datasets, we propose the\nmost exhaustive study of DNNs for TSC to date.\n",
    "topics": "{'Time Series Classification': 1.0, 'Time Series': 0.9999995}",
    "score": 0.8167832261
  },
  {
    "id": "2006.04406",
    "title": "Passive Batch Injection Training Technique: Boosting Network Performance\n  by Injecting Mini-Batches from a different Data Distribution",
    "abstract": "  This work presents a novel training technique for deep neural networks that\nmakes use of additional data from a distribution that is different from that of\nthe original input data. This technique aims to reduce overfitting and improve\nthe generalization performance of the network. Our proposed technique, namely\nPassive Batch Injection Training Technique (PBITT), even reduces the level of\noverfitting in networks that already use the standard techniques for reducing\noverfitting such as $L_2$ regularization and batch normalization, resulting in\nsignificant accuracy improvements. Passive Batch Injection Training Technique\n(PBITT) introduces a few passive mini-batches into the training process that\ncontain data from a distribution that is different from the input data\ndistribution. This technique does not increase the number of parameters in the\nfinal model and also does not increase the inference (test) time but still\nimproves the performance of deep CNNs. To the best of our knowledge, this is\nthe first work that makes use of different data distribution to aid the\ntraining of convolutional neural networks (CNNs). We thoroughly evaluate the\nproposed approach on standard architectures: VGG, ResNet, and WideResNet, and\non several popular datasets: CIFAR-10, CIFAR-100, SVHN, and ImageNet. We\nobserve consistent accuracy improvement by using the proposed technique. We\nalso show experimentally that the model trained by our technique generalizes\nwell to other tasks such as object detection on the MS-COCO dataset using\nFaster R-CNN. We present extensive ablations to validate the proposed approach.\nOur approach improves the accuracy of VGG-16 by a significant margin of 2.1%\nover the CIFAR-100 dataset.\n",
    "topics": "{'Object Detection': 0.9959371}",
    "score": 0.8167823761
  },
  {
    "id": "2007.08389",
    "title": "Device-Robust Acoustic Scene Classification Based on Two-Stage\n  Categorization and Data Augmentation",
    "abstract": "  In this technical report, we present a joint effort of four groups, namely\nGT, USTC, Tencent, and UKE, to tackle Task 1 - Acoustic Scene Classification\n(ASC) in the DCASE 2020 Challenge. Task 1 comprises two different sub-tasks:\n(i) Task 1a focuses on ASC of audio signals recorded with multiple (real and\nsimulated) devices into ten different fine-grained classes, and (ii) Task 1b\nconcerns with classification of data into three higher-level classes using\nlow-complexity solutions. For Task 1a, we propose a novel two-stage ASC system\nleveraging upon ad-hoc score combination of two convolutional neural networks\n(CNNs), classifying the acoustic input according to three classes, and then ten\nclasses, respectively. Four different CNN-based architectures are explored to\nimplement the two-stage classifiers, and several data augmentation techniques\nare also investigated. For Task 1b, we leverage upon a quantization method to\nreduce the complexity of two of our top-accuracy three-classes CNN-based\narchitectures. On Task 1a development data set, an ASC accuracy of 76.9\\% is\nattained using our best single classifier and data augmentation. An accuracy of\n81.9\\% is then attained by a final model fusion of our two-stage ASC\nclassifiers. On Task 1b development data set, we achieve an accuracy of 96.7\\%\nwith a model size smaller than 500KB. Code is available:\nhttps://github.com/MihawkHu/DCASE2020_task1.\n",
    "topics": "{'Scene Classification': 1.0, 'Data Augmentation': 1.0, 'Quantization': 0.9062174}",
    "score": 0.816701019
  },
  {
    "id": "1904.02363",
    "title": "Spatiotemporal CNN for Video Object Segmentation",
    "abstract": "  In this paper, we present a unified, end-to-end trainable spatiotemporal CNN\nmodel for VOS, which consists of two branches, i.e., the temporal coherence\nbranch and the spatial segmentation branch. Specifically, the temporal\ncoherence branch pretrained in an adversarial fashion from unlabeled video\ndata, is designed to capture the dynamic appearance and motion cues of video\nsequences to guide object segmentation. The spatial segmentation branch focuses\non segmenting objects accurately based on the learned appearance and motion\ncues. To obtain accurate segmentation results, we design a coarse-to-fine\nprocess to sequentially apply a designed attention module on multi-scale\nfeature maps, and concatenate them to produce the final prediction. In this\nway, the spatial segmentation branch is enforced to gradually concentrate on\nobject regions. These two branches are jointly fine-tuned on video segmentation\nsequences in an end-to-end manner. Several experiments are carried out on three\nchallenging datasets (i.e., DAVIS-2016, DAVIS-2017 and Youtube-Object) to show\nthat our method achieves favorable performance against the state-of-the-arts.\nCode is available at https://github.com/longyin880815/STCNN.\n",
    "topics": "{'Video Semantic Segmentation': 0.99978715, 'Visual Object Tracking': 0.99931264, 'Semi-Supervised Video Object Segmentation': 0.9982243, 'Semantic Segmentation': 0.9973929, 'Video Segmentation': 0.9932342, 'Video Object Segmentation': 0.98752916}",
    "score": 0.816646227
  },
  {
    "id": "1901.03554",
    "title": "CSGAN: Cyclic-Synthesized Generative Adversarial Networks for\n  Image-to-Image Transformation",
    "abstract": "  The primary motivation of Image-to-Image Transformation is to convert an\nimage of one domain to another domain. Most of the research has been focused on\nthe task of image transformation for a set of pre-defined domains. Very few\nworks are reported that actually developed a common framework for\nimage-to-image transformation for different domains. With the introduction of\nGenerative Adversarial Networks (GANs) as a general framework for the image\ngeneration problem, there is a tremendous growth in the area of image-to-image\ntransformation. Most of the research focuses over the suitable objective\nfunction for image-to-image transformation. In this paper, we propose a new\nCyclic-Synthesized Generative Adversarial Networks (CSGAN) for image-to-image\ntransformation. The proposed CSGAN uses a new objective function (loss) called\nCyclic-Synthesized Loss (CS) between the synthesized image of one domain and\ncycled image of another domain. The performance of the proposed CSGAN is\nevaluated on two benchmark image-to-image transformation datasets, including\nCUHK Face dataset and CMP Facades dataset. The results are computed using the\nwidely used evaluation metrics such as MSE, SSIM, PSNR, and LPIPS. The\nexperimental results of the proposed CSGAN approach are compared with the\nlatest state-of-the-art approaches such as GAN, Pix2Pix, DualGAN, CycleGAN and\nPS2GAN. The proposed CSGAN technique outperforms all the methods over CUHK\ndataset and exhibits the promising and comparable performance over Facades\ndataset in terms of both qualitative and quantitative measures. The code is\navailable at https://github.com/KishanKancharagunta/CSGAN.\n",
    "topics": "{'SSIM': 0.99999964, 'Image Generation': 0.9673273, 'Image-to-Image Translation': 0.6107312}",
    "score": 0.8166352479
  },
  {
    "id": "2004.13665",
    "title": "A novel Region of Interest Extraction Layer for Instance Segmentation",
    "abstract": "  Given the wide diffusion of deep neural network architectures for computer\nvision tasks, several new applications are nowadays more and more feasible.\nAmong them, a particular attention has been recently given to instance\nsegmentation, by exploiting the results achievable by two-stage networks (such\nas Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex\narchitectures, a crucial role is played by the Region of Interest (RoI)\nextraction layer, devoted to extracting a coherent subset of features from a\nsingle Feature Pyramid Network (FPN) layer attached on top of a backbone.\n  This paper is motivated by the need to overcome the limitations of existing\nRoI extractors which select only one (the best) layer from FPN. Our intuition\nis that all the layers of FPN retain useful information. Therefore, the\nproposed layer (called Generic RoI Extractor - GRoIE) introduces non-local\nbuilding blocks and attention mechanisms to boost the performance.\n  A comprehensive ablation study at component level is conducted to find the\nbest set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can\nbe integrated seamlessly with every two-stage architecture for both object\ndetection and instance segmentation tasks. Therefore, the improvements brought\nabout by the use of GRoIE in different state-of-the-art architectures are also\nevaluated. The proposed layer leads up to gain a 1.1% AP improvement on\nbounding box detection and 1.7% AP improvement on instance segmentation.\n  The code is publicly available on GitHub repository at\nhttps://github.com/IMPLabUniPr/mmdetection/tree/groie_dev\n",
    "topics": "{'Instance Segmentation': 1.0, 'Semantic Segmentation': 0.9999914, 'Object Detection': 0.9929945}",
    "score": 0.8166250118
  },
  {
    "id": "2004.03455",
    "title": "Deep Learning Based Multi-Label Text Classification of UNGA Resolutions",
    "abstract": "  The main goal of this research is to produce a useful software for United\nNations (UN), that could help to speed up the process of qualifying the UN\ndocuments following the Sustainable Development Goals (SDGs) in order to\nmonitor the progresses at the world level to fight poverty, discrimination,\nclimate changes. In fact human labeling of UN documents would be a daunting\ntask given the size of the impacted corpus. Thus, automatic labeling must be\nadopted at least as a first step of a multi-phase process to reduce the overall\neffort of cataloguing and classifying. Deep Learning (DL) is nowadays one of\nthe most powerful tools for state-of-the-art (SOTA) AI for this task, but very\noften it comes with the cost of an expensive and error-prone preparation of a\ntraining-set. In the case of multi-label text classification of domain-specific\ntext it seems that we cannot effectively adopt DL without a big-enough\ndomain-specific training-set. In this paper, we show that this is not always\ntrue. In fact we propose a novel method that is able, through statistics like\nTF-IDF, to exploit pre-trained SOTA DL models (such as the Universal Sentence\nEncoder) without any need for traditional transfer learning or any other\nexpensive training procedure. We show the effectiveness of our method in a\nlegal context, by classifying UN Resolutions according to their most related\nSDGs.\n",
    "topics": "{'Text Classification': 0.9999994, 'Transfer Learning': 0.9866926}",
    "score": 0.8166105369
  },
  {
    "id": "2007.10497",
    "title": "CovidDeep: SARS-CoV-2/COVID-19 Test Based on Wearable Medical Sensors\n  and Efficient Neural Networks",
    "abstract": "  The novel coronavirus (SARS-CoV-2) has led to a pandemic. Due to its highly\ncontagious nature, it has spread rapidly, resulting in major disruption to\npublic health. In addition, it has also had a severe negative impact on the\nworld economy. As a result, it is widely recognized now that widespread testing\nis key to containing the spread of the disease and opening up the economy.\nHowever, the current testing regime has been unable to keep up with testing\ndemands. Hence, there is a need for an alternative approach for repeated\nlarge-scale testing of COVID-19. The emergence of wearable medical sensors\n(WMSs) and novel machine learning methods, such as deep neural networks (DNNs),\npoints to a promising approach to address this challenge. WMSs enable\ncontinuous and user-transparent monitoring of the physiological signals.\nHowever, disease detection based on WMSs/DNNs and their deployment on\nresource-constrained edge devices remain challenging problems. In this work, we\npropose CovidDeep, a framework that combines efficient DNNs with commercially\navailable WMSs for pervasive testing of the coronavirus. We collected data from\n87 individuals, spanning four cohorts including healthy, asymptomatic (but\nSARS-CoV-2-positive) as well as moderately and severely symptomatic COVID-19\npatients. We trained DNNs on various subsets of the features extracted from six\nWMS and questionnaire categories to perform ablation studies to determine which\nsubsets are most efficacious in terms of test accuracy for a four-way\nclassification. The highest test accuracy obtained was 99.4%. Since different\nWMS subsets may be more accessible (in terms of cost, availability, etc.) to\ndifferent sets of people, we hope these DNN models will provide users with\nample flexibility. The resultant DNNs can be easily deployed on edge devices,\ne.g., smartwatch or smartphone, which also has the benefit of preserving\npatient privacy.\n",
    "topics": "{}",
    "score": 0.8164632038
  },
  {
    "id": "2010.06107",
    "title": "Universal Model for 3D Medical Image Analysis",
    "abstract": "  Deep Learning-based methods recently have achieved remarkable progress in\nmedical image analysis, but heavily rely on massive amounts of labeled training\ndata. Transfer learning from pre-trained models has been proposed as a standard\npipeline on medical image analysis to address this bottleneck. Despite their\nsuccess, the existing pre-trained models are mostly not tuned for multi-modal\nmulti-task generalization in medical domains. Specifically, their training data\nare either from non-medical domain or in single modality, failing to attend to\nthe problem of performance degradation with cross-modal transfer. Furthermore,\nthere is no effort to explicitly extract multi-level features required by a\nvariety of downstream tasks. To overcome these limitations, we propose\nUniversal Model, a transferable and generalizable pre-trained model for 3D\nmedical image analysis. A unified self-supervised learning scheme is leveraged\nto learn representations from multiple unlabeled source datasets with different\nmodalities and distinctive scan regions. A modality invariant adversarial\nlearning module is further introduced to improve the cross-modal\ngeneralization. To fit a wide range of tasks, a simple yet effective scale\nclassifier is incorporated to capture multi-level visual representations. To\nvalidate the effectiveness of the Universal Model, we perform extensive\nexperimental analysis on five target tasks, covering multiple imaging\nmodalities, distinctive scan regions, and different analysis tasks. Compared\nwith both public 3D pre-trained models and newly investigated 3D\nself-supervised learning methods, Universal Model demonstrates superior\ngeneralizability, manifested by its higher performance, stronger robustness and\nfaster convergence. The pre-trained Universal Model is available at:\n\\href{https://github.com/xm-cmic/Universal-Model}{https://github.com/xm-cmic/Universal-Model}.\n",
    "topics": "{'Self-Supervised Learning': 1.0}",
    "score": 0.816450603
  },
  {
    "id": "1110.2416",
    "title": "Supervised learning of short and high-dimensional temporal sequences for\n  life science measurements",
    "abstract": "  The analysis of physiological processes over time are often given by\nspectrometric or gene expression profiles over time with only few time points\nbut a large number of measured variables. The analysis of such temporal\nsequences is challenging and only few methods have been proposed. The\ninformation can be encoded time independent, by means of classical expression\ndifferences for a single time point or in expression profiles over time.\nAvailable methods are limited to unsupervised and semi-supervised settings. The\npredictive variables can be identified only by means of wrapper or\npost-processing techniques. This is complicated due to the small number of\nsamples for such studies. Here, we present a supervised learning approach,\ntermed Supervised Topographic Mapping Through Time (SGTM-TT). It learns a\nsupervised mapping of the temporal sequences onto a low dimensional grid. We\nutilize a hidden markov model (HMM) to account for the time domain and\nrelevance learning to identify the relevant feature dimensions most predictive\nover time. The learned mapping can be used to visualize the temporal sequences\nand to predict the class of a new sequence. The relevance learning permits the\nidentification of discriminating masses or gen expressions and prunes\ndimensions which are unnecessary for the classification task or encode mainly\nnoise. In this way we obtain a very efficient learning system for temporal\nsequences. The results indicate that using simultaneous supervised learning and\nmetric adaptation significantly improves the prediction accuracy for\nsynthetically and real life data in comparison to the standard techniques. The\ndiscriminating features, identified by relevance learning, compare favorably\nwith the results of alternative methods. Our method permits the visualization\nof the data on a low dimensional grid, highlighting the observed temporal\nstructure.\n",
    "topics": "{}",
    "score": 0.8163882345
  },
  {
    "id": "1703.06902",
    "title": "A Comparison of deep learning methods for environmental sound",
    "abstract": "  Environmental sound detection is a challenging application of machine\nlearning because of the noisy nature of the signal, and the small amount of\n(labeled) data that is typically available. This work thus presents a\ncomparison of several state-of-the-art Deep Learning models on the IEEE\nchallenge on Detection and Classification of Acoustic Scenes and Events (DCASE)\n2016 challenge task and data, classifying sounds into one of fifteen common\nindoor and outdoor acoustic scenes, such as bus, cafe, car, city center, forest\npath, library, train, etc. In total, 13 hours of stereo audio recordings are\navailable, making this one of the largest datasets available. We perform\nexperiments on six sets of features, including standard Mel-frequency cepstral\ncoefficients (MFCC), Binaural MFCC, log Mel-spectrum and two different large-\nscale temporal pooling features extracted using OpenSMILE. On these features,\nwe apply five models: Gaussian Mixture Model (GMM), Deep Neural Network (DNN),\nRecurrent Neural Network (RNN), Convolutional Deep Neural Net- work (CNN) and\ni-vector. Using the late-fusion approach, we improve the performance of the\nbaseline 72.5% by 15.6% in 4-fold Cross Validation (CV) avg. accuracy and 11%\nin test accuracy, which matches the best result of the DCASE 2016 challenge.\nWith large feature sets, deep neural network models out- perform traditional\nmethods and achieve the best performance among all the studied methods.\nConsistent with other work, the best performing single model is the\nnon-temporal DNN model, which we take as evidence that sounds in the DCASE\nchallenge do not exhibit strong temporal dynamics.\n",
    "topics": "{}",
    "score": 0.8163717348
  },
  {
    "id": "1910.04030",
    "title": "Cribriform pattern detection in prostate histopathological images using\n  deep learning models",
    "abstract": "  Architecture, size, and shape of glands are most important patterns used by\npathologists for assessment of cancer malignancy in prostate histopathological\ntissue slides. Varying structures of glands along with cumbersome manual\nobservations may result in subjective and inconsistent assessment. Cribriform\ngland with irregular border is an important feature in Gleason pattern 4. We\npropose using deep neural networks for cribriform pattern classification in\nprostate histopathological images. $163708$ Hematoxylin and Eosin (H\\&E)\nstained images were extracted from histopathologic tissue slides of $19$\npatients with prostate cancer and annotated for cribriform patterns. Our\nautomated image classification system analyses the H\\&E images to classify them\nas either `Cribriform' or `Non-cribriform'. Our system uses various deep\nlearning approaches and hand-crafted image pixel intensity-based features. We\npresent our results for cribriform pattern detection across various parameters\nand configuration allowed by our system. The combination of fine-tuned deep\nlearning models outperformed the state-of-art nuclei feature based methods. Our\nimage classification system achieved the testing accuracy of $85.93~\\pm~7.54$\n(cross-validated) and $88.04~\\pm~5.63$ ( additional unseen test set) across\nthree folds. In this paper, we present an annotated cribriform dataset along\nwith analysis of deep learning models and hand-crafted features for cribriform\npattern detection in prostate histopathological images.\n",
    "topics": "{'Image Classification': 0.99999154}",
    "score": 0.8163449004
  },
  {
    "id": "2007.05492",
    "title": "XSleepNet: Multi-View Sequential Model for Automatic Sleep Staging",
    "abstract": "  Automating sleep staging is vital to scale up sleep assessment and diagnosis\nto millions of people experiencing sleep deprivation and disorders and to\nenable longitudinal sleep monitoring in home environments. Learning from raw\npolysomnography signals and their derived time-frequency images has been\nprevalent. However, learning from multi-view inputs (e.g. both the raw signals\nand the time-frequency images) for sleep staging is difficult and not well\nunderstood. This work proposes a sequence-to-sequence sleep staging model,\nXSleepNet, that is capable of learning a joint representation from both raw\nsignals and time-frequency images effectively. Since different views often\ngeneralize (and overfit) at different rates, the proposed network is trained in\nsuch a way that the learning pace on each view is adapted based on their\ngeneralization/overfitting behavior. In simple terms, the learning on a\nparticular view is speeded up when it is generalizing well and slowed down when\nit is overfitting. View-specific generalization/overfitting measures are\ncomputed on-the-fly during the training course and used to derive weights to\nblend the gradients from different views. As a result, the network is able to\nretain representation power of different views in the joint features which\nrepresent the underlying distribution better than those learned by each\nindividual view alone. Furthermore, the XSleepNet architecture is principally\ndesigned to gain robustness to the amount of training data and to increase the\ncomplementarity between the input views. Experimental results on five databases\nof different size show that XSleepNet consistently results in better\nperformance than the single-view baselines as well as the multi-view baseline\nwith a simple fusion strategy. Finally, XSleepNet outperforms all prior sleep\nstaging methods and sets new state-of-the-art results on the experimental\ndatabases.\n",
    "topics": "{'MULTI-VIEW LEARNING': 0.9956772}",
    "score": 0.8163006895
  },
  {
    "id": "2004.12231",
    "title": "Deep DIH : Statistically Inferred Reconstruction of Digital In-Line\n  Holography by Deep Learning",
    "abstract": "  Digital in-line holography is commonly used to reconstruct 3D images from 2D\nholograms for microscopic objects. One of the technical challenges that arise\nin the signal processing stage is removing the twin image that is caused by the\nphase-conjugate wavefront from the recorded holograms. Twin image removal is\ntypically formulated as a non-linear inverse problem due to the irreversible\nscattering process when generating the hologram. Recently, end-to-end deep\nlearning-based methods have been utilized to reconstruct the object wavefront\n(as a surrogate for the 3D structure of the object) directly from a single-shot\nin-line digital hologram. However, massive data pairs are required to train\ndeep learning models for acceptable reconstruction precision. In contrast to\ntypical image processing problems, well-curated datasets for in-line digital\nholography does not exist. Also, the trained model highly influenced by the\nmorphological properties of the object and hence can vary for different\napplications. Therefore, data collection can be prohibitively cumbersome in\npractice as a major hindrance to using deep learning for digital holography. In\nthis paper, we proposed a novel implementation of autoencoder-based deep\nlearning architecture for single-shot hologram reconstruction solely based on\nthe current sample without the need for massive datasets to train the model.\nThe simulations results demonstrate the superior performance of the proposed\nmethod compared to the state of the art single-shot compressive digital in-line\nhologram reconstruction method.\n",
    "topics": "{}",
    "score": 0.8162580937
  },
  {
    "id": "1703.06676",
    "title": "I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation",
    "abstract": "  Translating information between text and image is a fundamental problem in\nartificial intelligence that connects natural language processing and computer\nvision. In the past few years, performance in image caption generation has seen\nsignificant improvement through the adoption of recurrent neural networks\n(RNN). Meanwhile, text-to-image generation begun to generate plausible images\nusing datasets of specific categories like birds and flowers. We've even seen\nimage generation from multi-category datasets such as the Microsoft Common\nObjects in Context (MSCOCO) through the use of generative adversarial networks\n(GANs). Synthesizing objects with a complex shape, however, is still\nchallenging. For example, animals and humans have many degrees of freedom,\nwhich means that they can take on many complex shapes. We propose a new\ntraining method called Image-Text-Image (I2T2I) which integrates text-to-image\nand image-to-text (image captioning) synthesis to improve the performance of\ntext-to-image synthesis. We demonstrate that %the capability of our method to\nunderstand the sentence descriptions, so as to I2T2I can generate better\nmulti-categories images using MSCOCO than the state-of-the-art. We also\ndemonstrate that I2T2I can achieve transfer learning by using a pre-trained\nimage captioning module to generate human images on the MPII Human Pose\n",
    "topics": "{'Image Generation': 0.99999917, 'Image Captioning': 0.9999869, 'Data Augmentation': 0.94999266, 'Transfer Learning': 0.44313997}",
    "score": 0.8160207654
  },
  {
    "id": "1407.5976",
    "title": "Detection of Sclerotic Spine Metastases via Random Aggregation of Deep\n  Convolutional Neural Network Classifications",
    "abstract": "  Automated detection of sclerotic metastases (bone lesions) in Computed\nTomography (CT) images has potential to be an important tool in clinical\npractice and research. State-of-the-art methods show performance of 79%\nsensitivity or true-positive (TP) rate, at 10 false-positives (FP) per volume.\nWe design a two-tiered coarse-to-fine cascade framework to first operate a\nhighly sensitive candidate generation system at a maximum sensitivity of ~92%\nbut with high FP level (~50 per patient). Regions of interest (ROI) for lesion\ncandidates are generated in this step and function as input for the second\ntier. In the second tier we generate N 2D views, via scale, random\ntranslations, and rotations with respect to each ROI centroid coordinates.\nThese random views are used to train a deep Convolutional Neural Network (CNN)\nclassifier. In testing, the CNN is employed to assign individual probabilities\nfor a new set of N random views that are averaged at each ROI to compute a\nfinal per-candidate classification probability. This second tier behaves as a\nhighly selective process to reject difficult false positives while preserving\nhigh sensitivities. We validate the approach on CT images of 59 patients (49\nwith sclerotic metastases and 10 normal controls). The proposed method reduces\nthe number of FP/vol. from 4 to 1.2, 7 to 3, and 12 to 9.5 when comparing a\nsensitivity rates of 60%, 70%, and 80% respectively in testing. The\nArea-Under-the-Curve (AUC) is 0.834. The results show marked improvement upon\nprevious work.\n",
    "topics": "{'Computed Tomography (CT)': 0.99824595}",
    "score": 0.8160176748
  },
  {
    "id": "1610.04861",
    "title": "Digital Makeup from Internet Images",
    "abstract": "  We present a novel approach of color transfer between images by exploring\ntheir high-level semantic information. First, we set up a database which\nconsists of the collection of downloaded images from the internet, which are\nsegmented automatically by using matting techniques. We then, extract image\nforegrounds from both source and multiple target images. Then by using image\nmatting algorithms, the system extracts the semantic information such as faces,\nlips, teeth, eyes, eyebrows, etc., from the extracted foregrounds of the source\nimage. And, then the color is transferred between corresponding parts with the\nsame semantic information. Next we get the color transferred result by\nseamlessly compositing different parts together using alpha blending. In the\nfinal step, we present an efficient method of color consistency to optimize the\ncolor of a collection of images showing the common scene. The main advantage of\nour method over existing techniques is that it does not need face matching, as\none could use more than one target images. It is not restricted to head shot\nimages as we can also change the color style in the wild. Moreover, our\nalgorithm does not require to choose the same color style, same pose and image\nsize between source and target images. Our algorithm is not restricted to\none-to-one image color transfer and can make use of more than one target images\nto transfer the color in different parts in the source image. Comparing with\nother approaches, our algorithm is much better in color blending in the input\ndata.\n",
    "topics": "{'Color Constancy': 0.86516094}",
    "score": 0.8159667197
  },
  {
    "id": "1709.04577",
    "title": "DeepVoting: A Robust and Explainable Deep Network for Semantic Part\n  Detection under Partial Occlusion",
    "abstract": "  In this paper, we study the task of detecting semantic parts of an object,\ne.g., a wheel of a car, under partial occlusion. We propose that all models\nshould be trained without seeing occlusions while being able to transfer the\nlearned knowledge to deal with occlusions. This setting alleviates the\ndifficulty in collecting an exponentially large dataset to cover occlusion\npatterns and is more essential. In this scenario, the proposal-based deep\nnetworks, like RCNN-series, often produce unsatisfactory results, because both\nthe proposal extraction and classification stages may be confused by the\nirrelevant occluders. To address this, [25] proposed a voting mechanism that\ncombines multiple local visual cues to detect semantic parts. The semantic\nparts can still be detected even though some visual cues are missing due to\nocclusions. However, this method is manually-designed, thus is hard to be\noptimized in an end-to-end manner.\n  In this paper, we present DeepVoting, which incorporates the robustness shown\nby [25] into a deep network, so that the whole pipeline can be jointly\noptimized. Specifically, it adds two layers after the intermediate features of\na deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the\nevidence of local visual cues, and the second layer performs a voting mechanism\nby utilizing the spatial relationship between visual cues and semantic parts.\nWe also propose an improved version DeepVoting+ by learning visual cues from\ncontext outside objects. In experiments, DeepVoting achieves significantly\nbetter performance than several baseline methods, including Faster-RCNN, for\nsemantic part detection under occlusion. In addition, DeepVoting enjoys\nexplainability as the detection results can be diagnosed via looking up the\nvoting cues.\n",
    "topics": "{}",
    "score": 0.8158681693
  },
  {
    "id": "1809.05825",
    "title": "Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN\n  Trained on Synthetic Data",
    "abstract": "  The ability to segment unknown objects in depth images has potential to\nenhance robot skills in grasping and object tracking. Recent computer vision\nresearch has demonstrated that Mask R-CNN can be trained to segment specific\ncategories of objects in RGB images when massive hand-labeled datasets are\navailable. As generating these datasets is time consuming, we instead train\nwith synthetic depth images. Many robots now use depth sensors, and recent\nresults suggest training on synthetic depth data can transfer successfully to\nthe real world. We present a method for automated dataset generation and\nrapidly generate a synthetic training dataset of 50,000 depth images and\n320,000 object masks using simulated heaps of 3D CAD models. We train a variant\nof Mask R-CNN with domain randomization on the generated dataset to perform\ncategory-agnostic instance segmentation without any hand-labeled data and we\nevaluate the trained network, which we refer to as Synthetic Depth (SD) Mask\nR-CNN, on a set of real, high-resolution depth images of challenging,\ndensely-cluttered bins containing objects with highly-varied geometry. SD Mask\nR-CNN outperforms point cloud clustering baselines by an absolute 15% in\nAverage Precision and 20% in Average Recall on COCO benchmarks, and achieves\nperformance levels similar to a Mask R-CNN trained on a massive, hand-labeled\nRGB dataset and fine-tuned on real images from the experimental setup. We\ndeploy the model in an instance-specific grasping pipeline to demonstrate its\nusefulness in a robotics application. Code, the synthetic training dataset, and\nsupplementary material are available at https://bit.ly/2letCuE.\n",
    "topics": "{'Instance Segmentation': 0.9996195, 'Semantic Segmentation': 0.55530614}",
    "score": 0.8157270198
  },
  {
    "id": "2006.00654",
    "title": "A multimodal approach for multi-label movie genre classification",
    "abstract": "  Movie genre classification is a challenging task that has increasingly\nattracted the attention of researchers. In this paper, we addressed the\nmulti-label classification of the movie genres in a multimodal way. For this\npurpose, we created a dataset composed of trailer video clips, subtitles,\nsynopses, and movie posters taken from 152,622 movie titles from The Movie\nDatabase. The dataset was carefully curated and organized, and it was also made\navailable as a contribution of this work. Each movie of the dataset was labeled\naccording to a set of eighteen genre labels. We extracted features from these\ndata using different kinds of descriptors, namely Mel Frequency Cepstral\nCoefficients, Statistical Spectrum Descriptor , Local Binary Pattern with\nspectrograms, Long-Short Term Memory, and Convolutional Neural Networks. The\ndescriptors were evaluated using different classifiers, such as BinaryRelevance\nand ML-kNN. We have also investigated the performance of the combination of\ndifferent classifiers/features using a late fusion strategy, which obtained\nencouraging results. Based on the F-Score metric, our best result, 0.628, was\nobtained by the fusion of a classifier created using LSTM on the synopses, and\na classifier created using CNN on movie trailer frames. When considering the\nAUC-PR metric, the best result, 0.673, was also achieved by combining those\nrepresentations, but in addition, a classifier based on LSTM created from the\nsubtitles was used. These results corroborate the existence of complementarity\namong classifiers based on different sources of information in this field of\napplication. As far as we know, this is the most comprehensive study developed\nin terms of the diversity of multimedia sources of information to perform movie\ngenre classification.\n",
    "topics": "{'Multi-Label Classification': 0.9939698}",
    "score": 0.8156292305
  },
  {
    "id": "1908.03057",
    "title": "Sim-to-Real Learning for Casualty Detection from Ground Projected Point\n  Cloud Data",
    "abstract": "  This paper addresses the problem of human body detection---particularly a\nhuman body lying on the ground (a.k.a. casualty)---using point cloud data. This\nability to detect a casualty is one of the most important features of mobile\nrescue robots, in order for them to be able to operate autonomously. We propose\na deep-learning-based casualty detection method using a deep convolutional\nneural network (CNN). This network is trained to be able to detect a casualty\nusing a point-cloud data input. In the method we propose, the point cloud input\nis pre-processed to generate a depth image-like ground-projected heightmap.\nThis heightmap is generated based on the projected distance of each point onto\nthe detected ground plane within the point cloud data. The generated heightmap\n-- in image form -- is then used as an input for the CNN to detect a human body\nlying on the ground. To train the neural network, we propose a novel\nsim-to-real approach, in which the network model is trained using synthetic\ndata obtained in simulation and then tested on real sensor data. To make the\nmodel transferable to real data implementations, during the training we adopt\nspecific data augmentation strategies with the synthetic training data. The\nexperimental results show that data augmentation introduced during the training\nprocess is essential for improving the performance of the trained model on real\ndata. More specifically, the results demonstrate that the data augmentations on\nraw point-cloud data have contributed to a considerable improvement of the\ntrained model performance.\n",
    "topics": "{'Data Augmentation': 0.99999046}",
    "score": 0.8155841341
  },
  {
    "id": "2001.02329",
    "title": "Emo-CNN for Perceiving Stress from Audio Signals: A Brain Chemistry\n  Approach",
    "abstract": "  Emotion plays a key role in many applications like healthcare, to gather\npatients emotional behavior. There are certain emotions which are given more\nimportance due to their effectiveness in understanding human feelings. In this\npaper, we propose an approach that models human stress from audio signals. The\nresearch challenge in speech emotion detection is defining the very meaning of\nstress and being able to categorize it in a precise manner. Supervised Machine\nLearning models, including state of the art Deep Learning classification\nmethods, rely on the availability of clean and labelled data. One of the\nproblems in affective computation and emotion detection is the limited amount\nof annotated data of stress. The existing labelled stress emotion datasets are\nhighly subjective to the perception of the annotator.\n  We address the first issue of feature selection by exploiting the use of\ntraditional MFCC features in Convolutional Neural Network. Our experiments show\nthat Emo-CNN consistently and significantly outperforms the popular existing\nmethods over multiple datasets. It achieves 90.2% categorical accuracy on the\nEmo-DB dataset. To tackle the second and the more significant problem of\nsubjectivity in stress labels, we use Lovheim's cube, which is a 3-dimensional\nprojection of emotions. The cube aims at explaining the relationship between\nthese neurotransmitters and the positions of emotions in 3D space. The learnt\nemotion representations from the Emo-CNN are mapped to the cube using three\ncomponent PCA (Principal Component Analysis) which is then used to model human\nstress. This proposed approach not only circumvents the need for labelled\nstress data but also complies with the psychological theory of emotions given\nby Lovheim's cube. We believe that this work is the first step towards creating\na connection between Artificial Intelligence and the chemistry of human\nemotions.\n",
    "topics": "{'Feature Selection': 0.7518629}",
    "score": 0.8155567639
  },
  {
    "id": "1910.04855",
    "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task\n  Learning and ArcFace",
    "abstract": "  Affective computing has been largely limited in terms of available data\nresources. The need to collect and annotate diverse in-the-wild datasets has\nbecome apparent with the rise of deep learning models, as the default approach\nto address any computer vision task. Some in-the-wild databases have been\nrecently proposed. However: i) their size is small, ii) they are not\naudiovisual, iii) only a small part is manually annotated, iv) they contain a\nsmall number of subjects, or v) they are not annotated for all main behavior\ntasks (valence-arousal estimation, action unit detection and basic expression\nclassification). To address these, we substantially extend the largest\navailable in-the-wild database (Aff-Wild) to study continuous emotions such as\nvalence and arousal. Furthermore, we annotate parts of the database with basic\nexpressions and action units. As a consequence, for the first time, this allows\nthe joint study of all three types of behavior states. We call this database\nAff-Wild2. We conduct extensive experiments with CNN and CNN-RNN architectures\nthat use visual and audio modalities; these networks are trained on Aff-Wild2\nand their performance is then evaluated on 10 publicly available emotion\ndatabases. We show that the networks achieve state-of-the-art performance for\nthe emotion recognition tasks. Additionally, we adapt the ArcFace loss function\nin the emotion recognition context and use it for training two new networks on\nAff-Wild2 and then re-train them in a variety of diverse expression recognition\ndatabases. The networks are shown to improve the existing state-of-the-art. The\ndatabase, emotion recognition models and source code are available at\nhttp://ibug.doc.ic.ac.uk/resources/aff-wild2.\n",
    "topics": "{'Emotion Recognition': 0.9999862, 'Multi-Task Learning': 0.9025097}",
    "score": 0.8153490353
  },
  {
    "id": "2003.12040",
    "title": "Pseudo-Labeling for Small Lesion Detection on Diabetic Retinopathy\n  Images",
    "abstract": "  Diabetic retinopathy (DR) is a primary cause of blindness in working-age\npeople worldwide. About 3 to 4 million people with diabetes become blind\nbecause of DR every year. Diagnosis of DR through color fundus images is a\ncommon approach to mitigate such problem. However, DR diagnosis is a difficult\nand time consuming task, which requires experienced clinicians to identify the\npresence and significance of many small features on high resolution images.\nConvolutional Neural Network (CNN) has proved to be a promising approach for\nautomatic biomedical image analysis recently. In this work, we investigate\nlesion detection on DR fundus images with CNN-based object detection methods.\nLesion detection on fundus images faces two unique challenges. The first one is\nthat our dataset is not fully labeled, i.e., only a subset of all lesion\ninstances are marked. Not only will these unlabeled lesion instances not\ncontribute to the training of the model, but also they will be mistakenly\ncounted as false negatives, leading the model move to the opposite direction.\nThe second challenge is that the lesion instances are usually very small,\nmaking them difficult to be found by normal object detectors. To address the\nfirst challenge, we introduce an iterative training algorithm for the\nsemi-supervised method of pseudo-labeling, in which a considerable number of\nunlabeled lesion instances can be discovered to boost the performance of the\nlesion detector. For the small size targets problem, we extend both the input\nsize and the depth of feature pyramid network (FPN) to produce a large CNN\nfeature map, which can preserve the detail of small lesions and thus enhance\nthe effectiveness of the lesion detector. The experimental results show that\nour proposed methods significantly outperform the baselines.\n",
    "topics": "{'Object Detection': 0.9428376}",
    "score": 0.8151876857
  },
  {
    "id": "2003.06112",
    "title": "Graph Convolutional Topic Model for Data Streams",
    "abstract": "  Learning hidden topics in data streams has been paid a great deal of\nattention by researchers with a lot of proposed methods, but exploiting prior\nknowledge in general and a knowledge graph in particular has not been taken\ninto adequate consideration in these methods. Prior knowledge that is derived\nfrom human knowledge (e.g. Wordnet) or a pre-trained model (e.g.Word2vec) is\nvery valuable and useful to help topic models work better, especially on short\ntexts. However, previous work often ignores this resource, or it can only\nutilize prior knowledge of a vector form in a simple way. In this paper, we\npropose a novel graph convolutional topic model (GCTM) which integrates graph\nconvolutional networks (GCN) into a topic model and a learning method which\nlearns the networks and the topic model simultaneously for data streams. In\neach minibatch, our method not only can exploit an external knowledge graph but\nalso can balance between the external and old knowledge to perform well on new\ndata. We conduct extensive experiments to evaluate our method with both human\ngraph knowledge(Wordnet) and a graph built from pre-trained word embeddings\n(Word2vec). The experimental results show that our method achieves\nsignificantly better performances than the state-of-the-art baselines in terms\nof probabilistic predictive measure and topic coherence. In particular, our\nmethod can work well when dealing with short texts as well as concept drift.\nThe implementation of GCTM is available at\nhttps://github.com/bachtranxuan/GCTM.git.\n",
    "topics": "{'Topic Models': 0.99931526, 'Word Embeddings': 0.8289966}",
    "score": 0.8151605272
  },
  {
    "id": "2005.11724",
    "title": "Learning to Transfer Graph Embeddings for Inductive Graph based\n  Recommendation",
    "abstract": "  With the increasing availability of videos, how to edit them and present the\nmost interesting parts to users, i.e., video highlight, has become an urgent\nneed with many broad applications. As users'visual preferences are subjective\nand vary from person to person, previous generalized video highlight extraction\nmodels fail to tailor to users' unique preferences. In this paper, we study the\nproblem of personalized video highlight recommendation with rich visual\ncontent. By dividing each video into non-overlapping segments, we formulate the\nproblem as a personalized segment recommendation task with many new segments in\nthe test stage. The key challenges of this problem lie in: the cold-start users\nwith limited video highlight records in the training data and new segments\nwithout any user ratings at the test stage. In this paper, we propose an\ninductive Graph based Transfer learning framework for personalized video\nhighlight Recommendation (TransGRec). TransGRec is composed of two parts: a\ngraph neural network followed by an item embedding transfer network.\nSpecifically, the graph neural network part exploits the higher-order proximity\nbetween users and segments to alleviate the user cold-start problem. The\ntransfer network is designed to approximate the learned item embeddings from\ngraph neural networks by taking each item's visual content as input, in order\nto tackle the new segment problem in the test phase. We design two detailed\nimplementations of the transfer learning optimization function, and we show how\nthe two parts of TransGRec can be efficiently optimized with different transfer\nlearning optimization functions. Extensive experimental results on a real-world\ndataset clearly show the effectiveness of our proposed model.\n",
    "topics": "{'Transfer Learning': 0.9999651}",
    "score": 0.8151175536
  },
  {
    "id": "2007.13826",
    "title": "Large Scale Subject Category Classification of Scholarly Papers with\n  Deep Attentive Neural Networks",
    "abstract": "  Subject categories of scholarly papers generally refer to the knowledge\ndomain(s) to which the papers belong, examples being computer science or\nphysics. Subject category information can be used for building faceted search\nfor digital library search engines. This can significantly assist users in\nnarrowing down their search space of relevant documents. Unfortunately, many\nacademic papers do not have such information as part of their metadata.\nExisting methods for solving this task usually focus on unsupervised learning\nthat often relies on citation networks. However, a complete list of papers\nciting the current paper may not be readily available. In particular, new\npapers that have few or no citations cannot be classified using such methods.\nHere, we propose a deep attentive neural network (DANN) that classifies\nscholarly papers using only their abstracts. The network is trained using 9\nmillion abstracts from Web of Science (WoS). We also use the WoS schema that\ncovers 104 subject categories. The proposed network consists of two\nbi-directional recurrent neural networks followed by an attention layer. We\ncompare our model against baselines by varying the architecture and text\nrepresentation. Our best model achieves micro-F1 measure of 0.76 with F1 of\nindividual subject categories ranging from 0.50-0.95. The results showed the\nimportance of retraining word embedding models to maximize the vocabulary\noverlap and the effectiveness of the attention mechanism. The combination of\nword vectors with TFIDF outperforms character and sentence level embedding\nmodels. We discuss imbalanced samples and overlapping categories and suggest\npossible strategies for mitigation. We also determine the subject category\ndistribution in CiteSeerX by classifying a random sample of one million\nacademic papers.\n",
    "topics": "{}",
    "score": 0.8151129936
  },
  {
    "id": "2008.05865",
    "title": "DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image\n  Synthesis",
    "abstract": "  Synthesizing high-resolution realistic images from text descriptions is a\nchallenging task. Almost all existing text-to-image methods employ stacked\ngenerative adversarial networks as the backbone, utilize cross-modal attention\nmechanisms to fuse text and image features, and use extra networks to ensure\ntext-image semantic consistency. The existing text-to-image models have three\nproblems: 1) For the backbone, there are multiple generators and discriminators\nstacked for generating different scales of images making the training process\nslow and inefficient. 2) For semantic consistency, the existing models employ\nextra networks to ensure the semantic consistency increasing the training\ncomplexity and bringing an additional computational cost. 3) For the text-image\nfeature fusion method, cross-modal attention is only applied a few times during\nthe generation process due to its computational cost impeding fusing the text\nand image features deeply. To solve these limitations, we propose 1) a novel\nsimplified text-to-image backbone which is able to synthesize high-quality\nimages directly by one pair of generator and discriminator, 2) a novel\nregularization method called Matching-Aware zero-centered Gradient Penalty\nwhich promotes the generator to synthesize more realistic and text-image\nsemantic consistent images without introducing extra networks, 3) a novel\nfusion module called Deep Text-Image Fusion Block which can exploit the\nsemantics of text descriptions effectively and fuse text and image features\ndeeply during the generation process. Compared with the previous text-to-image\nmodels, our DF-GAN is simpler and more efficient and achieves better\nperformance. Extensive experiments and ablation studies on both Caltech-UCSD\nBirds 200 and COCO datasets demonstrate the superiority of the proposed model\nin comparison to state-of-the-art models.\n",
    "topics": "{'Image Generation': 0.961815}",
    "score": 0.8151118934
  },
  {
    "id": "1912.01811",
    "title": "Drone-based Joint Density Map Estimation, Localization and Tracking with\n  Space-Time Multi-Scale Attention Network",
    "abstract": "  This paper proposes a space-time multi-scale attention network (STANet) to\nsolve density map estimation, localization and tracking in dense crowds of\nvideo clips captured by drones with arbitrary crowd density, perspective, and\nflight altitude. Our STANet method aggregates multi-scale feature maps in\nsequential frames to exploit the temporal coherency, and then predict the\ndensity maps, localize the targets, and associate them in crowds\nsimultaneously. A coarse-to-fine process is designed to gradually apply the\nattention module on the aggregated multi-scale feature maps to enforce the\nnetwork to exploit the discriminative space-time features for better\nperformance. The whole network is trained in an end-to-end manner with the\nmulti-task loss, formed by three terms, i.e., the density map loss,\nlocalization loss and association loss. The non-maximal suppression followed by\nthe min-cost flow framework is used to generate the trajectories of targets' in\nscenarios. Since existing crowd counting datasets merely focus on crowd\ncounting in static cameras rather than density map estimation, counting and\ntracking in crowds on drones, we have collected a new large-scale drone-based\ndataset, DroneCrowd, formed by 112 video clips with 33,600 high resolution\nframes (i.e., 1920x1080) captured in 70 different scenarios. With intensive\namount of effort, our dataset provides 20,800 people trajectories with 4.8\nmillion head annotations and several video-level attributes in sequences.\nExtensive experiments are conducted on two challenging public datasets, i.e.,\nShanghaitech and UCF-QNRF, and our DroneCrowd, to demonstrate that STANet\nachieves favorable performance against the state-of-the-arts. The datasets and\ncodes can be found at https://github.com/VisDrone.\n",
    "topics": "{'Crowd Counting': 1.0}",
    "score": 0.8150296363
  },
  {
    "id": "1908.05317",
    "title": "Recognition of Ischaemia and Infection in Diabetic Foot Ulcers: Dataset\n  and Techniques",
    "abstract": "  Recognition and analysis of Diabetic Foot Ulcers (DFU) using computerized\nmethods is an emerging research area with the evolution of image-based machine\nlearning algorithms. Existing research using visual computerized methods mainly\nfocuses on recognition, detection, and segmentation of the visual appearance of\nthe DFU as well as tissue classification. According to DFU medical\nclassification systems, the presence of infection (bacteria in the wound) and\nischaemia (inadequate blood supply) has important clinical implications for DFU\nassessment, which are used to predict the risk of amputation. In this work, we\npropose a new dataset and computer vision techniques to identify the presence\nof infection and ischaemia in DFU. This is the first time a DFU dataset with\nground truth labels of ischaemia and infection cases is introduced for research\npurposes. For the handcrafted machine learning approach, we propose a new\nfeature descriptor, namely the Superpixel Color Descriptor. Then we use the\nEnsemble Convolutional Neural Network (CNN) model for more effective\nrecognition of ischaemia and infection. We propose to use a natural\ndata-augmentation method, which identifies the region of interest on foot\nimages and focuses on finding the salient features existing in this area.\nFinally, we evaluate the performance of our proposed techniques on binary\nclassification, i.e. ischaemia versus non-ischaemia and infection versus\nnon-infection. Overall, our method performed better in the classification of\nischaemia than infection. We found that our proposed Ensemble CNN deep learning\nalgorithms performed better for both classification tasks as compared to\nhandcrafted machine learning algorithms, with 90% accuracy in ischaemia\nclassification and 73% in infection classification.\n",
    "topics": "{}",
    "score": 0.8149725308
  },
  {
    "id": "1910.04792",
    "title": "Unsupervised video summarization framework using keyframe extraction and\n  video skimming",
    "abstract": "  Video is one of the robust sources of information and the consumption of\nonline and offline videos has reached an unprecedented level in the last few\nyears. A fundamental challenge of extracting information from videos is a\nviewer has to go through the complete video to understand the context, as\nopposed to an image where the viewer can extract information from a single\nframe. Apart from context understanding, it almost impossible to create a\nuniversal summarized video for everyone, as everyone has their own bias of\nkeyframe, e.g; In a soccer game, a coach person might consider those frames\nwhich consist of information on player placement, techniques, etc; however, a\nperson with less knowledge about a soccer game, will focus more on frames which\nconsist of goals and score-board. Therefore, if we were to tackle problem video\nsummarization through a supervised learning path, it will require extensive\npersonalized labeling of data. In this paper, we attempt to solve video\nsummarization through unsupervised learning by employing traditional\nvision-based algorithmic methodologies for accurate feature extraction from\nvideo frames. We have also proposed a deep learning-based feature extraction\nfollowed by multiple clustering methods to find an effective way of summarizing\na video by interesting key-frame extraction. We have compared the performance\nof these approaches on the SumMe dataset and showcased that using deep\nlearning-based feature extraction has been proven to perform better in case of\ndynamic viewpoint videos.\n",
    "topics": "{'Video Summarization': 1.0}",
    "score": 0.8149720014
  },
  {
    "id": "1901.09613",
    "title": "Hybrid Machine Learning Approach to Popularity Prediction of Newly\n  Released Contents for Online Video Streaming Service",
    "abstract": "  In the industry of video content providers such as VOD and IPTV, predicting\nthe popularity of video contents in advance is critical not only from a\nmarketing perspective but also from a network optimization perspective. By\npredicting whether the content will be successful or not in advance, the\ncontent file, which is large, is efficiently deployed in the proper service\nproviding server, leading to network cost optimization. Many previous studies\nhave done view count prediction research to do this. However, the studies have\nbeen making predictions based on historical view count data from users. In this\ncase, the contents had been published to the users and already deployed on a\nservice server. These approaches make possible to efficiently deploy a content\nalready published but are impossible to use for a content that is not be\npublished. To address the problems, this research proposes a hybrid machine\nlearning approach to the classification model for the popularity prediction of\nnewly video contents which is not published. In this paper, we create a new\nvariable based on the related content of the specific content and divide entire\ndataset by the characteristics of the contents. Next, the prediction is\nperformed using XGBoosting and deep neural net based model according to the\ndata characteristics of the cluster. Our model uses metadata for contents for\nprediction, so we use categorical embedding techniques to solve the sparsity of\ncategorical variables and make them learn efficiently for the deep neural net\nmodel. As well, we use the FTRL-proximal algorithm to solve the problem of the\nview-count volatility of video content. We achieve overall better performance\nthan the previous standalone method with a dataset from one of the top\nstreaming service company.\n",
    "topics": "{}",
    "score": 0.8149574436
  },
  {
    "id": "2006.16670",
    "title": "EndoSLAM Dataset and An Unsupervised Monocular Visual Odometry and Depth\n  Estimation Approach for Endoscopic Videos: Endo-SfMLearner",
    "abstract": "  Deep learning techniques hold promise to develop dense topography\nreconstruction and pose estimation methods for endoscopic videos. However,\ncurrently available datasets do not support effective quantitative\nbenchmarking. In this paper, we introduce a comprehensive endoscopic SLAM\ndataset consisting of 3D point cloud data for six porcine organs, capsule and\nstandard endoscopy recordings as well as synthetically generated data. A Panda\nrobotic arm, two commercially available capsule endoscopes, two conventional\nendoscopes with different camera properties, and two high precision 3D scanners\nwere employed to collect data from 8 ex-vivo porcine gastrointestinal\n(GI)-tract organs. In total, 35 sub-datasets are provided with 6D pose ground\ntruth for the ex-vivo part: 18 sub-dataset for colon, 12 sub-datasets for\nstomach and 5 sub-datasets for small intestine, while four of these contain\npolyp-mimicking elevations carried out by an expert gastroenterologist.\nSynthetic capsule endoscopy frames from GI-tract with both depth and pose\nannotations are included to facilitate the study of simulation-to-real transfer\nlearning algorithms. Additionally, we propound Endo-SfMLearner, an unsupervised\nmonocular depth and pose estimation method that combines residual networks with\nspatial attention module in order to dictate the network to focus on\ndistinguishable and highly textured tissue regions. The proposed approach makes\nuse of a brightness-aware photometric loss to improve the robustness under fast\nframe-to-frame illumination changes. To exemplify the use-case of the EndoSLAM\ndataset, the performance of Endo-SfMLearner is extensively compared with the\nstate-of-the-art. The codes and the link for the dataset are publicly available\nat https://github.com/CapsuleEndoscope/EndoSLAM. A video demonstrating the\nexperimental setup and procedure is accessible through\nhttps://www.youtube.com/watch?v=G_LCe0aWWdQ.\n",
    "topics": "{'Visual Odometry': 0.9999995, 'Pose Estimation': 0.99958485, 'Simultaneous Localization and Mapping': 0.9987375, 'Depth Estimation': 0.98771566, 'Transfer Learning': 0.90661365}",
    "score": 0.8148015637
  },
  {
    "id": "2004.06912",
    "title": "Combining Visible Light and Infrared Imaging for Efficient Detection of\n  Respiratory Infections such as COVID-19 on Portable Device",
    "abstract": "  Coronavirus Disease 2019 (COVID-19) has become a serious global epidemic in\nthe past few months and caused huge loss to human society worldwide. For such a\nlarge-scale epidemic, early detection and isolation of potential virus carriers\nis essential to curb the spread of the epidemic. Recent studies have shown that\none important feature of COVID-19 is the abnormal respiratory status caused by\nviral infections. During the epidemic, many people tend to wear masks to reduce\nthe risk of getting sick. Therefore, in this paper, we propose a portable\nnon-contact method to screen the health condition of people wearing masks\nthrough analysis of the respiratory characteristics. The device mainly consists\nof a FLIR one thermal camera and an Android phone. This may help identify those\npotential patients of COVID-19 under practical scenarios such as pre-inspection\nin schools and hospitals. In this work, we perform the health screening through\nthe combination of the RGB and thermal videos obtained from the dual-mode\ncamera and deep learning architecture.We first accomplish a respiratory data\ncapture technique for people wearing masks by using face recognition. Then, a\nbidirectional GRU neural network with attention mechanism is applied to the\nrespiratory data to obtain the health screening result. The results of\nvalidation experiments show that our model can identify the health status on\nrespiratory with the accuracy of 83.7\\% on the real-world dataset. The abnormal\nrespiratory data and part of normal respiratory data are collected from Ruijin\nHospital Affiliated to The Shanghai Jiao Tong University Medical School. Other\nnormal respiratory data are obtained from healthy people around our\nresearchers. This work demonstrates that the proposed portable and intelligent\nhealth screening device can be used as a pre-scan method for respiratory\ninfections, which may help fight the current COVID-19 epidemic.\n",
    "topics": "{'Face Recognition': 0.90846777}",
    "score": 0.8147687834
  },
  {
    "id": "2006.00033",
    "title": "Learning stochastic object models from medical imaging measurements\n  using Progressively-Growing AmbientGANs",
    "abstract": "  It has been advocated that medical imaging systems and reconstruction\nalgorithms should be assessed and optimized by use of objective measures of\nimage quality that quantify the performance of an observer at specific\ndiagnostic tasks. One important source of variability that can significantly\nlimit observer performance is variation in the objects to-be-imaged. This\nsource of variability can be described by stochastic object models (SOMs). A\nSOM is a generative model that can be employed to establish an ensemble of\nto-be-imaged objects with prescribed statistical properties. In order to\naccurately model variations in anatomical structures and object textures, it is\ndesirable to establish SOMs from experimental imaging measurements acquired by\nuse of a well-characterized imaging system. Deep generative neural networks,\nsuch as generative adversarial networks (GANs) hold great potential for this\ntask. However, conventional GANs are typically trained by use of reconstructed\nimages that are influenced by the effects of measurement noise and the\nreconstruction process. To circumvent this, an AmbientGAN has been proposed\nthat augments a GAN with a measurement operator. However, the original\nAmbientGAN could not immediately benefit from modern training procedures, such\nas progressive growing, which limited its ability to be applied to\nrealistically sized medical image data. To circumvent this, in this work, a new\nProgressive Growing AmbientGAN (ProAmGAN) strategy is developed for\nestablishing SOMs from medical imaging measurements. Stylized numerical studies\ncorresponding to common medical imaging modalities are conducted to demonstrate\nand validate the proposed method for establishing SOMs.\n",
    "topics": "{}",
    "score": 0.8147327281
  },
  {
    "id": "1803.07764",
    "title": "Estimating defectiveness of source code: A predictive model using GitHub\n  content",
    "abstract": "  Two key contributions presented in this paper are: i) A method for building a\ndataset containing source code features extracted from source files taken from\nOpen Source Software (OSS) and associated bug reports, ii) A predictive model\nfor estimating defectiveness of a given source code. These artifacts can be\nuseful for building tools and techniques pertaining to several automated\nsoftware engineering areas such as bug localization, code review, and\nrecommendation and program repair.\n  In order to achieve our goal, we first extract coding style information (e.g.\nrelated to programming language constructs used in the source code) for source\ncode files present on GitHub. Then the information available in bug reports (if\nany) associated with these source code files are extracted. Thus fetched un(/\nsemi)-structured information is then transformed into a structured knowledge\nbase. We considered more than 30400 source code files from 20 different GitHub\nrepositories with about 14950 associated bug reports across 4 bug tracking\nportals. The source code files considered are written in four programming\nlanguages (viz., C, C++, Java, and Python) and belong to different types of\napplications.\n  A machine learning (ML) model for estimating the defectiveness of a given\ninput source code is then trained using the knowledge base. In order to pick\nthe best ML model, we evaluated 8 different ML algorithms such as Random\nForest, K Nearest Neighbour and SVM with around 50 parameter configurations to\ncompare their performance on our tasks. One of our findings shows that best\nK-fold (with k=5) cross-validation results are obtained with the NuSVM\ntechnique that gives a mean F1 score of 0.914.\n",
    "topics": "{}",
    "score": 0.8146377833
  },
  {
    "id": "2001.01202",
    "title": "Deep Face Representations for Differential Morphing Attack Detection",
    "abstract": "  The vulnerability of facial recognition systems to face morphing attacks is\nwell known. Many different approaches for morphing attack detection have been\nproposed in the scientific literature. However, the morphing attack detection\nalgorithms proposed so far have only been trained and tested on datasets whose\ndistributions of image characteristics are either very limited (e.g. only\ncreated with a single morphing tool) or rather unrealistic (e.g. no print-scan\ntransformation). As a consequence, these methods easily overfit on certain\nimage types and the results presented cannot be expected to apply to real-world\nscenarios. For example, the results of the latest NIST Face Recognition Vendor\nTest MORPH show that the submitted MAD algorithms lack robustness and\nperformance when considering unseen and challenging datasets. In this work,\nsubsets of the FERET and FRGCv2 face databases are used to create a large\nrealistic database for training and testing of morphing attack detection\nalgorithms, containing a large number of ICAO-compliant bona fide facial\nimages, corresponding unconstrained probe images, and morphed images created\nwith four different tools. Furthermore, multiple post-processings are applied\non the reference images, e.g. print-scan and JPEG2000 compression. On this\ndatabase, previously proposed differential morphing algorithms are evaluated\nand compared. In addition, the application of deep face representations for\ndifferential morphing attack detection algorithms is investigated. It is shown\nthat algorithms based on deep face representations can achieve very high\ndetection performance (less than 3\\%~\\mbox{D-EER}) and robustness with respect\nto various post-processings. Finally, the limitations of the developed methods\nare analyzed.\n",
    "topics": "{'Face Recognition': 0.9834689}",
    "score": 0.81460351
  },
  {
    "id": "1807.10581",
    "title": "Multi-Scale Gradual Integration CNN for False Positive Reduction in\n  Pulmonary Nodule Detection",
    "abstract": "  Lung cancer is a global and dangerous disease, and its early detection is\ncrucial to reducing the risks of mortality. In this regard, it has been of\ngreat interest in developing a computer-aided system for pulmonary nodules\ndetection as early as possible on thoracic CT scans. In general, a nodule\ndetection system involves two steps: (i) candidate nodule detection at a high\nsensitivity, which captures many false positives and (ii) false positive\nreduction from candidates. However, due to the high variation of nodule\nmorphological characteristics and the possibility of mistaking them for\nneighboring organs, candidate nodule detection remains a challenge. In this\nstudy, we propose a novel Multi-scale Gradual Integration Convolutional Neural\nNetwork (MGI-CNN), designed with three main strategies: (1) to use multi-scale\ninputs with different levels of contextual information, (2) to use abstract\ninformation inherent in different input scales with gradual integration, and\n(3) to learn multi-stream feature integration in an end-to-end manner. To\nverify the efficacy of the proposed network, we conducted exhaustive\nexperiments on the LUNA16 challenge datasets by comparing the performance of\nthe proposed method with state-of-the-art methods in the literature. On two\ncandidate subsets of the LUNA16 dataset, i.e., V1 and V2, our method achieved\nan average CPM of 0.908 (V1) and 0.942 (V2), outperforming comparable methods\nby a large margin. Our MGI-CNN is implemented in Python using TensorFlow and\nthe source code is available from 'https://github.com/ku-milab/MGICNN.'\n",
    "topics": "{}",
    "score": 0.8145959479
  },
  {
    "id": "1908.11714",
    "title": "Multi-Modal Fusion for End-to-End RGB-T Tracking",
    "abstract": "  We propose an end-to-end tracking framework for fusing the RGB and TIR\nmodalities in RGB-T tracking. Our baseline tracker is DiMP (Discriminative\nModel Prediction), which employs a carefully designed target prediction network\ntrained end-to-end using a discriminative loss. We analyze the effectiveness of\nmodality fusion in each of the main components in DiMP, i.e. feature extractor,\ntarget estimation network, and classifier. We consider several fusion\nmechanisms acting at different levels of the framework, including pixel-level,\nfeature-level and response-level. Our tracker is trained in an end-to-end\nmanner, enabling the components to learn how to fuse the information from both\nmodalities. As data to train our model, we generate a large-scale RGB-T dataset\nby considering an annotated RGB tracking dataset (GOT-10k) and synthesizing\npaired TIR images using an image-to-image translation approach. We perform\nextensive experiments on VOT-RGBT2019 dataset and RGBT210 dataset, evaluating\neach type of modality fusing on each model component. The results show that the\nproposed fusion mechanisms improve the performance of the single modality\ncounterparts. We obtain our best results when fusing at the feature-level on\nboth the IoU-Net and the model predictor, obtaining an EAO score of 0.391 on\nVOT-RGBT2019 dataset. With this fusion mechanism we achieve the\nstate-of-the-art performance on RGBT210 dataset.\n",
    "topics": "{'Image-to-Image Translation': 0.9879699}",
    "score": 0.8144113637
  },
  {
    "id": "2005.13171",
    "title": "Precisely Predicting Acute Kidney Injury with Convolutional Neural\n  Network Based on Electronic Health Record Data",
    "abstract": "  The incidence of Acute Kidney Injury (AKI) commonly happens in the Intensive\nCare Unit (ICU) patients, especially in the adults, which is an independent\nrisk factor affecting short-term and long-term mortality. Though researchers in\nrecent years highlight the early prediction of AKI, the performance of existing\nmodels are not precise enough. The objective of this research is to precisely\npredict AKI by means of Convolutional Neural Network on Electronic Health\nRecord (EHR) data. The data sets used in this research are two public\nElectronic Health Record (EHR) databases: MIMIC-III and eICU database. In this\nstudy, we take several Convolutional Neural Network models to train and test\nour AKI predictor, which can precisely predict whether a certain patient will\nsuffer from AKI after admission in ICU according to the last measurements of\nthe 16 blood gas and demographic features. The research is based on Kidney\nDisease Improving Global Outcomes (KDIGO) criteria for AKI definition. Our work\ngreatly improves the AKI prediction precision, and the best AUROC is up to\n0.988 on MIMIC-III data set and 0.936 on eICU data set, both of which\noutperform the state-of-art predictors. And the dimension of the input vector\nused in this predictor is much fewer than that used in other existing\nresearches. Compared with the existing AKI predictors, the predictor in this\nwork greatly improves the precision of early prediction of AKI by using the\nConvolutional Neural Network architecture and a more concise input vector.\nEarly and precise prediction of AKI will bring much benefit to the decision of\ntreatment, so it is believed that our work is a very helpful clinical\napplication.\n",
    "topics": "{}",
    "score": 0.8144037665
  },
  {
    "id": "1811.09897",
    "title": "Conditional Recurrent Flow: Conditional Generation of Longitudinal\n  Samples with Applications to Neuroimaging",
    "abstract": "  Generative models using neural network have opened a door to large-scale\nstudies for various application domains, especially for studies that suffer\nfrom lack of real samples to obtain statistically robust inference. Typically,\nthese generative models would train on existing data to learn the underlying\ndistribution of the measurements (e.g., images) in latent spaces conditioned on\ncovariates (e.g., image labels), and generate independent samples that are\nidentically distributed in the latent space. Such models may work for\ncross-sectional studies, however, they are not suitable to generate data for\nlongitudinal studies that focus on \"progressive\" behavior in a sequence of\ndata. In practice, this is a quite common case in various neuroimaging studies\nwhose goal is to characterize a trajectory of pathologies of a specific disease\neven from early stages. This may be too ambitious especially when the sample\nsize is small (e.g., up to a few hundreds). Motivated from the setup above, we\nseek to develop a conditional generative model for longitudinal data generation\nby designing an invertable neural network. Inspired by recurrent nature of\nlongitudinal data, we propose a novel neural network that incorporates\nrecurrent subnetwork and context gating to include smooth transition in a\nsequence of generated data. Our model is validated on a video sequence dataset\nand a longitudinal AD dataset with various experimental settings for\nqualitative and quantitative evaluations of the generated samples. The results\nwith the AD dataset captures AD specific group differences with sufficiently\ngenerated longitudinal samples that are consistent with existing literature,\nwhich implies a great potential to be applicable to other disease studies.\n",
    "topics": "{}",
    "score": 0.8143758157
  },
  {
    "id": "1807.02004",
    "title": "Calamari - A High-Performance Tensorflow-based Deep Learning Package for\n  Optical Character Recognition",
    "abstract": "  Optical Character Recognition (OCR) on contemporary and historical data is\nstill in the focus of many researchers. Especially historical prints require\nbook specific trained OCR models to achieve applicable results (Springmann and\nL\\\"udeling, 2016, Reul et al., 2017a). To reduce the human effort for manually\nannotating ground truth (GT) various techniques such as voting and pretraining\nhave shown to be very efficient (Reul et al., 2018a, Reul et al., 2018b).\nCalamari is a new open source OCR line recognition software that both uses\nstate-of-the art Deep Neural Networks (DNNs) implemented in Tensorflow and\ngiving native support for techniques such as pretraining and voting. The\ncustomizable network architectures constructed of Convolutional Neural Networks\n(CNNS) and Long-ShortTerm-Memory (LSTM) layers are trained by the so-called\nConnectionist Temporal Classification (CTC) algorithm of Graves et al. (2006).\nOptional usage of a GPU drastically reduces the computation times for both\ntraining and prediction. We use two different datasets to compare the\nperformance of Calamari to OCRopy, OCRopus3, and Tesseract 4. Calamari reaches\na Character Error Rate (CER) of 0.11% on the UW3 dataset written in modern\nEnglish and 0.18% on the DTA19 dataset written in German Fraktur, which\nconsiderably outperforms the results of the existing softwares.\n",
    "topics": "{'Optical Character Recognition': 1.0}",
    "score": 0.8141593108
  },
  {
    "id": "1611.05128",
    "title": "Designing Energy-Efficient Convolutional Neural Networks using\n  Energy-Aware Pruning",
    "abstract": "  Deep convolutional neural networks (CNNs) are indispensable to\nstate-of-the-art computer vision algorithms. However, they are still rarely\ndeployed on battery-powered mobile devices, such as smartphones and wearable\ngadgets, where vision algorithms can enable many revolutionary real-world\napplications. The key limiting factor is the high energy consumption of CNN\nprocessing due to its high computational complexity. While there are many\nprevious efforts that try to reduce the CNN model size or amount of\ncomputation, we find that they do not necessarily result in lower energy\nconsumption, and therefore do not serve as a good metric for energy cost\nestimation.\n  To close the gap between CNN design and energy consumption optimization, we\npropose an energy-aware pruning algorithm for CNNs that directly uses energy\nconsumption estimation of a CNN to guide the pruning process. The energy\nestimation methodology uses parameters extrapolated from actual hardware\nmeasurements that target realistic battery-powered system setups. The proposed\nlayer-by-layer pruning algorithm also prunes more aggressively than previously\nproposed pruning methods by minimizing the error in output feature maps instead\nof filter weights. For each layer, the weights are first pruned and then\nlocally fine-tuned with a closed-form least-square solution to quickly restore\nthe accuracy. After all layers are pruned, the entire network is further\nglobally fine-tuned using back-propagation. With the proposed pruning method,\nthe energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x,\nrespectively, with less than 1% top-5 accuracy loss. Finally, we show that\npruning the AlexNet with a reduced number of target classes can greatly\ndecrease the number of weights but the energy reduction is limited.\n  Energy modeling tool and energy-aware pruned models available at\nhttp://eyeriss.mit.edu/energy.html\n",
    "topics": "{}",
    "score": 0.8141188478
  },
  {
    "id": "2008.12617",
    "title": "Simulation-supervised deep learning for analysing organelles states and\n  behaviour in living cells",
    "abstract": "  In many real-world scientific problems, generating ground truth (GT) for\nsupervised learning is almost impossible. The causes include limitations\nimposed by scientific instrument, physical phenomenon itself, or the complexity\nof modeling. Performing artificial intelligence (AI) tasks such as\nsegmentation, tracking, and analytics of small sub-cellular structures such as\nmitochondria in microscopy videos of living cells is a prime example. The 3D\nblurring function of microscope, digital resolution from pixel size, optical\nresolution due to the character of light, noise characteristics, and complex 3D\ndeformable shapes of mitochondria, all contribute to making this problem GT\nhard. Manual segmentation of 100s of mitochondria across 1000s of frames and\nthen across many such videos is not only herculean but also physically\ninaccurate because of the instrument and phenomena imposed limitations.\nUnsupervised learning produces less than optimal results and accuracy is\nimportant if inferences relevant to therapy are to be derived. In order to\nsolve this unsurmountable problem, we bring modeling and deep learning to a\nnexus. We show that accurate physics based modeling of microscopy data\nincluding all its limitations can be the solution for generating simulated\ntraining datasets for supervised learning. We show here that our\nsimulation-supervised segmentation approach is a great enabler for studying\nmitochondrial states and behaviour in heart muscle cells, where mitochondria\nhave a significant role to play in the health of the cells. We report\nunprecedented mean IoU score of 91% for binary segmentation (19% better than\nthe best performing unsupervised approach) of mitochondria in actual microscopy\nvideos of living cells. We further demonstrate the possibility of performing\nmulti-class classification, tracking, and morphology associated analytics at\nthe scale of individual mitochondrion.\n",
    "topics": "{'Multi-class Classification': 0.9999485}",
    "score": 0.814097866
  },
  {
    "id": "1912.11642",
    "title": "Competing Ratio Loss for Discriminative Multi-class Image Classification",
    "abstract": "  The development of deep convolutional neural network architecture is critical\nto the improvement of image classification task performance. Many image\nclassification studies use deep convolutional neural network and focus on\nmodifying the network structure to improve image classification performance.\nConversely, our study focuses on loss function design. Cross-entropy Loss (CEL)\nhas been widely used for training deep convolutional neural network for the\ntask of multi-class classification. Although CEL has been successfully\nimplemented in several image classification tasks, it only focuses on the\nposterior probability of the correct class. For this reason, a negative log\nlikelihood ratio loss (NLLR) was proposed to better differentiate between the\ncorrect class and the competing incorrect ones. However, during the training of\nthe deep convolutional neural network, the value of NLLR is not always positive\nor negative, which severely affects the convergence of NLLR. Our proposed\ncompeting ratio loss (CRL) calculates the posterior probability ratio between\nthe correct class and the competing incorrect classes to further enlarge the\nprobability difference between the correct and incorrect classes. We added\nhyperparameters to CRL, thereby ensuring its value to be positive and that the\nupdate size of backpropagation is suitable for the CRL's fast convergence. To\ndemonstrate the performance of CRL, we conducted experiments on general image\nclassification tasks (CIFAR10/100, SVHN, ImageNet), the fine-grained image\nclassification tasks (CUB200-2011 and Stanford Car), and the challenging face\nage estimation task (using Adience). Experimental results show the\neffectiveness and robustness of the proposed loss function on different deep\nconvolutional neural network architectures and different image classification\ntasks.\n",
    "topics": "{'Image Classification': 1.0, 'Fine-Grained Image Classification': 0.9998646, 'Age Estimation': 0.9985207, 'Multi-class Classification': 0.99779236}",
    "score": 0.8140349167
  },
  {
    "id": "2006.14887",
    "title": "Ensemble Transfer Learning for Emergency Landing Field Identification on\n  Moderate Resource Heterogeneous Kubernetes Cluster",
    "abstract": "  The full loss of thrust of an aircraft requires fast and reliable decisions\nof the pilot. If no published landing field is within reach, an emergency\nlanding field must be selected. The choice of a suitable emergency landing\nfield denotes a crucial task to avoid unnecessary damage of the aircraft, risk\nfor the civil population as well as the crew and all passengers on board.\nEspecially in case of instrument meteorological conditions it is indispensable\nto use a database of suitable emergency landing fields. Thus, based on public\navailable digital orthographic photos and digital surface models, we created\nvarious datasets with different sample sizes to facilitate training and testing\nof neural networks. Each dataset consists of a set of data layers. The best\ncompositions of these data layers as well as the best performing transfer\nlearning models are selected. Subsequently, certain hyperparameters of the\nchosen models for each sample size are optimized with Bayesian and Bandit\noptimization. The hyperparameter tuning is performed with a self-made\nKubernetes cluster. The models outputs were investigated with respect to the\ninput data by the utilization of layer-wise relevance propagation. With\noptimized models we created an ensemble model to improve the segmentation\nperformance. Finally, an area around the airport of Arnsberg in North\nRhine-Westphalia was segmented and emergency landing fields are identified,\nwhile the verification of the final approach's obstacle clearance is left\nunconsidered. These emergency landing fields are stored in a PostgreSQL\ndatabase.\n",
    "topics": "{'Transfer Learning': 0.9992473}",
    "score": 0.8139782289
  },
  {
    "id": "1607.02046",
    "title": "MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild",
    "abstract": "  This paper addresses the problem of 3D human pose estimation in the wild. A\nsignificant challenge is the lack of training data, i.e., 2D images of humans\nannotated with 3D poses. Such data is necessary to train state-of-the-art CNN\narchitectures. Here, we propose a solution to generate a large set of\nphotorealistic synthetic images of humans with 3D pose annotations. We\nintroduce an image-based synthesis engine that artificially augments a dataset\nof real images with 2D human pose annotations using 3D Motion Capture (MoCap)\ndata. Given a candidate 3D pose our algorithm selects for each joint an image\nwhose 2D pose locally matches the projected 3D pose. The selected images are\nthen combined to generate a new synthetic image by stitching local image\npatches in a kinematically constrained manner. The resulting images are used to\ntrain an end-to-end CNN for full-body 3D pose estimation. We cluster the\ntraining data into a large number of pose classes and tackle pose estimation as\na K-way classification problem. Such an approach is viable only with large\ntraining sets such as ours. Our method outperforms the state of the art in\nterms of 3D pose estimation in controlled environments (Human3.6M) and shows\npromising results for in-the-wild images (LSP). This demonstrates that CNNs\ntrained on artificial images generalize well to real images.\n",
    "topics": "{'Pose Estimation': 1.0, 'Motion Capture': 0.99996734, '3D Pose Estimation': 0.9997614, '3D Human Pose Estimation': 0.99563897, 'Data Augmentation': 0.9508773}",
    "score": 0.8139548164
  },
  {
    "id": "1905.10488",
    "title": "GAN2GAN: Generative Noise Learning for Iterative Blind Denoising with\n  Single Noisy Images",
    "abstract": "  We tackle a challenging blind image denoising problem, in which only single\ndistinct noisy images are available for training a denoiser, and no information\nabout noise is known, except for it being zero-mean, additive, and independent\nof the clean image. In such a setting, which often occurs in practice, it is\nnot possible to train a denoiser with the standard discriminative training or\nwith the recently developed Noise2Noise (N2N) training; the former requires the\nunderlying clean image for the given noisy image, and the latter requires two\nindependently realized noisy image pair for a clean image. To that end, we\npropose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise)\nmethod that first learns a generative model that can 1) simulate the noise in\nthe given noisy images and 2) generate a rough, noisy estimates of the clean\nimages, then 3) iteratively trains a denoiser with subsequently synthesized\nnoisy image pairs (as in N2N), obtained from the generative model. In results,\nwe show the denoiser trained with our GAN2GAN achieves an impressive denoising\nperformance on both synthetic and real-world datasets for the blind denoising\nsetting; it almost approaches the performance of the standard\ndiscriminatively-trained or N2N-trained models that have more information than\nours, and it significantly outperforms the recent baseline for the same\nsetting, e.g., Noise2Void, and a more conventional yet strong one, BM3D.\n",
    "topics": "{'Denoising': 1.0, 'Image Denoising': 0.9999703}",
    "score": 0.8139446735
  },
  {
    "id": "2009.10401",
    "title": "Dynamic Fusion based Federated Learning for COVID-19 Detection",
    "abstract": "  Medical diagnostic image analysis (e.g., CT scan or X-Ray) using machine\nlearning is an efficient and accurate way to detect COVID-19 infections.\nHowever, sharing diagnostic images across medical institutions is usually not\nallowed due to the concern of patients' privacy. This causes the issue of\ninsufficient datasets for training the image classification model. Federated\nlearning is an emerging privacy-preserving machine learning paradigm that\nproduces an unbiased global model based on the received updates of local models\ntrained by clients without exchanging clients' local data. Nevertheless, the\ndefault setting of federated learning introduces huge communication cost of\ntransferring model updates and can hardly ensure model performance when data\nheterogeneity of clients heavily exists. To improve communication efficiency\nand model performance, in this paper, we propose a novel dynamic fusion-based\nfederated learning approach for medical diagnostic image analysis to detect\nCOVID-19 infections. First, we design an architecture for dynamic fusion-based\nfederated learning systems to analyse medical diagnostic images. Further, we\npresent a dynamic fusion method to dynamically decide the participating clients\naccording to their local model performance and schedule the model fusion-based\non participating clients' training time. In addition, we summarise a category\nof medical diagnostic image datasets for COVID-19 detection, which can be used\nby the machine learning community for image analysis. The evaluation results\nshow that the proposed approach is feasible and performs better than the\ndefault setting of federated learning in terms of model performance,\ncommunication efficiency and fault tolerance.\n",
    "topics": "{'Federated Learning': 1.0, 'Image Classification': 0.72845405, 'Decision Making': 0.48340544}",
    "score": 0.8139033131
  },
  {
    "id": "1712.09527",
    "title": "Co-Morbidity Exploration on Wearables Activity Data Using Unsupervised\n  Pre-training and Multi-Task Learning",
    "abstract": "  Physical activity and sleep play a major role in the prevention and\nmanagement of many chronic conditions. It is not a trivial task to understand\ntheir impact on chronic conditions. Currently, data from electronic health\nrecords (EHRs), sleep lab studies, and activity/sleep logs are used. The rapid\nincrease in the popularity of wearable health devices provides a significant\nnew data source, making it possible to track the user's lifestyle real-time\nthrough web interfaces, both to consumer as well as their healthcare provider,\npotentially. However, at present there is a gap between lifestyle data (e.g.,\nsleep, physical activity) and clinical outcomes normally captured in EHRs. This\nis a critical barrier for the use of this new source of signal for healthcare\ndecision making. Applying deep learning to wearables data provides a new\nopportunity to overcome this barrier.\n  To address the problem of the unavailability of clinical data from a major\nfraction of subjects and unrepresentative subject populations, we propose a\nnovel unsupervised (task-agnostic) time-series representation learning\ntechnique called act2vec. act2vec learns useful features by taking into account\nthe co-occurrence of activity levels along with periodicity of human activity\npatterns. The learned representations are then exploited to boost the\nperformance of disorder-specific supervised learning models. Furthermore, since\nmany disorders are often related to each other, a phenomenon referred to as\nco-morbidity, we use a multi-task learning framework for exploiting the shared\nstructure of disorder inducing life-style choices partially captured in the\nwearables data. Empirical evaluation using actigraphy data from 4,124 subjects\nshows that our proposed method performs and generalizes substantially better\nthan the conventional time-series symbolic representational methods and\ntask-specific deep learning models.\n",
    "topics": "{'Multi-Task Learning': 0.99999964, 'Unsupervised Pre-training': 0.99971527, 'Time Series': 0.9956619, 'Decision Making': 0.9723832, 'Representation Learning': 0.9559929}",
    "score": 0.8138977195
  },
  {
    "id": "1812.10532",
    "title": "A Unified Learning Based Framework for Light Field Reconstruction from\n  Coded Projections",
    "abstract": "  Light field presents a rich way to represent the 3D world by capturing the\nspatio-angular dimensions of the visual signal. However, the popular way of\ncapturing light field (LF) via a plenoptic camera presents spatio-angular\nresolution trade-off. Computational imaging techniques such as compressive\nlight field and programmable coded aperture reconstruct full sensor resolution\nLF from coded projections obtained by multiplexing the incoming spatio-angular\nlight field. Here, we present a unified learning framework that can reconstruct\nLF from a variety of multiplexing schemes with minimal number of coded images\nas input. We consider three light field capture schemes: heterodyne capture\nscheme with code placed near the sensor, coded aperture scheme with code at the\ncamera aperture and finally the dual exposure scheme of capturing a\nfocus-defocus pair where there is no explicit coding. Our algorithm consists of\nthree stages 1) we recover the all-in-focus image from the coded image 2) we\nestimate the disparity maps for all the LF views from the coded image and the\nall-in-focus image, 3) we then render the LF by warping the all-in-focus image\nusing disparity maps and refine it. For these three stages we propose three\ndeep neural networks - ViewNet, DispairtyNet and RefineNet. Our reconstructions\nshow that our learning algorithm achieves state-of-the-art results for all the\nthree multiplexing schemes. Especially, our LF reconstructions from\nfocus-defocus pair is comparable to other learning-based view synthesis\napproaches from multiple images. Thus, our work paves the way for capturing\nhigh-resolution LF (~ a megapixel) using conventional cameras such as DSLRs.\nPlease check our supplementary materials\n$\\href{https://docs.google.com/presentation/d/1Vr-F8ZskrSd63tvnLfJ2xmEXY6OBc1Rll3XeOAtc11I/}{online}$\nto better appreciate the reconstructed light fields.\n",
    "topics": "{}",
    "score": 0.8138128833
  },
  {
    "id": "2007.02684",
    "title": "On the Influence of Ageing on Face Morph Attacks: Vulnerability and\n  Detection",
    "abstract": "  Face morphing attacks have raised critical concerns as they demonstrate a new\nvulnerability of Face Recognition Systems (FRS), which are widely deployed in\nborder control applications. The face morphing process uses the images from\nmultiple data subjects and performs an image blending operation to generate a\nmorphed image of high quality. The generated morphed image exhibits similar\nvisual characteristics corresponding to the biometric characteristics of the\ndata subjects that contributed to the composite image and thus making it\ndifficult for both humans and FRS, to detect such attacks. In this paper, we\nreport a systematic investigation on the vulnerability of the\nCommercial-Off-The-Shelf (COTS) FRS when morphed images under the influence of\nageing are presented. To this extent, we have introduced a new morphed face\ndataset with ageing derived from the publicly available MORPH II face dataset,\nwhich we refer to as MorphAge dataset. The dataset has two bins based on age\nintervals, the first bin - MorphAge-I dataset has 1002 unique data subjects\nwith the age variation of 1 year to 2 years while the MorphAge-II dataset\nconsists of 516 data subjects whose age intervals are from 2 years to 5 years.\nTo effectively evaluate the vulnerability for morphing attacks, we also\nintroduce a new evaluation metric, namely the Fully Mated Morphed Presentation\nMatch Rate (FMMPMR), to quantify the vulnerability effectively in a realistic\nscenario. Extensive experiments are carried out by using two different COTS FRS\n(COTS I - Cognitec and COTS II - Neurotechnology) to quantify the vulnerability\nwith ageing. Further, we also evaluate five different Morph Attack Detection\n(MAD) techniques to benchmark their detection performance with ageing.\n",
    "topics": "{'Face Recognition': 0.9982577}",
    "score": 0.8137105484
  },
  {
    "id": "1907.01115",
    "title": "Neural Semantic Parsing with Anonymization for Command Understanding in\n  General-Purpose Service Robots",
    "abstract": "  Service robots are envisioned to undertake a wide range of tasks at the\nrequest of users. Semantic parsing is one way to convert natural language\ncommands given to these robots into executable representations. Methods for\ncreating semantic parsers, however, rely either on large amounts of data or on\nengineered lexical features and parsing rules, which has limited their\napplication in robotics. To address this challenge, we propose an approach that\nleverages neural semantic parsing methods in combination with contextual word\nembeddings to enable the training of a semantic parser with little data and\nwithout domain specific parser engineering. Key to our approach is the use of\nan anonymized target representation which is more easily learned by the parser.\nIn most cases, this simplified representation can trivially be transformed into\nan executable format, and in others the parse can be completed through further\ninteraction with the user. We evaluate this approach in the context of the\nRoboCup@Home General Purpose Service Robot task, where we have collected a\ncorpus of paraphrased versions of commands from the standardized command\ngenerator. Our results show that neural semantic parsers can predict the\nlogical form of unseen commands with 89% accuracy. We release our data and the\ndetails of our models to encourage further development from the RoboCup and\nservice robotics communities.\n",
    "topics": "{'Semantic Parsing': 0.9999999, 'Word Embeddings': 0.70090663}",
    "score": 0.8136994157
  },
  {
    "id": "1904.00625",
    "title": "Med3D: Transfer Learning for 3D Medical Image Analysis",
    "abstract": "  The performance on deep learning is significantly affected by volume of\ntraining data. Models pre-trained from massive dataset such as ImageNet become\na powerful weapon for speeding up training convergence and improving accuracy.\nSimilarly, models based on large dataset are important for the development of\ndeep learning in 3D medical images. However, it is extremely challenging to\nbuild a sufficiently large dataset due to difficulty of data acquisition and\nannotation in 3D medical imaging. We aggregate the dataset from several medical\nchallenges to build 3DSeg-8 dataset with diverse modalities, target organs, and\npathologies. To extract general medical three-dimension (3D) features, we\ndesign a heterogeneous 3D network called Med3D to co-train multi-domain 3DSeg-8\nso as to make a series of pre-trained models. We transfer Med3D pre-trained\nmodels to lung segmentation in LIDC dataset, pulmonary nodule classification in\nLIDC dataset and liver segmentation on LiTS challenge. Experiments show that\nthe Med3D can accelerate the training convergence speed of target 3D medical\ntasks 2 times compared with model pre-trained on Kinetics dataset, and 10 times\ncompared with training from scratch as well as improve accuracy ranging from 3%\nto 20%. Transferring our Med3D model on state-the-of-art DenseASPP segmentation\nnetwork, in case of single model, we achieve 94.6\\% Dice coefficient which\napproaches the result of top-ranged algorithms on the LiTS challenge.\n",
    "topics": "{'Transfer Learning': 0.73645586}",
    "score": 0.8135704501
  },
  {
    "id": "2003.06583",
    "title": "From W-Net to CDGAN: Bi-temporal Change Detection via Deep Learning\n  Techniques",
    "abstract": "  Traditional change detection methods usually follow the image differencing,\nchange feature extraction and classification framework, and their performance\nis limited by such simple image domain differencing and also the hand-crafted\nfeatures. Recently, the success of deep convolutional neural networks (CNNs)\nhas widely spread across the whole field of computer vision for their powerful\nrepresentation abilities. In this paper, we therefore address the remote\nsensing image change detection problem with deep learning techniques. We\nfirstly propose an end-to-end dual-branch architecture, termed as the W-Net,\nwith each branch taking as input one of the two bi-temporal images as in the\ntraditional change detection models. In this way, CNN features with more\npowerful representative abilities can be obtained to boost the final detection\nperformance. Also, W-Net performs differencing in the feature domain rather\nthan in the traditional image domain, which greatly alleviates loss of useful\ninformation for determining the changes. Furthermore, by reformulating change\ndetection as an image translation problem, we apply the recently popular\nGenerative Adversarial Network (GAN) in which our W-Net serves as the\nGenerator, leading to a new GAN architecture for change detection which we call\nCDGAN. To train our networks and also facilitate future research, we construct\na large scale dataset by collecting images from Google Earth and provide\ncarefully manually annotated ground truths. Experiments show that our proposed\nmethods can provide fine-grained change detection results superior to the\nexisting state-of-the-art baselines.\n",
    "topics": "{'Change Point Detection': 0.7355012}",
    "score": 0.8135556929
  },
  {
    "id": "1802.04028",
    "title": "Automatic Generation of Language-Independent Features for Cross-Lingual\n  Classification",
    "abstract": "  Many applications require categorization of text documents using predefined\ncategories. The main approach to performing text categorization is learning\nfrom labeled examples. For many tasks, it may be difficult to find examples in\none language but easy in others. The problem of learning from examples in one\nor more languages and classifying (categorizing) in another is called\ncross-lingual learning. In this work, we present a novel approach that solves\nthe general cross-lingual text categorization problem. Our method generates,\nfor each training document, a set of language-independent features. Using these\nfeatures for training yields a language-independent classifier. At the\nclassification stage, we generate language-independent features for the\nunlabeled document, and apply the classifier on the new representation.\n  To build the feature generator, we utilize a hierarchical\nlanguage-independent ontology, where each concept has a set of support\ndocuments for each language involved. In the preprocessing stage, we use the\nsupport documents to build a set of language-independent feature generators,\none for each language. The collection of these generators is used to map any\ndocument into the language-independent feature space.\n  Our methodology works on the most general cross-lingual text categorization\nproblems, being able to learn from any mix of languages and classify documents\nin any other language. We also present a method for exploiting the hierarchical\nstructure of the ontology to create virtual supporting documents for languages\nthat do not have them. We tested our method, using Wikipedia as our ontology,\non the most commonly used test collections in cross-lingual text\ncategorization, and found that it outperforms existing methods.\n",
    "topics": "{'Text Categorization': 1.0}",
    "score": 0.813531081
  },
  {
    "id": "1905.08965",
    "title": "Segmentation-Aware Image Denoising without Knowing True Segmentation",
    "abstract": "  Several recent works discussed application-driven image restoration neural\nnetworks, which are capable of not only removing noise in images but also\npreserving their semantic-aware details, making them suitable for various\nhigh-level computer vision tasks as the pre-processing step. However, such\napproaches require extra annotations for their high-level vision tasks, in\norder to train the joint pipeline using hybrid losses. The availability of\nthose annotations is yet often limited to a few image sets, potentially\nrestricting the general applicability of these methods to denoising more unseen\nand unannotated images. Motivated by that, we propose a segmentation-aware\nimage denoising model dubbed U-SAID, based on a novel unsupervised approach\nwith a pixel-wise uncertainty loss. U-SAID does not need any ground-truth\nsegmentation map, and thus can be applied to any image dataset. It generates\ndenoised images with comparable or even better quality, and the denoised\nresults show stronger robustness for subsequent semantic segmentation tasks,\nwhen compared to either its supervised counterpart or classical\n\"application-agnostic\" denoisers. Moreover, we demonstrate the superior\ngeneralizability of U-SAID in three-folds, by plugging its \"universal\" denoiser\nwithout fine-tuning: (1) denoising unseen types of images; (2) denoising as\npre-processing for segmenting unseen noisy images; and (3) denoising for unseen\nhigh-level tasks. Extensive experiments demonstrate the effectiveness,\nrobustness and generalizability of the proposed U-SAID over various popular\nimage sets.\n",
    "topics": "{'Image Denoising': 1.0, 'Denoising': 1.0, 'Image Restoration': 0.99983704, 'Semantic Segmentation': 0.99200726}",
    "score": 0.8134990636
  },
  {
    "id": "1912.05945",
    "title": "Towards a Robust Classifier: An MDL-Based Method for Generating\n  Adversarial Examples",
    "abstract": "  We address the problem of adversarial examples in machine learning where an\nadversary tries to misguide a classifier by making functionality-preserving\nmodifications to original samples. We assume a black-box scenario where the\nadversary has access to only the feature set, and the final hard-decision\noutput of the classifier. We propose a method to generate adversarial examples\nusing the minimum description length (MDL) principle. Our final aim is to\nimprove the robustness of the classifier by considering generated examples in\nrebuilding the classifier. We evaluate our method for the application of static\nmalware detection in portable executable (PE) files. We consider API calls of\nPE files as their distinguishing features where the feature vector is a binary\nvector representing the presence-absence of API calls. In our method, we first\ncreate a dataset of benign samples by querying the target classifier. We next\nconstruct a code table of frequent patterns for the compression of this dataset\nusing the MDL principle. We finally generate an adversarial example\ncorresponding to a malware sample by selecting and adding a pattern from the\nbenign code table to the malware sample. The selected pattern is the one that\nminimizes the length of the compressed adversarial example given the code\ntable. This modification preserves the functionalities of the original malware\nsample as all original API calls are kept, and only some new API calls are\nadded. Considering a neural network, we show that the evasion rate is 78.24\npercent for adversarial examples compared to 8.16 percent for original malware\nsamples. This shows the effectiveness of our method in generating examples that\nneed to be considered in rebuilding the classifier.\n",
    "topics": "{'Malware Detection': 0.9999999}",
    "score": 0.8133803473
  },
  {
    "id": "2005.11061",
    "title": "Vulnerability of deep neural networks for detecting COVID-19 cases from\n  chest X-ray images to universal adversarial attacks",
    "abstract": "  Under the epidemic of the novel coronavirus disease 2019 (COVID-19), chest\nX-ray computed tomography imaging is being used for effectively screening\nCOVID-19 patients. The development of computer-aided systems based on deep\nneural networks (DNNs) has been advanced, to rapidly and accurately detect\nCOVID-19 cases, because the need for expert radiologists, who are limited in\nnumber, forms a bottleneck for the screening. However, so far, the\nvulnerability of DNN-based systems has been poorly evaluated, although DNNs are\nvulnerable to a single perturbation, called universal adversarial perturbation\n(UAP), which can induce DNN failure in most classification tasks. Thus, we\nfocus on representative DNN models for detecting COVID-19 cases from chest\nX-ray images and evaluate their vulnerability to UAPs generated using simple\niterative algorithms. We consider nontargeted UAPs, which cause a task failure\nresulting in an input being assigned an incorrect label, and targeted UAPs,\nwhich cause the DNN to classify an input into a specific class. The results\ndemonstrate that the models are vulnerable to nontargeted and targeted UAPs,\neven in case of small UAPs. In particular, 2% norm of the UPAs to the average\nnorm of an image in the image dataset achieves >85% and >90% success rates for\nthe nontargeted and targeted attacks, respectively. Due to the nontargeted\nUAPs, the DNN models judge most chest X-ray images as COVID-19 cases. The\ntargeted UAPs make the DNN models classify most chest X-ray images into a given\ntarget class. The results indicate that careful consideration is required in\npractical applications of DNNs to COVID-19 diagnosis; in particular, they\nemphasize the need for strategies to address security concerns. As an example,\nwe show that iterative fine-tuning of the DNN models using UAPs improves the\nrobustness of the DNN models against UAPs.\n",
    "topics": "{'COVID-19 Diagnosis': 0.9967623}",
    "score": 0.8133579105
  },
  {
    "id": "2008.11505",
    "title": "Cross-regional oil palm tree counting and detection via multi-level\n  attention domain adaptation network",
    "abstract": "  Providing an accurate evaluation of palm tree plantation in a large region\ncan bring meaningful impacts in both economic and ecological aspects. However,\nthe enormous spatial scale and the variety of geological features across\nregions has made it a grand challenge with limited solutions based on manual\nhuman monitoring efforts. Although deep learning based algorithms have\ndemonstrated potential in forming an automated approach in recent years, the\nlabelling efforts needed for covering different features in different regions\nlargely constrain its effectiveness in large-scale problems. In this paper, we\npropose a novel domain adaptive oil palm tree detection method, i.e., a\nMulti-level Attention Domain Adaptation Network (MADAN) to reap cross-regional\noil palm tree counting and detection. MADAN consists of 4 procedures: First, we\nadopted a batch-instance normalization network (BIN) based feature extractor\nfor improving the generalization ability of the model, integrating batch\nnormalization and instance normalization. Second, we embedded a multi-level\nattention mechanism (MLA) into our architecture for enhancing the\ntransferability, including a feature level attention and an entropy level\nattention. Then we designed a minimum entropy regularization (MER) to increase\nthe confidence of the classifier predictions through assigning the entropy\nlevel attention value to the entropy penalty. Finally, we employed a sliding\nwindow-based prediction and an IOU based post-processing approach to attain the\nfinal detection results. We conducted comprehensive ablation experiments using\nthree different satellite images of large-scale oil palm plantation area with\nsix transfer tasks. MADAN improves the detection accuracy by 14.98% in terms of\naverage F1-score compared with the Baseline method (without DA), and performs\n3.55%-14.49% better than existing domain adaptation methods.\n",
    "topics": "{'Domain Adaptation': 0.9956748}",
    "score": 0.8132246559
  },
  {
    "id": "2008.03511",
    "title": "Single-Shot Two-Pronged Detector with Rectified IoU Loss",
    "abstract": "  In the CNN based object detectors, feature pyramids are widely exploited to\nalleviate the problem of scale variation across object instances. These object\ndetectors, which strengthen features via a top-down pathway and lateral\nconnections, are mainly to enrich the semantic information of low-level\nfeatures, but ignore the enhancement of high-level features. This can lead to\nan imbalance between different levels of features, in particular a serious lack\nof detailed information in the high-level features, which makes it difficult to\nget accurate bounding boxes. In this paper, we introduce a novel two-pronged\ntransductive idea to explore the relationship among different layers in both\nbackward and forward directions, which can enrich the semantic information of\nlow-level features and detailed information of high-level features at the same\ntime. Under the guidance of the two-pronged idea, we propose a Two-Pronged\nNetwork (TPNet) to achieve bidirectional transfer between high-level features\nand low-level features, which is useful for accurately detecting object at\ndifferent scales. Furthermore, due to the distribution imbalance between the\nhard and easy samples in single-stage detectors, the gradient of localization\nloss is always dominated by the hard examples that have poor localization\naccuracy. This will enable the model to be biased toward the hard samples. So\nin our TPNet, an adaptive IoU based localization loss, named Rectified IoU\n(RIoU) loss, is proposed to rectify the gradients of each kind of samples. The\nRectified IoU loss increases the gradients of examples with high IoU while\nsuppressing the gradients of examples with low IoU, which can improve the\noverall localization accuracy of model. Extensive experiments demonstrate the\nsuperiority of our TPNet and RIoU loss.\n",
    "topics": "{}",
    "score": 0.813224034
  },
  {
    "id": "1904.03845",
    "title": "Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A\n  New Benchmark",
    "abstract": "  Person re-identification (Re-ID) benefits greatly from the accurate\nannotations of existing datasets (e.g., CUHK03 [1] and Market-1501 [2]), which\nare quite expensive because each image in these datasets has to be assigned\nwith a proper label. In this work, we ease the annotation of Re-ID by replacing\nthe accurate annotation with inaccurate annotation, i.e., we group the images\ninto bags in terms of time and assign a bag-level label for each bag. This\ngreatly reduces the annotation effort and leads to the creation of a\nlarge-scale Re-ID benchmark called SYSU-30$k$. The new benchmark contains $30k$\nindividuals, which is about $20$ times larger than CUHK03 ($1.3k$ individuals)\nand Market-1501 ($1.5k$ individuals), and $30$ times larger than ImageNet ($1k$\ncategories). It sums up to 29,606,918 images. Learning a Re-ID model with\nbag-level annotation is called the weakly supervised Re-ID problem. To solve\nthis problem, we introduce a differentiable graphical model to capture the\ndependencies from all images in a bag and generate a reliable pseudo label for\neach person image. The pseudo label is further used to supervise the learning\nof the Re-ID model. When compared with the fully supervised Re-ID models, our\nmethod achieves state-of-the-art performance on SYSU-30$k$ and other datasets.\nThe code, dataset, and pretrained model will be available at\n\\url{https://github.com/wanggrun/SYSU-30k}.\n",
    "topics": "{'Person Re-Identification': 0.9999999}",
    "score": 0.8130236249
  },
  {
    "id": "1706.00556",
    "title": "r-BTN: Cross-domain Face Composite and Synthesis from Limited Facial\n  Patches",
    "abstract": "  We start by asking an interesting yet challenging question, \"If an eyewitness\ncan only recall the eye features of the suspect, such that the forensic artist\ncan only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig.\n1), can advanced computer vision techniques help generate the whole face\nimage?\" A more generalized question is that if a large proportion (e.g., more\nthan 50%) of the face/sketch is missing, can a realistic whole face\nsketch/image still be estimated. Existing face completion and generation\nmethods either do not conduct domain transfer learning or can not handle large\nmissing area. For example, the inpainting approach tends to blur the generated\nregion when the missing area is large (i.e., more than 50%). In this paper, we\nexploit the potential of deep learning networks in filling large missing region\n(e.g., as high as 95% missing) and generating realistic faces with\nhigh-fidelity in cross domains. We propose the recursive generation by\nbidirectional transformation networks (r-BTN) that recursively generates a\nwhole face/sketch from a small sketch/face patch. The large missing area and\nthe cross domain challenge make it difficult to generate satisfactory results\nusing a unidirectional cross-domain learning structure. On the other hand, a\nforward and backward bidirectional learning between the face and sketch domains\nwould enable recursive estimation of the missing region in an incremental\nmanner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarial\nconstraint to encourage the generation of realistic faces/sketches. Extensive\nexperiments have been conducted to demonstrate the superior performance from\nr-BTN as compared to existing potential solutions.\n",
    "topics": "{'Face Generation': 0.32747814}",
    "score": 0.8129909593
  },
  {
    "id": "1810.12186",
    "title": "DeepSphere: Efficient spherical Convolutional Neural Network with\n  HEALPix sampling for cosmological applications",
    "abstract": "  Convolutional Neural Networks (CNNs) are a cornerstone of the Deep Learning\ntoolbox and have led to many breakthroughs in Artificial Intelligence. These\nnetworks have mostly been developed for regular Euclidean domains such as those\nsupporting images, audio, or video. Because of their success, CNN-based methods\nare becoming increasingly popular in Cosmology. Cosmological data often comes\nas spherical maps, which make the use of the traditional CNNs more complicated.\nThe commonly used pixelization scheme for spherical maps is the Hierarchical\nEqual Area isoLatitude Pixelisation (HEALPix). We present a spherical CNN for\nanalysis of full and partial HEALPix maps, which we call DeepSphere. The\nspherical CNN is constructed by representing the sphere as a graph. Graphs are\nversatile data structures that can act as a discrete representation of a\ncontinuous manifold. Using the graph-based representation, we define many of\nthe standard CNN operations, such as convolution and pooling. With filters\nrestricted to being radial, our convolutions are equivariant to rotation on the\nsphere, and DeepSphere can be made invariant or equivariant to rotation. This\nway, DeepSphere is a special case of a graph CNN, tailored to the HEALPix\nsampling of the sphere. This approach is computationally more efficient than\nusing spherical harmonics to perform convolutions. We demonstrate the method on\na classification problem of weak lensing mass maps from two cosmological models\nand compare the performance of the CNN with that of two baseline classifiers.\nThe results show that the performance of DeepSphere is always superior or equal\nto both of these baselines. For high noise levels and for data covering only a\nsmaller fraction of the sphere, DeepSphere achieves typically 10% better\nclassification accuracy than those baselines. Finally, we show how learned\nfilters can be visualized to introspect the neural network.\n",
    "topics": "{}",
    "score": 0.8128464902
  },
  {
    "id": "1912.10166",
    "title": "MedCAT -- Medical Concept Annotation Tool",
    "abstract": "  Biomedical documents such as Electronic Health Records (EHRs) contain a large\namount of information in an unstructured format. The data in EHRs is a hugely\nvaluable resource documenting clinical narratives and decisions, but whilst the\ntext can be easily understood by human doctors it is challenging to use in\nresearch and clinical applications. To uncover the potential of biomedical\ndocuments we need to extract and structure the information they contain. The\ntask at hand is Named Entity Recognition and Linking (NER+L). The number of\nentities, ambiguity of words, overlapping and nesting make the biomedical area\nsignificantly more difficult than many others. To overcome these difficulties,\nwe have developed the Medical Concept Annotation Tool (MedCAT), an open-source\nunsupervised approach to NER+L. MedCAT uses unsupervised machine learning to\ndisambiguate entities. It was validated on MIMIC-III (a freely accessible\ncritical care database) and MedMentions (Biomedical papers annotated with\nmentions from the Unified Medical Language System). In case of NER+L, the\ncomparison with existing tools shows that MedCAT improves the previous best\nwith only unsupervised learning (F1=0.848 vs 0.691 for disease detection;\nF1=0.710 vs. 0.222 for general concept detection). A qualitative analysis of\nthe vector embeddings learnt by MedCAT shows that it captures latent medical\nknowledge available in EHRs (MIMIC-III). Unsupervised learning can improve the\nperformance of large scale entity extraction, but it has some limitations when\nworking with only a couple of entities and a small dataset. In that case\noptions are supervised learning or active learning, both of which are supported\nin MedCAT via the MedCATtrainer extension. Our approach can detect and link\nmillions of different biomedical concepts with state-of-the-art performance,\nwhilst being lightweight, fast and easy to use.\n",
    "topics": "{'Named Entity Recognition': 0.6756513}",
    "score": 0.8128002257
  },
  {
    "id": "1803.11395",
    "title": "Contrast-Oriented Deep Neural Networks for Salient Object Detection",
    "abstract": "  Deep convolutional neural networks have become a key element in the recent\nbreakthrough of salient object detection. However, existing CNN-based methods\nare based on either patch-wise (region-wise) training and inference or fully\nconvolutional networks. Methods in the former category are generally\ntime-consuming due to severe storage and computational redundancies among\noverlapping patches. To overcome this deficiency, methods in the second\ncategory attempt to directly map a raw input image to a predicted dense\nsaliency map in a single network forward pass. Though being very efficient, it\nis arduous for these methods to detect salient objects of different scales or\nsalient regions with weak semantic information. In this paper, we develop\nhybrid contrast-oriented deep neural networks to overcome the aforementioned\nlimitations. Each of our deep networks is composed of two complementary\ncomponents, including a fully convolutional stream for dense prediction and a\nsegment-level spatial pooling stream for sparse saliency inference. We further\npropose an attentional module that learns weight maps for fusing the two\nsaliency predictions from these two streams. A tailored alternate scheme is\ndesigned to train these deep networks by fine-tuning pre-trained baseline\nmodels. Finally, a customized fully connected CRF model incorporating a salient\ncontour feature embedding can be optionally applied as a post-processing step\nto improve spatial coherence and contour positioning in the fused result from\nthese two streams. Extensive experiments on six benchmark datasets demonstrate\nthat our proposed model can significantly outperform the state of the art in\nterms of all popular evaluation metrics.\n",
    "topics": "{'RGB Salient Object Detection': 0.9999907, 'Object Detection': 0.99980885}",
    "score": 0.8127367479
  },
  {
    "id": "1711.05941",
    "title": "Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints\n  for Action Recognition",
    "abstract": "  Human skeleton joints are popular for action analysis since they can be\neasily extracted from videos to discard background noises. However, current\nskeleton representations do not fully benefit from machine learning with CNNs.\nWe propose \"Skepxels\" a spatio-temporal representation for skeleton sequences\nto fully exploit the \"local\" correlations between joints using the 2D\nconvolution kernels of CNN. We transform skeleton videos into images of\nflexible dimensions using Skepxels and develop a CNN-based framework for\neffective human action recognition using the resulting images. Skepxels encode\nrich spatio-temporal information about the skeleton joints in the frames by\nmaximizing a unique distance metric, defined collaboratively over the distinct\njoint arrangements used in the skeletal image. Moreover, they are flexible in\nencoding compound semantic notions such as location and speed of the joints.\nThe proposed action recognition exploits the representation in a hierarchical\nmanner by first capturing the micro-temporal relations between the skeleton\njoints with the Skepxels and then exploiting their macro-temporal relations by\ncomputing the Fourier Temporal Pyramids over the CNN features of the skeletal\nimages. We extend the Inception-ResNet CNN architecture with the proposed\nmethod and improve the state-of-the-art accuracy by 4.4% on the large scale NTU\nhuman activity dataset. On the medium-sized N-UCLA and UTH-MHAD datasets, our\nmethod outperforms the existing results by 5.7% and 9.3% respectively.\n",
    "topics": "{'Action Recognition': 1.0, 'Temporal Action Localization': 0.9999999, 'Skeleton Based Action Recognition': 0.9932869, '3D Action Recognition': 0.6707951}",
    "score": 0.8125691946
  },
  {
    "id": "1911.04699",
    "title": "Deep Generative Models Strike Back! Improving Understanding and\n  Evaluation in Light of Unmet Expectations for OoD Data",
    "abstract": "  Advances in deep generative and density models have shown impressive capacity\nto model complex probability density functions in lower-dimensional space.\nAlso, applying such models to high-dimensional image data to model the PDF has\nshown poor generalization, with out-of-distribution data being assigned equal\nor higher likelihood than in-sample data. Methods to deal with this have been\nproposed that deviate from a fully unsupervised approach, requiring large\nensembles or additional knowledge about the data, not commonly available in the\nreal-world. In this work, the previously offered reasoning behind these issues\nis challenged empirically, and it is shown that data-sets such as MNIST\nfashion/digits and CIFAR10/SVHN are trivially separable and have no overlap on\ntheir respective data manifolds that explains the higher OoD likelihood. Models\nlike masked autoregressive flows and block neural autoregressive flows are\nshown to not suffer from OoD likelihood issues to the extent of GLOW,\nPixelCNN++, and real NVP. A new avenue is also explored which involves a change\nof basis to a new space of the same dimension with an orthonormal unitary basis\nof eigenvectors before modeling. In the test data-sets and models, this aids in\npushing down the relative likelihood of the contrastive OoD data set and\nimprove discrimination results. The significance of the density of the original\nspace is maintained, while invertibility remains tractable. Finally, a look to\nthe previous generation of generative models in the form of probabilistic\nprincipal component analysis is inspired, and revisited for the same data-sets\nand shown to work really well for discriminating anomalies based on likelihood\nin a fully unsupervised fashion compared with pixelCNN++, GLOW, and real NVP\nwith less complexity and faster training. Also, dimensionality reduction using\nPCA is shown to improve anomaly detection in generative models.\n",
    "topics": "{'Anomaly Detection': 0.99994874, 'Dimensionality Reduction': 0.99614894}",
    "score": 0.8125204545
  },
  {
    "id": "2002.02143",
    "title": "Pose-Aware Instance Segmentation Framework from Cone Beam CT Images for\n  Tooth Segmentation",
    "abstract": "  Individual tooth segmentation from cone beam computed tomography (CBCT)\nimages is an essential prerequisite for an anatomical understanding of\northodontic structures in several applications, such as tooth reformation\nplanning and implant guide simulations. However, the presence of severe metal\nartifacts in CBCT images hinders the accurate segmentation of each individual\ntooth. In this study, we propose a neural network for pixel-wise labeling to\nexploit an instance segmentation framework that is robust to metal artifacts.\nOur method comprises of three steps: 1) image cropping and realignment by pose\nregressions, 2) metal-robust individual tooth detection, and 3) segmentation.\nWe first extract the alignment information of the patient by pose regression\nneural networks to attain a volume-of-interest (VOI) region and realign the\ninput image, which reduces the inter-overlapping area between tooth bounding\nboxes. Then, individual tooth regions are localized within a VOI realigned\nimage using a convolutional detector. We improved the accuracy of the detector\nby employing non-maximum suppression and multiclass classification metrics in\nthe region proposal network. Finally, we apply a convolutional neural network\n(CNN) to perform individual tooth segmentation by converting the pixel-wise\nlabeling task to a distance regression task. Metal-intensive image augmentation\nis also employed for a robust segmentation of metal artifacts. The result shows\nthat our proposed method outperforms other state-of-the-art methods, especially\nfor teeth with metal artifacts. The primary significance of the proposed method\nis two-fold: 1) an introduction of pose-aware VOI realignment followed by a\nrobust tooth detection and 2) a metal-robust CNN framework for accurate tooth\nsegmentation.\n",
    "topics": "{'Region Proposal': 1.0, 'Instance Segmentation': 1.0, 'Image Augmentation': 0.9991911, 'Semantic Segmentation': 0.9969021}",
    "score": 0.8124887567
  },
  {
    "id": "1808.01097",
    "title": "CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images",
    "abstract": "  We present a simple yet efficient approach capable of training deep neural\nnetworks on large-scale weakly-supervised web images, which are crawled raw\nfrom the Internet by using text queries, without any human annotation. We\ndevelop a principled learning strategy by leveraging curriculum learning, with\nthe goal of handling a massive amount of noisy labels and data imbalance\neffectively. We design a new learning curriculum by measuring the complexity of\ndata using its distribution density in a feature space, and rank the complexity\nin an unsupervised manner. This allows for an efficient implementation of\ncurriculum learning on large-scale web images, resulting in a high-performance\nCNN model, where the negative impact of noisy labels is reduced substantially.\nImportantly, we show by experiments that those images with highly noisy labels\ncan surprisingly improve the generalization capability of the model, by serving\nas a manner of regularization. Our approaches obtain state-of-the-art\nperformance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101.\nWith an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on\nthe WebVision challenge for 1000-category classification. This result was the\ntop performance by a wide margin, outperforming second place by a nearly 50%\nrelative error rate. Code and models are available at:\nhttps://github.com/MalongTech/CurriculumNet .\n",
    "topics": "{'Curriculum Learning': 1.0}",
    "score": 0.8124841913
  },
  {
    "id": "2004.03042",
    "title": "COVID-MobileXpert: On-Device COVID-19 Patient Triage and Follow-up using\n  Chest X-rays",
    "abstract": "  During the COVID-19 pandemic, there has been an emerging need for rapid,\ndedicated, and point-of-care COVID-19 patient disposition techniques to\noptimize resource utilization and clinical workflow. In view of this need, we\npresent COVID-MobileXpert: a lightweight deep neural network (DNN) based mobile\napp that can use chest X-ray (CXR) for COVID-19 case screening and radiological\ntrajectory prediction. We design and implement a novel three-player knowledge\ntransfer and distillation (KTD) framework including a pre-trained attending\nphysician (AP) network that extracts CXR imaging features from a large scale of\nlung disease CXR images, a fine-tuned resident fellow (RF) network that learns\nthe essential CXR imaging features to discriminate COVID-19 from pneumonia\nand/or normal cases with a small amount of COVID-19 cases, and a trained\nlightweight medical student (MS) network to perform on-device COVID-19 patient\ntriage and follow-up. To tackle the challenge of vastly similar and dominant\nfore- and background in medical images, we employ novel loss functions and\ntraining schemes for the MS network to learn the robust features. We\ndemonstrate the significant potential of COVID-MobileXpert for rapid deployment\nvia extensive experiments with diverse MS architecture and tuning parameter\nsettings. The source codes for cloud and mobile based models are available from\nthe following url: https://github.com/xinli0928/COVID-Xray.\n",
    "topics": "{'Transfer Learning': 0.994128, 'Computed Tomography (CT)': 0.94941443, 'Trajectory Prediction': 0.7587221}",
    "score": 0.8124587202
  },
  {
    "id": "2006.05220",
    "title": "Rethinking Localization Map: Towards Accurate Object Perception with\n  Self-Enhancement Maps",
    "abstract": "  Recently, remarkable progress has been made in weakly supervised object\nlocalization (WSOL) to promote object localization maps. The common practice of\nevaluating these maps applies an indirect and coarse way, i.e., obtaining tight\nbounding boxes which can cover high-activation regions and calculating\nintersection-over-union (IoU) scores between the predicted and ground-truth\nboxes. This measurement can evaluate the ability of localization maps to some\nextent, but we argue that the maps should be measured directly and delicately,\ni.e., comparing the maps with the ground-truth object masks pixel-wisely. To\nfulfill the direct evaluation, we annotate pixel-level object masks on the\nILSVRC validation set. We propose to use IoU-Threshold curves for evaluating\nthe real quality of localization maps. Beyond the amended evaluation metric and\nannotated object masks, this work also introduces a novel self-enhancement\nmethod to harvest accurate object localization maps and object boundaries with\nonly category labels as supervision. We propose a two-stage approach to\ngenerate the localization maps by simply comparing the similarity of point-wise\nfeatures between the high-activation and the rest pixels. Based on the\npredicted localization maps, we explore to estimate object boundaries on a very\nlarge dataset. A hard-negative suppression loss is proposed for obtaining fine\nboundaries. We conduct extensive experiments on the ILSVRC and CUB benchmarks.\nIn particular, the proposed Self-Enhancement Maps achieve the state-of-the-art\nlocalization accuracy of 54.88% on ILSVRC. The code and the annotated masks are\nreleased at https://github.com/xiaomengyc/SEM.\n",
    "topics": "{'Object Localization': 1.0, 'Weakly-Supervised Object Localization': 0.99999726}",
    "score": 0.8124390788
  },
  {
    "id": "2010.05495",
    "title": "Increasing the Robustness of Semantic Segmentation Models with\n  Painting-by-Numbers",
    "abstract": "  For safety-critical applications such as autonomous driving, CNNs have to be\nrobust with respect to unavoidable image corruptions, such as image noise.\nWhile previous works addressed the task of robust prediction in the context of\nfull-image classification, we consider it for dense semantic segmentation. We\nbuild upon an insight from image classification that output robustness can be\nimproved by increasing the network-bias towards object shapes. We present a new\ntraining schema that increases this shape bias. Our basic idea is to\nalpha-blend a portion of the RGB training images with faked images, where each\nclass-label is given a fixed, randomly chosen color that is not likely to\nappear in real imagery. This forces the network to rely more strongly on shape\ncues. We call this data augmentation technique ``Painting-by-Numbers''. We\ndemonstrate the effectiveness of our training schema for DeepLabv3+ with\nvarious network backbones, MobileNet-V2, ResNets, and Xception, and evaluate it\non the Cityscapes dataset. With respect to our 16 different types of image\ncorruptions and 5 different network backbones, we are in 74% better than\ntraining with clean data. For cases where we are worse than a model trained\nwithout our training schema, it is mostly only marginally worse. However, for\nsome image corruptions such as images with noise, we see a considerable\nperformance gain of up to 25%.\n",
    "topics": "{'Autonomous Driving': 0.9999262, 'Data Augmentation': 0.99747187, 'Semantic Segmentation': 0.99651176, 'Image Classification': 0.97139156}",
    "score": 0.8123846959
  },
  {
    "id": "1907.01062",
    "title": "DeepTEGINN: Deep Learning Based Tools to Extract Graphs from Images of\n  Neural Networks",
    "abstract": "  In the brain, the structure of a network of neurons defines how these neurons\nimplement the computations that underlie the mind and the behavior of animals\nand humans. Provided that we can describe the network of neurons as a graph, we\ncan employ methods from graph theory to investigate its structure or use\ncellular automata to mathematically assess its function. Although, software for\nthe analysis of graphs and cellular automata are widely available. Graph\nextraction from the image of networks of brain cells remains difficult. Nervous\ntissue is heterogeneous, and differences in anatomy may reflect relevant\ndifferences in function. Here we introduce a deep learning based toolbox to\nextracts graphs from images of brain tissue. This toolbox provides an\neasy-to-use framework allowing system neuroscientists to generate graphs based\non images of brain tissue by combining methods from image processing, deep\nlearning, and graph theory. The goals are to simplify the training and usage of\ndeep learning methods for computer vision and facilitate its integration into\ngraph extraction pipelines. In this way, the toolbox provides an alternative to\nthe required laborious manual process of tracing, sorting and classifying. We\nexpect to democratize the machine learning methods to a wider community of\nusers beyond the computer vision experts and improve the time-efficiency of\ngraph extraction from large brain image datasets, which may lead to further\nunderstanding of the human mind.\n",
    "topics": "{}",
    "score": 0.8122616818
  },
  {
    "id": "1811.03447",
    "title": "Microscopic Nuclei Classification, Segmentation and Detection with\n  improved Deep Convolutional Neural Network (DCNN) Approaches",
    "abstract": "  Due to cellular heterogeneity, cell nuclei classification, segmentation, and\ndetection from pathological images are challenging tasks. In the last few\nyears, Deep Convolutional Neural Networks (DCNN) approaches have been shown\nstate-of-the-art (SOTA) performance on histopathological imaging in different\nstudies. In this work, we have proposed different advanced DCNN models and\nevaluated for nuclei classification, segmentation, and detection. First, the\nDensely Connected Recurrent Convolutional Network (DCRN) model is used for\nnuclei classification. Second, Recurrent Residual U-Net (R2U-Net) is applied\nfor nuclei segmentation. Third, the R2U-Net regression model which is named\nUD-Net is used for nuclei detection from pathological images. The experiments\nare conducted with different datasets including Routine Colon Cancer(RCC)\nclassification and detection dataset, and Nuclei Segmentation Challenge 2018\ndataset. The experimental results show that the proposed DCNN models provide\nsuperior performance compared to the existing approaches for nuclei\nclassification, segmentation, and detection tasks. The results are evaluated\nwith different performance metrics including precision, recall, Dice\nCoefficient (DC), Means Squared Errors (MSE), F1-score, and overall accuracy.\nWe have achieved around 3.4% and 4.5% better F-1 score for nuclei\nclassification and detection tasks compared to recently published DCNN based\nmethod. In addition, R2U-Net shows around 92.15% testing accuracy in term of\nDC. These improved methods will help for pathological practices for better\nquantitative analysis of nuclei in Whole Slide Images(WSI) which ultimately\nwill help for better understanding of different types of cancer in clinical\nworkflow.\n",
    "topics": "{'whole slide images': 0.9999589}",
    "score": 0.8122612748
  },
  {
    "id": "1712.03541",
    "title": "An Architecture Combining Convolutional Neural Network (CNN) and Support\n  Vector Machine (SVM) for Image Classification",
    "abstract": "  Convolutional neural networks (CNNs) are similar to \"ordinary\" neural\nnetworks in the sense that they are made up of hidden layers consisting of\nneurons with \"learnable\" parameters. These neurons receive inputs, performs a\ndot product, and then follows it with a non-linearity. The whole network\nexpresses the mapping between raw image pixels and their class scores.\nConventionally, the Softmax function is the classifier used at the last layer\nof this network. However, there have been studies (Alalshekmubarak and Smith,\n2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited\nstudies introduce the usage of linear support vector machine (SVM) in an\nartificial neural network architecture. This project is yet another take on the\nsubject, and is inspired by (Tang, 2013). Empirical data has shown that the\nCNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST\ndataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax\nwas able to achieve a test accuracy of ~99.23% using the same dataset. Both\nmodels were also tested on the recently-published Fashion-MNIST dataset (Xiao,\nRasul, and Vollgraf, 2017), which is suppose to be a more difficult image\nclassification dataset than MNIST (Zalandoresearch, 2017). This proved to be\nthe case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax\nreached a test accuracy of ~91.86%. The said results may be improved if data\npreprocessing techniques were employed on the datasets, and if the base CNN\nmodel was a relatively more sophisticated than the one used in this study.\n",
    "topics": "{'Image Classification': 0.9999087}",
    "score": 0.8122336906
  },
  {
    "id": "1908.09067",
    "title": "Plexus Convolutional Neural Network (PlexusNet): A novel neural network\n  architecture for histologic image analysis",
    "abstract": "  Different convolutional neural network (CNN) models have been tested for\ntheir application in histological image analyses. However, these models are\nprone to overfitting due to their large parameter capacity, requiring more data\nor valuable computational resources for model training. Given these\nlimitations, we introduced a novel architecture (termed PlexusNet). We utilized\n310 Hematoxylin and Eosin stained (H&E) annotated histological images of\nprostate cancer cases from TCGA-PRAD and Stanford University and 398 H&E whole\nslides images from the Camelyon 2016 challenge. PlexusNet-architecture -derived\nmodels were compared to models derived from several existing \"state of the art\"\narchitectures. We measured discrimination accuracy, calibration, and clinical\nutility. An ablation study was conducted to study the effect of each component\nof PlexusNet on model performance. A well-fitted PlexusNet-based model\ndelivered comparable classification performance (AUC: 0.963) in distinguishing\nprostate cancer from healthy tissues, although it was at least 23 times\nsmaller, had a better model calibration and clinical utility than the\ncomparison models. A separate smaller PlexusNet model accurately detected\nslides with breast cancer metastases (AUC: 0.978); it helped reduce the slide\nnumber to examine by 43.8% without consequences, although its parameter\ncapacity was 200 times smaller than ResNet18. We found that the partitioning of\nthe development set influences the model calibration for all models. However,\nwith PlexusNet architecture, we could achieve comparable well-calibrated models\ntrained on different partitions. In conclusion, PlexusNet represents a novel\nmodel architecture for histological image analysis that achieves classification\nperformance comparable to other models while providing orders-of-magnitude\nparameter reduction.\n",
    "topics": "{'whole slide images': 0.6922319}",
    "score": 0.8119729999
  },
  {
    "id": "1711.04291",
    "title": "Scale out for large minibatch SGD: Residual network training on\n  ImageNet-1K with improved accuracy and reduced time to train",
    "abstract": "  For the past 5 years, the ILSVRC competition and the ImageNet dataset have\nattracted a lot of interest from the Computer Vision community, allowing for\nstate-of-the-art accuracy to grow tremendously. This should be credited to the\nuse of deep artificial neural network designs. As these became more complex,\nthe storage, bandwidth, and compute requirements increased. This means that\nwith a non-distributed approach, even when using the most high-density server\navailable, the training process may take weeks, making it prohibitive.\nFurthermore, as datasets grow, the representation learning potential of deep\nnetworks grows as well by using more complex models. This synchronicity\ntriggers a sharp increase in the computational requirements and motivates us to\nexplore the scaling behaviour on petaflop scale supercomputers. In this paper\nwe will describe the challenges and novel solutions needed in order to train\nResNet-50 in this large scale environment. We demonstrate above 90\\% scaling\nefficiency and a training time of 28 minutes using up to 104K x86 cores. This\nis supported by software tools from Intel's ecosystem. Moreover, we show that\nwith regular 90 - 120 epoch train runs we can achieve a top-1 accuracy as high\nas 77\\% for the unmodified ResNet-50 topology. We also introduce the novel\nCollapsed Ensemble (CE) technique that allows us to obtain a 77.5\\% top-1\naccuracy, similar to that of a ResNet-152, while training a unmodified\nResNet-50 topology for the same fixed training budget. All ResNet-50 models as\nwell as the scripts needed to replicate them will be posted shortly.\n",
    "topics": "{'Representation Learning': 0.97997427}",
    "score": 0.8117750541
  },
  {
    "id": "1909.12874",
    "title": "Geomorphological Analysis Using Unpiloted Aircraft Systems, Structure\n  from Motion, and Deep Learning",
    "abstract": "  We present a pipeline for geomorphological analysis that uses structure from\nmotion (SfM) and deep learning on close-range aerial imagery to estimate\nspatial distributions of rock traits (size, roundness, and orientation) along a\ntectonic fault scarp. The properties of the rocks on the fault scarp derive\nfrom the combination of initial volcanic fracturing and subsequent tectonic and\ngeomorphic fracturing, and our pipeline allows scientists to leverage UAS-based\nimagery to gain a better understanding of such surface processes. We start by\nusing SfM on aerial imagery to produce georeferenced orthomosaics and digital\nelevation models (DEM). A human expert then annotates rocks on a set of image\ntiles sampled from the orthomosaics, and these annotations are used to train a\ndeep neural network to detect and segment individual rocks in the entire site.\nThe extracted semantic information (rock masks) on large volumes of unlabeled,\nhigh-resolution SfM products allows subsequent structural analysis and shape\ndescriptors to estimate rock size, roundness, and orientation. We present\nresults of two experiments conducted along a fault scarp in the Volcanic\nTablelands near Bishop, California. We conducted the first, proof-of-concept\nexperiment with a DJI Phantom 4 Pro equipped with an RGB camera and inspected\nif elevation information assisted instance segmentation from RGB channels.\nRock-trait histograms along and across the fault scarp were obtained with the\nneural network inference. In the second experiment, we deployed a hexrotor and\na multispectral camera to produce a DEM and five spectral orthomosaics in red,\ngreen, blue, red edge, and near infrared. We focused on examining the\neffectiveness of different combinations of input channels in instance\nsegmentation.\n",
    "topics": "{'Instance Segmentation': 1.0, 'Semantic Segmentation': 0.99763453, 'Morphological Analysis': 0.6242493}",
    "score": 0.8117557001
  },
  {
    "id": "1908.05024",
    "title": "Person Re-identification in Aerial Imagery",
    "abstract": "  Nowadays, with the rapid development of consumer Unmanned Aerial Vehicles\n(UAVs), visual surveillance by utilizing the UAV platform has been very\nattractive. Most of the research works for UAV captured visual data are mainly\nfocused on the tasks of object detection and tracking. However, limited\nattention has been paid to the task of person Re-identification (ReID) which\nhas been widely studied in ordinary surveillance cameras with fixed\nemplacements. In this paper, to facilitate the research of person ReID in\naerial imagery, we collect a large scale airborne person ReID dataset named as\nPerson ReID for Aerial Imagery (PRAI-1581), which consists of 39,461 images of\n1581 person identities. The images of the dataset are shot by two DJI consumer\nUAVs flying at an altitude ranging from 20 to 60 meters above the ground, which\ncovers most of the real UAV surveillance scenarios. In addition, we propose to\nutilize subspace pooling of convolution feature maps to represent the input\nperson images. Our method can learn a discriminative and compact feature\nrepresentation for ReID in aerial imagery and can be trained in an end-to-end\nfashion efficiently. We conduct extensive experiments on the proposed dataset\nand the experimental results demonstrate that re-identify persons in aerial\nimagery is a challenging problem, where our method performs favorably against\nstate of the arts. Our dataset can be accessed via\n\\url{https://github.com/stormyoung/PRAI-1581}.\n",
    "topics": "{'Person Re-Identification': 0.99245644}",
    "score": 0.8116997315
  },
  {
    "id": "2003.02790",
    "title": "Event-Based Angular Velocity Regression with Spiking Networks",
    "abstract": "  Spiking Neural Networks (SNNs) are bio-inspired networks that process\ninformation conveyed as temporal spikes rather than numeric values. A spiking\nneuron of an SNN only produces a spike whenever a significant number of spikes\noccur within a short period of time. Due to their spike-based computational\nmodel, SNNs can process output from event-based, asynchronous sensors without\nany pre-processing at extremely lower power unlike standard artificial neural\nnetworks. This is possible due to specialized neuromorphic hardware that\nimplements the highly-parallelizable concept of SNNs in silicon. Yet, SNNs have\nnot enjoyed the same rise of popularity as artificial neural networks. This not\nonly stems from the fact that their input format is rather unconventional but\nalso due to the challenges in training spiking networks. Despite their temporal\nnature and recent algorithmic advances, they have been mostly evaluated on\nclassification problems. We propose, for the first time, a temporal regression\nproblem of numerical values given events from an event camera. We specifically\ninvestigate the prediction of the 3-DOF angular velocity of a rotating event\ncamera with an SNN. The difficulty of this problem arises from the prediction\nof angular velocities continuously in time directly from irregular,\nasynchronous event-based input. Directly utilising the output of event cameras\nwithout any pre-processing ensures that we inherit all the benefits that they\nprovide over conventional cameras. That is high-temporal resolution,\nhigh-dynamic range and no motion blur. To assess the performance of SNNs on\nthis task, we introduce a synthetic event camera dataset generated from\nreal-world panoramic images and show that we can successfully train an SNN to\nperform angular velocity regression.\n",
    "topics": "{}",
    "score": 0.8115404419
  },
  {
    "id": "1902.09094",
    "title": "DRAMNet: Authentication based on Physical Unique Features of DRAM Using\n  Deep Convolutional Neural Networks",
    "abstract": "  Nowadays, there is an increasing interest in the development of Autonomous\nVehicles (AV). However, there are two types of attack challenges that can\naffect AVs and are yet to be resolved, i.e., sensor attacks and vehicle access\nattacks. This paper, to the best of our knowledge, is the first work that\nproposes a novel authentication scheme involving DRAM power-up unique features\nusing deep Convolutional Neural Network (CNN), which can be used to implement\nsecure access control of autonomous vehicles. Our approach consists of two\nparts. First, we convert raw power-up sequence data from DRAM cells into a\ntwo-dimensional (2D) format to generate a DRAM image structure. Second, we\napply deep CNN to DRAM images, in order to extract unique features from each\nmemory to classify them for authentication. To evaluate our proposed approach,\nwe utilize data from three Commercial-Off-The-Shelf (COTS) DRAMs taken under\nvarious environmental and other conditions (high/low temperature, high/low\nsupply voltage and aging effects). Based on our results, our proposed\nauthentication method ``DRAMNet'' achieves 98.63% accuracy and 98.49%\nprecision. In comparison to other state-of-the-art CNN architectures, such as\nthe AlexNet and VGGNet models, our DRAMNet approach fares equally well or\nbetter than them.\n",
    "topics": "{'Autonomous Vehicles': 1.0}",
    "score": 0.8114785553
  },
  {
    "id": "1909.07113",
    "title": "TextSR: Content-Aware Text Super-Resolution Guided by Recognition",
    "abstract": "  Scene text recognition has witnessed rapid development with the advance of\nconvolutional neural networks. Nonetheless, most of the previous methods may\nnot work well in recognizing text with low resolution which is often seen in\nnatural scene images. An intuitive solution is to introduce super-resolution\ntechniques as pre-processing. However, conventional super-resolution methods in\nthe literature mainly focus on reconstructing the detailed texture of natural\nimages, which typically do not work well for text due to the unique\ncharacteristics of text. To tackle these problems, in this work, we propose a\ncontent-aware text super-resolution network to generate the information desired\nfor text recognition. In particular, we design an end-to-end network that can\nperform super-resolution and text recognition simultaneously. Different from\nprevious super-resolution methods, we use the loss of text recognition as the\nText Perceptual Loss to guide the training of the super-resolution network, and\nthus it pays more attention to the text content, rather than the irrelevant\nbackground area. Extensive experiments on several challenging benchmarks\ndemonstrate the effectiveness of our proposed method in restoring a sharp\nhigh-resolution image from a small blurred one, and show that the recognition\nperformance clearly boosts up the performance of text recognizer. To our\nknowledge, this is the first work focusing on text super-resolution. Code will\nbe released in https://github.com/xieenze/TextSR.\n",
    "topics": "{'Super-Resolution': 1.0, 'Super Resolution': 1.0, 'Scene Text Recognition': 0.99998534, 'Scene Text': 0.9999231}",
    "score": 0.8114780011
  },
  {
    "id": "1811.01609",
    "title": "ConvS2S-VC: Fully convolutional sequence-to-sequence voice conversion",
    "abstract": "  This paper proposes a voice conversion (VC) method using sequence-to-sequence\n(seq2seq or S2S) learning, which flexibly converts not only the voice\ncharacteristics but also the pitch contour and duration of input speech. The\nproposed method, called ConvS2S-VC, has three key features. First, it uses a\nmodel with a fully convolutional architecture. This is particularly\nadvantageous in that it is suitable for parallel computations using GPUs. It is\nalso beneficial since it enables effective normalization techniques such as\nbatch normalization to be used for all the hidden layers in the networks.\nSecond, it achieves many-to-many conversion by simultaneously learning mappings\namong multiple speakers using only a single model instead of separately\nlearning mappings between each speaker pair using a different model. This\nenables the model to fully utilize available training data collected from\nmultiple speakers by capturing common latent features that can be shared across\ndifferent speakers. Owing to this structure, our model works reasonably well\neven without source speaker information, thus making it able to handle\nany-to-many conversion tasks. Third, we introduce a mechanism, called the\nconditional batch normalization that switches batch normalization layers in\naccordance with the target speaker. This particular mechanism has been found to\nbe extremely effective for our many-to-many conversion model. We conducted\nspeaker identity conversion experiments and found that ConvS2S-VC obtained\nhigher sound quality and speaker similarity than baseline methods. We also\nfound from audio examples that it could perform well in various tasks including\nemotional expression conversion, electrolaryngeal speech enhancement, and\nEnglish accent conversion.\n",
    "topics": "{'Voice Conversion': 1.0, 'Speech Enhancement': 0.74085677}",
    "score": 0.8114150122
  },
  {
    "id": "1707.08149",
    "title": "Patch-based Carcinoma Detection on Confocal Laser Endomicroscopy Images\n  -- A Cross-Site Robustness Assessment",
    "abstract": "  Deep learning technologies such as convolutional neural networks (CNN)\nprovide powerful methods for image recognition and have recently been employed\nin the field of automated carcinoma detection in confocal laser endomicroscopy\n(CLE) images. CLE is a (sub-)surface microscopic imaging technique that reaches\nmagnifications of up to 1000x and is thus suitable for in vivo structural\ntissue analysis. In this work, we aim to evaluate the prospects of a priorly\ndeveloped deep learning-based algorithm targeted at the identification of oral\nsquamous cell carcinoma with regard to its generalization to further anatomic\nlocations of squamous cell carcinomas in the area of head and neck. We applied\nthe algorithm on images acquired from the vocal fold area of five patients with\nhistologically verified squamous cell carcinoma and presumably healthy control\nimages of the clinically normal contra-lateral vocal cord. We find that the\nnetwork trained on the oral cavity data reaches an accuracy of 89.45% and an\narea-under-the-curve (AUC) value of 0.955, when applied on the vocal cords\ndata. Compared to the state of the art, we achieve very similar results, yet\nwith an algorithm that was trained on a completely disjunct data set.\nConcatenating both data sets yielded further improvements in cross-validation\nwith an accuracy of 90.81% and AUC of 0.970. In this study, for the first time\nto our knowledge, a deep learning mechanism for the identification of oral\ncarcinomas using CLE Images could be applied to other disciplines in the area\nof head and neck. This study shows the prospect of the algorithmic approach to\ngeneralize well on other malignant entities of the head and neck, regardless of\nthe anatomical location and furthermore in an examiner-independent manner.\n",
    "topics": "{}",
    "score": 0.8113918085
  },
  {
    "id": "1905.12305",
    "title": "Fusion of Heterogeneous Earth Observation Data for the Classification of\n  Local Climate Zones",
    "abstract": "  This paper proposes a novel framework for fusing multi-temporal,\nmultispectral satellite images and OpenStreetMap (OSM) data for the\nclassification of local climate zones (LCZs). Feature stacking is the most\ncommonly-used method of data fusion but does not consider the heterogeneity of\nmultimodal optical images and OSM data, which becomes its main drawback. The\nproposed framework processes two data sources separately and then combines them\nat the model level through two fusion models (the landuse fusion model and\nbuilding fusion model), which aim to fuse optical images with landuse and\nbuildings layers of OSM data, respectively. In addition, a new approach to\ndetecting building incompleteness of OSM data is proposed. The proposed\nframework was trained and tested using data from the 2017 IEEE GRSS Data Fusion\nContest, and further validated on one additional test set containing test\nsamples which are manually labeled in Munich and New York. Experimental results\nhave indicated that compared to the feature stacking-based baseline framework\nthe proposed framework is effective in fusing optical images with OSM data for\nthe classification of LCZs with high generalization capability on a large\nscale. The classification accuracy of the proposed framework outperforms the\nbaseline framework by more than 6% and 2%, while testing on the test set of\n2017 IEEE GRSS Data Fusion Contest and the additional test set, respectively.\nIn addition, the proposed framework is less sensitive to spectral diversities\nof optical satellite images and thus achieves more stable classification\nperformance than state-of-the art frameworks.\n",
    "topics": "{}",
    "score": 0.8113788488
  },
  {
    "id": "1905.12439",
    "title": "Towards robust audio spoofing detection: a detailed comparison of\n  traditional and learned features",
    "abstract": "  Automatic speaker verification, like every other biometric system, is\nvulnerable to spoofing attacks. Using only a few minutes of recorded voice of a\ngenuine client of a speaker verification system, attackers can develop a\nvariety of spoofing attacks that might trick such systems. Detecting these\nattacks using the audio cues present in the recordings is an important\nchallenge. Most existing spoofing detection systems depend on knowing the used\nspoofing technique. With this research, we aim at overcoming this limitation,\nby examining robust audio features, both traditional and those learned through\nan autoencoder, that are generalizable over different types of replay spoofing.\nFurthermore, we provide a detailed account of all the steps necessary in\nsetting up state-of-the-art audio feature detection, pre-, and postprocessing,\nsuch that the (non-audio expert) machine learning researcher can implement such\nsystems. Finally, we evaluate the performance of our robust replay speaker\ndetection system with a wide variety and different combinations of both\nextracted and machine learned audio features on the `out in the wild' ASVspoof\n2017 dataset. This dataset contains a variety of new spoofing configurations.\nSince our focus is on examining which features will ensure robustness, we base\nour system on a traditional Gaussian Mixture Model-Universal Background Model.\nWe then systematically investigate the relative contribution of each feature\nset. The fused models, based on both the known audio features and the machine\nlearned features respectively, have a comparable performance with an Equal\nError Rate (EER) of 12. The final best performing model, which obtains an EER\nof 10.8, is a hybrid model that contains both known and machine learned\nfeatures, thus revealing the importance of incorporating both types of features\nwhen developing a robust spoofing prediction model.\n",
    "topics": "{'Speaker Verification': 1.0}",
    "score": 0.8113626733
  },
  {
    "id": "1902.04236",
    "title": "RespNet: A deep learning model for extraction of respiration from\n  photoplethysmogram",
    "abstract": "  Respiratory ailments afflict a wide range of people and manifests itself\nthrough conditions like asthma and sleep apnea. Continuous monitoring of\nchronic respiratory ailments is seldom used outside the intensive care ward due\nto the large size and cost of the monitoring system. While Electrocardiogram\n(ECG) based respiration extraction is a validated approach, its adoption is\nlimited by access to a suitable continuous ECG monitor. Recently, due to the\nwidespread adoption of wearable smartwatches with in-built Photoplethysmogram\n(PPG) sensor, it is being considered as a viable candidate for continuous and\nunobtrusive respiration monitoring. Research in this domain, however, has been\npredominantly focussed on estimating respiration rate from PPG. In this work, a\nnovel end-to-end deep learning network called RespNet is proposed to perform\nthe task of extracting the respiration signal from a given input PPG as opposed\nto extracting respiration rate. The proposed network was trained and tested on\ntwo different datasets utilizing different modalities of reference respiration\nsignal recordings. Also, the similarity and performance of the proposed network\nagainst two conventional signal processing approaches for extracting\nrespiration signal were studied. The proposed method was tested on two\nindependent datasets with a Mean Squared Error of 0.262 and 0.145. The\nCross-Correlation coefficient of the respective datasets were found to be 0.933\nand 0.931. The reported errors and similarity was found to be better than\nconventional approaches. The proposed approach would aid clinicians to provide\ncomprehensive evaluation of sleep-related respiratory conditions and chronic\nrespiratory ailments while being comfortable and inexpensive for the patient.\n",
    "topics": "{}",
    "score": 0.811303734
  },
  {
    "id": "1807.09193",
    "title": "GRAINS: Generative Recursive Autoencoders for INdoor Scenes",
    "abstract": "  We present a generative neural network which enables us to generate plausible\n3D indoor scenes in large quantities and varieties, easily and highly\nefficiently. Our key observation is that indoor scene structures are inherently\nhierarchical. Hence, our network is not convolutional; it is a recursive neural\nnetwork or RvNN. Using a dataset of annotated scene hierarchies, we train a\nvariational recursive autoencoder, or RvNN-VAE, which performs scene object\ngrouping during its encoding phase and scene generation during decoding.\nSpecifically, a set of encoders are recursively applied to group 3D objects\nbased on support, surround, and co-occurrence relations in a scene, encoding\ninformation about object spatial properties, semantics, and their relative\npositioning with respect to other objects in the hierarchy. By training a\nvariational autoencoder (VAE), the resulting fixed-length codes roughly follow\na Gaussian distribution. A novel 3D scene can be generated hierarchically by\nthe decoder from a randomly sampled code from the learned distribution. We coin\nour method GRAINS, for Generative Recursive Autoencoders for INdoor Scenes. We\ndemonstrate the capability of GRAINS to generate plausible and diverse 3D\nindoor scenes and compare with existing methods for 3D scene synthesis. We show\napplications of GRAINS including 3D scene modeling from 2D layouts, scene\nediting, and semantic scene segmentation via PointNet whose performance is\nboosted by the large quantity and variety of 3D scenes generated by our method.\n",
    "topics": "{'Scene Parsing': 0.98455566, 'Scene Segmentation': 0.92490876}",
    "score": 0.8112969334
  },
  {
    "id": "1904.00781",
    "title": "RILOD: Near Real-Time Incremental Learning for Object Detection at the\n  Edge",
    "abstract": "  Object detection models shipped with camera-equipped edge devices cannot\ncover the objects of interest for every user. Therefore, the incremental\nlearning capability is a critical feature for a robust and personalized object\ndetection system that many applications would rely on. In this paper, we\npresent an efficient yet practical system, RILOD, to incrementally train an\nexisting object detection model such that it can detect new object classes\nwithout losing its capability to detect old classes. The key component of RILOD\nis a novel incremental learning algorithm that trains end-to-end for one-stage\ndeep object detection models only using training data of new object classes.\nSpecifically to avoid catastrophic forgetting, the algorithm distills three\ntypes of knowledge from the old model to mimic the old model's behavior on\nobject classification, bounding box regression and feature extraction. In\naddition, since the training data for the new classes may not be available, a\nreal-time dataset construction pipeline is designed to collect training images\non-the-fly and automatically label the images with both category and bounding\nbox annotations. We have implemented RILOD under both edge-cloud and edge-only\nsetups. Experiment results show that the proposed system can learn to detect a\nnew object class in just a few minutes, including both dataset construction and\nmodel training. In comparison, traditional fine-tuning based method may take a\nfew hours for training, and in most cases would also need a tedious and costly\nmanual dataset labeling step.\n",
    "topics": "{'Object Detection': 1.0, 'Incremental Learning': 1.0, 'Object Classification': 0.99875915}",
    "score": 0.8112382731
  },
  {
    "id": "1709.05107",
    "title": "Multi-Label Zero-Shot Human Action Recognition via Joint Latent Ranking\n  Embedding",
    "abstract": "  Human action recognition refers to automatic recognizing human actions from a\nvideo clip. In reality, there often exist multiple human actions in a video\nstream. Such a video stream is often weakly-annotated with a set of relevant\nhuman action labels at a global level rather than assigning each label to a\nspecific video episode corresponding to a single action, which leads to a\nmulti-label learning problem. Furthermore, there are many meaningful human\nactions in reality but it would be extremely difficult to collect/annotate\nvideo clips regarding all of various human actions, which leads to a zero-shot\nlearning scenario. To the best of our knowledge, there is no work that has\naddressed all the above issues together in human action recognition. In this\npaper, we formulate a real-world human action recognition task as a multi-label\nzero-shot learning problem and propose a framework to tackle this problem in a\nholistic way. Our framework holistically tackles the issue of unknown temporal\nboundaries between different actions for multi-label learning and exploits the\nside information regarding the semantic relationship between different human\nactions for knowledge transfer. Consequently, our framework leads to a joint\nlatent ranking embedding for multi-label zero-shot human action recognition. A\nnovel neural architecture of two component models and an alternate learning\nalgorithm are proposed to carry out the joint latent ranking embedding\nlearning. Thus, multi-label zero-shot recognition is done by measuring\nrelatedness scores of action labels to a test video clip in the joint latent\nvisual and semantic embedding spaces. We evaluate our framework with different\nsettings, including a novel data split scheme designed especially for\nevaluating multi-label zero-shot learning, on two datasets: Breakfast and\nCharades. The experimental results demonstrate the effectiveness of our\nframework.\n",
    "topics": "{'Temporal Action Localization': 1.0, 'Multi-Label Learning': 1.0, 'Action Recognition': 1.0, 'Zero-Shot Learning': 0.9999974, 'Transfer Learning': 0.90498847}",
    "score": 0.8110139206
  },
  {
    "id": "2009.11705",
    "title": "Gated Res2Net for Multivariate Time Series Analysis",
    "abstract": "  Multivariate time series analysis is an important problem in data mining\nbecause of its widespread applications. With the increase of time series data\navailable for training, implementing deep neural networks in the field of time\nseries analysis is becoming common. Res2Net, a recently proposed backbone, can\nfurther improve the state-of-the-art networks as it improves the multi-scale\nrepresentation ability through connecting different groups of filters. However,\nRes2Net ignores the correlations of the feature maps and lacks the control on\nthe information interaction process. To address that problem, in this paper, we\npropose a backbone convolutional neural network based on the thought of gated\nmechanism and Res2Net, namely Gated Res2Net (GRes2Net), for multivariate time\nseries analysis. The hierarchical residual-like connections are influenced by\ngates whose values are calculated based on the original feature maps, the\nprevious output feature maps and the next input feature maps thus considering\nthe correlations between the feature maps more effectively. Through the\nutilization of gated mechanism, the network can control the process of\ninformation sending hence can better capture and utilize the both the temporal\ninformation and the correlations between the feature maps. We evaluate the\nGRes2Net on four multivariate time series datasets including two classification\ndatasets and two forecasting datasets. The results demonstrate that GRes2Net\nhave better performances over the state-of-the-art methods thus indicating the\nsuperiority\n",
    "topics": "{'Time Series Analysis': 1.0, 'Time Series': 0.99999833}",
    "score": 0.8109526513
  },
  {
    "id": "1807.09666",
    "title": "Person re-identification across different datasets with multi-task\n  learning",
    "abstract": "  This paper presents an approach to tackle the re-identification problem. This\nis a challenging problem due to the large variation of pose, illumination or\ncamera view. More and more datasets are available to train machine learning\nmodels for person re-identification. These datasets vary in conditions: cameras\nnumbers, camera positions, location, season, in size, i.e. number of images,\nnumber of different identities. Finally in labeling: there are datasets\nannotated with attributes while others are not. To deal with this variety of\ndatasets we present in this paper an approach to take information from\ndifferent datasets to build a system which performs well on all of them. Our\nmodel is based on a Convolutional Neural Network (CNN) and trained using\nmultitask learning. Several losses are used to extract the different\ninformation available in the different datasets. Our main task is learned with\na classification loss. To reduce the intra-class variation we experiment with\nthe center loss. Our paper ends with a performance evaluation in which we\ndiscuss the influence of the different losses on the global re-identification\nperformance. We show that with our method, we are able to build a system that\nperforms well on different datasets and simultaneously extracts attributes. We\nalso show that our system outperforms recent re-identification works on two\ndatasets.\n",
    "topics": "{'Person Re-Identification': 1.0, 'Multi-Task Learning': 0.99957126}",
    "score": 0.8109256521
  },
  {
    "id": "1807.00882",
    "title": "Deep convolutional encoder-decoder networks for uncertainty\n  quantification of dynamic multiphase flow in heterogeneous media",
    "abstract": "  Surrogate strategies are used widely for uncertainty quantification of\ngroundwater models in order to improve computational efficiency. However, their\napplication to dynamic multiphase flow problems is hindered by the curse of\ndimensionality, the saturation discontinuity due to capillarity effects, and\nthe time-dependence of the multi-output responses. In this paper, we propose a\ndeep convolutional encoder-decoder neural network methodology to tackle these\nissues. The surrogate modeling task is transformed to an image-to-image\nregression strategy. This approach extracts high-level coarse features from the\nhigh-dimensional input permeability images using an encoder, and then refines\nthe coarse features to provide the output pressure/saturation images through a\ndecoder. A training strategy combining a regression loss and a segmentation\nloss is proposed in order to better approximate the discontinuous saturation\nfield. To characterize the high-dimensional time-dependent outputs of the\ndynamic system, time is treated as an additional input to the network that is\ntrained using pairs of input realizations and of the corresponding system\noutputs at a limited number of time instances. The proposed method is evaluated\nusing a geological carbon storage process-based multiphase flow model with a\n2500-dimensional stochastic permeability field. With a relatively small number\nof training data, the surrogate model is capable of accurately characterizing\nthe spatio-temporal evolution of the pressure and discontinuous CO2 saturation\nfields and can be used efficiently to compute the statistics of the system\nresponses.\n",
    "topics": "{}",
    "score": 0.8108993295
  },
  {
    "id": "1909.12158",
    "title": "Fast and Effective Adaptation of Facial Action Unit Detection Deep Model",
    "abstract": "  Detecting facial action units (AU) is one of the fundamental steps in\nautomatic recognition of facial expression of emotions and cognitive states.\nThough there have been a variety of approaches proposed for this task, most of\nthese models are trained only for the specific target AUs, and as such they\nfail to easily adapt to the task of recognition of new AUs (i.e., those not\ninitially used to train the target models). In this paper, we propose a deep\nlearning approach for facial AU detection that can easily and in a fast manner\nadapt to a new AU or target subject by leveraging only a few labeled samples\nfrom the new task (either an AU or subject). To this end, we propose a modeling\napproach based on the notion of the model-agnostic meta-learning, originally\nproposed for the general image recognition/detection tasks (e.g., the character\nrecognition from the Omniglot dataset). Specifically, each subject and/or AU is\ntreated as a new learning task and the model learns to adapt based on the\nknowledge of the previous tasks (the AUs and subjects used to pre-train the\ntarget models). Thus, given a new subject or AU, this meta-knowledge (that is\nshared among training and test tasks) is used to adapt the model to the new\ntask using the notion of deep learning and model-agnostic meta-learning. We\nshow on two benchmark datasets (BP4D and DISFA) for facial AU detection that\nthe proposed approach can be easily adapted to new tasks (AUs/subjects). Using\nonly a few labeled examples from these tasks, the model achieves large\nimprovements over the baselines (i.e., non-adapted models).\n",
    "topics": "{'Meta-Learning': 0.9999583}",
    "score": 0.8108903773
  },
  {
    "id": "1906.01095",
    "title": "Robust Gaussian Process Regression for Real-Time High Precision GPS\n  Signal Enhancement",
    "abstract": "  Satellite-based positioning system such as GPS often suffers from large\namount of noise that degrades the positioning accuracy dramatically especially\nin real-time applications. In this work, we consider a data-mining approach to\nenhance the GPS signal. We build a large-scale high precision GPS receiver grid\nsystem to collect real-time GPS signals for training. The Gaussian Process (GP)\nregression is chosen to model the vertical Total Electron Content (vTEC)\ndistribution of the ionosphere of the Earth. Our experiments show that the\nnoise in the real-time GPS signals often exceeds the breakdown point of the\nconventional robust regression methods resulting in sub-optimal system\nperformance. We propose a three-step approach to address this challenge. In the\nfirst step we perform a set of signal validity tests to separate the signals\ninto clean and dirty groups. In the second step, we train an initial model on\nthe clean signals and then reweigting the dirty signals based on the residual\nerror. A final model is retrained on both the clean signals and the reweighted\ndirty signals. In the theoretical analysis, we prove that the proposed\nthree-step approach is able to tolerate much higher noise level than the\nvanilla robust regression methods if two reweighting rules are followed. We\nvalidate the superiority of the proposed method in our real-time high precision\npositioning system against several popular state-of-the-art robust regression\nmethods. Our method achieves centimeter positioning accuracy in the benchmark\nregion with probability $78.4\\%$ , outperforming the second best baseline\nmethod by a margin of $8.3\\%$. The benchmark takes 6 hours on 20,000 CPU cores\nor 14 years on a single CPU.\n",
    "topics": "{}",
    "score": 0.8108228736
  },
  {
    "id": "1704.00509",
    "title": "Truncating Wide Networks using Binary Tree Architectures",
    "abstract": "  Recent study shows that a wide deep network can obtain accuracy comparable to\na deeper but narrower network. Compared to narrower and deeper networks, wide\nnetworks employ relatively less number of layers and have various important\nbenefits, such that they have less running time on parallel computing devices,\nand they are less affected by gradient vanishing problems. However, the\nparameter size of a wide network can be very large due to use of large width of\neach layer in the network. In order to keep the benefits of wide networks\nmeanwhile improve the parameter size and accuracy trade-off of wide networks,\nwe propose a binary tree architecture to truncate architecture of wide networks\nby reducing the width of the networks. More precisely, in the proposed\narchitecture, the width is continuously reduced from lower layers to higher\nlayers in order to increase the expressive capacity of network with a less\nincrease on parameter size. Also, to ease the gradient vanishing problem,\nfeatures obtained at different layers are concatenated to form the output of\nour architecture. By employing the proposed architecture on a baseline wide\nnetwork, we can construct and train a new network with same depth but\nconsiderably less number of parameters. In our experimental analyses, we\nobserve that the proposed architecture enables us to obtain better parameter\nsize and accuracy trade-off compared to baseline networks using various\nbenchmark image classification datasets. The results show that our model can\ndecrease the classification error of baseline from 20.43% to 19.22% on\nCifar-100 using only 28% of parameters that baseline has. Code is available at\nhttps://github.com/ZhangVision/bitnet.\n",
    "topics": "{'Image Classification': 0.98912245}",
    "score": 0.8106581932
  },
  {
    "id": "2004.05085",
    "title": "Beyond Disentangled Representations: An Attentive Angular Distillation\n  Approach to Large-scale Lightweight Age-Invariant Face Recognition",
    "abstract": "  Disentangled representations have been commonly adopted to Age-invariant Face\nRecognition (AiFR) tasks. However, these methods have reached some limitations\nwith (1) the requirement of large-scale face recognition (FR) training data\nwith age labels, which is limited in practice; (2) heavy deep network\narchitecture for high performance; and (3) their evaluations are usually taken\nplace on age-related face databases while neglecting the standard large-scale\nFR databases to guarantee its robustness. This work presents a novel Attentive\nAngular Distillation (AAD) approach to Large-scale Lightweight AiFR that\novercomes these limitations. Given two high-performance heavy networks as\nteachers with different specialized knowledge, AAD introduces a learning\nparadigm to efficiently distill the age-invariant attentive and angular\nknowledge from those teachers to a lightweight student network making it more\npowerful with higher FR accuracy and robust against age factor. Consequently,\nAAD approach is able to take the advantages of both FR datasets with and\nwithout age labels to train an AiFR model. Far apart from prior distillation\nmethods mainly focusing on accuracy and compression ratios in closed-set\nproblems, our AAD aims to solve the open-set problem, i.e. large-scale face\nrecognition. Evaluations on LFW, IJB-B and IJB-C Janus, AgeDB and\nMegaFace-FGNet with one million distractors have demonstrated the efficiency of\nthe proposed approach. This work also presents a new longitudinal face aging\n(LogiFace) database for further studies in age-related facial problems in\nfuture.\n",
    "topics": "{'Face Recognition': 1.0}",
    "score": 0.8106446736
  },
  {
    "id": "1805.10445",
    "title": "Fine-Grained Age Estimation in the wild with Attention LSTM Networks",
    "abstract": "  Age estimation from a single face image has been an essential task in the\nfield of human-computer interaction and computer vision, which has a wide range\nof practical application values. Accuracy of age estimation of face images in\nthe wild is relatively low for existing methods, because they only take into\naccount the global features, while neglecting the fine-grained features of\nage-sensitive areas. We propose a novel method based on our attention long\nshort-term memory (AL) network for fine-grained age estimation in the wild,\ninspired by the fine-grained categories and the visual attention mechanism.\nThis method combines the residual networks (ResNets) or the residual network of\nresidual network (RoR) models with LSTM units to construct AL-ResNets or AL-RoR\nnetworks to extract local features of age-sensitive regions, which effectively\nimproves the age estimation accuracy. First, a ResNets or a RoR model\npretrained on ImageNet dataset is selected as the basic model, which is then\nfine-tuned on the IMDB-WIKI-101 dataset for age estimation. Then, we fine-tune\nthe ResNets or the RoR on the target age datasets to extract the global\nfeatures of face images. To extract the local features of age-sensitive\nregions, the LSTM unit is then presented to obtain the coordinates of the\nagesensitive region automatically. Finally, the age group classification is\nconducted directly on the Adience dataset, and age-regression experiments are\nperformed by the Deep EXpectation algorithm (DEX) on MORPH Album 2, FG-NET and\n15/16LAP datasets. By combining the global and the local features, we obtain\nour final prediction results. Experimental results illustrate the effectiveness\nand robustness of the proposed AL-ResNets or AL-RoR for age estimation in the\nwild, where it achieves better state-of-the-art performance than all other\nconvolutional neural network.\n",
    "topics": "{'Age Estimation': 0.9999975}",
    "score": 0.8105252234
  },
  {
    "id": "1812.03368",
    "title": "Unsupervised Learning of Monocular Depth Estimation with Bundle\n  Adjustment, Super-Resolution and Clip Loss",
    "abstract": "  We present a novel unsupervised learning framework for single view depth\nestimation using monocular videos. It is well known in 3D vision that enlarging\nthe baseline can increase the depth estimation accuracy, and jointly optimizing\na set of camera poses and landmarks is essential. In previous monocular\nunsupervised learning frameworks, only part of the photometric and geometric\nconstraints within a sequence are used as supervisory signals. This may result\nin a short baseline and overfitting. Besides, previous works generally estimate\na low resolution depth from a low resolution impute image. The low resolution\ndepth is then interpolated to recover the original resolution. This strategy\nmay generate large errors on object boundaries, as the depth of background and\nforeground are mixed to yield the high resolution depth. In this paper, we\nintroduce a bundle adjustment framework and a super-resolution network to solve\nthe above two problems. In bundle adjustment, depths and poses of an image\nsequence are jointly optimized, which increases the baseline by establishing\nthe relationship between farther frames. The super resolution network learns to\nestimate a high resolution depth from a low resolution image. Additionally, we\nintroduce the clip loss to deal with moving objects and occlusion. Experimental\nresults on the KITTI dataset show that the proposed algorithm outperforms the\nstate-of-the-art unsupervised methods using monocular sequences, and achieves\ncomparable or even better result compared to unsupervised methods using stereo\nsequences.\n",
    "topics": "{'Depth Estimation': 0.99999976, 'Monocular Depth Estimation': 0.99986494, 'Super-Resolution': 0.99967396, 'Super Resolution': 0.9942152}",
    "score": 0.810522769
  },
  {
    "id": "1811.11823",
    "title": "Semantic Part Detection via Matching: Learning to Generalize to Novel\n  Viewpoints from Limited Training Data",
    "abstract": "  Detecting semantic parts of an object is a challenging task in computer\nvision, particularly because it is hard to construct large annotated datasets\ndue to the difficulty of annotating semantic parts. In this paper we present an\napproach which learns from a small training dataset of annotated semantic\nparts, where the object is seen from a limited range of viewpoints, but\ngeneralizes to detect semantic parts from a much larger range of viewpoints.\nOur approach is based on a matching algorithm for finding accurate spatial\ncorrespondence between two images, which enables semantic parts annotated on\none image to be transplanted to another. In particular, this enables images in\nthe training dataset to be matched to a virtual 3D model of the object (for\nsimplicity, we assume that the object viewpoint can be estimated by standard\ntechniques). Then a clustering algorithm is used to annotate the semantic parts\nof the 3D virtual model. This virtual 3D model can be used to synthesize\nannotated images from a large range of viewpoint. These can be matched to\nimages in the test set, using the same matching algorithm, to detect semantic\nparts in novel viewpoints of the object. Our algorithm is very simple,\nintuitive, and contains very few parameters. We evaluate our approach in the\ncar subclass of the VehicleSemanticPart dataset. We show it outperforms\nstandard deep network approaches and, in particular, performs much better on\nnovel viewpoints. For facilitating the future research, code is available:\nhttps://github.com/ytongbai/SemanticPartDetection\n",
    "topics": "{'3D Reconstruction': 0.47352645}",
    "score": 0.8103585497
  },
  {
    "id": "2004.03374",
    "title": "Complete CVDL Methodology for Investigating Hydrodynamic Instabilities",
    "abstract": "  In fluid dynamics, one of the most important research fields is hydrodynamic\ninstabilities and their evolution in different flow regimes. The investigation\nof said instabilities is concerned with the highly non-linear dynamics.\nCurrently, three main methods are used for understanding of such phenomenon -\nnamely analytical models, experiments and simulations - and all of them are\nprimarily investigated and correlated using human expertise. In this work we\nclaim and demonstrate that a major portion of this research effort could and\nshould be analysed using recent breakthrough advancements in the field of\nComputer Vision with Deep Learning (CVDL, or Deep Computer-Vision).\nSpecifically, we target and evaluate specific state-of-the-art techniques -\nsuch as Image Retrieval, Template Matching, Parameters Regression and\nSpatiotemporal Prediction - for the quantitative and qualitative benefits they\nprovide. In order to do so we focus in this research on one of the most\nrepresentative instabilities, the Rayleigh-Taylor one, simulate its behaviour\nand create an open-sourced state-of-the-art annotated database (RayleAI).\nFinally, we use adjusted experimental results and novel physical loss\nmethodologies to validate the correspondence of the predicted results to actual\nphysical reality to prove the models efficiency. The techniques which were\ndeveloped and proved in this work can be served as essential tools for\nphysicists in the field of hydrodynamics for investigating a variety of\nphysical systems, and also could be used via Transfer Learning to other\ninstabilities research. A part of the techniques can be easily applied on\nalready exist simulation results. All models as well as the data-set that was\ncreated for this work, are publicly available at:\nhttps://github.com/scientific-computing-nrcn/SimulAI.\n",
    "topics": "{'Image Retrieval': 0.6686927}",
    "score": 0.8101083675
  },
  {
    "id": "2002.09283",
    "title": "MODMA dataset: a Multi-modal Open Dataset for Mental-disorder Analysis",
    "abstract": "  According to the World Health Organization, the number of mental disorder\npatients, especially depression patients, has grown rapidly and become a\nleading contributor to the global burden of disease. However, the present\ncommon practice of depression diagnosis is based on interviews and clinical\nscales carried out by doctors, which is not only labor-consuming but also\ntime-consuming. One important reason is due to the lack of physiological\nindicators for mental disorders. With the rising of tools such as data mining\nand artificial intelligence, using physiological data to explore new possible\nphysiological indicators of mental disorder and creating new applications for\nmental disorder diagnosis has become a new research hot topic. However, good\nquality physiological data for mental disorder patients are hard to acquire. We\npresent a multi-modal open dataset for mental-disorder analysis. The dataset\nincludes EEG and audio data from clinically depressed patients and matching\nnormal controls. All our patients were carefully diagnosed and selected by\nprofessional psychiatrists in hospitals. The EEG dataset includes not only data\ncollected using traditional 128-electrodes mounted elastic cap, but also a\nnovel wearable 3-electrode EEG collector for pervasive applications. The\n128-electrodes EEG signals of 53 subjects were recorded as both in resting\nstate and under stimulation; the 3-electrode EEG signals of 55 subjects were\nrecorded in resting state; the audio data of 52 subjects were recorded during\ninterviewing, reading, and picture description. We encourage other researchers\nin the field to use it for testing their methods of mental-disorder analysis.\n",
    "topics": "{'EEG': 1.0}",
    "score": 0.8101068442
  },
  {
    "id": "2008.09648",
    "title": "Semantic Segmentation and Data Fusion of Microsoft Bing 3D Cities and\n  Small UAV-based Photogrammetric Data",
    "abstract": "  With state-of-the-art sensing and photogrammetric techniques, Microsoft Bing\nMaps team has created over 125 highly detailed 3D cities from 11 different\ncountries that cover hundreds of thousands of square kilometer areas. The 3D\ncity models were created using the photogrammetric technique with\nhigh-resolution images that were captured from aircraft-mounted cameras. Such a\nlarge 3D city database has caught the attention of the US Army for creating\nvirtual simulation environments to support military operations. However, the 3D\ncity models do not have semantic information such as buildings, vegetation, and\nground and cannot allow sophisticated user-level and system-level interaction.\nAt I/ITSEC 2019, the authors presented a fully automated data segmentation and\nobject information extraction framework for creating simulation terrain using\nUAV-based photogrammetric data. This paper discusses the next steps in\nextending our designed data segmentation framework for segmenting 3D city data.\nIn this study, the authors first investigated the strengths and limitations of\nthe existing framework when applied to the Bing data. The main differences\nbetween UAV-based and aircraft-based photogrammetric data are highlighted. The\ndata quality issues in the aircraft-based photogrammetric data, which can\nnegatively affect the segmentation performance, are identified. Based on the\nfindings, a workflow was designed specifically for segmenting Bing data while\nconsidering its characteristics. In addition, since the ultimate goal is to\ncombine the use of both small unmanned aerial vehicle (UAV) collected data and\nthe Bing data in a virtual simulation environment, data from these two sources\nneeded to be aligned and registered together. To this end, the authors also\nproposed a data registration workflow that utilized the traditional iterative\nclosest point (ICP) with the extracted semantic information.\n",
    "topics": "{'Semantic Segmentation': 0.98451257}",
    "score": 0.8099632935
  },
  {
    "id": "1810.09683",
    "title": "Unsupervised Features Extraction for Binary Similarity Using Graph\n  Embedding Neural Networks",
    "abstract": "  In this paper we consider the binary similarity problem that consists in\ndetermining if two binary functions are similar only considering their compiled\nform. This problem is know to be crucial in several application scenarios, such\nas copyright disputes, malware analysis, vulnerability detection, etc. The\ncurrent state-of-the-art solutions in this field work by creating an embedding\nmodel that maps binary functions into vectors in $\\mathbb{R}^{n}$. Such\nembedding model captures syntactic and semantic similarity between binaries,\ni.e., similar binary functions are mapped to points that are close in the\nvector space. This strategy has many advantages, one of them is the possibility\nto precompute embeddings of several binary functions, and then compare them\nwith simple geometric operations (e.g., dot product). In [32] functions are\nfirst transformed in Annotated Control Flow Graphs (ACFGs) constituted by\nmanually engineered features and then graphs are embedded into vectors using a\ndeep neural network architecture. In this paper we propose and test several\nways to compute annotated control flow graphs that use unsupervised approaches\nfor feature learning, without incurring a human bias. Our methods are inspired\nafter techniques used in the natural language processing community (e.g., we\nuse word2vec to encode assembly instructions). We show that our approach is\nindeed successful, and it leads to better performance than previous\nstate-of-the-art solutions. Furthermore, we report on a qualitative analysis of\nfunctions embeddings. We found interesting cases in which embeddings are\nclustered according to the semantic of the original binary function.\n",
    "topics": "{'Semantic Similarity': 0.9854667, 'Semantic Textual Similarity': 0.94535685, 'Graph Embedding': 0.9180348}",
    "score": 0.8098691923
  },
  {
    "id": "1803.08323",
    "title": "Prioritized Multi-View Stereo Depth Map Generation Using Confidence\n  Prediction",
    "abstract": "  In this work, we propose a novel approach to prioritize the depth map\ncomputation of multi-view stereo (MVS) to obtain compact 3D point clouds of\nhigh quality and completeness at low computational cost. Our prioritization\napproach operates before the MVS algorithm is executed and consists of two\nsteps. In the first step, we aim to find a good set of matching partners for\neach view. In the second step, we rank the resulting view clusters (i.e. key\nviews with matching partners) according to their impact on the fulfillment of\ndesired quality parameters such as completeness, ground resolution and\naccuracy. Additional to geometric analysis, we use a novel machine learning\ntechnique for training a confidence predictor. The purpose of this confidence\npredictor is to estimate the chances of a successful depth reconstruction for\neach pixel in each image for one specific MVS algorithm based on the RGB images\nand the image constellation. The underlying machine learning technique does not\nrequire any ground truth or manually labeled data for training, but instead\nadapts ideas from depth map fusion for providing a supervision signal. The\ntrained confidence predictor allows us to evaluate the quality of image\nconstellations and their potential impact to the resulting 3D reconstruction\nand thus builds a solid foundation for our prioritization approach. In our\nexperiments, we are thus able to reach more than 70% of the maximal reachable\nquality fulfillment using only 5% of the available images as key views. For\nevaluating our approach within and across different domains, we use two\ncompletely different scenarios, i.e. cultural heritage preservation and\nreconstruction of single family houses.\n",
    "topics": "{'3D Reconstruction': 0.99933714}",
    "score": 0.809718198
  },
  {
    "id": "1903.06536",
    "title": "Multi-Representational Learning for Offline Signature Verification using\n  Multi-Loss Snapshot Ensemble of CNNs",
    "abstract": "  Offline Signature Verification (OSV) is a challenging pattern recognition\ntask, especially in presence of skilled forgeries that are not available during\ntraining. This study aims to tackle its challenges and meet the substantial\nneed for generalization for OSV by examining different loss functions for\nConvolutional Neural Network (CNN). We adopt our new approach to OSV by asking\ntwo questions: 1. which classification loss provides more generalization for\nfeature learning in OSV? , and 2. How integration of different losses into a\nunified multi-loss function lead to an improved learning framework? These\nquestions are studied based on analysis of three loss functions, including\ncross entropy, Cauchy-Schwarz divergence, and hinge loss. According to\ncomplementary features of these losses, we combine them into a dynamic\nmulti-loss function and propose a novel ensemble framework for simultaneous use\nof them in CNN. Our proposed Multi-Loss Snapshot Ensemble (MLSE) consists of\nseveral sequential trials. In each trial, a dominant loss function is selected\nfrom the multi-loss set, and the remaining losses act as a regularizer.\nDifferent trials learn diverse representations for each input based on\nsignature identification task. This multi-representation set is then employed\nfor the verification task. An ensemble of SVMs is trained on these\nrepresentations, and their decisions are finally combined according to the\nselection of most generalizable SVM for each user. We conducted two sets of\nexperiments based on two different protocols of OSV, i.e., writer-dependent and\nwriter-independent on three signature datasets: GPDS-Synthetic, MCYT, and\nUT-SIG. Based on the writer-dependent OSV protocol, we achieved substantial\nimprovements over the best EERs in the literature. The results of the second\nset of experiments also confirmed the robustness to the arrival of new users\nenrolled in the OSV system.\n",
    "topics": "{'Question Answering': 0.430433}",
    "score": 0.8096962645
  },
  {
    "id": "1903.10536",
    "title": "Gene Expression based Survival Prediction for Cancer Patients: A Topic\n  Modeling Approach",
    "abstract": "  Cancer is one of the leading cause of death, worldwide. Many believe that\ngenomic data will enable us to better predict the survival time of these\npatients, which will lead to better, more personalized treatment options and\npatient care. As standard survival prediction models have a hard time coping\nwith the high-dimensionality of such gene expression (GE) data, many projects\nuse some dimensionality reduction techniques to overcome this hurdle. We\nintroduce a novel methodology, inspired by topic modeling from the natural\nlanguage domain, to derive expressive features from the high-dimensional GE\ndata. There, a document is represented as a mixture over a relatively small\nnumber of topics, where each topic corresponds to a distribution over the\nwords; here, to accommodate the heterogeneity of a patient's cancer, we\nrepresent each patient (~document) as a mixture over cancer-topics, where each\ncancer-topic is a mixture over GE values (~words). This required some\nextensions to the standard LDA model eg: to accommodate the \"real-valued\"\nexpression values - leading to our novel \"discretized\" Latent Dirichlet\nAllocation (dLDA) procedure. We initially focus on the METABRIC dataset, which\ndescribes breast cancer patients using the r=49,576 GE values, from\nmicroarrays. Our results show that our approach provides survival estimates\nthat are more accurate than standard models, in terms of the standard\nConcordance measure. We then validate this approach by running it on the\nPan-kidney (KIPAN) dataset, over r=15,529 GE values - here using the mRNAseq\nmodality - and find that it again achieves excellent results. In both cases, we\nalso show that the resulting model is calibrated, using the recent\n\"D-calibrated\" measure. These successes, in two different cancer types and\nexpression modalities, demonstrates the generality, and the effectiveness, of\nthis approach.\n",
    "topics": "{'Survival Analysis': 0.7587104, 'Dimensionality Reduction': 0.7419174}",
    "score": 0.8096209569
  },
  {
    "id": "2005.10986",
    "title": "A Convolutional Neural Network with Parallel Multi-Scale Spatial Pooling\n  to Detect Temporal Changes in SAR Images",
    "abstract": "  In synthetic aperture radar (SAR) image change detection, it is quite\nchallenging to exploit the changing information from the noisy difference image\nsubject to the speckle. In this paper, we propose a multi-scale spatial pooling\n(MSSP) network to exploit the changed information from the noisy difference\nimage. Being different from the traditional convolutional network with only\nmono-scale pooling kernels, in the proposed method, multi-scale pooling kernels\nare equipped in a convolutional network to exploit the spatial context\ninformation on changed regions from the difference image. Furthermore, to\nverify the generalization of the proposed method, we apply our proposed method\nto the cross-dataset bitemporal SAR image change detection, where the MSSP\nnetwork (MSSP-Net) is trained on a dataset and then applied to an unknown\ntesting dataset. We compare the proposed method with other state-of-arts and\nthe comparisons are performed on four challenging datasets of bitemporal SAR\nimages. Experimental results demonstrate that our proposed method obtains\ncomparable results with S-PCA-Net on YR-A and YR-B dataset and outperforms\nother state-of-art methods, especially on the Sendai-A and Sendai-B datasets\nwith more complex scenes. More important, MSSP-Net is more efficient than\nS-PCA-Net and convolutional neural networks (CNN) with less executing time in\nboth training and testing phases.\n",
    "topics": "{}",
    "score": 0.8093896389
  },
  {
    "id": "1911.06185",
    "title": "Convolutional Neural Network for Convective Storm Nowcasting Using 3D\n  Doppler Weather Radar Data",
    "abstract": "  Convective storms are one of the severe weather hazards found during the warm\nseason. Doppler weather radar is the only operational instrument that can\nfrequently sample the detailed structure of convective storm which has a small\nspatial scale and short lifetime. For the challenging task of short-term\nconvective storm forecasting, 3-D radar images contain information about the\nprocesses in convective storm. However, effectively extracting such information\nfrom multisource raw data has been problematic due to a lack of methodology and\ncomputation limitations. Recent advancements in deep learning techniques and\ngraphics processing units now make it possible. This article investigates the\nfeasibility and performance of an end-to-end deep learning nowcasting method.\nThe nowcasting problem was transformed into a classification problem first, and\nthen, a deep learning method that uses a convolutional neural network was\npresented to make predictions. On the first layer of CNN, a cross-channel 3D\nconvolution was proposed to fuse 3D raw data. The CNN method eliminates the\nhandcrafted feature engineering, i.e., the process of using domain knowledge of\nthe data to manually design features. Operationally produced historical data of\nthe Beijing-Tianjin-Hebei region in China was used to train the nowcasting\nsystem and evaluate its performance; 3737332 samples were collected in the\ntraining data set. The experimental results show that the deep learning method\nimproves nowcasting skills compared with traditional machine learning methods.\n",
    "topics": "{'Feature Engineering': 0.999622}",
    "score": 0.8093259696
  },
  {
    "id": "2005.07518",
    "title": "Temperate Fish Detection and Classification: a Deep Learning based\n  Approach",
    "abstract": "  A wide range of applications in marine ecology extensively uses underwater\ncameras. Still, to efficiently process the vast amount of data generated, we\nneed to develop tools that can automatically detect and recognize species\ncaptured on film. Classifying fish species from videos and images in natural\nenvironments can be challenging because of noise and variation in illumination\nand the surrounding habitat. In this paper, we propose a two-step deep learning\napproach for the detection and classification of temperate fishes without\npre-filtering. The first step is to detect each single fish in an image,\nindependent of species and sex. For this purpose, we employ the You Only Look\nOnce (YOLO) object detection technique. In the second step, we adopt a\nConvolutional Neural Network (CNN) with the Squeeze-and-Excitation (SE)\narchitecture for classifying each fish in the image without pre-filtering. We\napply transfer learning to overcome the limited training samples of temperate\nfishes and to improve the accuracy of the classification. This is done by\ntraining the object detection model with ImageNet and the fish classifier via a\npublic dataset (Fish4Knowledge), whereupon both the object detection and\nclassifier are updated with temperate fishes of interest. The weights obtained\nfrom pre-training are applied to post-training as a priori. Our solution\nachieves the state-of-the-art accuracy of 99.27\\% on the pre-training. The\npercentage values for accuracy on the post-training are good; 83.68\\% and\n87.74\\% with and without image augmentation, respectively, indicating that the\nsolution is viable with a more extensive dataset.\n",
    "topics": "{'Object Detection': 0.99999964, 'Image Augmentation': 0.99994874, 'Transfer Learning': 0.9943633}",
    "score": 0.8091086025
  },
  {
    "id": "1805.12218",
    "title": "Convolutional Embedded Networks for Population Scale Clustering and\n  Bio-ancestry Inferencing",
    "abstract": "  The study of genetic variants can help find correlating population groups to\nidentify cohorts that are predisposed to common diseases and explain\ndifferences in disease susceptibility and how patients react to drugs. Machine\nlearning algorithms are increasingly being applied to identify interacting GVs\nto understand their complex phenotypic traits. Since the performance of a\nlearning algorithm not only depends on the size and nature of the data but also\non the quality of underlying representation, deep neural networks can learn\nnon-linear mappings that allow transforming GVs data into more clustering and\nclassification friendly representations than manual feature selection. In this\npaper, we proposed convolutional embedded networks in which we combine two DNN\narchitectures called convolutional embedded clustering and convolutional\nautoencoder classifier for clustering individuals and predicting geographic\nethnicity based on GVs, respectively. We employed CAE-based representation\nlearning on 95 million GVs from the 1000 genomes and Simons genome diversity\nprojects. Quantitative and qualitative analyses with a focus on accuracy and\nscalability show that our approach outperforms state-of-the-art approaches such\nas VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted\npopulation groups in 22 hours with an adjusted rand index of 0.915, the\nnormalized mutual information of 0.92, and the clustering accuracy of 89%.\nContrarily, the CAE classifier can predict the geographic ethnicity of unknown\nsamples with an F1 and Mathews correlation coefficient(MCC) score of 0.9004 and\n0.8245, respectively. To provide interpretations of the predictions, we\nidentify significant biomarkers using gradient boosted trees(GBT) and SHAP.\nOverall, our approach is transparent and faster than the baseline methods, and\nscalable for 5% to 100% of the full human genome.\n",
    "topics": "{'Feature Selection': 0.9556009, 'Representation Learning': 0.95475453}",
    "score": 0.8090666676
  },
  {
    "id": "1912.06640",
    "title": "SPIN: A High Speed, High Resolution Vision Dataset for Tracking and\n  Action Recognition in Ping Pong",
    "abstract": "  We introduce a new high resolution, high frame rate stereo video dataset,\nwhich we call SPIN, for tracking and action recognition in the game of ping\npong. The corpus consists of ping pong play with three main annotation streams\nthat can be used to learn tracking and action recognition models -- tracking of\nthe ping pong ball and poses of humans in the videos and the spin of the ball\nbeing hit by humans. The training corpus consists of 53 hours of data with\nlabels derived from previous models in a semi-supervised method. The testing\ncorpus contains 1 hour of data with the same information, except that crowd\ncompute was used to obtain human annotations of the ball position, from which\nball spin has been derived. Along with the dataset we introduce several\nbaseline models that were trained on this data. The models were specifically\nchosen to be able to perform inference at the same rate as the images are\ngenerated -- specifically 150 fps. We explore the advantages of multi-task\ntraining on this data, and also show interesting properties of ping pong ball\ntrajectories that are derived from our observational data, rather than from\nprior physics models. To our knowledge this is the first large scale dataset of\nping pong; we offer it to the community as a rich dataset that can be used for\na large variety of machine learning and vision tasks such as tracking, pose\nestimation, semi-supervised and unsupervised learning and generative modeling.\n",
    "topics": "{'Action Recognition': 1.0, 'Pose Estimation': 0.9638916}",
    "score": 0.8090177582
  },
  {
    "id": "2007.02517",
    "title": "EDSL: An Encoder-Decoder Architecture with Symbol-Level Features for\n  Printed Mathematical Expression Recognition",
    "abstract": "  Printed Mathematical expression recognition (PMER) aims to transcribe a\nprinted mathematical expression image into a structural expression, such as\nLaTeX expression. It is a crucial task for many applications, including\nautomatic question recommendation, automatic problem solving and analysis of\nthe students, etc. Currently, the mainstream solutions rely on solving image\ncaptioning tasks, all addressing image summarization. As such, these methods\ncan be suboptimal for solving MER problem.\n  In this paper, we propose a new method named EDSL, shorted for\nencoder-decoder with symbol-level features, to identify the printed\nmathematical expressions from images. The symbol-level image encoder of EDSL\nconsists of segmentation module and reconstruction module. By performing\nsegmentation module, we identify all the symbols and their spatial information\nfrom images in an unsupervised manner. We then design a novel reconstruction\nmodule to recover the symbol dependencies after symbol segmentation.\nEspecially, we employ a position correction attention mechanism to capture the\nspatial relationships between symbols. To alleviate the negative impact from\nlong output, we apply the transformer model for transcribing the encoded image\ninto the sequential and structural output. We conduct extensive experiments on\ntwo real datasets to verify the effectiveness and rationality of our proposed\nEDSL method. The experimental results have illustrated that EDSL has achieved\n92.7\\% and 89.0\\% in evaluation metric Match, which are 3.47\\% and 4.04\\%\nhigher than the state-of-the-art method. Our code and datasets are available at\nhttps://github.com/abcAnonymous/EDSL .\n",
    "topics": "{'Image Captioning': 0.99894375}",
    "score": 0.809006786
  },
  {
    "id": "1409.8576",
    "title": "Data Imputation through the Identification of Local Anomalies",
    "abstract": "  We introduce a comprehensive and statistical framework in a model free\nsetting for a complete treatment of localized data corruptions due to severe\nnoise sources, e.g., an occluder in the case of a visual recording. Within this\nframework, we propose i) a novel algorithm to efficiently separate, i.e.,\ndetect and localize, possible corruptions from a given suspicious data instance\nand ii) a Maximum A Posteriori (MAP) estimator to impute the corrupted data. As\na generalization to Euclidean distance, we also propose a novel distance\nmeasure, which is based on the ranked deviations among the data attributes and\nempirically shown to be superior in separating the corruptions. Our algorithm\nfirst splits the suspicious instance into parts through a binary partitioning\ntree in the space of data attributes and iteratively tests those parts to\ndetect local anomalies using the nominal statistics extracted from an\nuncorrupted (clean) reference data set. Once each part is labeled as anomalous\nvs normal, the corresponding binary patterns over this tree that characterize\ncorruptions are identified and the affected attributes are imputed. Under a\ncertain conditional independency structure assumed for the binary patterns, we\nanalytically show that the false alarm rate of the introduced algorithm in\ndetecting the corruptions is independent of the data and can be directly set\nwithout any parameter tuning. The proposed framework is tested over several\nwell-known machine learning data sets with synthetically generated corruptions;\nand experimentally shown to produce remarkable improvements in terms of\nclassification purposes with strong corruption separation capabilities. Our\nexperiments also indicate that the proposed algorithms outperform the typical\napproaches and are robust to varying training phase conditions.\n",
    "topics": "{'Imputation': 0.99204755}",
    "score": 0.8089904269
  },
  {
    "id": "1705.08106",
    "title": "Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action\n  Recognition",
    "abstract": "  It remains a challenge to efficiently extract spatialtemporal information\nfrom skeleton sequences for 3D human action recognition. Although most recent\naction recognition methods are based on Recurrent Neural Networks which present\noutstanding performance, one of the shortcomings of these methods is the\ntendency to overemphasize the temporal information. Since 3D convolutional\nneural network(3D CNN) is a powerful tool to simultaneously learn features from\nboth spatial and temporal dimensions through capturing the correlations between\nthree dimensional signals, this paper proposes a novel two-stream model using\n3D CNN. To our best knowledge, this is the first application of 3D CNN in\nskeleton-based action recognition. Our method consists of three stages. First,\nskeleton joints are mapped into a 3D coordinate space and then encoding the\nspatial and temporal information, respectively. Second, 3D CNN models are\nseperately adopted to extract deep features from two streams. Third, to enhance\nthe ability of deep features to capture global relationships, we extend every\nstream into multitemporal version. Extensive experiments on the SmartHome\ndataset and the large-scale NTU RGB-D dataset demonstrate that our method\noutperforms most of RNN-based methods, which verify the complementary property\nbetween spatial and temporal information and the robustness to noise.\n",
    "topics": "{'Skeleton Based Action Recognition': 1.0, 'Action Recognition': 1.0, 'Temporal Action Localization': 0.99999416, '3D Action Recognition': 0.9982882}",
    "score": 0.8089317329
  },
  {
    "id": "2002.07551",
    "title": "Hierarchical Transformer Network for Utterance-level Emotion Recognition",
    "abstract": "  While there have been significant advances in de-tecting emotions in text, in\nthe field of utter-ance-level emotion recognition (ULER), there are still many\nproblems to be solved. In this paper, we address some challenges in ULER in\ndialog sys-tems. (1) The same utterance can deliver different emotions when it\nis in different contexts or from different speakers. (2) Long-range contextual\nin-formation is hard to effectively capture. (3) Unlike the traditional text\nclassification problem, this task is supported by a limited number of datasets,\namong which most contain inadequate conversa-tions or speech. To address these\nproblems, we propose a hierarchical transformer framework (apart from the\ndescription of other studies, the \"transformer\" in this paper usually refers to\nthe encoder part of the transformer) with a lower-level transformer to model\nthe word-level input and an upper-level transformer to capture the context of\nutterance-level embeddings. We use a pretrained language model bidirectional\nencoder representa-tions from transformers (BERT) as the lower-level\ntransformer, which is equivalent to introducing external data into the model\nand solve the problem of data shortage to some extent. In addition, we add\nspeaker embeddings to the model for the first time, which enables our model to\ncapture the in-teraction between speakers. Experiments on three dialog emotion\ndatasets, Friends, EmotionPush, and EmoryNLP, demonstrate that our proposed\nhierarchical transformer network models achieve 1.98%, 2.83%, and 3.94%\nimprovement, respec-tively, over the state-of-the-art methods on each dataset\nin terms of macro-F1.\n",
    "topics": "{'Emotion Recognition': 0.9994703, 'Language Modelling': 0.3355163}",
    "score": 0.8088786401
  },
  {
    "id": "1905.07529",
    "title": "Multinomial Distribution Learning for Effective Neural Architecture\n  Search",
    "abstract": "  Architectures obtained by Neural Architecture Search (NAS) have achieved\nhighly competitive performance in various computer vision tasks. However, the\nprohibitive computation demand of forward-backward propagation in deep neural\nnetworks and searching algorithms makes it difficult to apply NAS in practice.\nIn this paper, we propose a Multinomial Distribution Learning for extremely\neffective NAS,which considers the search space as a joint multinomial\ndistribution, i.e., the operation between two nodes is sampled from this\ndistribution, and the optimal network structure is obtained by the operations\nwith the most likely probability in this distribution. Therefore, NAS can be\ntransformed to a multinomial distribution learning problem, i.e., the\ndistribution is optimized to have a high expectation of the performance.\nBesides, a hypothesis that the performance ranking is consistent in every\ntraining epoch is proposed and demonstrated to further accelerate the learning\nprocess. Experiments on CIFAR10 and ImageNet demonstrate the effectiveness of\nour method. On CIFAR-10, the structure searched by our method achieves 2.55%\ntest error, while being 6.0x (only 4 GPU hours on GTX1080Ti) faster compared\nwith state-of-the-art NAS algorithms. On ImageNet, our model achieves 75.2%\ntop1 accuracy under MobileNet settings (MobileNet V1/V2), while being 1.2x\nfaster with measured GPU latency. Test code with pre-trained models are\navailable at https://github.com/tanglang96/MDENAS\n",
    "topics": "{'Neural Architecture Search': 1.0}",
    "score": 0.808876892
  },
  {
    "id": "1908.04018",
    "title": "An overlapping-free leaf segmentation method for plant point clouds",
    "abstract": "  Automatic leaf segmentation, as well as identification and classification\nmethods that built upon it, are able to provide immediate monitoring for plant\ngrowth status to guarantee the output. Although 3D plant point clouds contain\nabundant phenotypic features, plant leaves are usually distributed in clusters\nand are sometimes seriously overlapped in the canopy. Therefore, it is still a\nbig challenge to automatically segment each individual leaf from a highly\ncrowded plant canopy in 3D for plant phenotyping purposes. In this work, we\npropose an overlapping-free individual leaf segmentation method for plant point\nclouds using the 3D filtering and facet region growing. In order to separate\nleaves with different overlapping situations, we develop a new 3D joint\nfiltering operator, which integrates a Radius-based Outlier Filter (RBOF) and a\nSurface Boundary Filter (SBF) to help to separate occluded leaves. By\nintroducing the facet over-segmentation and facet-based region growing, the\nnoise in segmentation is suppressed and labeled leaf centers can expand to\ntheir whole leaves, respectively. Our method can work on point clouds generated\nfrom three types of 3D imaging platforms, and also suitable for different kinds\nof plant species. In experiments, it obtains a point-level cover rate of 97%\nfor Epipremnum aureum, 99% for Monstera deliciosa, 99% for Calathea makoyana,\nand 87% for Hedera nepalensis sample plants. At the leaf level, our method\nreaches an average Recall at 100.00%, a Precision at 99.33%, and an average\nF-measure at 99.66%, respectively. The proposed method can also facilitate the\nautomatic traits estimation of each single leaf (such as the leaf area, length,\nand width), which has potential to become a highly effective tool for plant\nresearch and agricultural engineering.\n",
    "topics": "{}",
    "score": 0.8088679362
  },
  {
    "id": "1902.09904",
    "title": "Diagnosis of Alzheimer's Disease via Multi-modality 3D Convolutional\n  Neural Network",
    "abstract": "  Alzheimer's Disease (AD) is one of the most concerned neurodegenerative\ndiseases. In the last decade, studies on AD diagnosis attached great\nsignificance to artificial intelligence (AI)-based diagnostic algorithms. Among\nthe diverse modality imaging data, T1-weighted MRI and 18F-FDGPET are widely\nresearched for this task. In this paper, we propose a novel convolutional\nneural network (CNN) to fuse the multi-modality information including T1-MRI\nand FDG-PDT images around the hippocampal area for the diagnosis of AD.\nDifferent from the traditional machine learning algorithms, this method does\nnot require manually extracted features, and utilizes the stateof-art 3D\nimage-processing CNNs to learn features for the diagnosis and prognosis of AD.\nTo validate the performance of the proposed network, we trained the classifier\nwith paired T1-MRI and FDG-PET images using the ADNI datasets, including 731\nNormal (NL) subjects, 647 AD subjects, 441 stable MCI (sMCI) subjects and 326\nprogressive MCI (pMCI) subjects. We obtained the maximal accuracies of 90.10%\nfor NL/AD task, 87.46% for NL/pMCI task, and 76.90% for sMCI/pMCI task. The\nproposed framework yields comparative results against state-of-the-art\napproaches. Moreover, the experimental results have demonstrated that (1)\nsegmentation is not a prerequisite by using CNN, (2) the hippocampal area\nprovides enough information to give a reference to AD diagnosis. Keywords:\nAlzheimer's Disease, Multi-modality, Image Classification, CNN, Deep Learning,\nHippocampal\n",
    "topics": "{'Image Classification': 0.95027536}",
    "score": 0.8088388621
  },
  {
    "id": "1811.08705",
    "title": "Inline Detection of Domain Generation Algorithms with Context-Sensitive\n  Word Embeddings",
    "abstract": "  Domain generation algorithms (DGAs) are frequently employed by malware to\ngenerate domains used for connecting to command-and-control (C2) servers.\nRecent work in DGA detection leveraged deep learning architectures like\nconvolutional neural networks (CNNs) and character-level long short-term memory\nnetworks (LSTMs) to classify domains. However, these classifiers perform poorly\nwith wordlist-based DGA families, which generate domains by pseudorandomly\nconcatenating dictionary words. We propose a novel approach that combines\ncontext-sensitive word embeddings with a simple fully-connected classifier to\nperform classification of domains based on word-level information. The word\nembeddings were pre-trained on a large unrelated corpus and left frozen during\nthe training on domain data. The resulting small number of trainable parameters\nenabled extremely short training durations, while the transfer of language\nknowledge stored in the representations allowed for high-performing models with\nsmall training datasets. We show that this architecture reliably outperformed\nexisting techniques on wordlist-based DGA families with just 30 DGA training\nexamples and achieved state-of-the-art performance with around 100 DGA training\nexamples, all while requiring an order of magnitude less time to train compared\nto current techniques. Of special note is the technique's performance on the\nmatsnu DGA: the classifier attained a 89.5% detection rate with a 1:1,000 false\npositive rate (FPR) after training on only 30 examples of the DGA domains, and\na 91.2% detection rate with a 1:10,000 FPR after 90 examples. Considering that\nsome of these DGAs have wordlists of several hundred words, our results\ndemonstrate that this technique does not rely on the classifier learning the\nDGA wordlists. Instead, the classifier is able to learn the semantic signatures\nof the wordlist-based DGA families.\n",
    "topics": "{'Word Embeddings': 0.9350149}",
    "score": 0.8088151852
  },
  {
    "id": "1711.04022",
    "title": "Deep Within-Class Covariance Analysis for Robust Audio Representation\n  Learning",
    "abstract": "  Convolutional Neural Networks (CNNs) can learn effective features, though\nhave been shown to suffer from a performance drop when the distribution of the\ndata changes from training to test data. In this paper we analyze the internal\nrepresentations of CNNs and observe that the representations of unseen data in\neach class, spread more (with higher variance) in the embedding space of the\nCNN compared to representations of the training data. More importantly, this\ndifference is more extreme if the unseen data comes from a shifted\ndistribution. Based on this observation, we objectively evaluate the degree of\nrepresentation's variance in each class via eigenvalue decomposition on the\nwithin-class covariance of the internal representations of CNNs and observe the\nsame behaviour. This can be problematic as larger variances might lead to\nmis-classification if the sample crosses the decision boundary of its class. We\napply nearest neighbor classification on the representations and empirically\nshow that the embeddings with the high variance actually have significantly\nworse KNN classification performances, although this could not be foreseen from\ntheir end-to-end classification results. To tackle this problem, we propose\nDeep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that\nsignificantly reduces the within-class covariance of a DNN's representation,\nimproving performance on unseen test data from a shifted distribution. We\nempirically evaluate DWCCA on two datasets for Acoustic Scene Classification\n(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA\nsignificantly improve the network's internal representation, it also increases\nthe end-to-end classification accuracy, especially when the test set exhibits a\ndistribution shift. By adding DWCCA to a VGG network, we achieve around 6\npercentage points improvement in the case of a distribution mismatch.\n",
    "topics": "{'Scene Classification': 0.99999905, 'Representation Learning': 0.9125402}",
    "score": 0.8087573531
  },
  {
    "id": "2008.05924",
    "title": "DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions\n  in the Wild",
    "abstract": "  Recently, facial expression recognition (FER) in the wild has gained a lot of\nresearchers' attention because it is a valuable topic to enable the FER\ntechniques to move from the laboratory to the real applications. In this paper,\nwe focus on this challenging but interesting topic and make contributions from\nthree aspects. First, we present a new large-scale 'in-the-wild' dynamic facial\nexpression database, DFEW (Dynamic Facial Expression in the Wild), consisting\nof over 16,000 video clips from thousands of movies. These video clips contain\nvarious challenging interferences in practical scenarios such as extreme\nillumination, occlusions, and capricious pose changes. Second, we propose a\nnovel method called Expression-Clustered Spatiotemporal Feature Learning\n(EC-STFL) framework to deal with dynamic FER in the wild. Third, we conduct\nextensive benchmark experiments on DFEW using a lot of spatiotemporal deep\nfeature learning methods as well as our proposed EC-STFL. Experimental results\nshow that DFEW is a well-designed and challenging database, and the proposed\nEC-STFL can promisingly improve the performance of existing spatiotemporal deep\nneural networks in coping with the problem of dynamic FER in the wild. Our DFEW\ndatabase is publicly available and can be freely downloaded from\nhttps://dfew-dataset.github.io/.\n",
    "topics": "{'Facial Expression Recognition': 0.9999993}",
    "score": 0.8087563164
  },
  {
    "id": "1708.02412",
    "title": "Wasserstein CNN: Learning Invariant Features for NIR-VIS Face\n  Recognition",
    "abstract": "  Heterogeneous face recognition (HFR) aims to match facial images acquired\nfrom different sensing modalities with mission-critical applications in\nforensics, security and commercial sectors. However, HFR is a much more\nchallenging problem than traditional face recognition because of large\nintra-class variations of heterogeneous face images and limited training\nsamples of cross-modality face image pairs. This paper proposes a novel\napproach namely Wasserstein CNN (convolutional neural networks, or WCNN for\nshort) to learn invariant features between near-infrared and visual face images\n(i.e. NIR-VIS face recognition). The low-level layers of WCNN are trained with\nwidely available face images in visual spectrum. The high-level layer is\ndivided into three parts, i.e., NIR layer, VIS layer and NIR-VIS shared layer.\nThe first two layers aims to learn modality-specific features and NIR-VIS\nshared layer is designed to learn modality-invariant feature subspace.\nWasserstein distance is introduced into NIR-VIS shared layer to measure the\ndissimilarity between heterogeneous feature distributions. So W-CNN learning\naims to achieve the minimization of Wasserstein distance between NIR\ndistribution and VIS distribution for invariant deep feature representation of\nheterogeneous face images. To avoid the over-fitting problem on small-scale\nheterogeneous face data, a correlation prior is introduced on the\nfully-connected layers of WCNN network to reduce parameter space. This prior is\nimplemented by a low-rank constraint in an end-to-end network. The joint\nformulation leads to an alternating minimization for deep feature\nrepresentation at training stage and an efficient computation for heterogeneous\ndata at testing stage. Extensive experiments on three challenging NIR-VIS face\nrecognition databases demonstrate the significant superiority of Wasserstein\nCNN over state-of-the-art methods.\n",
    "topics": "{'Face Recognition': 1.0}",
    "score": 0.8087494756
  },
  {
    "id": "1809.10196",
    "title": "Computer-Aided Diagnosis of Label-Free 3-D Optical Coherence Microscopy\n  Images of Human Cervical Tissue",
    "abstract": "  Objective: Ultrahigh-resolution optical coherence microscopy (OCM) has\nrecently demonstrated its potential for accurate diagnosis of human cervical\ndiseases. One major challenge for clinical adoption, however, is the steep\nlearning curve clinicians need to overcome to interpret OCM images. Developing\nan intelligent technique for computer-aided diagnosis (CADx) to accurately\ninterpret OCM images will facilitate clinical adoption of the technology and\nimprove patient care. Methods: 497 high-resolution 3-D OCM volumes (600\ncross-sectional images each) were collected from 159 ex vivo specimens of 92\nfemale patients. OCM image features were extracted using a convolutional neural\nnetwork (CNN) model, concatenated with patient information (e.g., age, HPV\nresults), and classified using a support vector machine classifier. Ten-fold\ncross-validations were utilized to test the performance of the CADx method in a\nfive-class classification task and a binary classification task. Results: An\n88.3 plus or minus 4.9% classification accuracy was achieved for five\nfine-grained classes of cervical tissue, namely normal, ectropion, low-grade\nand high-grade squamous intraepithelial lesions (LSIL and HSIL), and cancer. In\nthe binary classification task (low-risk [normal, ectropion and LSIL] vs.\nhigh-risk [HSIL and cancer]), the CADx method achieved an area-under-the-curve\n(AUC) value of 0.959 with an 86.7 plus or minus 11.4% sensitivity and 93.5 plus\nor minus 3.8% specificity. Conclusion: The proposed deep-learning based CADx\nmethod outperformed three human experts. It was also able to identify\nmorphological characteristics in OCM images that were consistent with\nhistopathological interpretations. Significance: Label-free OCM imaging,\ncombined with deep-learning based CADx methods, hold a great promise to be used\nin clinical settings for the effective screening and diagnosis of cervical\ndiseases.\n",
    "topics": "{}",
    "score": 0.8086814107
  },
  {
    "id": "1903.03893",
    "title": "A Hybrid GA-PSO Method for Evolving Architecture and Short Connections\n  of Deep Convolutional Neural Networks",
    "abstract": "  Image classification is a difficult machine learning task, where\nConvolutional Neural Networks (CNNs) have been applied for over 20 years in\norder to solve the problem. In recent years, instead of the traditional way of\nonly connecting the current layer with its next layer, shortcut connections\nhave been proposed to connect the current layer with its forward layers apart\nfrom its next layer, which has been proved to be able to facilitate the\ntraining process of deep CNNs. However, there are various ways to build the\nshortcut connections, it is hard to manually design the best shortcut\nconnections when solving a particular problem, especially given the design of\nthe network architecture is already very challenging.\n  In this paper, a hybrid evolutionary computation (EC) method is proposed to\n\\textit{automatically} evolve both the architecture of deep CNNs and the\nshortcut connections. Three major contributions of this work are: Firstly, a\nnew encoding strategy is proposed to encode a CNN, where the architecture and\nthe shortcut connections are encoded separately; Secondly, a hybrid two-level\nEC method, which combines particle swarm optimisation and genetic algorithms,\nis developed to search for the optimal CNNs; Lastly, an adjustable learning\nrate is introduced for the fitness evaluations, which provides a better\nlearning rate for the training process given a fixed number of epochs. The\nproposed algorithm is evaluated on three widely used benchmark datasets of\nimage classification and compared with 12 peer Non-EC based competitors and one\nEC based competitor. The experimental results demonstrate that the proposed\nmethod outperforms all of the peer competitors in terms of classification\naccuracy.\n",
    "topics": "{'Image Classification': 0.99943703}",
    "score": 0.8085935655
  },
  {
    "id": "1810.10804",
    "title": "Fast Neural Architecture Search of Compact Semantic Segmentation Models\n  via Auxiliary Cells",
    "abstract": "  Automated design of neural network architectures tailored for a specific task\nis an extremely promising, albeit inherently difficult, avenue to explore.\nWhile most results in this domain have been achieved on image classification\nand language modelling problems, here we concentrate on dense per-pixel tasks,\nin particular, semantic image segmentation using fully convolutional networks.\nIn contrast to the aforementioned areas, the design choices of a fully\nconvolutional network require several changes, ranging from the sort of\noperations that need to be used---e.g., dilated convolutions---to a solving of\na more difficult optimisation problem. In this work, we are particularly\ninterested in searching for high-performance compact segmentation\narchitectures, able to run in real-time using limited resources. To achieve\nthat, we intentionally over-parameterise the architecture during the training\ntime via a set of auxiliary cells that provide an intermediate supervisory\nsignal and can be omitted during the evaluation phase. The design of the\nauxiliary cell is emitted by a controller, a neural network with the fixed\nstructure trained using reinforcement learning. More crucially, we demonstrate\nhow to efficiently search for these architectures within limited time and\ncomputational budgets. In particular, we rely on a progressive strategy that\nterminates non-promising architectures from being further trained, and on\nPolyak averaging coupled with knowledge distillation to speed-up the\nconvergence. Quantitatively, in 8 GPU-days our approach discovers a set of\narchitectures performing on-par with state-of-the-art among compact models on\nthe semantic segmentation, pose estimation and depth prediction tasks. Code\nwill be made available here: https://github.com/drsleep/nas-segm-pytorch\n",
    "topics": "{'Semantic Segmentation': 0.99996006, 'Language Modelling': 0.9845258, 'Depth Estimation': 0.77616435, 'Neural Architecture Search': 0.5060731}",
    "score": 0.8085489322
  },
  {
    "id": "1811.12296",
    "title": "Face Detection in the Operating Room: Comparison of State-of-the-art\n  Methods and a Self-supervised Approach",
    "abstract": "  Purpose: Face detection is a needed component for the automatic analysis and\nassistance of human activities during surgical procedures. Efficient face\ndetection algorithms can indeed help to detect and identify the persons present\nin the room, and also be used to automatically anonymize the data. However,\ncurrent algorithms trained on natural images do not generalize well to the\noperating room (OR) images. In this work, we provide a comparison of\nstate-of-the-art face detectors on OR data and also present an approach to\ntrain a face detector for the OR by exploiting non-annotated OR images.\nMethods: We propose a comparison of 6 state-of-the-art face detectors on\nclinical data using Multi-View Operating Room Faces (MVOR-Faces), a dataset of\noperating room images capturing real surgical activities. We then propose to\nuse self-supervision, a domain adaptation method, for the task of face\ndetection in the OR. The approach makes use of non-annotated images to\nfine-tune a state-of-the-art detector for the OR without using any human\nsupervision. Results: The results show that the best model, namely the tiny\nface detector, yields an average precision of 0.536 at Intersection over Union\n(IoU) of 0.5. Our self-supervised model using non-annotated clinical data\noutperforms this result by 9.2%. Conclusion: We present the first comparison of\nstate-of-the-art face detectors on operating room images and show that results\ncan be significantly improved by using self-supervision on non-annotated data.\n",
    "topics": "{'Face Detection': 1.0, 'Domain Adaptation': 0.9076155}",
    "score": 0.8084331853
  },
  {
    "id": "1805.10531",
    "title": "Unsupervised Learning with Stein's Unbiased Risk Estimator",
    "abstract": "  Learning from unlabeled and noisy data is one of the grand challenges of\nmachine learning. As such, it has seen a flurry of research with new ideas\nproposed continuously. In this work, we revisit a classical idea: Stein's\nUnbiased Risk Estimator (SURE). We show that, in the context of image recovery,\nSURE and its generalizations can be used to train convolutional neural networks\n(CNNs) for a range of image denoising and recovery problems without any ground\ntruth data.\n  Specifically, our goal is to reconstruct an image $x$ from a noisy linear\ntransformation (measurement) of the image. We consider two scenarios: one where\nno additional data is available and one where we have measurements of other\nimages that are drawn from the same noisy distribution as $x$, but have no\naccess to the clean images. Such is the case, for instance, in the context of\nmedical imaging, microscopy, and astronomy, where noise-less ground truth data\nis rarely available.\n  We show that in this situation, SURE can be used to estimate the\nmean-squared-error loss associated with an estimate of $x$. Using this estimate\nof the loss, we train networks to perform denoising and compressed sensing\nrecovery. In addition, we also use the SURE framework to partially explain and\nimprove upon an intriguing results presented by Ulyanov et al. in \"Deep Image\nPrior\": that a network initialized with random weights and fit to a single\nnoisy image can effectively denoise that image.\n  Public implementations of the networks and methods described in this paper\ncan be found at https://github.com/ricedsp/D-AMP_Toolbox.\n",
    "topics": "{'Denoising': 0.9999913, 'Image Denoising': 0.99995434}",
    "score": 0.8084292166
  },
  {
    "id": "1908.09745",
    "title": "A Semantics-Guided Class Imbalance Learning Model for Zero-Shot\n  Classification",
    "abstract": "  Zero-Shot Classification (ZSC) equips the learned model with the ability to\nrecognize the visual instances from the novel classes via constructing the\ninteractions between the visual and the semantic modalities. In contrast to the\ntraditional image classification, ZSC is easily suffered from the\nclass-imbalance issue since it is more concerned with the class-level knowledge\ntransfer capability. In the real world, the class samples follow a long-tailed\ndistribution, and the discriminative information in the sample-scarce seen\nclasses is hard to be transferred to the related unseen classes in the\ntraditional batch-based training manner, which degrades the overall\ngeneralization ability a lot. Towards alleviating the class imbalance issue in\nZSC, we propose a sample-balanced training process to encourage all training\nclasses to contribute equally to the learned model. Specifically, we randomly\nselect the same number of images from each class across all training classes to\nform a training batch to ensure that the sample-scarce classes contribute\nequally as those classes with sufficient samples during each iteration.\nConsidering that the instances from the same class differ in class\nrepresentativeness, we further develop an efficient semantics-guided feature\nfusion model to obtain discriminative class visual prototype for the following\nvisual-semantic interaction process via distributing different weights to the\nselected samples based on their class representativeness. Extensive experiments\non three imbalanced ZSC benchmark datasets for both the Traditional ZSC (TZSC)\nand the Generalized ZSC (GZSC) tasks demonstrate our approach achieves\npromising results especially for the unseen categories those are closely\nrelated to the sample-scarce seen categories.\n",
    "topics": "{'Transfer Learning': 0.9697753, 'Image Classification': 0.9520793, 'Zero-Shot Learning': 0.9000829}",
    "score": 0.8083883518
  },
  {
    "id": "1808.08517",
    "title": "An Incremental Construction of Deep Neuro Fuzzy System for Continual\n  Learning of Non-stationary Data Streams",
    "abstract": "  Existing FNNs are mostly developed under a shallow network configuration\nhaving lower generalization power than those of deep structures. This paper\nproposes a novel self-organizing deep FNN, namely DEVFNN. Fuzzy rules can be\nautomatically extracted from data streams or removed if they play limited role\nduring their lifespan. The structure of the network can be deepened on demand\nby stacking additional layers using a drift detection method which not only\ndetects the covariate drift, variations of input space, but also accurately\nidentifies the real drift, dynamic changes of both feature space and target\nspace. DEVFNN is developed under the stacked generalization principle via the\nfeature augmentation concept where a recently developed algorithm, namely\ngClass, drives the hidden layer. It is equipped by an automatic feature\nselection method which controls activation and deactivation of input attributes\nto induce varying subsets of input features. A deep network simplification\nprocedure is put forward using the concept of hidden layer merging to prevent\nuncontrollable growth of dimensionality of input space due to the nature of\nfeature augmentation approach in building a deep network structure. DEVFNN\nworks in the sample-wise fashion and is compatible for data stream\napplications. The efficacy of DEVFNN has been thoroughly evaluated using seven\ndatasets with non-stationary properties under the prequential test-then-train\nprotocol. It has been compared with four popular continual learning algorithms\nand its shallow counterpart where DEVFNN demonstrates improvement of\nclassification accuracy. Moreover, it is also shown that the concept drift\ndetection method is an effective tool to control the depth of network structure\nwhile the hidden layer merging scenario is capable of simplifying the network\ncomplexity of a deep network with negligible compromise of generalization\nperformance.\n",
    "topics": "{'Continual Learning': 0.9999995, 'Feature Selection': 0.4881876}",
    "score": 0.8083621162
  },
  {
    "id": "2010.03975",
    "title": "Synthesising clinically realistic Chest X-rays using Generative\n  Adversarial Networks",
    "abstract": "  Chest x-rays are one of the most commonly performed medical investigations\nglobally and are vital to identifying a number of conditions. These images are\nhowever protected under patient confidentiality and as such require the removal\nof identifying information as well as ethical clearance to be released.\nGenerative adversarial networks (GANs) are a branch of deep learning which are\ncapable of producing synthetic samples of a desired distribution. Image\ngeneration is one such application with recent advances enabling the production\nof high-resolution images, a feature vital to the utility of x-rays given the\nscale of various pathologies. We apply the Progressive Growing GAN (PGGAN) to\nthe task of chest x-ray generation with the goal of being able to produce\nimages without any ethical concerns that may be used for medical education or\nin other machine learning work. We evaluate the properties of the generated\nx-rays with a practicing radiologist and demonstrate that high-quality,\nrealistic images can be produced with global features consistent with\npathologies seen in the NIH dataset. Improvements in the reproduction of\nsmall-scale details remains for future work. We train a classification model on\nthe NIH images and evaluate the distribution of disease labels across the\ngenerated samples. We find that the model is capable of reproducing all the\nabnormalities in a similar proportion to the source image distribution as\nlabelled by the classifier. We additionally demonstrate that the latent space\ncan be optimised to produce images of a particular class despite unconditional\ntraining, with the model producing related features and complications for the\nclass of interest. We also validate the application of the Fr'echet Inception\nDistance (FID) to x-ray images and determine that the PGGAN reproduces x-ray\nimages with an FID of 8.02, which is similar to other high resolution tasks.\n",
    "topics": "{'Image Generation': 0.81138784}",
    "score": 0.8083560767
  },
  {
    "id": "1811.11057",
    "title": "Fast Object Detection in Compressed Video",
    "abstract": "  Object detection in videos has drawn increasing attention since it is more\npractical in real scenarios. Most of the deep learning methods use CNNs to\nprocess each decoded frame in a video stream individually. However, the free of\ncharge yet valuable motion information already embedded in the video\ncompression format is usually overlooked. In this paper, we propose a fast\nobject detection method by taking advantage of this with a novel Motion aided\nMemory Network (MMNet). The MMNet has two major advantages: 1) It significantly\naccelerates the procedure of feature extraction for compressed videos. It only\nneed to run a complete recognition network for I-frames, i.e. a few reference\nframes in a video, and it produces the features for the following P frames\n(predictive frames) with a light weight memory network, which runs fast; 2)\nUnlike existing methods that establish an additional network to model motion of\nframes, we take full advantage of both motion vectors and residual errors that\nare freely available in video streams. To our best knowledge, the MMNet is the\nfirst work that investigates a deep convolutional detector on compressed\nvideos. Our method is evaluated on the large-scale ImageNet VID dataset, and\nthe results show that it is 3x times faster than single image detector R-FCN\nand 10x times faster than high-performance detector MANet at a minor accuracy\nloss.\n",
    "topics": "{'Object Detection': 0.99999833, 'Video Compression': 0.9999887, 'Real-Time Object Detection': 0.9999167}",
    "score": 0.8083479464
  },
  {
    "id": "1704.08772",
    "title": "Deep Face Deblurring",
    "abstract": "  Blind deblurring consists a long studied task, however the outcomes of\ngeneric methods are not effective in real world blurred images. Domain-specific\nmethods for deblurring targeted object categories, e.g. text or faces,\nfrequently outperform their generic counterparts, hence they are attracting an\nincreasing amount of attention. In this work, we develop such a domain-specific\nmethod to tackle deblurring of human faces, henceforth referred to as face\ndeblurring. Studying faces is of tremendous significance in computer vision,\nhowever face deblurring has yet to demonstrate some convincing results. This\ncan be partly attributed to the combination of i) poor texture and ii) highly\nstructure shape that yield the contour/gradient priors (that are typically\nused) sub-optimal. In our work instead of making assumptions over the prior, we\nadopt a learning approach by inserting weak supervision that exploits the\nwell-documented structure of the face. Namely, we utilise a deep network to\nperform the deblurring and employ a face alignment technique to pre-process\neach face. We additionally surpass the requirement of the deep network for\nthousands training samples, by introducing an efficient framework that allows\nthe generation of a large dataset. We utilised this framework to create 2MF2, a\ndataset of over two million frames. We conducted experiments with real world\nblurred facial images and report that our method returns a result close to the\nsharp natural latent image.\n",
    "topics": "{'Deblurring': 1.0}",
    "score": 0.8083255649
  },
  {
    "id": "1910.08071",
    "title": "Context-Aware Saliency Detection for Image Retargeting Using\n  Convolutional Neural Networks",
    "abstract": "  Image retargeting is the task of making images capable of being displayed on\nscreens with different sizes. This work should be done so that high-level\nvisual information and low-level features such as texture remain as intact as\npossible to the human visual system, while the output image may have different\ndimensions. Thus, simple methods such as scaling and cropping are not adequate\nfor this purpose. In recent years, researchers have tried to improve the\nexisting retargeting methods and introduce new ones. However, a specific method\ncannot be utilized to retarget all types of images. In other words, different\nimages require different retargeting methods. Image retargeting has a close\nrelationship to image saliency detection, which is relatively a new image\nprocessing task. Earlier saliency detection methods were based on local and\nglobal but low-level image information. These methods are called bottom-up\nmethods. On the other hand, newer approaches are top-down and mixed methods\nthat consider the high level and semantic information of the image too. In this\npaper, we introduce the proposed methods in both saliency detection and\nretargeting. For the saliency detection, the use of image context and semantic\nsegmentation are examined, and a novel mixed bottom-up, and top-down saliency\ndetection method is introduced. After saliency detection, a modified version of\nan existing retargeting method is utilized for retargeting the images. The\nresults suggest that the proposed image retargeting pipeline has excellent\nperformance compared to other tested methods. Also, the subjective evaluations\non the Pascal dataset can be used as a retargeting quality assessment dataset\nfor further research.\n",
    "topics": "{'Saliency Detection': 1.0}",
    "score": 0.8082264491
  },
  {
    "id": "2005.03912",
    "title": "An Extensive Study on Cross-Dataset Bias and Evaluation Metrics\n  Interpretation for Machine Learning applied to Gastrointestinal Tract\n  Abnormality Classification",
    "abstract": "  Precise and efficient automated identification of Gastrointestinal (GI) tract\ndiseases can help doctors treat more patients and improve the rate of disease\ndetection and identification. Currently, automatic analysis of diseases in the\nGI tract is a hot topic in both computer science and medical-related journals.\nNevertheless, the evaluation of such an automatic analysis is often incomplete\nor simply wrong. Algorithms are often only tested on small and biased datasets,\nand cross-dataset evaluations are rarely performed. A clear understanding of\nevaluation metrics and machine learning models with cross datasets is crucial\nto bring research in the field to a new quality level. Towards this goal, we\npresent comprehensive evaluations of five distinct machine learning models\nusing Global Features and Deep Neural Networks that can classify 16 different\nkey types of GI tract conditions, including pathological findings, anatomical\nlandmarks, polyp removal conditions, and normal findings from images captured\nby common GI tract examination instruments. In our evaluation, we introduce\nperformance hexagons using six performance metrics such as recall, precision,\nspecificity, accuracy, F1-score, and Matthews Correlation Coefficient to\ndemonstrate how to determine the real capabilities of models rather than\nevaluating them shallowly. Furthermore, we perform cross-dataset evaluations\nusing different datasets for training and testing. With these cross-dataset\nevaluations, we demonstrate the challenge of actually building a generalizable\nmodel that could be used across different hospitals. Our experiments clearly\nshow that more sophisticated performance metrics and evaluation methods need to\nbe applied to get reliable models rather than depending on evaluations of the\nsplits of the same dataset, i.e., the performance metrics should always be\ninterpreted together rather than relying on a single metric.\n",
    "topics": "{}",
    "score": 0.8080397571
  },
  {
    "id": "2006.09179",
    "title": "Reconstruction of turbulent data with deep generative models for\n  semantic inpainting from TURB-Rot database",
    "abstract": "  We study the applicability of tools developed by the computer vision\ncommunity for features learning and semantic image inpainting to perform data\nreconstruction of fluid turbulence configurations. The aim is twofold. First,\nwe explore on a quantitative basis, the capability of Convolutional Neural\nNetworks embedded in a Deep Generative Adversarial Model (Deep-GAN) to generate\nmissing data in turbulence, a paradigmatic high dimensional chaotic system. In\nparticular, we investigate their use in reconstructing two-dimensional damaged\nsnapshots extracted from a large database of numerical configurations of 3d\nturbulence in the presence of rotation, a case with multi-scale random features\nwhere both large-scale organised structures and small-scale highly intermittent\nand non-Gaussian fluctuations are present. Second, following a reverse\nengineering approach, we aim to rank the input flow properties (features) in\nterms of their qualitative and quantitative importance to obtain a better set\nof reconstructed fields. We present two approaches both based on Context\nEncoders. The first one infers the missing data via a minimization of the L2\npixel-wise reconstruction loss, plus a small adversarial penalisation. The\nsecond searches for the closest encoding of the corrupted flow configuration\nfrom a previously trained generator. Finally, we present a comparison with a\ndifferent data assimilation tool, based on Nudging, an equation-informed\nunbiased protocol, well known in the numerical weather prediction community.\nThe TURB-Rot database, \\url{http://smart-turb.roma2.infn.it}, of roughly 300K\n2d turbulent images is released and details on how to download it are given.\n",
    "topics": "{'Image Inpainting': 1.0}",
    "score": 0.8080024385
  },
  {
    "id": "1706.00712",
    "title": "Convolutional Neural Networks for Medical Image Analysis: Full Training\n  or Fine Tuning?",
    "abstract": "  Training a deep convolutional neural network (CNN) from scratch is difficult\nbecause it requires a large amount of labeled training data and a great deal of\nexpertise to ensure proper convergence. A promising alternative is to fine-tune\na CNN that has been pre-trained using, for instance, a large set of labeled\nnatural images. However, the substantial differences between natural and\nmedical images may advise against such knowledge transfer. In this paper, we\nseek to answer the following central question in the context of medical image\nanalysis: \\emph{Can the use of pre-trained deep CNNs with sufficient\nfine-tuning eliminate the need for training a deep CNN from scratch?} To\naddress this question, we considered 4 distinct medical imaging applications in\n3 specialties (radiology, cardiology, and gastroenterology) involving\nclassification, detection, and segmentation from 3 different imaging\nmodalities, and investigated how the performance of deep CNNs trained from\nscratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner.\nOur experiments consistently demonstrated that (1) the use of a pre-trained CNN\nwith adequate fine-tuning outperformed or, in the worst case, performed as well\nas a CNN trained from scratch; (2) fine-tuned CNNs were more robust to the size\nof training sets than CNNs trained from scratch; (3) neither shallow tuning nor\ndeep tuning was the optimal choice for a particular application; and (4) our\nlayer-wise fine-tuning scheme could offer a practical way to reach the best\nperformance for the application at hand based on the amount of available data.\n",
    "topics": "{'Transfer Learning': 0.9924264}",
    "score": 0.8079034569
  },
  {
    "id": "1606.00915",
    "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,\n  Atrous Convolution, and Fully Connected CRFs",
    "abstract": "  In this work we address the task of semantic image segmentation with Deep\nLearning and make three main contributions that are experimentally shown to\nhave substantial practical merit. First, we highlight convolution with\nupsampled filters, or 'atrous convolution', as a powerful tool in dense\nprediction tasks. Atrous convolution allows us to explicitly control the\nresolution at which feature responses are computed within Deep Convolutional\nNeural Networks. It also allows us to effectively enlarge the field of view of\nfilters to incorporate larger context without increasing the number of\nparameters or the amount of computation. Second, we propose atrous spatial\npyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP\nprobes an incoming convolutional feature layer with filters at multiple\nsampling rates and effective fields-of-views, thus capturing objects as well as\nimage context at multiple scales. Third, we improve the localization of object\nboundaries by combining methods from DCNNs and probabilistic graphical models.\nThe commonly deployed combination of max-pooling and downsampling in DCNNs\nachieves invariance but has a toll on localization accuracy. We overcome this\nby combining the responses at the final DCNN layer with a fully connected\nConditional Random Field (CRF), which is shown both qualitatively and\nquantitatively to improve localization performance. Our proposed \"DeepLab\"\nsystem sets the new state-of-art at the PASCAL VOC-2012 semantic image\nsegmentation task, reaching 79.7% mIOU in the test set, and advances the\nresults on three other datasets: PASCAL-Context, PASCAL-Person-Part, and\nCityscapes. All of our code is made publicly available online.\n",
    "topics": "{'Semantic Segmentation': 0.9991685}",
    "score": 0.8079000101
  },
  {
    "id": "1811.01704",
    "title": "ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural\n  Networks",
    "abstract": "  Deep Neural Networks (DNNs) typically require massive amount of computation\nresource in inference tasks for computer vision applications. Quantization can\nsignificantly reduce DNN computation and storage by decreasing the bitwidth of\nnetwork encodings. Recent research affirms that carefully selecting the\nquantization levels for each layer can preserve the accuracy while pushing the\nbitwidth below eight bits. However, without arduous manual effort, this deep\nquantization can lead to significant accuracy loss, leaving it in a position of\nquestionable utility. As such, deep quantization opens a large hyper-parameter\nspace (bitwidth of the layers), the exploration of which is a major challenge.\nWe propose a systematic approach to tackle this problem, by automating the\nprocess of discovering the quantization levels through an end-to-end deep\nreinforcement learning framework (ReLeQ). We adapt policy optimization methods\nto the problem of quantization, and focus on finding the best design decisions\nin choosing the state and action spaces, network architecture and training\nframework, as well as the tuning of various hyperparamters. We show how ReLeQ\ncan balance speed and quality, and provide an asymmetric general solution for\nquantization of a large variety of deep networks (AlexNet, CIFAR-10, LeNet,\nMobileNet-V1, ResNet-20, SVHN, and VGG-11) that virtually preserves the\naccuracy (=< 0.3% loss) while minimizing the computation and storage cost. With\nthese DNNs, ReLeQ enables conventional hardware to achieve 2.2x speedup over\n8-bit execution. Similarly, a custom DNN accelerator achieves 2.0x speedup and\nenergy reduction compared to 8-bit runs. These encouraging results mark ReLeQ\nas the initial step towards automating the deep quantization of neural\nnetworks.\n",
    "topics": "{'Quantization': 1.0}",
    "score": 0.8078877078
  },
  {
    "id": "1911.07042",
    "title": "Automatic Annotation of Hip Anatomy in Fluoroscopy for Robust and\n  Efficient 2D/3D Registration",
    "abstract": "  Fluoroscopy is the standard imaging modality used to guide hip surgery and is\ntherefore a natural sensor for computer-assisted navigation. In order to\nefficiently solve the complex registration problems presented during\nnavigation, human-assisted annotations of the intraoperative image are\ntypically required. This manual initialization interferes with the surgical\nworkflow and diminishes any advantages gained from navigation. We propose a\nmethod for fully automatic registration using annotations produced by a neural\nnetwork. Neural networks are trained to simultaneously segment anatomy and\nidentify landmarks in fluoroscopy. Training data is obtained using an\nintraoperatively incompatible 2D/3D registration of hip anatomy. Ground truth\n2D labels are established using projected 3D annotations. Intraoperative\nregistration couples an intensity-based strategy with annotations inferred by\nthe network and requires no human assistance. Ground truth labels were obtained\nin 366 fluoroscopic images across 6 cadaveric specimens. In a\nleave-one-subject-out experiment, networks obtained mean dice coefficients for\nleft and right hemipelves, left and right femurs of 0.86, 0.87, 0.90, and 0.84.\nThe mean 2D landmark error was 5.0 mm. The pelvis was registered within 1\ndegree for 86% of the images when using the proposed intraoperative approach\nwith an average runtime of 7 seconds. In comparison, an intensity-only approach\nwithout manual initialization, registered the pelvis to 1 degree in 18% of\nimages. We have created the first accurately annotated, non-synthetic, dataset\nof hip fluoroscopy. By using these annotations as training data for neural\nnetworks, state of the art performance in fluoroscopic segmentation and\nlandmark localization was achieved. Integrating these annotations allows for a\nrobust, fully automatic, and efficient intraoperative registration during\nfluoroscopic navigation of the hip.\n",
    "topics": "{'Image Registration': 0.78328985}",
    "score": 0.8078508599
  },
  {
    "id": "1604.07507",
    "title": "Once for All: a Two-flow Convolutional Neural Network for Visual\n  Tracking",
    "abstract": "  One of the main challenges of visual object tracking comes from the arbitrary\nappearance of objects. Most existing algorithms try to resolve this problem as\nan object-specific task, i.e., the model is trained to regenerate or classify a\nspecific object. As a result, the model need to be initialized and retrained\nfor different objects. In this paper, we propose a more generic approach\nutilizing a novel two-flow convolutional neural network (named YCNN). The YCNN\ntakes two inputs (one is object image patch, the other is search image patch),\nthen outputs a response map which predicts how likely the object appears in a\nspecific location. Unlike those object-specific approach, the YCNN is trained\nto measure the similarity between two image patches. Thus it will not be\nconfined to any specific object. Furthermore the network can be end-to-end\ntrained to extract both shallow and deep convolutional features which are\ndedicated for visual tracking. And once properly trained, the YCNN can be\napplied to track all kinds of objects without further training and updating.\nBenefiting from the once-for-all model, our algorithm is able to run at a very\nhigh speed of 45 frames-per-second. The experiments on 51 sequences also show\nthat our algorithm achieves an outstanding performance.\n",
    "topics": "{'Visual Tracking': 1.0, 'Object Tracking': 0.99999857, 'Visual Object Tracking': 0.99998355}",
    "score": 0.8078040066
  },
  {
    "id": "1905.07991",
    "title": "Deep Transfer Learning Methods for Colon Cancer Classification in\n  Confocal Laser Microscopy Images",
    "abstract": "  Purpose: The gold standard for colorectal cancer metastases detection in the\nperitoneum is histological evaluation of a removed tissue sample. For feedback\nduring interventions, real-time in-vivo imaging with confocal laser microscopy\nhas been proposed for differentiation of benign and malignant tissue by manual\nexpert evaluation. Automatic image classification could improve the surgical\nworkflow further by providing immediate feedback.\n  Methods: We analyze the feasibility of classifying tissue from confocal laser\nmicroscopy in the colon and peritoneum. For this purpose, we adopt both\nclassical and state-of-the-art convolutional neural networks to directly learn\nfrom the images. As the available dataset is small, we investigate several\ntransfer learning strategies including partial freezing variants and full\nfine-tuning. We address the distinction of different tissue types, as well as\nbenign and malignant tissue.\n  Results: We present a thorough analysis of transfer learning strategies for\ncolorectal cancer with confocal laser microscopy. In the peritoneum, metastases\nare classified with an AUC of 97.1 and in the colon, the primarius is\nclassified with an AUC of 73.1. In general, transfer learning substantially\nimproves performance over training from scratch. We find that the optimal\ntransfer learning strategy differs for models and classification tasks.\n  Conclusions: We demonstrate that convolutional neural networks and transfer\nlearning can be used to identify cancer tissue with confocal laser microscopy.\nWe show that there is no generally optimal transfer learning strategy and model\nas well as task-specific engineering is required. Given the high performance\nfor the peritoneum, even with a small dataset, application for intraoperative\ndecision support could be feasible.\n",
    "topics": "{'Transfer Learning': 1.0, 'Image Classification': 0.96523553}",
    "score": 0.807803688
  },
  {
    "id": "1901.07012",
    "title": "Understanding the Impact of Label Granularity on CNN-based Image\n  Classification",
    "abstract": "  In recent years, supervised learning using Convolutional Neural Networks\n(CNNs) has achieved great success in image classification tasks, and large\nscale labeled datasets have contributed significantly to this achievement.\nHowever, the definition of a label is often application dependent. For example,\nan image of a cat can be labeled as \"cat\" or perhaps more specifically \"Persian\ncat.\" We refer to this as label granularity. In this paper, we conduct\nextensive experiments using various datasets to demonstrate and analyze how and\nwhy training based on fine-grain labeling, such as \"Persian cat\" can improve\nCNN accuracy on classifying coarse-grain classes, in this case \"cat.\" The\nexperimental results show that training CNNs with fine-grain labels improves\nboth network's optimization and generalization capabilities, as intuitively it\nencourages the network to learn more features, and hence increases\nclassification accuracy on coarse-grain classes under all datasets considered.\nMoreover, fine-grain labels enhance data efficiency in CNN training. For\nexample, a CNN trained with fine-grain labels and only 40% of the total\ntraining data can achieve higher accuracy than a CNN trained with the full\ntraining dataset and coarse-grain labels. These results point to two possible\napplications of this work: (i) with sufficient human resources, one can improve\nCNN performance by re-labeling the dataset with fine-grain labels, and (ii)\nwith limited human resources, to improve CNN performance, rather than\ncollecting more training data, one may instead use fine-grain labels for the\ndataset. We further propose a metric called Average Confusion Ratio to\ncharacterize the effectiveness of fine-grain labeling, and show its use through\nextensive experimentation. Code is available at\nhttps://github.com/cmu-enyac/Label-Granularity.\n",
    "topics": "{'Image Classification': 0.9996014}",
    "score": 0.8077593704
  },
  {
    "id": "0804.3361",
    "title": "A New Approach to Automated Epileptic Diagnosis Using EEG and\n  Probabilistic Neural Network",
    "abstract": "  Epilepsy is one of the most common neurological disorders that greatly impair\npatient' daily lives. Traditional epileptic diagnosis relies on tedious visual\nscreening by neurologists from lengthy EEG recording that requires the presence\nof seizure (ictal) activities. Nowadays, there are many systems helping the\nneurologists to quickly find interesting segments of the lengthy signal by\nautomatic seizure detection. However, we notice that it is very difficult, if\nnot impossible, to obtain long-term EEG data with seizure activities for\nepilepsy patients in areas lack of medical resources and trained neurologists.\nTherefore, we propose to study automated epileptic diagnosis using interictal\nEEG data that is much easier to collect than ictal data. The authors are not\naware of any report on automated EEG diagnostic system that can accurately\ndistinguish patients' interictal EEG from the EEG of normal people. The\nresearch presented in this paper, therefore, aims to develop an automated\ndiagnostic system that can use interictal EEG data to diagnose whether the\nperson is epileptic. Such a system should also detect seizure activities for\nfurther investigation by doctors and potential patient monitoring. To develop\nsuch a system, we extract four classes of features from the EEG data and build\na Probabilistic Neural Network (PNN) fed with these features. Leave-one-out\ncross-validation (LOO-CV) on a widely used epileptic-normal data set reflects\nan impressive 99.5% accuracy of our system on distinguishing normal people's\nEEG from patient's interictal EEG. We also find our system can be used in\npatient monitoring (seizure detection) and seizure focus localization, with\n96.7% and 77.5% accuracy respectively on the data set.\n",
    "topics": "{'EEG': 1.0}",
    "score": 0.8077582825
  },
  {
    "id": "1804.04543",
    "title": "Forecasting Future Humphrey Visual Fields Using Deep Learning",
    "abstract": "  Purpose: To determine if deep learning networks could be trained to forecast\na future 24-2 Humphrey Visual Field (HVF).\n  Participants: All patients who obtained a HVF 24-2 at the University of\nWashington.\n  Methods: All datapoints from consecutive 24-2 HVFs from 1998 to 2018 were\nextracted from a University of Washington database. Ten-fold cross validation\nwith a held out test set was used to develop the three main phases of model\ndevelopment: model architecture selection, dataset combination selection, and\ntime-interval model training with transfer learning, to train a deep learning\nartificial neural network capable of generating a point-wise visual field\nprediction.\n  Results: More than 1.7 million perimetry points were extracted to the\nhundredth decibel from 32,443 24-2 HVFs. The best performing model with 20\nmillion trainable parameters, CascadeNet-5, was selected. The overall MAE for\nthe test set was 2.47 dB (95% CI: 2.45 dB to 2.48 dB). The 100 fully trained\nmodels were able to successfully predict progressive field loss in glaucomatous\neyes up to 5.5 years in the future with a correlation of 0.92 between the MD of\npredicted and actual future HVF (p < 2.2 x 10 -16 ) and an average difference\nof 0.41 dB.\n  Conclusions: Using unfiltered real-world datasets, deep learning networks\nshow an impressive ability to not only learn spatio-temporal HVF changes but\nalso to generate predictions for future HVFs up to 5.5 years, given only a\nsingle HVF.\n",
    "topics": "{'Transfer Learning': 0.9879925}",
    "score": 0.8077413336
  },
  {
    "id": "1609.01859",
    "title": "Automatic Visual Theme Discovery from Joint Image and Text Corpora",
    "abstract": "  A popular approach to semantic image understanding is to manually tag images\nwith keywords and then learn a mapping from vi- sual features to keywords.\nManually tagging images is a subjective pro- cess and the same or very similar\nvisual contents are often tagged with different keywords. Furthermore, not all\ntags have the same descriptive power for visual contents and large vocabulary\navailable from natural language could result in a very diverse set of keywords.\nIn this paper, we propose an unsupervised visual theme discovery framework as a\nbetter (more compact, efficient and effective) alternative to semantic\nrepresen- tation of visual contents. We first show that tag based annotation\nlacks consistency and compactness for describing visually similar contents. We\nthen learn the visual similarity between tags based on the visual features of\nthe images containing the tags. At the same time, we use a natural language\nprocessing technique (word embedding) to measure the seman- tic similarity\nbetween tags. Finally, we cluster tags into visual themes based on their visual\nsimilarity and semantic similarity measures using a spectral clustering\nalgorithm. We conduct user studies to evaluate the effectiveness and\nrationality of the visual themes discovered by our unsu- pervised algorithm and\nobtains promising result. We then design three common computer vision tasks,\nexample based image search, keyword based image search and image labelling to\nexplore potential applica- tion of our visual themes discovery framework. In\nexperiments, visual themes significantly outperforms tags on semantic image\nunderstand- ing and achieve state-of-art performance in all three tasks. This\nagain demonstrate the effectiveness and versatility of proposed framework.\n",
    "topics": "{'Image Retrieval': 0.9963102, 'Semantic Similarity': 0.68242216}",
    "score": 0.8077193497
  },
  {
    "id": "1911.09249",
    "title": "Semantic Segmentation of Thigh Muscle using 2.5D Deep Learning Network\n  Trained with Limited Datasets",
    "abstract": "  Purpose: We propose a 2.5D deep learning neural network (DLNN) to\nautomatically classify thigh muscle into 11 classes and evaluate its\nclassification accuracy over 2D and 3D DLNN when trained with limited datasets.\nEnables operator invariant quantitative assessment of the thigh muscle volume\nchange with respect to the disease progression. Materials and methods:\nRetrospective datasets consist of 48 thigh volume (TV) cropped from CT DICOM\nimages. Cropped volumes were aligned with femur axis and resample in 2 mm\nvoxel-spacing. Proposed 2.5D DLNN consists of three 2D U-Net trained with\naxial, coronal and sagittal muscle slices respectively. A voting algorithm was\nused to combine the output of U-Nets to create final segmentation. 2.5D U-Net\nwas trained on PC with 38 TV and the remaining 10 TV were used to evaluate\nsegmentation accuracy of 10 classes within Thigh. The result segmentation of\nboth left and right thigh were de-cropped to original CT volume space. Finally,\nsegmentation accuracies were compared between proposed DLNN and 2D/3D U-Net.\nResults: Average segmentation DSC score accuracy of all classes with 2.5D U-Net\nas 91.18% and Average Surface distance (ASD) accuracy as 0.84 mm. We found,\nmean DSC score for 2D U-Net was 3.3% lower than the that of 2.5D U-Net and mean\nDSC score of 3D U-Net was 5.7% lower than that of 2.5D U-Net when trained with\nsame datasets. Conclusion: We achieved a faster computationally efficient and\nautomatic segmentation of thigh muscle into 11 classes with reasonable\naccuracy. Enables quantitative evaluation of muscle atrophy with disease\nprogression.\n",
    "topics": "{'Semantic Segmentation': 0.95516896}",
    "score": 0.807665051
  },
  {
    "id": "2010.05352",
    "title": "MoCo Pretraining Improves Representation and Transferability of Chest\n  X-ray Models",
    "abstract": "  Self-supervised approaches such as Momentum Contrast (MoCo) can leverage\nunlabeled data to produce pretrained models for subsequent fine-tuning on\nlabeled data. While MoCo has demonstrated promising results on natural image\nclassification tasks, its application to medical imaging tasks like chest X-ray\ninterpretation has been limited. Chest X-ray interpretation is fundamentally\ndifferent from natural image classification in ways that may limit the\napplicability of self-supervised approaches. In this work, we investigate\nwhether MoCo-pretraining leads to better representations or initializations for\nchest X-ray interpretation. We conduct MoCo-pretraining on CheXpert, a large\nlabeled dataset of X-rays, followed by supervised fine-tuning experiments on\nthe pleural effusion task. Using 0.1% of labeled training data, we find that a\nlinear model trained on MoCo-pretrained representations outperforms one trained\non representations without MoCo-pretraining by an AUC of 0.096 (95% CI 0.061,\n0.130), indicating that MoCo-pretrained representations are of higher quality.\nFurthermore, a model fine-tuned end-to-end with MoCo-pretraining outperforms\nits non-MoCo-pretrained counterpart by an AUC of 0.037 (95% CI 0.015, 0.062)\nwith the 0.1% label fraction. These AUC improvements are observed for all label\nfractions for both the linear model and an end-to-end fine-tuned model with the\ngreater improvements for smaller label fractions. Finally, we observe similar\nresults on a small, target chest X-ray dataset (Shenzhen dataset for\ntuberculosis) with MoCo-pretraining done on the source dataset (CheXpert),\nwhich suggests that pretraining on unlabeled X-rays can provide transfer\nlearning benefits for a target task. Our study demonstrates that\nMoCo-pretraining provides high-quality representations and transferable\ninitializations for chest X-ray interpretation.\n",
    "topics": "{'Image Classification': 0.9903679, 'Transfer Learning': 0.9820403}",
    "score": 0.8076363186
  },
  {
    "id": "1801.05412",
    "title": "An Automated System for Epilepsy Detection using EEG Brain Signals based\n  on Deep Learning Approach",
    "abstract": "  Epilepsy is a neurological disorder and for its detection, encephalography\n(EEG) is a commonly used clinical approach. Manual inspection of EEG brain\nsignals is a time-consuming and laborious process, which puts heavy burden on\nneurologists and affects their performance. Several automatic techniques have\nbeen proposed using traditional approaches to assist neurologists in detecting\nbinary epilepsy scenarios e.g. seizure vs. non-seizure or normal vs. ictal.\nThese methods do not perform well when classifying ternary case e.g. ictal vs.\nnormal vs. inter-ictal; the maximum accuracy for this case by the\nstate-of-the-art-methods is 97+-1%. To overcome this problem, we propose a\nsystem based on deep learning, which is an ensemble of pyramidal\none-dimensional convolutional neural network (P-1D-CNN) models. In a CNN model,\nthe bottleneck is the large number of learnable parameters. P-1D-CNN works on\nthe concept of refinement approach and it results in 60% fewer parameters\ncompared to traditional CNN models. Further to overcome the limitations of\nsmall amount of data, we proposed augmentation schemes for learning P-1D-CNN\nmodel. In almost all the cases concerning epilepsy detection, the proposed\nsystem gives an accuracy of 99.1+-0.9% on the University of Bonn dataset.\n",
    "topics": "{'EEG': 1.0, 'Small Data Image Classification': 0.9897752}",
    "score": 0.8074696901
  },
  {
    "id": "1505.00308",
    "title": "Multi-Object Classification and Unsupervised Scene Understanding Using\n  Deep Learning Features and Latent Tree Probabilistic Models",
    "abstract": "  Deep learning has shown state-of-art classification performance on datasets\nsuch as ImageNet, which contain a single object in each image. However,\nmulti-object classification is far more challenging. We present a unified\nframework which leverages the strengths of multiple machine learning methods,\nviz deep learning, probabilistic models and kernel methods to obtain\nstate-of-art performance on Microsoft COCO, consisting of non-iconic images. We\nincorporate contextual information in natural images through a conditional\nlatent tree probabilistic model (CLTM), where the object co-occurrences are\nconditioned on the extracted fc7 features from pre-trained Imagenet CNN as\ninput. We learn the CLTM tree structure using conditional pairwise\nprobabilities for object co-occurrences, estimated through kernel methods, and\nwe learn its node and edge potentials by training a new 3-layer neural network,\nwhich takes fc7 features as input. Object classification is carried out via\ninference on the learnt conditional tree model, and we obtain significant gain\nin precision-recall and F-measures on MS-COCO, especially for difficult object\ncategories. Moreover, the latent variables in the CLTM capture scene\ninformation: the images with top activations for a latent node have common\nthemes such as being a grasslands or a food scene, and on on. In addition, we\nshow that a simple k-means clustering of the inferred latent nodes alone\nsignificantly improves scene classification performance on the MIT-Indoor\ndataset, without the need for any retraining, and without using scene labels\nduring training. Thus, we present a unified framework for multi-object\nclassification and unsupervised scene understanding.\n",
    "topics": "{'Scene Understanding': 1.0, 'Scene Classification': 0.99997175, 'Object Classification': 0.99930334}",
    "score": 0.8074066494
  },
  {
    "id": "1907.03698",
    "title": "TrackNet: A Deep Learning Network for Tracking High-speed and Tiny\n  Objects in Sports Applications",
    "abstract": "  Ball trajectory data are one of the most fundamental and useful information\nin the evaluation of players' performance and analysis of game strategies.\nAlthough vision-based object tracking techniques have been developed to analyze\nsport competition videos, it is still challenging to recognize and position a\nhigh-speed and tiny ball accurately. In this paper, we develop a deep learning\nnetwork, called TrackNet, to track the tennis ball from broadcast videos in\nwhich the ball images are small, blurry, and sometimes with afterimage tracks\nor even invisible. The proposed heatmap-based deep learning network is trained\nto not only recognize the ball image from a single frame but also learn flying\npatterns from consecutive frames. TrackNet takes images with a size of\n$640\\times360$ to generate a detection heatmap from either a single frame or\nseveral consecutive frames to position the ball and can achieve high precision\neven on public domain videos. The network is evaluated on the video of the\nmen's singles final at the 2017 Summer Universiade, which is available on\nYouTube. The precision, recall, and F1-measure of TrackNet reach $99.7\\%$,\n$97.3\\%$, and $98.5\\%$, respectively. To prevent overfitting, 9 additional\nvideos are partially labeled together with a subset from the previous dataset\nto implement 10-fold cross-validation, and the precision, recall, and\nF1-measure are $95.3\\%$, $75.7\\%$, and $84.3\\%$, respectively. A conventional\nimage processing algorithm is also implemented to compare with TrackNet. Our\nexperiments indicate that TrackNet outperforms conventional method by a big\nmargin and achieves exceptional ball tracking performance. The dataset and demo\nvideo are available at https://nol.cs.nctu.edu.tw/ndo3je6av9/.\n",
    "topics": "{'Object Tracking': 0.9999896}",
    "score": 0.8072979159
  },
  {
    "id": "1805.04288",
    "title": "Piecewise classifier mappings: Learning fine-grained learners for novel\n  categories with few examples",
    "abstract": "  Humans are capable of learning a new fine-grained concept with very little\nsupervision, \\emph{e.g.}, few exemplary images for a species of bird, yet our\nbest deep learning systems need hundreds or thousands of labeled examples. In\nthis paper, we try to reduce this gap by studying the fine-grained image\nrecognition problem in a challenging few-shot learning setting, termed few-shot\nfine-grained recognition (FSFG). The task of FSFG requires the learning systems\nto build classifiers for novel fine-grained categories from few examples (only\none or less than five). To solve this problem, we propose an end-to-end\ntrainable deep network which is inspired by the state-of-the-art fine-grained\nrecognition model and is tailored for the FSFG task.\n  Specifically, our network consists of a bilinear feature learning module and\na classifier mapping module: while the former encodes the discriminative\ninformation of an exemplar image into a feature vector, the latter maps the\nintermediate feature into the decision boundary of the novel category. The key\nnovelty of our model is a \"piecewise mappings\" function in the classifier\nmapping module, which generates the decision boundary via learning a set of\nmore attainable sub-classifiers in a more parameter-economic way. We learn the\nexemplar-to-classifier mapping based on an auxiliary dataset in a meta-learning\nfashion, which is expected to be able to generalize to novel categories. By\nconducting comprehensive experiments on three fine-grained datasets, we\ndemonstrate that the proposed method achieves superior performance over the\ncompeting baselines.\n",
    "topics": "{'Few-Shot Learning': 0.999091, 'Meta-Learning': 0.992141}",
    "score": 0.8072451233
  },
  {
    "id": "1801.09975",
    "title": "Preparation of Improved Turkish DataSet for Sentiment Analysis in Social\n  Media",
    "abstract": "  A public dataset, with a variety of properties suitable for sentiment\nanalysis [1], event prediction, trend detection and other text mining\napplications, is needed in order to be able to successfully perform analysis\nstudies. The vast majority of data on social media is text-based and it is not\npossible to directly apply machine learning processes into these raw data,\nsince several different processes are required to prepare the data before the\nimplementation of the algorithms. For example, different misspellings of same\nword enlarge the word vector space unnecessarily, thereby it leads to reduce\nthe success of the algorithm and increase the computational power requirement.\nThis paper presents an improved Turkish dataset with an effective spelling\ncorrection algorithm based on Hadoop [2]. The collected data is recorded on the\nHadoop Distributed File System and the text based data is processed by\nMapReduce programming model. This method is suitable for the storage and\nprocessing of large sized text based social media data. In this study, movie\nreviews have been automatically recorded with Apache ManifoldCF (MCF) [3] and\ndata clusters have been created. Various methods compared such as Levenshtein\nand Fuzzy String Matching have been proposed to create a public dataset from\ncollected data. Experimental results show that the proposed algorithm, which\ncan be used as an open source dataset in sentiment analysis studies, have been\nperformed successfully to the detection and correction of spelling errors.\n",
    "topics": "{'Spelling Correction': 0.9998241, 'Sentiment Analysis': 0.8624649}",
    "score": 0.8072357029
  },
  {
    "id": "1903.03313",
    "title": "A Mutual Bootstrapping Model for Automated Skin Lesion Segmentation and\n  Classification",
    "abstract": "  Automated skin lesion segmentation and classification are two most essential\nand related tasks in the computer-aided diagnosis of skin cancer. Despite their\nprevalence, deep learning models are usually designed for only one task,\nignoring the potential benefits in jointly performing both tasks. In this\npaper, we propose the mutual bootstrapping deep convolutional neural networks\n(MB-DCNN) model for simultaneous skin lesion segmentation and classification.\nThis model consists of a coarse segmentation network (coarse-SN), a mask-guided\nclassification network (mask-CN), and an enhanced segmentation network\n(enhanced-SN). On one hand, the coarse-SN generates coarse lesion masks that\nprovide a prior bootstrapping for mask-CN to help it locate and classify skin\nlesions accurately. On the other hand, the lesion localization maps produced by\nmask-CN are then fed into enhanced-SN, aiming to transfer the localization\ninformation learned by mask-CN to enhanced-SN for accurate lesion segmentation.\nIn this way, both segmentation and classification networks mutually transfer\nknowledge between each other and facilitate each other in a bootstrapping way.\nMeanwhile, we also design a novel rank loss and jointly use it with the Dice\nloss in segmentation networks to address the issues caused by class imbalance\nand hard-easy pixel imbalance. We evaluate the proposed MB-DCNN model on the\nISIC-2017 and PH2 datasets, and achieve a Jaccard index of 80.4% and 89.4% in\nskin lesion segmentation and an average AUC of 93.8% and 97.7% in skin lesion\nclassification, which are superior to the performance of representative\nstate-of-the-art skin lesion segmentation and classification methods. Our\nresults suggest that it is possible to boost the performance of skin lesion\nsegmentation and classification simultaneously via training a unified model to\nperform both tasks in a mutual bootstrapping way.\n",
    "topics": "{'Lesion Segmentation': 1.0, 'Lesion Classification': 0.99824035}",
    "score": 0.8072105444
  },
  {
    "id": "1806.07486",
    "title": "Standard Plane Detection in 3D Fetal Ultrasound Using an Iterative\n  Transformation Network",
    "abstract": "  Standard scan plane detection in fetal brain ultrasound (US) forms a crucial\nstep in the assessment of fetal development. In clinical settings, this is done\nby manually manoeuvring a 2D probe to the desired scan plane. With the advent\nof 3D US, the entire fetal brain volume containing these standard planes can be\neasily acquired. However, manual standard plane identification in 3D volume is\nlabour-intensive and requires expert knowledge of fetal anatomy. We propose a\nnew Iterative Transformation Network (ITN) for the automatic detection of\nstandard planes in 3D volumes. ITN uses a convolutional neural network to learn\nthe relationship between a 2D plane image and the transformation parameters\nrequired to move that plane towards the location/orientation of the standard\nplane in the 3D volume. During inference, the current plane image is passed\niteratively to the network until it converges to the standard plane location.\nWe explore the effect of using different transformation representations as\nregression outputs of ITN. Under a multi-task learning framework, we introduce\nadditional classification probability outputs to the network to act as\nconfidence measures for the regressed transformation parameters in order to\nfurther improve the localisation accuracy. When evaluated on 72 US volumes of\nfetal brain, our method achieves an error of 3.83mm/12.7 degrees and\n3.80mm/12.6 degrees for the transventricular and transcerebellar planes\nrespectively and takes 0.46s per plane. Source code is publicly available at\nhttps://github.com/yuanwei1989/plane-detection.\n",
    "topics": "{'3D Reconstruction': 0.950454, 'Language Modelling': 0.4552411}",
    "score": 0.8071940914
  },
  {
    "id": "1909.01498",
    "title": "Demystifying Brain Tumour Segmentation Networks: Interpretability and\n  Uncertainty Analysis",
    "abstract": "  The accurate automatic segmentation of gliomas and its intra-tumoral\nstructures is important not only for treatment planning but also for follow-up\nevaluations. Several methods based on 2D and 3D Deep Neural Networks (DNN) have\nbeen developed to segment brain tumors and to classify different categories of\ntumors from different MRI modalities. However, these networks are often\nblack-box models and do not provide any evidence regarding the process they\ntake to perform this task. Increasing transparency and interpretability of such\ndeep learning techniques are necessary for the complete integration of such\nmethods into medical practice. In this paper, we explore various techniques to\nexplain the functional organization of brain tumor segmentation models and to\nextract visualizations of internal concepts to understand how these networks\nachieve highly accurate tumor segmentations. We use the BraTS 2018 dataset to\ntrain three different networks with standard architectures and outline\nsimilarities and differences in the process that these networks take to segment\nbrain tumors. We show that brain tumor segmentation networks learn certain\nhuman-understandable disentangled concepts on a filter level. We also show that\nthey take a top-down or hierarchical approach to localizing the different parts\nof the tumor. We then extract visualizations of some internal feature maps and\nalso provide a measure of uncertainty with regards to the outputs of the models\nto give additional qualitative evidence about the predictions of these\nnetworks. We believe that the emergence of such human-understandable\norganization and concepts might aid in the acceptance and integration of such\nmethods in medical diagnosis.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'Brain Tumor Segmentation': 1.0, 'Medical Diagnosis': 0.9999999}",
    "score": 0.8071922059
  },
  {
    "id": "2009.09182",
    "title": "ENAS4D: Efficient Multi-stage CNN Architecture Search for Dynamic\n  Inference",
    "abstract": "  Dynamic inference is a feasible way to reduce the computational cost of\nconvolutional neural network(CNN), which can dynamically adjust the computation\nfor each input sample. One of the ways to achieve dynamic inference is to use\nmulti-stage neural network, which contains a sub-network with prediction layer\nat each stage. The inference of a input sample can exit from early stage if the\nprediction of the stage is confident enough. However, design a multi-stage CNN\narchitecture is a non-trivial task. In this paper, we introduce a general\nframework, ENAS4D, which can efficiently search for optimal multi-stage CNN\narchitecture for dynamic inference in a well-designed search space. Firstly, we\npropose a method to construct the search space with multi-stage convolution.\nThe search space include different numbers of layers, different kernel sizes\nand different numbers of channels for each stage and the resolution of input\nsamples. Then, we train a once-for-all network that supports to sample diverse\nmulti-stage CNN architecture. A specialized multi-stage network can be obtained\nfrom the once-for-all network without additional training. Finally, we devise a\nmethod to efficiently search for the optimal multi-stage network that trades\nthe accuracy off the computational cost taking the advantage of once-for-all\nnetwork. The experiments on the ImageNet classification task demonstrate that\nthe multi-stage CNNs searched by ENAS4D consistently outperform the\nstate-of-the-art method for dyanmic inference. In particular, the network\nachieves 74.4% ImageNet top-1 accuracy under 185M average MACs.\n",
    "topics": "{}",
    "score": 0.8070991662
  },
  {
    "id": "2008.00119",
    "title": "CorrSigNet: Learning CORRelated Prostate Cancer SIGnatures from\n  Radiology and Pathology Images for Improved Computer Aided Diagnosis",
    "abstract": "  Magnetic Resonance Imaging (MRI) is widely used for screening and staging\nprostate cancer. However, many prostate cancers have subtle features which are\nnot easily identifiable on MRI, resulting in missed diagnoses and alarming\nvariability in radiologist interpretation. Machine learning models have been\ndeveloped in an effort to improve cancer identification, but current models\nlocalize cancer using MRI-derived features, while failing to consider the\ndisease pathology characteristics observed on resected tissue. In this paper,\nwe propose CorrSigNet, an automated two-step model that localizes prostate\ncancer on MRI by capturing the pathology features of cancer. First, the model\nlearns MRI signatures of cancer that are correlated with corresponding\nhistopathology features using Common Representation Learning. Second, the model\nuses the learned correlated MRI features to train a Convolutional Neural\nNetwork to localize prostate cancer. The histopathology images are used only in\nthe first step to learn the correlated features. Once learned, these correlated\nfeatures can be extracted from MRI of new patients (without histopathology or\nsurgery) to localize cancer. We trained and validated our framework on a unique\ndataset of 75 patients with 806 slices who underwent MRI followed by\nprostatectomy surgery. We tested our method on an independent test set of 20\nprostatectomy patients (139 slices, 24 cancerous lesions, 1.12M pixels) and\nachieved a per-pixel sensitivity of 0.81, specificity of 0.71, AUC of 0.86 and\na per-lesion AUC of $0.96 \\pm 0.07$, outperforming the current state-of-the-art\naccuracy in predicting prostate cancer using MRI.\n",
    "topics": "{'Representation Learning': 0.8871752}",
    "score": 0.8070945725
  },
  {
    "id": "1703.08050",
    "title": "Is Second-order Information Helpful for Large-scale Visual Recognition?",
    "abstract": "  By stacking layers of convolution and nonlinearity, convolutional networks\n(ConvNets) effectively learn from low-level to high-level features and\ndiscriminative representations. Since the end goal of large-scale recognition\nis to delineate complex boundaries of thousands of classes, adequate\nexploration of feature distributions is important for realizing full potentials\nof ConvNets. However, state-of-the-art works concentrate only on deeper or\nwider architecture design, while rarely exploring feature statistics higher\nthan first-order. We take a step towards addressing this problem. Our method\nconsists in covariance pooling, instead of the most commonly used first-order\npooling, of high-level convolutional features. The main challenges involved are\nrobust covariance estimation given a small sample of large-dimensional features\nand usage of the manifold structure of covariance matrices. To address these\nchallenges, we present a Matrix Power Normalized Covariance (MPN-COV) method.\nWe develop forward and backward propagation formulas regarding the nonlinear\nmatrix functions such that MPN-COV can be trained end-to-end. In addition, we\nanalyze both qualitatively and quantitatively its advantage over the well-known\nLog-Euclidean metric. On the ImageNet 2012 validation set, by combining MPN-COV\nwe achieve over 4%, 3% and 2.5% gains for AlexNet, VGG-M and VGG-16,\nrespectively; integration of MPN-COV into 50-layer ResNet outperforms\nResNet-101 and is comparable to ResNet-152. The source code will be available\non the project page: http://www.peihuali.org/MPN-COV\n",
    "topics": "{'Object Recognition': 0.893445}",
    "score": 0.8068347513
  },
  {
    "id": "2008.09092",
    "title": "Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data\n  Generation",
    "abstract": "  Procedural models are being widely used to synthesize scenes for graphics,\ngaming, and to create (labeled) synthetic datasets for ML. In order to produce\nrealistic and diverse scenes, a number of parameters governing the procedural\nmodels have to be carefully tuned by experts. These parameters control both the\nstructure of scenes being generated (e.g. how many cars in the scene), as well\nas parameters which place objects in valid configurations. Meta-Sim aimed at\nautomatically tuning parameters given a target collection of real images in an\nunsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition\nto parameters, which is a challenging problem due to its discrete nature.\nMeta-Sim2 proceeds by learning to sequentially sample rule expansions from a\ngiven probabilistic scene grammar. Due to the discrete nature of the problem,\nwe use Reinforcement Learning to train our model, and design a feature space\ndivergence between our synthesized and target images that is key to successful\ntraining. Experiments on a real driving dataset show that, without any\nsupervision, we can successfully learn to generate data that captures discrete\nstructural statistics of objects, such as their frequency, in real images. We\nalso show that this leads to downstream improvement in the performance of an\nobject detector trained on our generated dataset as opposed to other baseline\nsimulation methods. Project page:\nhttps://nv-tlabs.github.io/meta-sim-structure/.\n",
    "topics": "{}",
    "score": 0.8066631207
  },
  {
    "id": "1906.01796",
    "title": "One-pass Multi-task Networks with Cross-task Guided Attention for Brain\n  Tumor Segmentation",
    "abstract": "  Class imbalance has emerged as one of the major challenges for medical image\nsegmentation. The model cascade (MC) strategy significantly alleviates the\nclass imbalance issue via running a set of individual deep models for\ncoarse-to-fine segmentation. Despite its outstanding performance, however, this\nmethod leads to undesired system complexity and also ignores the correlation\namong the models. To handle these flaws, we propose a light-weight deep model,\ni.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better\nthan MC does, while requiring only one-pass computation. First, OM-Net\nintegrates the separate segmentation tasks into one deep model, which consists\nof shared parameters to learn joint features, as well as task-specific\nparameters to learn discriminative features. Second, to more effectively\noptimize OM-Net, we take advantage of the correlation among tasks to design\nboth an online training data transfer strategy and a curriculum learning-based\ntraining strategy. Third, we further propose sharing prediction results between\ntasks and design a cross-task guided attention (CGA) module which can\nadaptively recalibrate channel-wise feature responses based on the\ncategory-specific statistics. Finally, a simple yet effective post-processing\nmethod is introduced to refine the segmentation results. Extensive experiments\nare conducted to demonstrate the effectiveness of the proposed techniques. Most\nimpressively, we achieve state-of-the-art performance on the BraTS 2015 testing\nset and BraTS 2017 online validation set. Using these proposed approaches, we\nalso won joint third place in the BraTS 2018 challenge among 64 participating\nteams. The code is publicly available at\nhttps://github.com/chenhong-zhou/OM-Net.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'Brain Tumor Segmentation': 1.0, 'Medical Image Segmentation': 0.99992824, 'Semantic Segmentation': 0.94983584, 'Curriculum Learning': 0.53491646}",
    "score": 0.8066290398
  },
  {
    "id": "1804.04988",
    "title": "Convolutional Neural Networks for Skull-stripping in Brain MR Imaging\n  using Consensus-based Silver standard Masks",
    "abstract": "  Convolutional neural networks (CNN) for medical imaging are constrained by\nthe number of annotated data required in the training stage. Usually, manual\nannotation is considered to be the \"gold standard\". However, medical imaging\ndatasets that include expert manual segmentation are scarce as this step is\ntime-consuming, and therefore expensive. Moreover, single-rater manual\nannotation is most often used in data-driven approaches making the network\noptimal with respect to only that single expert. In this work, we propose a CNN\nfor brain extraction in magnetic resonance (MR) imaging, that is fully trained\nwith what we refer to as silver standard masks. Our method consists of 1)\ndeveloping a dataset with \"silver standard\" masks as input, and implementing\nboth 2) a tri-planar method using parallel 2D U-Net-based CNNs (referred to as\nCONSNet) and 3) an auto-context implementation of CONSNet. The term CONSNet\nrefers to our integrated approach, i.e., training with silver standard masks\nand using a 2D U-Net-based architecture. Our results showed that we\noutperformed (i.e., larger Dice coefficients) the current state-of-the-art SS\nmethods. Our use of silver standard masks reduced the cost of manual\nannotation, decreased inter-intra-rater variability, and avoided CNN\nsegmentation super-specialization towards one specific manual annotation\nguideline that can occur when gold standard masks are used. Moreover, the usage\nof silver standard masks greatly enlarges the volume of input annotated data\nbecause we can relatively easily generate labels for unlabeled data. In\naddition, our method has the advantage that, once trained, it takes only a few\nseconds to process a typical brain image volume using modern hardware, such as\na high-end graphics processing unit. In contrast, many of the other competitive\nmethods have processing times in the order of minutes.\n",
    "topics": "{}",
    "score": 0.8066265358
  },
  {
    "id": "1704.01137",
    "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks",
    "abstract": "  Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety\nof machine learning tasks and are deployed in increasing numbers of products\nand services. However, the computational requirements of training and\nevaluating large-scale DNNs are growing at a much faster pace than the\ncapabilities of the underlying hardware platforms that they are executed upon.\nIn this work, we propose Dynamic Variable Effort Deep Neural Networks\n(DyVEDeep) to reduce the computational requirements of DNNs during inference.\nPrevious efforts propose specialized hardware implementations for DNNs,\nstatically prune the network, or compress the weights. Complementary to these\napproaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in\nthe inputs to DNNs to improve their compute efficiency with comparable\nclassification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms\nthat, in the course of processing an input, identify how critical a group of\ncomputations are to classify the input. DyVEDeep dynamically focuses its\ncompute effort only on the critical computa- tions, while skipping or\napproximating the rest. We propose 3 effort knobs that operate at different\nlevels of granularity viz. neuron, feature and layer levels. We build DyVEDeep\nversions for 5 popular image recognition benchmarks - one for CIFAR-10 and four\nfor ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across\nall benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar\noperations, which translates to 1.8x-2.3x performance improvement over a\nCaffe-based implementation, with < 0.5% loss in accuracy.\n",
    "topics": "{}",
    "score": 0.8065774904
  },
  {
    "id": "1603.03183",
    "title": "Exploring Context with Deep Structured models for Semantic Segmentation",
    "abstract": "  State-of-the-art semantic image segmentation methods are mostly based on\ntraining deep convolutional neural networks (CNNs). In this work, we proffer to\nimprove semantic segmentation with the use of contextual information. In\nparticular, we explore `patch-patch' context and `patch-background' context in\ndeep CNNs. We formulate deep structured models by combining CNNs and\nConditional Random Fields (CRFs) for learning the patch-patch context between\nimage regions. Specifically, we formulate CNN-based pairwise potential\nfunctions to capture semantic correlations between neighboring patches.\nEfficient piecewise training of the proposed deep structured model is then\napplied in order to avoid repeated expensive CRF inference during the course of\nback propagation. For capturing the patch-background context, we show that a\nnetwork design with traditional multi-scale image inputs and sliding pyramid\npooling is very effective for improving performance. We perform comprehensive\nevaluation of the proposed method. We achieve new state-of-the-art performance\non a number of challenging semantic segmentation datasets including $NYUDv2$,\n$PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$,\n$SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report an\nintersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset.\n",
    "topics": "{'Semantic Segmentation': 1.0}",
    "score": 0.8065577733
  },
  {
    "id": "1811.01811",
    "title": "Active Deep Learning Attacks under Strict Rate Limitations for Online\n  API Calls",
    "abstract": "  Machine learning has been applied to a broad range of applications and some\nof them are available online as application programming interfaces (APIs) with\neither free (trial) or paid subscriptions. In this paper, we study adversarial\nmachine learning in the form of back-box attacks on online classifier APIs. We\nstart with a deep learning based exploratory (inference) attack, which aims to\nbuild a classifier that can provide similar classification results (labels) as\nthe target classifier. To minimize the difference between the labels returned\nby the inferred classifier and the target classifier, we show that the deep\nlearning based exploratory attack requires a large number of labeled training\ndata samples. These labels can be collected by calling the online API, but\nusually there is some strict rate limitation on the number of allowed API\ncalls. To mitigate the impact of limited training data, we develop an active\nlearning approach that first builds a classifier based on a small number of API\ncalls and uses this classifier to select samples to further collect their\nlabels. Then, a new classifier is built using more training data samples. This\nupdating process can be repeated multiple times. We show that this active\nlearning approach can build an adversarial classifier with a small statistical\ndifference from the target classifier using only a limited number of training\ndata samples. We further consider evasion and causative (poisoning) attacks\nbased on the inferred classifier that is built by the exploratory attack.\nEvasion attack determines samples that the target classifier is likely to\nmisclassify, whereas causative attack provides erroneous training data samples\nto reduce the reliability of the re-trained classifier. The success of these\nattacks show that adversarial machine learning emerges as a feasible threat in\nthe realistic case with limited training data.\n",
    "topics": "{'Active Learning': 0.9764722}",
    "score": 0.8064995806
  },
  {
    "id": "1701.06487",
    "title": "Dirty Pixels: Optimizing Image Classification Architectures for Raw\n  Sensor Data",
    "abstract": "  Real-world sensors suffer from noise, blur, and other imperfections that make\nhigh-level computer vision tasks like scene segmentation, tracking, and scene\nunderstanding difficult. Making high-level computer vision networks robust is\nimperative for real-world applications like autonomous driving, robotics, and\nsurveillance. We propose a novel end-to-end differentiable architecture for\njoint denoising, deblurring, and classification that makes classification\nrobust to realistic noise and blur. The proposed architecture dramatically\nimproves the accuracy of a classification network in low light and other\nchallenging conditions, outperforming alternative approaches such as retraining\nthe network on noisy and blurry images and preprocessing raw sensor inputs with\nconventional denoising and deblurring algorithms. The architecture learns\ndenoising and deblurring pipelines optimized for classification whose outputs\ndiffer markedly from those of state-of-the-art denoising and deblurring\nmethods, preserving fine detail at the cost of more noise and artifacts. Our\nresults suggest that the best low-level image processing for computer vision is\ndifferent from existing algorithms designed to produce visually pleasing\nimages. The principles used to design the proposed architecture easily extend\nto other high-level computer vision tasks and image formation models, providing\na general framework for integrating low-level and high-level image processing.\n",
    "topics": "{'Denoising': 1.0, 'Deblurring': 1.0, 'Scene Understanding': 0.9999999, 'Scene Segmentation': 0.9995433, 'Autonomous Driving': 0.99805164, 'Image Classification': 0.86479425}",
    "score": 0.8063706611
  },
  {
    "id": "1905.09894",
    "title": "PHom-GeM: Persistent Homology for Generative Models",
    "abstract": "  Generative neural network models, including Generative Adversarial Network\n(GAN) and Auto-Encoders (AE), are among the most popular neural network models\nto generate adversarial data. The GAN model is composed of a generator that\nproduces synthetic data and of a discriminator that discriminates between the\ngenerator's output and the true data. AE consist of an encoder which maps the\nmodel distribution to a latent manifold and of a decoder which maps the latent\nmanifold to a reconstructed distribution. However, generative models are known\nto provoke chaotically scattered reconstructed distribution during their\ntraining, and consequently, incomplete generated adversarial distributions.\nCurrent distance measures fail to address this problem because they are not\nable to acknowledge the shape of the data manifold, i.e. its topological\nfeatures, and the scale at which the manifold should be analyzed. We propose\nPersistent Homology for Generative Models, PHom-GeM, a new methodology to\nassess and measure the distribution of a generative model. PHom-GeM minimizes\nan objective function between the true and the reconstructed distributions and\nuses persistent homology, the study of the topological features of a space at\ndifferent spatial resolutions, to compare the nature of the true and the\ngenerated distributions. Our experiments underline the potential of persistent\nhomology for Wasserstein GAN in comparison to Wasserstein AE and Variational\nAE. The experiments are conducted on a real-world data set particularly\nchallenging for traditional distance measures and generative neural network\nmodels. PHom-GeM is the first methodology to propose a topological distance\nmeasure, the bottleneck distance, for generative models used to compare\nadversarial samples in the context of credit card transactions.\n",
    "topics": "{}",
    "score": 0.8063323446
  },
  {
    "id": "1804.10938",
    "title": "Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge,\n  Deep Architectures, and Beyond",
    "abstract": "  Automatic understanding of human affect using visual signals is of great\nimportance in everyday human-machine interactions. Appraising human emotional\nstates, behaviors and reactions displayed in real-world settings, can be\naccomplished using latent continuous dimensions (e.g., the circumplex model of\naffect). Valence (i.e., how positive or negative is an emotion) & arousal\n(i.e., power of the activation of the emotion) constitute popular and effective\naffect representations. Nevertheless, the majority of collected datasets this\nfar, although containing naturalistic emotional states, have been captured in\nhighly controlled recording conditions. In this paper, we introduce the\nAff-Wild benchmark for training and evaluating affect recognition algorithms.\nWe also report on the results of the First Affect-in-the-wild Challenge that\nwas organized in conjunction with CVPR 2017 on the Aff-Wild database and was\nthe first ever challenge on the estimation of valence and arousal in-the-wild.\nFurthermore, we design and extensively train an end-to-end deep neural\narchitecture which performs prediction of continuous emotion dimensions based\non visual cues. The proposed deep learning architecture, AffWildNet, includes\nconvolutional & recurrent neural network layers, exploiting the invariant\nproperties of convolutional features, while also modeling temporal dynamics\nthat arise in human behavior via the recurrent layers. The AffWildNet produced\nstate-of-the-art results on the Aff-Wild Challenge. We then exploit the AffWild\ndatabase for learning features, which can be used as priors for achieving best\nperformances both for dimensional, as well as categorical emotion recognition,\nusing the RECOLA, AFEW-VA and EmotiW datasets, compared to all other methods\ndesigned for the same goal. The database and emotion recognition models are\navailable at http://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge.\n",
    "topics": "{'Emotion Recognition': 0.99973077}",
    "score": 0.8063058729
  },
  {
    "id": "1804.10822",
    "title": "A Bimodal Learning Approach to Assist Multi-sensory Effects\n  Synchronization",
    "abstract": "  In mulsemedia applications, traditional media content (text, image, audio,\nvideo, etc.) can be related to media objects that target other human senses\n(e.g., smell, haptics, taste). Such applications aim at bridging the virtual\nand real worlds through sensors and actuators. Actuators are responsible for\nthe execution of sensory effects (e.g., wind, heat, light), which produce\nsensory stimulations on the users. In these applications sensory stimulation\nmust happen in a timely manner regarding the other traditional media content\nbeing presented. For example, at the moment in which an explosion is presented\nin the audiovisual content, it may be adequate to activate actuators that\nproduce heat and light. It is common to use some declarative multimedia\nauthoring language to relate the timestamp in which each media object is to be\npresented to the execution of some sensory effect. One problem in this setting\nis that the synchronization of media objects and sensory effects is done\nmanually by the author(s) of the application, a process which is time-consuming\nand error prone. In this paper, we present a bimodal neural network\narchitecture to assist the synchronization task in mulsemedia applications. Our\napproach is based on the idea that audio and video signals can be used\nsimultaneously to identify the timestamps in which some sensory effect should\nbe executed. Our learning architecture combines audio and video signals for the\nprediction of scene components. For evaluation purposes, we construct a dataset\nbased on Google's AudioSet. We provide experiments to validate our bimodal\narchitecture. Our results show that the bimodal approach produces better\nresults when compared to several variants of unimodal architectures.\n",
    "topics": "{}",
    "score": 0.8062006574
  },
  {
    "id": "1803.07950",
    "title": "End-to-End Video Captioning with Multitask Reinforcement Learning",
    "abstract": "  Although end-to-end (E2E) learning has led to impressive progress on a\nvariety of visual understanding tasks, it is often impeded by hardware\nconstraints (e.g., GPU memory) and is prone to overfitting. When it comes to\nvideo captioning, one of the most challenging benchmark tasks in computer\nvision, those limitations of E2E learning are especially amplified by the fact\nthat both the input videos and output captions are lengthy sequences. Indeed,\nstate-of-the-art methods for video captioning process video frames by\nconvolutional neural networks and generate captions by unrolling recurrent\nneural networks. If we connect them in an E2E manner, the resulting model is\nboth memory-consuming and data-hungry, making it extremely hard to train. In\nthis paper, we propose a multitask reinforcement learning approach to training\nan E2E video captioning model. The main idea is to mine and construct as many\neffective tasks (e.g., attributes, rewards, and the captions) as possible from\nthe human captioned videos such that they can jointly regulate the search space\nof the E2E neural network, from which an E2E video captioning model can be\nfound and generalized to the testing phase. To the best of our knowledge, this\nis the first video captioning model that is trained end-to-end from the raw\nvideo input to the caption output. Experimental results show that such a model\noutperforms existing ones to a large margin on two benchmark video captioning\ndatasets.\n",
    "topics": "{'Video Captioning': 1.0}",
    "score": 0.8061215928
  },
  {
    "id": "1811.07548",
    "title": "iQIYI-VID: A Large Dataset for Multi-modal Person Identification",
    "abstract": "  Person identification in the wild is very challenging due to great variation\nin poses, face quality, clothes, makeup and so on. Traditional research, such\nas face recognition, person re-identification, and speaker recognition, often\nfocuses on a single modal of information, which is inadequate to handle all the\nsituations in practice. Multi-modal person identification is a more promising\nway that we can jointly utilize face, head, body, audio features, and so on. In\nthis paper, we introduce iQIYI-VID, the largest video dataset for multi-modal\nperson identification. It is composed of 600K video clips of 5,000 celebrities.\nThese video clips are extracted from 400K hours of online videos of various\ntypes, ranging from movies, variety shows, TV series, to news broadcasting. All\nvideo clips pass through a careful human annotation process, and the error rate\nof labels is lower than 0.2\\%. We evaluated the state-of-art models of face\nrecognition, person re-identification, and speaker recognition on the iQIYI-VID\ndataset. Experimental results show that these models are still far from being\nperfect for the task of person identification in the wild. We proposed a\nMulti-modal Attention module to fuse multi-modal features that can improve\nperson identification considerably. We have released the dataset online to\npromote multi-modal person identification research.\n",
    "topics": "{'Speaker Recognition': 1.0, 'Person Re-Identification': 0.9999877, 'Face Recognition': 0.999713}",
    "score": 0.8061097593
  },
  {
    "id": "1904.08421",
    "title": "A large-scale field test on word-image classification in large\n  historical document collections using a traditional and two deep-learning\n  methods",
    "abstract": "  This technical report describes a practical field test on word-image\nclassification in a very large collection of more than 300 diverse handwritten\nhistorical manuscripts, with 1.6 million unique labeled images and more than 11\nmillion images used in testing. Results indicate that several deep-learning\ntests completely failed (mean accuracy 83%). In the tests with more than 1000\noutput units (lexical words) in one-hot encoding for classification,\nperformance steeply drops to almost zero percent accuracy, even with a modest\nsize of the pre-final (i.e., penultimate) layer (150 units). A traditional\nfeature method (BOVW) displays a consistent performance over numbers of classes\nand numbers of training examples (mean accuracy 87%). Additional tests using\nnearest mean on the output of the pre-final layer of an Inception V3 network,\nfor each book, only yielded mediocre results (mean accuracy 49\\%), but was not\nsensitive to high numbers of classes. Notably, this experiment was only\npossible on the basis of labels that were harvested on the basis of a\ntraditional method which already works starting from a single labeled image per\nclass. It is expected that the performance of the failed deep learning tests\ncan be repaired, but only on the basis of human handcrafting (sic) of network\narchitecture and hyperparameters. When the failed problematic books are not\nconsidered, end-to-end CNN training yields about 95% accuracy. This average is\ndominated by a large subset of Chinese characters, performances for other\nscript styles being lower.\n",
    "topics": "{'Image Classification': 0.9357846}",
    "score": 0.806030784
  },
  {
    "id": "1902.05611",
    "title": "GeoGAN: A Conditional GAN with Reconstruction and Style Loss to Generate\n  Standard Layer of Maps from Satellite Images",
    "abstract": "  Automatically generating maps from satellite images is an important task.\nThere is a body of literature which tries to address this challenge. We created\na more expansive survey of the task by experimenting with different models and\nadding new loss functions to improve results. We created a database of pairs of\nsatellite images and the corresponding map of the area. Our model translates\nthe satellite image to the corresponding standard layer map image using three\nmain model architectures: (i) a conditional Generative Adversarial Network\n(GAN) which compresses the images down to a learned embedding, (ii) a generator\nwhich is trained as a normalizing flow (RealNVP) model, and (iii) a conditional\nGAN where the generator translates via a series of convolutions to the standard\nlayer of a map and the discriminator input is the concatenation of the\nreal/generated map and the satellite image. Model (iii) was by far the most\npromising of three models. To improve the results we also added a\nreconstruction loss and style transfer loss in addition to the GAN losses. The\nthird model architecture produced the best quality of sampled images. In\ncontrast to the other generative model where evaluation of the model is a\nchallenging problem. since we have access to the real map for a given satellite\nimage, we are able to assign a quantitative metric to the quality of the\ngenerated images in addition to inspecting them visually. While we are\ncontinuing to work on increasing the accuracy of the model, one challenge has\nbeen the coarse resolution of the data which upper-bounds the quality of the\nresults of our model. Nevertheless, as will be seen in the results, the\ngenerated map is more accurate in the features it produces since the generator\narchitecture demands a pixel-wise image translation/pixel-wise coloring. A\nvideo presentation summarizing this paper is available at:\nhttps://youtu.be/Ur0flOX-Ji0\n",
    "topics": "{'Style Transfer': 0.9983163}",
    "score": 0.8058749507
  },
  {
    "id": "1805.07777",
    "title": "DLBI: Deep learning guided Bayesian inference for structure\n  reconstruction of super-resolution fluorescence microscopy",
    "abstract": "  Super-resolution fluorescence microscopy, with a resolution beyond the\ndiffraction limit of light, has become an indispensable tool to directly\nvisualize biological structures in living cells at a nanometer-scale\nresolution. Despite advances in high-density super-resolution fluorescent\ntechniques, existing methods still have bottlenecks, including extremely long\nexecution time, artificial thinning and thickening of structures, and lack of\nability to capture latent structures. Here we propose a novel deep learning\nguided Bayesian inference approach, DLBI, for the time-series analysis of\nhigh-density fluorescent images. Our method combines the strength of deep\nlearning and statistical inference, where deep learning captures the underlying\ndistribution of the fluorophores that are consistent with the observed\ntime-series fluorescent images by exploring local features and correlation\nalong time-axis, and statistical inference further refines the ultrastructure\nextracted by deep learning and endues physical meaning to the final image.\nComprehensive experimental results on both real and simulated datasets\ndemonstrate that our method provides more accurate and realistic local patch\nand large-field reconstruction than the state-of-the-art method, the 3B\nanalysis, while our method is more than two orders of magnitude faster. The\nmain program is available at https://github.com/lykaust15/DLBI\n",
    "topics": "{'Bayesian Inference': 1.0, 'Super-Resolution': 0.99999976, 'Super Resolution': 0.9999995, 'Time Series': 0.9084679}",
    "score": 0.8058680552
  },
  {
    "id": "1810.09945",
    "title": "Analyzing Neuroimaging Data Through Recurrent Deep Learning Models",
    "abstract": "  The application of deep learning (DL) models to neuroimaging data poses\nseveral challenges, due to the high dimensionality, low sample size and complex\ntemporo-spatial dependency structure of these datasets. Even further, DL models\nact as as black-box models, impeding insight into the association of cognitive\nstate and brain activity. To approach these challenges, we introduce the\nDeepLight framework, which utilizes long short-term memory (LSTM) based DL\nmodels to analyze whole-brain functional Magnetic Resonance Imaging (fMRI)\ndata. To decode a cognitive state (e.g., seeing the image of a house),\nDeepLight separates the fMRI volume into a sequence of axial brain slices,\nwhich is then sequentially processed by an LSTM. To maintain interpretability,\nDeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby,\ndecomposing its decoding decision into the contributions of the single input\nvoxels to this decision. Importantly, the decomposition is performed on the\nlevel of single fMRI volumes, enabling DeepLight to study the associations\nbetween cognitive state and brain activity on several levels of data\ngranularity, from the level of the group down to the level of single time\npoints. To demonstrate the versatility of DeepLight, we apply it to a large\nfMRI dataset of the Human Connectome Project. We show that DeepLight\noutperforms conventional approaches of uni- and multivariate fMRI analysis in\ndecoding the cognitive states and in identifying the physiologically\nappropriate brain regions associated with these states. We further demonstrate\nDeepLight's ability to study the fine-grained temporo-spatial variability of\nbrain activity over sequences of single fMRI samples.\n",
    "topics": "{}",
    "score": 0.8057818802
  },
  {
    "id": "1810.02419",
    "title": "Towards High Resolution Video Generation with Progressive Growing of\n  Sliced Wasserstein GANs",
    "abstract": "  The extension of image generation to video generation turns out to be a very\ndifficult task, since the temporal dimension of videos introduces an extra\nchallenge during the generation process. Besides, due to the limitation of\nmemory and training stability, the generation becomes increasingly challenging\nwith the increase of the resolution/duration of videos. In this work, we\nexploit the idea of progressive growing of Generative Adversarial Networks\n(GANs) for higher resolution video generation. In particular, we begin to\nproduce video samples of low-resolution and short-duration, and then\nprogressively increase both resolution and duration alone (or jointly) by\nadding new spatiotemporal convolutional layers to the current networks.\nStarting from the learning on a very raw-level spatial appearance and temporal\nmovement of the video distribution, the proposed progressive method learns\nspatiotemporal information incrementally to generate higher resolution videos.\nFurthermore, we introduce a sliced version of Wasserstein GAN (SWGAN) loss to\nimprove the distribution learning on the video data of high-dimension and\nmixed-spatiotemporal distribution. SWGAN loss replaces the distance between\njoint distributions by that of one-dimensional marginal distributions, making\nthe loss easier to compute. We evaluate the proposed model on our collected\nface video dataset of 10,900 videos to generate photorealistic face videos of\n256x256x32 resolution. In addition, our model also reaches a record inception\nscore of 14.57 in unsupervised action recognition dataset UCF-101.\n",
    "topics": "{'Video Generation': 0.99997747, 'Action Recognition': 0.99978536, 'Temporal Action Localization': 0.9984884, 'Image Generation': 0.9942445}",
    "score": 0.8057619275
  },
  {
    "id": "1612.00534",
    "title": "Object Detection via Aspect Ratio and Context Aware Region-based\n  Convolutional Networks",
    "abstract": "  Jointly integrating aspect ratio and context has been extensively studied and\nshown performance improvement in traditional object detection systems such as\nthe DPMs. It, however, has been largely ignored in deep neural network based\ndetection systems. This paper presents a method of integrating a mixture of\nobject models and region-based convolutional networks for accurate object\ndetection. Each mixture component accounts for both object aspect ratio and\nmulti-scale contextual information explicitly: (i) it exploits a mixture of\ntiling configurations in the RoI pooling to remedy the warping artifacts caused\nby a single type RoI pooling (e.g., with equally-sized 7 x 7 cells), and to\nrespect the underlying object shapes more; (ii) it \"looks from both the inside\nand the outside of a RoI\" by incorporating contextual information at two\nscales: global context pooled from the whole image and local context pooled\nfrom the surrounding of a RoI. To facilitate accurate detection, this paper\nproposes a multi-stage detection scheme for integrating the mixture of object\nmodels, which utilizes the detection results of the model at the previous stage\nas the proposals for the current in both training and testing. The proposed\nmethod is called the aspect ratio and context aware region-based convolutional\nnetwork (ARC-R-CNN). In experiments, ARC-R-CNN shows very competitive results\nwith Faster R-CNN [41] and R-FCN [10] on two datasets: the PASCAL VOC and the\nMicrosoft COCO. It obtains significantly better mAP performance using high IoU\nthresholds on both datasets.\n",
    "topics": "{'Object Detection': 0.99999547}",
    "score": 0.8057604768
  },
  {
    "id": "2001.03329",
    "title": "Convolutional Neural Networks based Focal Loss for Class Imbalance\n  Problem: A Case Study of Canine Red Blood Cells Morphology Classification",
    "abstract": "  Morphologies of red blood cells are normally interpreted by a pathologist. It\nis time-consuming and laborious. Furthermore, a misclassified red blood cell\nmorphology will lead to false disease diagnosis and improper treatment. Thus, a\ndecent pathologist must truly be an expert in classifying red blood cell\nmorphology. In the past decade, many approaches have been proposed for\nclassifying human red blood cell morphology. However, those approaches have not\naddressed the class imbalance problem in classification. A class imbalance\nproblem---a problem where the numbers of samples in classes are very\ndifferent---is one of the problems that can lead to a biased model towards the\nmajority class. Due to the rarity of every type of abnormal blood cell\nmorphology, the data from the collection process are usually imbalanced. In\nthis study, we aimed to solve this problem specifically for classification of\ndog red blood cell morphology by using a Convolutional Neural Network (CNN)---a\nwell-known deep learning technique---in conjunction with a focal loss function,\nadept at handling class imbalance problem. The proposed technique was conducted\non a well-designed framework: two different CNNs were used to verify the\neffectiveness of the focal loss function and the optimal hyper-parameters were\ndetermined by 5-fold cross-validation. The experimental results show that both\nCNNs models augmented with the focal loss function achieved higher\n$F_{1}$-scores, compared to the models augmented with a conventional\ncross-entropy loss function that does not address class imbalance problem. In\nother words, the focal loss function truly enabled the CNNs models to be less\nbiased towards the majority class than the cross-entropy did in the\nclassification task of imbalanced dog red blood cell data.\n",
    "topics": "{}",
    "score": 0.8053941304
  },
  {
    "id": "1606.00370",
    "title": "Decoding Emotional Experience through Physiological Signal Processing",
    "abstract": "  There is an increasing consensus among re- searchers that making a computer\nemotionally intelligent with the ability to decode human affective states would\nallow a more meaningful and natural way of human-computer interactions (HCIs).\nOne unobtrusive and non-invasive way of recognizing human affective states\nentails the exploration of how physiological signals vary under different\nemotional experiences. In particular, this paper explores the correlation\nbetween autonomically-mediated changes in multimodal body signals and discrete\nemotional states. In order to fully exploit the information in each modality,\nwe have provided an innovative classification approach for three specific\nphysiological signals including Electromyogram (EMG), Blood Volume Pressure\n(BVP) and Galvanic Skin Response (GSR). These signals are analyzed as inputs to\nan emotion recognition paradigm based on fusion of a series of weak learners.\nOur proposed classification approach showed 88.1% recognition accuracy, which\noutperformed the conventional Support Vector Machine (SVM) classifier with 17%\naccuracy improvement. Furthermore, in order to avoid information redundancy and\nthe resultant over-fitting, a feature reduction method is proposed based on a\ncorrelation analysis to optimize the number of features required for training\nand validating each weak learner. Results showed that despite the feature space\ndimensionality reduction from 27 to 18 features, our methodology preserved the\nrecognition accuracy of about 85.0%. This reduction in complexity will get us\none step closer towards embedding this human emotion encoder in the wireless\nand wearable HCI platforms.\n",
    "topics": "{'Dimensionality Reduction': 0.9978903, 'Emotion Recognition': 0.9978174}",
    "score": 0.8053245279
  },
  {
    "id": "2004.08596",
    "title": "DAPnet: A double self-attention convolutional network for segmentation\n  of point clouds",
    "abstract": "  LiDAR point cloud has a complex structure and the 3D semantic labeling of it\nis a challenging task. Existing methods adopt data transformations without\nfully exploring contextual features, which are less efficient and accurate\nproblem. In this study, we propose a double self-attention convolutional\nnetwork, called DAPnet, by combining geometric and contextual features to\ngenerate better segmentation results. The double self-attention module\nincluding point attention module and group attention module originates from the\nself-attention mechanism to extract contextual features of terrestrial objects\nwith various shapes and scales. The contextual features extracted by these\nmodules represent the long-range dependencies between the data and are\nbeneficial to reducing the scale diversity of point cloud objects. The point\nattention module selectively enhances the features by modeling the\ninterdependencies of neighboring points. Meanwhile, the group attention module\nis used to emphasizes interdependent groups of points. We evaluate our method\nbased on the ISPRS 3D Semantic Labeling Contest dataset and find that our model\noutperforms the benchmark by 85.2% with an overall accuracy of 90.7%. The\nimprovements over powerline and car are 7.5% and 13%. By conducting ablation\ncomparison, we find that the point attention module is more effective for the\noverall improvement of the model than the group attention module, and the\nincorporation of the double self-attention module has an average of 7%\nimprovement on the pre-class accuracy of the classes. Moreover, the adoption of\nthe double self-attention module consumes a similar training time as the one\nwithout the attention module for model convergence. The experimental result\nshows the effectiveness and efficiency of the DAPnet for the segmentation of\nLiDAR point clouds. The source codes are available at\nhttps://github.com/RayleighChen/point-attention.\n",
    "topics": "{}",
    "score": 0.8052123717
  },
  {
    "id": "2006.08432",
    "title": "SD-RSIC: Summarization Driven Deep Remote Sensing Image Captioning",
    "abstract": "  Deep neural networks (DNNs) have been recently found popular for image\ncaptioning problems in remote sensing (RS). Existing DNN based approaches rely\non the availability of a training set made up of a high number of RS images\nwith their captions. However, captions of training images may contain redundant\ninformation (they can be repetitive or semantically similar to each other),\nresulting in information deficiency while learning a mapping from the image\ndomain to the language domain. To overcome this limitation, in this paper, we\npresent a novel Summarization Driven Remote Sensing Image Captioning (SD-RSIC)\napproach. The proposed approach consists of three main steps. The first step\nobtains the standard image captions by jointly exploiting convolutional neural\nnetworks (CNNs) with long short-term memory (LSTM) networks. The second step,\nunlike the existing RS image captioning methods, summarizes the ground-truth\ncaptions of each training image into a single caption by exploiting sequence to\nsequence neural networks and eliminates the redundancy present in the training\nset. The third step automatically defines the adaptive weights associated to\neach RS image to combine the standard captions with the summarized captions\nbased on the semantic content of the image. This is achieved by a novel\nadaptive weighting strategy defined in the context of LSTM networks.\nExperimental results obtained on the RSCID, UCM-Captions and Sydney-Captions\ndatasets show the effectiveness of the proposed approach compared to the\nstate-of-the-art RS image captioning approaches. The code of the proposed\napproach is publicly available at\nhttps://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC.\n",
    "topics": "{'Image Captioning': 1.0}",
    "score": 0.8051961698
  },
  {
    "id": "1603.05544",
    "title": "Accelerating Deep Neural Network Training with Inconsistent Stochastic\n  Gradient Descent",
    "abstract": "  SGD is the widely adopted method to train CNN. Conceptually it approximates\nthe population with a randomly sampled batch; then it evenly trains batches by\nconducting a gradient update on every batch in an epoch. In this paper, we\ndemonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle Pseudo\nRandom Sampling differentiate batches in training, which then affect learning\nspeeds on them. Because of this, the unbiased treatment of batches involved in\nSGD creates improper load balancing. To address this issue, we present\nInconsistent Stochastic Gradient Descent (ISGD) to dynamically vary training\neffort according to learning statuses on batches. Specifically ISGD leverages\ntechniques in Statistical Process Control to identify a undertrained batch.\nOnce a batch is undertrained, ISGD solves a new subproblem, a chasing logic\nplus a conservative constraint, to accelerate the training on the batch while\navoid drastic parameter changes. Extensive experiments on a variety of datasets\ndemonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is\n21.05\\% faster than SGD to reach 56\\% top1 accuracy under the exactly same\nexperiment setup. We also extend ISGD to work on multiGPU or heterogeneous\ndistributed system based on data parallelism, enabling the batch size to be the\nkey to scalability. Then we present the study of ISGD batch size to the\nlearning rate, parallelism, synchronization cost, system saturation and\nscalability. We conclude the optimal ISGD batch size is machine dependent.\nVarious experiments on a multiGPU system validate our claim. In particular,\nISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4\nNVIDIA TITAN X at the batch size of 1536.\n",
    "topics": "{}",
    "score": 0.8051526447
  },
  {
    "id": "1404.3366",
    "title": "Learning Deep Convolutional Features for MRI Based Alzheimer's Disease\n  Classification",
    "abstract": "  Effective and accurate diagnosis of Alzheimer's disease (AD) or mild\ncognitive impairment (MCI) can be critical for early treatment and thus has\nattracted more and more attention nowadays. Since first introduced, machine\nlearning methods have been gaining increasing popularity for AD related\nresearch. Among the various identified biomarkers, magnetic resonance imaging\n(MRI) are widely used for the prediction of AD or MCI. However, before a\nmachine learning algorithm can be applied, image features need to be extracted\nto represent the MRI images. While good representations can be pivotal to the\nclassification performance, almost all the previous studies typically rely on\nhuman labelling to find the regions of interest (ROI) which may be correlated\nto AD, such as hippocampus, amygdala, precuneus, etc. This procedure requires\ndomain knowledge and is costly and tedious.\n  Instead of relying on extraction of ROI features, it is more promising to\nremove manual ROI labelling from the pipeline and directly work on the raw MRI\nimages. In other words, we can let the machine learning methods to figure out\nthese informative and discriminative image structures for AD classification. In\nthis work, we propose to learn deep convolutional image features using\nunsupervised and supervised learning. Deep learning has emerged as a powerful\ntool in the machine learning community and has been successfully applied to\nvarious tasks. We thus propose to exploit deep features of MRI images based on\na pre-trained large convolutional neural network (CNN) for AD and MCI\nclassification, which spares the effort of manual ROI annotation process.\n",
    "topics": "{}",
    "score": 0.805151374
  },
  {
    "id": "1805.10620",
    "title": "Anomaly Detection and Localization in Crowded Scenes by Motion-field\n  Shape Description and Similarity-based Statistical Learning",
    "abstract": "  In crowded scenes, detection and localization of abnormal behaviors is\nchallenging in that high-density people make object segmentation and tracking\nextremely difficult. We associate the optical flows of multiple frames to\ncapture short-term trajectories and introduce the histogram-based shape\ndescriptor referred to as shape contexts to describe such short-term\ntrajectories. Furthermore, we propose a K-NN similarity-based statistical model\nto detect anomalies over time and space, which is an unsupervised one-class\nlearning algorithm requiring no clustering nor any prior assumption. Firstly,\nwe retrieve the K-NN samples from the training set in regard to the testing\nsample, and then use the similarities between every pair of the K-NN samples to\nconstruct a Gaussian model. Finally, the probabilities of the similarities from\nthe testing sample to the K-NN samples under the Gaussian model are calculated\nin the form of a joint probability. Abnormal events can be detected by judging\nwhether the joint probability is below predefined thresholds in terms of time\nand space, separately. Such a scheme can adapt to the whole scene, since the\nprobability computed as such is not affected by motion distortions arising from\nperspective distortion. We conduct experiments on real-world surveillance\nvideos, and the results demonstrate that the proposed method can reliably\ndetect and locate the abnormal events in the video sequences, outperforming the\nstate-of-the-art approaches.\n",
    "topics": "{'Anomaly Detection': 0.9967056, 'Semantic Segmentation': 0.99369025, 'Temporal Action Localization': 0.30909786}",
    "score": 0.8050799594
  },
  {
    "id": "2010.03544",
    "title": "A Self-supervised Approach for Semantic Indexing in the Context of\n  COVID-19 Pandemic",
    "abstract": "  The pandemic has accelerated the pace at which COVID-19 scientific papers are\npublished. In addition, the process of manually assigning semantic indexes to\nthese papers by experts is even more time-consuming and overwhelming in the\ncurrent health crisis. Therefore, there is an urgent need for automatic\nsemantic indexing models which can effectively scale-up to newly introduced\nconcepts and rapidly evolving distributions of the hyperfocused related\nliterature. In this research, we present a novel semantic indexing approach\nbased on the state-of-the-art self-supervised representation learning and\ntransformer encoding exclusively suitable for pandemic crises. We present a\ncase study on a novel dataset that is based on COVID-19 papers published and\nmanually indexed in PubMed. Our study shows that our self-supervised model\noutperforms the best performing models of BioASQ Task 8a by micro-F1 score of\n0.1 and LCA-F score of 0.08 on average. Our model also shows superior\nperformance on detecting the supplementary concepts which is quite important\nwhen the focus of the literature has drastically shifted towards specific\nconcepts related to the pandemic. Our study sheds light on the main challenges\nconfronting semantic indexing models during a pandemic, namely new domains and\ndrastic changes of their distributions, and as a superior alternative for such\nsituations, propose a model founded on approaches which have shown auspicious\nperformance in improving generalization and data efficiency in various NLP\ntasks. We also show the joint indexing of major Medical Subject Headings (MeSH)\nand supplementary concepts improves the overall performance.\n",
    "topics": "{'Representation Learning': 0.42375302}",
    "score": 0.8050678863
  },
  {
    "id": "1905.10695",
    "title": "Adversarial Distillation for Ordered Top-k Attacks",
    "abstract": "  Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, especially\nwhite-box targeted attacks. One scheme of learning attacks is to design a\nproper adversarial objective function that leads to the imperceptible\nperturbation for any test image (e.g., the Carlini-Wagner (C&W) method). Most\nmethods address targeted attacks in the Top-1 manner. In this paper, we propose\nto learn ordered Top-k attacks (k>= 1) for image classification tasks, that is\nto enforce the Top-k predicted labels of an adversarial example to be the k\n(randomly) selected and ordered labels (the ground-truth label is exclusive).\nTo this end, we present an adversarial distillation framework: First, we\ncompute an adversarial probability distribution for any given ordered Top-k\ntargeted labels with respect to the ground-truth of a test image. Then, we\nlearn adversarial examples by minimizing the Kullback-Leibler (KL) divergence\ntogether with the perturbation energy penalty, similar in spirit to the network\ndistillation method. We explore how to leverage label semantic similarities in\ncomputing the targeted distributions, leading to knowledge-oriented attacks. In\nexperiments, we thoroughly test Top-1 and Top-5 attacks in the ImageNet-1000\nvalidation dataset using two popular DNNs trained with clean ImageNet-1000\ntrain dataset, ResNet-50 and DenseNet-121. For both models, our proposed\nadversarial distillation approach outperforms the C&W method in the Top-1\nsetting, as well as other baseline methods. Our approach shows significant\nimprovement in the Top-5 setting against a strong modified C&W method.\n",
    "topics": "{'Image Classification': 0.9967732}",
    "score": 0.8050284699
  },
  {
    "id": "1901.02573",
    "title": "Interactive Image Segmentation using Label Propagation through Complex\n  Networks",
    "abstract": "  Interactive image segmentation is a topic of many studies in image\nprocessing. In a conventional approach, a user marks some pixels of the\nobject(s) of interest and background, and an algorithm propagates these labels\nto the rest of the image. This paper presents a new graph-based method for\ninteractive segmentation with two stages. In the first stage, nodes\nrepresenting pixels are connected to their $k$-nearest neighbors to build a\ncomplex network with the small-world property to propagate the labels quickly.\nIn the second stage, a regular network in a grid format is used to refine the\nsegmentation on the object borders. Despite its simplicity, the proposed method\ncan perform the task with high accuracy. Computer simulations are performed\nusing some real-world images to show its effectiveness in both two-classes and\nmulti-classes problems. It is also applied to all the images from the Microsoft\nGrabCut dataset for comparison, and the segmentation accuracy is comparable to\nthose achieved by some state-of-the-art methods, while it is faster than them.\nIn particular, it outperforms some recent approaches when the user input is\ncomposed only by a few \"scribbles\" draw over the objects. Its computational\ncomplexity is only linear on the image size at the best-case scenario and\nlinearithmic in the worst case.\n",
    "topics": "{'Semantic Segmentation': 0.9999993}",
    "score": 0.8049851794
  },
  {
    "id": "1909.04142",
    "title": "DaTscan SPECT Image Classification for Parkinson's Disease",
    "abstract": "  Parkinson's Disease (PD) is a neurodegenerative disease that currently does\nnot have a cure. In order to facilitate disease management and reduce the speed\nof symptom progression, early diagnosis is essential. The current clinical,\ndiagnostic approach is to have radiologists perform human visual analysis of\nthe degeneration of dopaminergic neurons in the substantia nigra region of the\nbrain. Clinically, dopamine levels are monitored through observing dopamine\ntransporter (DaT) activity. One method of DaT activity analysis is performed\nwith the injection of an Iodine-123 fluoropropyl (123I-FP-CIT) tracer combined\nwith single photon emission computerized tomography (SPECT) imaging. The tracer\nillustrates the region of interest in the resulting DaTscan SPECT images. Human\nvisual analysis is slow and vulnerable to subjectivity between radiologists, so\nthe goal was to develop an introductory implementation of a deep convolutional\nneural network that can objectively and accurately classify DaTscan SPECT\nimages as Parkinson's Disease or normal. This study illustrates the approach of\nusing a deep convolutional neural network and evaluates its performance on\nDaTscan SPECT image classification. The data used in this study was obtained\nthrough a database provided by the Parkinson's Progression Markers Initiative\n(PPMI). The deep neural network in this study utilizes the InceptionV3\narchitecture, 1st runner up in the 2015 ImageNet Large Scale Visual Recognition\nCompetition (ILSVRC), as a base model. A custom, binary classifier block was\nadded on top of this base. In order to account for the small dataset size, a\nten fold cross validation was implemented to evaluate the model's performance.\n",
    "topics": "{'Image Classification': 0.999466, 'Object Recognition': 0.8866007}",
    "score": 0.8045925214
  },
  {
    "id": "2003.12009",
    "title": "Multi-Lead ECG Classification via an Information-Based Attention\n  Convolutional Neural Network",
    "abstract": "  Objective: A novel structure based on channel-wise attention mechanism is\npresented in this paper. Embedding with the proposed structure, an efficient\nclassification model that accepts multi-lead electrocardiogram (ECG) as input\nis constructed. Methods: One-dimensional convolutional neural networks (CNN)\nhave proven to be effective in pervasive classification tasks, enabling the\nautomatic extraction of features while classifying targets. We implement the\nResidual connection and design a structure which can learn the weights from the\ninformation contained in different channels in the input feature map during the\ntraining process. An indicator named mean square deviation is introduced to\nmonitor the performance of a particular model segment in the classification\ntask on the two out of the five ECG classes. The data in the MIT-BIH arrhythmia\ndatabase is used and a series of control experiments is conducted. Results:\nUtilizing both leads of the ECG signals as input to the neural network\nclassifier can achieve better classification results than those from using\nsingle channel inputs in different application scenarios. Models embedded with\nthe channel-wise attention structure always achieve better scores on\nsensitivity and precision than the plain Resnet models. The proposed model\nexceeds the performance of most of the state-of-the-art models in ventricular\nectopic beats (VEB) classification, and achieves competitive scores for\nsupraventricular ectopic beats (SVEB). Conclusion: Adopting more lead ECG\nsignals as input can increase the dimensions of the input feature maps, helping\nto improve both the performance and generalization of the network model.\nSignificance: Due to its end-to-end characteristics, and the extensible\nintrinsic for multi-lead heart diseases diagnosing, the proposed model can be\nused for the real-time ECG tracking of ECG waveforms for Holter or wearable\ndevices.\n",
    "topics": "{'Electrocardiography (ECG)': 0.999253}",
    "score": 0.8044960561
  },
  {
    "id": "1909.02835",
    "title": "Running Event Visualization using Videos from Multiple Cameras",
    "abstract": "  Visualizing the trajectory of multiple runners with videos collected at\ndifferent points in a race could be useful for sports performance analysis. The\nvideos and the trajectories can also aid in athlete health monitoring. While\nthe runners unique ID and their appearance are distinct, the task is not\nstraightforward because the video data does not contain explicit information as\nto which runners appear in each of the videos. There is no direct supervision\nof the model in tracking athletes, only filtering steps to remove irrelevant\ndetections. Other factors of concern include occlusion of runners and harsh\nillumination. To this end, we identify two methods for runner identification at\ndifferent points of the event, for determining their trajectory. One is scene\ntext detection which recognizes the runners by detecting a unique 'bib number'\nattached to their clothes and the other is person re-identification which\ndetects the runners based on their appearance. We train our method without\nground truth but to evaluate the proposed methods, we create a ground truth\ndatabase which consists of video and frame interval information where the\nrunners appear. The videos in the dataset was recorded by nine cameras at\ndifferent locations during the a marathon event. This data is annotated with\nbib numbers of runners appearing in each video. The bib numbers of runners\nknown to occur in the frame are used to filter irrelevant text and numbers\ndetected. Except for this filtering step, no supervisory signal is used. The\nexperimental evidence shows that the scene text recognition method achieves an\nF1-score of 74. Combining the two methods, that is - using samples collected by\ntext spotter to train the re-identification model yields a higher F1-score of\n85.8. Re-training the person re-identification model with identified inliers\nyields a slight improvement in performance(F1 score of 87.8).\n",
    "topics": "{'Scene Text': 0.99999905, 'Person Re-Identification': 0.99994004, 'Scene Text Recognition': 0.3440849}",
    "score": 0.8044066273
  },
  {
    "id": "1806.03619",
    "title": "VoxelAtlasGAN: 3D Left Ventricle Segmentation on Echocardiography with\n  Atlas Guided Generation and Voxel-to-voxel Discrimination",
    "abstract": "  3D left ventricle (LV) segmentation on echocardiography is very important for\ndiagnosis and treatment of cardiac disease. It is not only because of that\nechocardiography is a real-time imaging technology and widespread in clinical\napplication, but also because of that LV segmentation on 3D echocardiography\ncan provide more full volume information of heart than LV segmentation on 2D\nechocardiography. However, 3D LV segmentation on echocardiography is still an\nopen and challenging task owing to the lower contrast, higher noise and data\ndimensionality, limited annotation of 3D echocardiography. In this paper, we\nproposed a novel real-time framework, i.e., VoxelAtlasGAN, for 3D LV\nsegmentation on 3D echocardiography. This framework has three contributions: 1)\nIt is based on voxel-to-voxel conditional generative adversarial nets (cGAN).\nFor the first time, cGAN is used for 3D LV segmentation on echocardiography.\nAnd cGAN advantageously fuses substantial 3D spatial context information from\n3D echocardiography by self-learning structured loss; 2) For the first time, it\nembeds the atlas into an end-to-end optimization framework, which uses 3D LV\natlas as a powerful prior knowledge to improve the inference speed, address the\nlower contrast and the limited annotation problems of 3D echocardiography; 3)\nIt combines traditional discrimination loss and the new proposed consistent\nconstraint, which further improves the generalization of the proposed\nframework. VoxelAtlasGAN was validated on 60 subjects on 3D echocardiography\nand it achieved satisfactory segmentation results and high inference speed. The\nmean surface distance is 1.85 mm, the mean hausdorff surface distance is 7.26\nmm, mean dice is 0.953, the correlation of EF is 0.918, and the mean inference\nspeed is 0.1s. These results have demonstrated that our proposed method has\ngreat potential for clinical application\n",
    "topics": "{}",
    "score": 0.8044014294
  },
  {
    "id": "1807.10850",
    "title": "Synthesizing CT from Ultrashort Echo-Time MR Images via Convolutional\n  Neural Networks",
    "abstract": "  With the increasing popularity of PET-MR scanners in clinical applications,\nsynthesis of CT images from MR has been an important research topic. Accurate\nPET image reconstruction requires attenuation correction, which is based on the\nelectron density of tissues and can be obtained from CT images. While CT\nmeasures electron density information for x-ray photons, MR images convey\ninformation about the magnetic properties of tissues. Therefore, with the\nadvent of PET-MR systems, the attenuation coefficients need to be indirectly\nestimated from MR images. In this paper, we propose a fully convolutional\nneural network (CNN) based method to synthesize head CT from ultra-short\necho-time (UTE) dual-echo MR images. Unlike traditional $T_1$-w images which do\nnot have any bone signal, UTE images show some signal for bone, which makes it\na good candidate for MR to CT synthesis. A notable advantage of our approach is\nthat accurate results were achieved with a small training data set. Using an\natlas of a single CT and dual-echo UTE pair, we train a deep neural network\nmodel to learn the transform of MR intensities to CT using patches. We compared\nour CNN based model with a state-of-the-art registration based as well as a\nBayesian model based CT synthesis method, and showed that the proposed CNN\nmodel outperforms both of them. We also compared the proposed model when only\n$T_1$-w images are available instead of UTE, and show that UTE images produce\nbetter synthesis than using just $T_1$-w images.\n",
    "topics": "{'Image Reconstruction': 0.99996793}",
    "score": 0.8043112618
  },
  {
    "id": "1508.04535",
    "title": "Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image\n  Retrieval and Person Re-identification",
    "abstract": "  Extracting informative image features and learning effective approximate\nhashing functions are two crucial steps in image retrieval . Conventional\nmethods often study these two steps separately, e.g., learning hash functions\nfrom a predefined hand-crafted feature space. Meanwhile, the bit lengths of\noutput hashing codes are preset in most previous methods, neglecting the\nsignificance level of different bits and restricting their practical\nflexibility. To address these issues, we propose a supervised learning\nframework to generate compact and bit-scalable hashing codes directly from raw\nimages. We pose hashing learning as a problem of regularized similarity\nlearning. Specifically, we organize the training images into a batch of triplet\nsamples, each sample containing two images with the same label and one with a\ndifferent label. With these triplet samples, we maximize the margin between\nmatched pairs and mismatched pairs in the Hamming space. In addition, a\nregularization term is introduced to enforce the adjacency consistency, i.e.,\nimages of similar appearances should have similar codes. The deep convolutional\nneural network is utilized to train the model in an end-to-end fashion, where\ndiscriminative image features and hash functions are simultaneously optimized.\nFurthermore, each bit of our hashing codes is unequally weighted so that we can\nmanipulate the code lengths by truncating the insignificant bits. Our framework\noutperforms state-of-the-arts on public benchmarks of similar image search and\nalso achieves promising results in the application of person re-identification\nin surveillance. It is also shown that the generated bit-scalable hashing codes\nwell preserve the discriminative powers with shorter code lengths.\n",
    "topics": "{'Image Retrieval': 0.9999975, 'Person Re-Identification': 0.99995935}",
    "score": 0.8042796437
  },
  {
    "id": "1902.11208",
    "title": "No Padding Please: Efficient Neural Handwriting Recognition",
    "abstract": "  Neural handwriting recognition (NHR) is the recognition of handwritten text\nwith deep learning models, such as multi-dimensional long short-term memory\n(MDLSTM) recurrent neural networks. Models with MDLSTM layers have achieved\nstate-of-the art results on handwritten text recognition tasks. While\nmulti-directional MDLSTM-layers have an unbeaten ability to capture the\ncomplete context in all directions, this strength limits the possibilities for\nparallelization, and therefore comes at a high computational cost. In this work\nwe develop methods to create efficient MDLSTM-based models for NHR,\nparticularly a method aimed at eliminating computation waste that results from\npadding. This proposed method, called example-packing, replaces wasteful\nstacking of padded examples with efficient tiling in a 2-dimensional grid. For\nword-based NHR this yields a speed improvement of factor 6.6 over an already\nefficient baseline of minimal padding for each batch separately. For line-based\nNHR the savings are more modest, but still significant. In addition to\nexample-packing, we propose: 1) a technique to optimize parallelization for\ndynamic graph definition frameworks including PyTorch, using convolutions with\ngrouping, 2) a method for parallelization across GPUs for variable-length\nexample batches. All our techniques are thoroughly tested on our own PyTorch\nre-implementation of MDLSTM-based NHR models. A thorough evaluation on the IAM\ndataset shows that our models are performing similar to earlier implementations\nof state-of-the-art models. Our efficient NHR model and some of the reusable\ntechniques discussed with it offer ways to realize relatively efficient models\nfor the omnipresent scenario of variable-length inputs in deep learning.\n",
    "topics": "{'Handwriting Recognition': 1.0}",
    "score": 0.8040630276
  },
  {
    "id": "cs/0212020",
    "title": "Learning Algorithms for Keyphrase Extraction",
    "abstract": "  Many academic journals ask their authors to provide a list of about five to\nfifteen keywords, to appear on the first page of each article. Since these key\nwords are often phrases of two or more words, we prefer to call them\nkeyphrases. There is a wide variety of tasks for which keyphrases are useful,\nas we discuss in this paper. We approach the problem of automatically\nextracting keyphrases from text as a supervised learning task. We treat a\ndocument as a set of phrases, which the learning algorithm must learn to\nclassify as positive or negative examples of keyphrases. Our first set of\nexperiments applies the C4.5 decision tree induction algorithm to this learning\ntask. We evaluate the performance of nine different configurations of C4.5. The\nsecond set of experiments applies the GenEx algorithm to the task. We developed\nthe GenEx algorithm specifically for automatically extracting keyphrases from\ntext. The experimental results support the claim that a custom-designed\nalgorithm (GenEx), incorporating specialized procedural domain knowledge, can\ngenerate better keyphrases than a generalpurpose algorithm (C4.5). Subjective\nhuman evaluation of the keyphrases generated by Extractor suggests that about\n80% of the keyphrases are acceptable to human readers. This level of\nperformance should be satisfactory for a wide variety of applications.\n",
    "topics": "{}",
    "score": 0.8040447331
  },
  {
    "id": "2004.04351",
    "title": "Multi-feature super-resolution network for cloth wrinkle synthesis",
    "abstract": "  Existing physical cloth simulators suffer from expensive computation and\ndifficulties in tuning mechanical parameters to get desired wrinkling\nbehaviors. Data-driven methods provide an alternative solution. It typically\nsynthesizes cloth animation at a much lower computational cost, and also\ncreates wrinkling effects that highly resemble the much controllable training\ndata. In this paper we propose a deep learning based method for synthesizing\ncloth animation with high resolution meshes. To do this we first create a\ndataset for training: a pair of low and high resolution meshes are simulated\nand their motions are synchronized. As a result the two meshes exhibit similar\nlarge-scale deformation but different small wrinkles. Each simulated mesh pair\nare then converted into a pair of low and high resolution \"images\" (a 2D array\nof samples), with each sample can be interpreted as any of three features: the\ndisplacement, the normal and the velocity. With these image pairs, we design a\nmulti-feature super-resolution (MFSR) network that jointly train an upsampling\nsynthesizer for the three features. The MFSR architecture consists of two key\ncomponents: a sharing module that takes multiple features as input to learn\nlow-level representations from corresponding super-resolution tasks\nsimultaneously; and task-specific modules focusing on various high-level\nsemantics. Frame-to-frame consistency is well maintained thanks to the proposed\nkinematics-based loss function. Our method achieves realistic results at high\nframe rates: 12-14 times faster than traditional physical simulation. We\ndemonstrate the performance of our method with various experimental scenes,\nincluding a dressed character with sophisticated collisions.\n",
    "topics": "{'Super-Resolution': 0.99999225, 'Super Resolution': 0.9999635}",
    "score": 0.8040394173
  },
  {
    "id": "1907.12244",
    "title": "A Fine-Grain Error Map Prediction and Segmentation Quality Assessment\n  Framework for Whole-Heart Segmentation",
    "abstract": "  When introducing advanced image computing algorithms, e.g., whole-heart\nsegmentation, into clinical practice, a common suspicion is how reliable the\nautomatically computed results are. In fact, it is important to find out the\nfailure cases and identify the misclassified pixels so that they can be\nexcluded or corrected for the subsequent analysis or diagnosis. However, it is\nnot a trivial problem to predict the errors in a segmentation mask when ground\ntruth (usually annotated by experts) is absent. In this work, we attempt to\naddress the pixel-wise error map prediction problem and the per-case mask\nquality assessment problem using a unified deep learning (DL) framework.\nSpecifically, we first formalize an error map prediction problem, then we\nconvert it to a segmentation problem and build a DL network to tackle it. We\nalso derive a quality indicator (QI) from a predicted error map to measure the\noverall quality of a segmentation mask. To evaluate the proposed framework, we\nperform extensive experiments on a public whole-heart segmentation dataset,\ni.e., MICCAI 2017 MMWHS. By 5-fold cross validation, we obtain an overall Dice\nscore of 0.626 for the error map prediction task, and observe a high Pearson\ncorrelation coefficient (PCC) of 0.972 between QI and the actual segmentation\naccuracy (Acc), as well as a low mean absolute error (MAE) of 0.0048 between\nthem, which evidences the efficacy of our method in both error map prediction\nand quality assessment.\n",
    "topics": "{'Semantic Segmentation': 0.5657776}",
    "score": 0.8039828877
  },
  {
    "id": "1709.07267",
    "title": "Yet Another ADNI Machine Learning Paper? Paving The Way Towards\n  Fully-reproducible Research on Classification of Alzheimer's Disease",
    "abstract": "  In recent years, the number of papers on Alzheimer's disease classification\nhas increased dramatically, generating interesting methodological ideas on the\nuse machine learning and feature extraction methods. However, practical impact\nis much more limited and, eventually, one could not tell which of these\napproaches are the most efficient. While over 90\\% of these works make use of\nADNI an objective comparison between approaches is impossible due to variations\nin the subjects included, image pre-processing, performance metrics and\ncross-validation procedures. In this paper, we propose a framework for\nreproducible classification experiments using multimodal MRI and PET data from\nADNI. The core components are: 1) code to automatically convert the full ADNI\ndatabase into BIDS format; 2) a modular architecture based on Nipype in order\nto easily plug-in different classification and feature extraction tools; 3)\nfeature extraction pipelines for MRI and PET data; 4) baseline classification\napproaches for unimodal and multimodal features. This provides a flexible\nframework for benchmarking different feature extraction and classification\ntools in a reproducible manner. We demonstrate its use on all (1519) baseline\nT1 MR images and all (1102) baseline FDG PET images from ADNI 1, GO and 2 with\nSPM-based feature extraction pipelines and three different classification\ntechniques (linear SVM, anatomically regularized SVM and multiple kernel\nlearning SVM). The highest accuracies achieved were: 91% for AD vs CN, 83% for\nMCIc vs CN, 75% for MCIc vs MCInc, 94% for AD-A$\\beta$+ vs CN-A$\\beta$- and 72%\nfor MCIc-A$\\beta$+ vs MCInc-A$\\beta$+. The code is publicly available at\nhttps://gitlab.icm-institute.org/aramislab/AD-ML (depends on the Clinica\nsoftware platform, publicly available at http://www.clinica.run).\n",
    "topics": "{}",
    "score": 0.8039301164
  },
  {
    "id": "1803.09824",
    "title": "Low-Shot Learning for the Semantic Segmentation of Remote Sensing\n  Imagery",
    "abstract": "  Recent advances in computer vision using deep learning with RGB imagery\n(e.g., object recognition and detection) have been made possible thanks to the\ndevelopment of large annotated RGB image datasets. In contrast, multispectral\nimage (MSI) and hyperspectral image (HSI) datasets contain far fewer labeled\nimages, in part due to the wide variety of sensors used. These annotations are\nespecially limited for semantic segmentation, or pixel-wise classification, of\nremote sensing imagery because it is labor intensive to generate image\nannotations. Low-shot learning algorithms can make effective inferences despite\nsmaller amounts of annotated data. In this paper, we study low-shot learning\nusing self-taught feature learning for semantic segmentation. We introduce 1)\nan improved self-taught feature learning framework for HSI and MSI data and 2)\na semi-supervised classification algorithm. When these are combined, they\nachieve state-of-the-art performance on remote sensing datasets that have\nlittle annotated training data available. These low-shot learning frameworks\nwill reduce the manual image annotation burden and improve semantic\nsegmentation performance for remote sensing imagery.\n",
    "topics": "{'Semantic Segmentation': 0.99998116, 'Few-Shot Image Classification': 0.98731774, 'Object Recognition': 0.927902, 'Few-Shot Learning': 0.69486}",
    "score": 0.8039056525
  },
  {
    "id": "1702.07985",
    "title": "A multi-task convolutional neural network for mega-city analysis using\n  very high resolution satellite imagery and geospatial data",
    "abstract": "  Mega-city analysis with very high resolution (VHR) satellite images has been\ndrawing increasing interest in the fields of city planning and social\ninvestigation. It is known that accurate land-use, urban density, and\npopulation distribution information is the key to mega-city monitoring and\nenvironmental studies. Therefore, how to generate land-use, urban density, and\npopulation distribution maps at a fine scale using VHR satellite images has\nbecome a hot topic. Previous studies have focused solely on individual tasks\nwith elaborate hand-crafted features and have ignored the relationship between\ndifferent tasks. In this study, we aim to propose a universal framework which\ncan: 1) automatically learn the internal feature representation from the raw\nimage data; and 2) simultaneously produce fine-scale land-use, urban density,\nand population distribution maps. For the first target, a deep convolutional\nneural network (CNN) is applied to learn the hierarchical feature\nrepresentation from the raw image data. For the second target, a novel\nCNN-based universal framework is proposed to process the VHR satellite images\nand generate the land-use, urban density, and population distribution maps. To\nthe best of our knowledge, this is the first CNN-based mega-city analysis\nmethod which can process a VHR remote sensing image with such a large data\nvolume. A VHR satellite image (1.2 m spatial resolution) of the center of Wuhan\ncovering an area of 2606 km2 was used to evaluate the proposed method. The\nexperimental results confirm that the proposed method can achieve a promising\naccuracy for land-use, urban density, and population distribution maps.\n",
    "topics": "{}",
    "score": 0.8037369887
  },
  {
    "id": "1803.06067",
    "title": "Dynamic-structured Semantic Propagation Network",
    "abstract": "  Semantic concept hierarchy is still under-explored for semantic segmentation\ndue to the inefficiency and complicated optimization of incorporating\nstructural inference into dense prediction. This lack of modeling semantic\ncorrelations also makes prior works must tune highly-specified models for each\ntask due to the label discrepancy across datasets. It severely limits the\ngeneralization capability of segmentation models for open set concept\nvocabulary and annotation utilization. In this paper, we propose a\nDynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic\nneuron graph by explicitly incorporating the semantic concept hierarchy into\nnetwork construction. Each neuron represents the instantiated module for\nrecognizing a specific type of entity such as a super-class (e.g. food) or a\nspecific concept (e.g. pizza). During training, DSSPN performs the\ndynamic-structured neuron computation graph by only activating a sub-graph of\nneurons for each image in a principled way. A dense semantic-enhanced neural\nblock is proposed to propagate the learned knowledge of all ancestor neurons\ninto each fine-grained child neuron for feature evolving. Another merit of such\nsemantic explainable structure is the ability of learning a unified model\nconcurrently on diverse datasets by selectively activating different neuron\nsub-graphs for each annotation at each step. Extensive experiments on four\npublic semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and\nMapillary) demonstrate the superiority of our DSSPN over state-of-the-art\nsegmentation models. Moreoever, we demonstrate a universal segmentation model\nthat is jointly trained on diverse datasets can surpass the performance of the\ncommon fine-tuning scheme for exploiting multiple domain knowledge.\n",
    "topics": "{'Semantic Segmentation': 0.99202055}",
    "score": 0.8036335253
  },
  {
    "id": "1512.00743",
    "title": "Recognizing Semantic Features in Faces using Deep Learning",
    "abstract": "  The human face constantly conveys information, both consciously and\nsubconsciously. However, as basic as it is for humans to visually interpret\nthis information, it is quite a big challenge for machines. Conventional\nsemantic facial feature recognition and analysis techniques are already in use\nand are based on physiological heuristics, but they suffer from lack of\nrobustness and high computation time. This thesis aims to explore ways for\nmachines to learn to interpret semantic information available in faces in an\nautomated manner without requiring manual design of feature detectors, using\nthe approach of Deep Learning. This thesis provides a study of the effects of\nvarious factors and hyper-parameters of deep neural networks in the process of\ndetermining an optimal network configuration for the task of semantic facial\nfeature recognition. This thesis explores the effectiveness of the system to\nrecognize the various semantic features (like emotions, age, gender, ethnicity\netc.) present in faces. Furthermore, the relation between the effect of\nhigh-level concepts on low level features is explored through an analysis of\nthe similarities in low-level descriptors of different semantic features. This\nthesis also demonstrates a novel idea of using a deep network to generate 3-D\nActive Appearance Models of faces from real-world 2-D images.\n  For a more detailed report on this work, please see [arXiv:1512.00743v1].\n",
    "topics": "{}",
    "score": 0.8036057689
  },
  {
    "id": "1910.12060",
    "title": "MAP-Net: Multi Attending Path Neural Network for Building Footprint\n  Extraction from Remote Sensed Imagery",
    "abstract": "  Accurately and efficiently extracting building footprints from a wide range\nof remote sensed imagery remains a challenge due to their complex structure,\nvariety of scales and diverse appearances. Existing convolutional neural\nnetwork (CNN)-based building extraction methods are complained that they cannot\ndetect the tiny buildings because the spatial information of CNN feature maps\nare lost during repeated pooling operations of the CNN, and the large buildings\nstill have inaccurate segmentation edges. Moreover, features extracted by a CNN\nare always partial which restricted by the size of the respective field, and\nlarge-scale buildings with low texture are always discontinuous and holey when\nextracted. This paper proposes a novel multi attending path neural network\n(MAP-Net) for accurately extracting multiscale building footprints and precise\nboundaries. MAP-Net learns spatial localization-preserved multiscale features\nthrough a multi-parallel path in which each stage is gradually generated to\nextract high-level semantic features with fixed resolution. Then, an attention\nmodule adaptively squeezes channel-wise features from each path for\noptimization, and a pyramid spatial pooling module captures global dependency\nfor refining discontinuous building footprints. Experimental results show that\nMAP-Net outperforms state-of-the-art (SOTA) algorithms in boundary localization\naccuracy as well as continuity of large buildings. Specifically, our method\nachieved 0.68\\%, 1.74\\%, 1.46\\% precision, and 1.50\\%, 1.53\\%, 0.82\\% IoU score\nimprovement without increasing computational complexity compared with the\nlatest HRNetv2 on the Urban 3D, Deep Globe and WHU datasets, respectively. The\nTensorFlow implementation is available at https://github.com/lehaifeng/MAPNet.\n",
    "topics": "{}",
    "score": 0.8035693281
  },
  {
    "id": "1803.11550",
    "title": "Multi-modal Disease Classification in Incomplete Datasets Using\n  Geometric Matrix Completion",
    "abstract": "  In large population-based studies and in clinical routine, tasks like disease\ndiagnosis and progression prediction are inherently based on a rich set of\nmulti-modal data, including imaging and other sensor data, clinical scores,\nphenotypes, labels and demographics. However, missing features, rater bias and\ninaccurate measurements are typical ailments of real-life medical datasets.\nRecently, it has been shown that deep learning with graph convolution neural\nnetworks (GCN) can outperform traditional machine learning in disease\nclassification, but missing features remain an open problem. In this work, we\nfollow up on the idea of modeling multi-modal disease classification as a\nmatrix completion problem, with simultaneous classification and non-linear\nimputation of features. Compared to methods before, we arrange subjects in a\ngraph-structure and solve classification through geometric matrix completion,\nwhich simulates a heat diffusion process that is learned and solved with a\nrecurrent neural network. We demonstrate the potential of this method on the\nADNI-based TADPOLE dataset and on the task of predicting the transition from\nMCI to Alzheimer's disease. With an AUC of 0.950 and classification accuracy of\n87%, our approach outperforms standard linear and non-linear classifiers, as\nwell as several state-of-the-art results in related literature, including a\nrecently proposed GCN-based approach.\n",
    "topics": "{'Matrix Completion': 0.9999999, 'Imputation': 0.98380303}",
    "score": 0.8035669637
  },
  {
    "id": "2007.04302",
    "title": "A Novel BGCapsule Network for Text Classification",
    "abstract": "  Several text classification tasks such as sentiment analysis, news\ncategorization, multi-label classification and opinion classification are\nchallenging problems even for modern deep learning networks. Recently, Capsule\nNetworks (CapsNets) are proposed for image classification. It has been shown\nthat CapsNets have several advantages over Convolutional Neural Networks\n(CNNs), while their validity in the domain of text has been less explored. In\nthis paper, we propose a novel hybrid architecture viz., BGCapsule, which is a\nCapsule model preceded by an ensemble of Bidirectional Gated Recurrent Units\n(BiGRU) for several text classification tasks. We employed an ensemble of\nBidirectional GRUs for feature extraction layer preceding the primary capsule\nlayer. The hybrid architecture, after performing basic pre-processing steps,\nconsists of five layers: an embedding layer based on GloVe, a BiGRU based\nensemble layer, a primary capsule layer, a flatten layer and fully connected\nReLU layer followed by a fully connected softmax layer. In order to evaluate\nthe effectiveness of BGCapsule, we conducted extensive experiments on five\nbenchmark datasets (ranging from 10,000 records to 700,000 records) including\nMovie Review (MR Imdb 2005), AG News dataset, Dbpedia ontology dataset, Yelp\nReview Full dataset and Yelp review polarity dataset. These benchmarks cover\nseveral text classification tasks such as news categorization, sentiment\nanalysis, multiclass classification, multi-label classification and opinion\nclassification. We found that our proposed architecture (BGCapsule) achieves\nbetter accuracy compared to the existing methods without the help of any\nexternal linguistic knowledge such as positive sentiment keywords and negative\nsentiment keywords. Further, BGCapsule converged faster compared to other\nextant techniques.\n",
    "topics": "{'Text Classification': 1.0, 'Multi-Label Classification': 0.9999999, 'Image Classification': 0.95030713, 'Sentiment Analysis': 0.92531544}",
    "score": 0.8034000253
  },
  {
    "id": "2005.02134",
    "title": "IPN Hand: A Video Dataset and Benchmark for Real-Time Continuous Hand\n  Gesture Recognition",
    "abstract": "  In the research community of continuous hand gesture recognition (HGR), the\ncurrent publicly available datasets lack real-world elements needed to build\nresponsive and efficient HGR systems. In this paper, we introduce a new\nbenchmark dataset named IPN Hand with sufficient size, variation, and\nreal-world elements able to train and evaluate deep neural networks. This\ndataset contains more than 4 000 gesture samples and 800 000 RGB frames from 50\ndistinct subjects. We design 13 different static and dynamic gestures focused\non interaction with touchless screens. We especially consider the scenario when\ncontinuous gestures are performed without transition states, and when subjects\nperform natural movements with their hands as non-gesture actions. Gestures\nwere collected from about 30 diverse scenes, with real-world variation in\nbackground and illumination. With our dataset, the performance of three 3D-CNN\nmodels is evaluated on the tasks of isolated and continuous real-time HGR.\nFurthermore, we analyze the possibility of increasing the recognition accuracy\nby adding multiple modalities derived from RGB frames, i.e., optical flow and\nsemantic segmentation, while keeping the real-time performance of the 3D-CNN\nmodel. Our empirical study also provides a comparison with the publicly\navailable nvGesture (NVIDIA) dataset. The experimental results show that the\nstate-of-the-art ResNext-101 model decreases about 30% accuracy when using our\nreal-world dataset, demonstrating that the IPN Hand dataset can be used as a\nbenchmark, and may help the community to step forward in the continuous HGR.\nOur dataset and pre-trained models used in the evaluation are publicly\navailable at https://github.com/GibranBenitez/IPN-hand.\n",
    "topics": "{'Gesture Recognition': 1.0, 'Hand-Gesture Recognition': 0.99999666, 'Hand Gesture Recognition': 0.9999907, 'Optical Flow Estimation': 0.9969572, 'Semantic Segmentation': 0.49234596}",
    "score": 0.8031554063
  },
  {
    "id": "1903.06837",
    "title": "Visual recognition in the wild by sampling deep similarity functions",
    "abstract": "  Recognising relevant objects or object states in its environment is a basic\ncapability for an autonomous robot. The dominant approach to object recognition\nin images and range images is classification by supervised machine learning,\nnowadays mostly with deep convolutional neural networks (CNNs). This works well\nfor target classes whose variability can be completely covered with training\nexamples. However, a robot moving in the wild, i.e., in an environment that is\nnot known at the time the recognition system is trained, will often face\n\\emph{domain shift}: the training data cannot be assumed to exhaustively cover\nall the within-class variability that will be encountered in the test data. In\nthat situation, learning is in principle possible, since the training set does\ncapture the defining properties, respectively dissimilarities, of the target\nclasses. But directly training a CNN to predict class probabilities is prone to\noverfitting to irrelevant correlations between the class labels and the\nspecific subset of the target class that is represented in the training set. We\nexplore the idea to instead learn a Siamese CNN that acts as similarity\nfunction between pairs of training examples. Class predictions are then\nobtained by measuring the similarities between a new test instance and the\ntraining samples. We show that the CNN embedding correctly recovers the\nrelative similarities to arbitrary class exemplars in the training set. And\nthat therefore few, randomly picked training exemplars are sufficient to\nachieve good predictions, making the procedure efficient.\n",
    "topics": "{'Object Recognition': 0.9899754}",
    "score": 0.8031547635
  },
  {
    "id": "2008.01699",
    "title": "MOR-UAV: A Benchmark Dataset and Baselines for Moving Object Recognition\n  in UAV Videos",
    "abstract": "  Visual data collected from Unmanned Aerial Vehicles (UAVs) has opened a new\nfrontier of computer vision that requires automated analysis of aerial\nimages/videos. However, the existing UAV datasets primarily focus on object\ndetection. An object detector does not differentiate between the moving and\nnon-moving objects. Given a real-time UAV video stream, how can we both\nlocalize and classify the moving objects, i.e. perform moving object\nrecognition (MOR)? The MOR is one of the essential tasks to support various UAV\nvision-based applications including aerial surveillance, search and rescue,\nevent recognition, urban and rural scene understanding.To the best of our\nknowledge, no labeled dataset is available for MOR evaluation in UAV videos.\nTherefore, in this paper, we introduce MOR-UAV, a large-scale video dataset for\nMOR in aerial videos. We achieve this by labeling axis-aligned bounding boxes\nfor moving objects which requires less computational resources than producing\npixel-level estimates. We annotate 89,783 moving object instances collected\nfrom 30 UAV videos, consisting of 10,948 frames in various scenarios such as\nweather conditions, occlusion, changing flying altitude and multiple camera\nviews. We assigned the labels for two categories of vehicles (car and heavy\nvehicle). Furthermore, we propose a deep unified framework MOR-UAVNet for MOR\nin UAV videos. Since, this is a first attempt for MOR in UAV videos, we present\n16 baseline results based on the proposed framework over the MOR-UAV dataset\nthrough quantitative and qualitative experiments. We also analyze the\nmotion-salient regions in the network through multiple layer visualizations.\nThe MOR-UAVNet works online at inference as it requires only few past frames.\nMoreover, it doesn't require predefined target initialization from user.\nExperiments also demonstrate that the MOR-UAV dataset is quite challenging.\n",
    "topics": "{'Object Recognition': 0.9992416, 'Object Detection': 0.87468964}",
    "score": 0.8030809728
  },
  {
    "id": "2008.12873",
    "title": "Background Splitting: Finding Rare Classes in a Sea of Background",
    "abstract": "  We focus on the real-world problem of training accurate deep models for image\nclassification of a small number of rare categories. In these scenarios, almost\nall images belong to the background category in the dataset (>95% of the\ndataset is background). We demonstrate that both standard fine-tuning\napproaches and state-of-the-art approaches for training on imbalanced datasets\ndo not produce accurate deep models in the presence of this extreme imbalance.\nOur key observation is that the extreme imbalance due to the background\ncategory can be drastically reduced by leveraging visual knowledge from an\nexisting pre-trained model. Specifically, the background category is \"split\"\ninto smaller and more coherent pseudo-categories during training using a\npre-trained model. We incorporate background splitting into an image\nclassification model by adding an auxiliary loss that learns to mimic the\npredictions of the existing, pre-trained image classification model. Note that\nthis process is automatic and requires no additional manual labels. The\nauxiliary loss regularizes the feature representation of the shared network\ntrunk by requiring it to discriminate between previously homogeneous background\ninstances and reduces overfitting to the small number of rare category\npositives. We also show that BG splitting can be combined with other background\nimbalance methods to further improve performance. We evaluate our method on a\nmodified version of the iNaturalist dataset where only a small subset of rare\ncategory labels are available during training (all other images are labeled as\nbackground). By jointly learning to recognize ImageNet categories and selected\niNaturalist categories, our approach yields performance that is 42.3 mAP points\nhigher than a fine-tuning baseline when 99.98% of the data is background, and\n8.3 mAP points higher than SotA baselines when 98.30% of the data is\nbackground.\n",
    "topics": "{'Image Classification': 0.999913}",
    "score": 0.8029710591
  },
  {
    "id": "1901.08930",
    "title": "Active Anomaly Detection via Ensembles: Insights, Algorithms, and\n  Interpretability",
    "abstract": "  Anomaly detection (AD) task corresponds to identifying the true anomalies\nfrom a given set of data instances. AD algorithms score the data instances and\nproduce a ranked list of candidate anomalies, which are then analyzed by a\nhuman to discover the true anomalies. However, this process can be laborious\nfor the human analyst when the number of false-positives is very high.\nTherefore, in many real-world AD applications including computer security and\nfraud prevention, the anomaly detector must be configurable by the human\nanalyst to minimize the effort on false positives.\n  In this paper, we study the problem of active learning to automatically tune\nensemble of anomaly detectors to maximize the number of true anomalies\ndiscovered. We make four main contributions towards this goal. First, we\npresent an important insight that explains the practical successes of AD\nensembles and how ensembles are naturally suited for active learning. Second,\nwe present several algorithms for active learning with tree-based AD ensembles.\nThese algorithms help us to improve the diversity of discovered anomalies,\ngenerate rule sets for improved interpretability of anomalous instances, and\nadapt to streaming data settings in a principled manner. Third, we present a\nnovel algorithm called GLocalized Anomaly Detection (GLAD) for active learning\nwith generic AD ensembles. GLAD allows end-users to retain the use of simple\nand understandable global anomaly detectors by automatically learning their\nlocal relevance to specific data instances using label feedback. Fourth, we\npresent extensive experiments to evaluate our insights and algorithms. Our\nresults show that in addition to discovering significantly more anomalies than\nstate-of-the-art unsupervised baselines, our active learning algorithms under\nthe streaming-data setup are competitive with the batch setup.\n",
    "topics": "{'Anomaly Detection': 1.0, 'Active Learning': 0.9999999}",
    "score": 0.8029488994
  },
  {
    "id": "1702.03833",
    "title": "Estimation of the volume of the left ventricle from MRI images using\n  deep neural networks",
    "abstract": "  Segmenting human left ventricle (LV) in magnetic resonance imaging (MRI)\nimages and calculating its volume are important for diagnosing cardiac\ndiseases. In 2016, Kaggle organized a competition to estimate the volume of LV\nfrom MRI images. The dataset consisted of a large number of cases, but only\nprovided systole and diastole volumes as labels. We designed a system based on\nneural networks to solve this problem. It began with a detector combined with a\nneural network classifier for detecting regions of interest (ROIs) containing\nLV chambers. Then a deep neural network named hypercolumns fully convolutional\nnetwork was used to segment LV in ROIs. The 2D segmentation results were\nintegrated across different images to estimate the volume. With ground-truth\nvolume labels, this model was trained end-to-end. To improve the result, an\nadditional dataset with only segmentation label was used. The model was trained\nalternately on these two datasets with different types of teaching signals. We\nalso proposed a variance estimation method for the final prediction. Our\nalgorithm ranked the 4th on the test set in this competition.\n",
    "topics": "{}",
    "score": 0.8029029031
  },
  {
    "id": "2007.09453",
    "title": "Robust Image Classification Using A Low-Pass Activation Function and DCT\n  Augmentation",
    "abstract": "  Convolutional Neural Network's (CNN's) performance disparity on clean and\ncorrupted datasets has recently been noticed. In this work, we analyse common\ncorruptions from a frequency perspective, i.e., High Frequency corruptions or\nHFc (e.g., noise) and Low Frequency corruptions or LFc (e.g., blur). A common\nsignal processing solution to HFc is low-pass filtering. Intriguingly, the\nde-facto Activation Function (AF) used in modern CNNs, i.e., ReLU does not have\nany filtering mechanism resulting in unstable performance on HFc. In this work,\nwe propose a family of novel AFs with low-pass filtering to improve robustness\nagainst HFc (we call it Low-Pass ReLU or LP-ReLU). To deal with LFc, we further\nenhance the AFs with Discrete Cosine Transform (DCT) based augmentation.\nLP-ReLU coupled with DCT augmentation, enables a deep network to tackle a\nvariety of corruptions. We evaluate our method's performance on CIFAR-10-C and\nTiny ImageNet-C datasets and achieve improvements of 5.1% and 7.2% in accuracy\nrespectively compared to the State-Of-The-Art (SOTA). We further evaluate our\nmethod's performance stability on a variety of perturbations available in\nCIFAR-10-P and Tiny ImageNet-P. We also achieve new SOTA results in these\nexperiments. We also devise a decision space visualisation process to further\nstrengthen the understanding regarding CNN's lack of robustness against\ncorrupted data.\n",
    "topics": "{'Image Classification': 0.98628753}",
    "score": 0.8028419194
  },
  {
    "id": "1910.04963",
    "title": "Interaction Relational Network for Mutual Action Recognition",
    "abstract": "  Person-person mutual action recognition (also referred to as interaction\nrecognition) is an important research branch of human activity analysis.\nCurrent solutions in the field are mainly dominated by CNNs, GCNs and LSTMs.\nThese approaches often consist of complicated architectures and mechanisms to\nembed the relationships between the two persons on the architecture itself, to\nensure the interaction patterns can be properly learned. In this paper, we\npropose a more simple yet very powerful architecture, named Interaction\nRelational Network (IRN), which utilizes minimal prior knowledge about the\nstructure of the human body. We drive the network to identify by itself how to\nrelate the body parts from the individuals interacting. In order to better\nrepresent the interaction, we define two different relationships, leading to\nspecialized architectures and models for each. These multiple relationship\nmodels will then be fused into a single and special architecture, in order to\nleverage both streams of information for further enhancing the relational\nreasoning capability. Furthermore we define important structured pair-wise\noperations to extract meaningful extra information from each pair of joints --\ndistance and motion. Ultimately, with the coupling of an LSTM, our IRN is\ncapable of paramount sequential relational reasoning. These important\nextensions we made to our network can also be valuable to other problems that\nrequire sophisticated relational reasoning. Our solution is able to achieve\nstate-of-the-art performance on the traditional interaction recognition\ndatasets SBU and UT, and also on the mutual actions from the large-scale NTU\nRGB+D and NTU RGB+D 120 datasets.\n",
    "topics": "{'Action Recognition': 0.9999993, 'Relational Reasoning': 0.3077992}",
    "score": 0.802750785
  },
  {
    "id": "1811.06666",
    "title": "Ground Plane Polling for 6DoF Pose Estimation of Objects on the Road",
    "abstract": "  This paper introduces an approach to produce accurate 3D detection boxes for\nobjects on the ground using single monocular images. We do so by merging 2D\nvisual cues, 3D object dimensions, and ground plane constraints to produce\nboxes that are robust against small errors and incorrect predictions. First, we\ntrain a single-shot convolutional neural network (CNN) that produces multiple\nvisual and geometric cues of interest: 2D bounding boxes, 2D keypoints of\ninterest, coarse object orientations and object dimensions. Subsets of these\ncues are then used to poll probable ground planes from a pre-computed database\nof ground planes, to identify the \"best fit\" plane with highest consensus. Once\nidentified, the \"best fit\" plane provides enough constraints to successfully\nconstruct the desired 3D detection box, without directly predicting the 6DoF\npose of the object. The entire ground plane polling (GPP) procedure is\nconstructed as a non-parametrized layer of the CNN that outputs the desired\n\"best fit\" plane and the corresponding 3D keypoints, which together define the\nfinal 3D bounding box. Doing so allows us to poll thousands of different ground\nplane configurations without adding considerable overhead, while also creating\na single CNN that directly produces the desired output without the need for\npost processing. We evaluate our method on the 2D detection and orientation\nestimation benchmark from the challenging KITTI dataset, and provide additional\ncomparisons for 3D metrics of importance. This single-stage, single-pass CNN\nresults in superior localization and orientation estimation compared to more\ncomplex and computationally expensive monocular approaches.\n",
    "topics": "{'Pose Estimation': 0.90491784}",
    "score": 0.802689644
  },
  {
    "id": "1908.07896",
    "title": "Enabling hyperparameter optimization in sequential autoencoders for\n  spiking neural data",
    "abstract": "  Continuing advances in neural interfaces have enabled simultaneous monitoring\nof spiking activity from hundreds to thousands of neurons. To interpret these\nlarge-scale data, several methods have been proposed to infer latent dynamic\nstructure from high-dimensional datasets. One recent line of work uses\nrecurrent neural networks in a sequential autoencoder (SAE) framework to\nuncover dynamics. SAEs are an appealing option for modeling nonlinear dynamical\nsystems, and enable a precise link between neural activity and behavior on a\nsingle-trial basis. However, the very large parameter count and complexity of\nSAEs relative to other models has caused concern that SAEs may only perform\nwell on very large training sets. We hypothesized that with a method to\nsystematically optimize hyperparameters (HPs), SAEs might perform well even in\ncases of limited training data. Such a breakthrough would greatly extend their\napplicability. However, we find that SAEs applied to spiking neural data are\nprone to a particular form of overfitting that cannot be detected using\nstandard validation metrics, which prevents standard HP searches. We develop\nand test two potential solutions: an alternate validation method (\"sample\nvalidation\") and a novel regularization method (\"coordinated dropout\"). These\ninnovations prevent overfitting quite effectively, and allow us to test whether\nSAEs can achieve good performance on limited data through large-scale HP\noptimization. When applied to data from motor cortex recorded while monkeys\nmade reaches in various directions, large-scale HP optimization allowed SAEs to\nbetter maintain performance for small dataset sizes. Our results should greatly\nextend the applicability of SAEs in extracting latent dynamics from sparse,\nmultidimensional data, such as neural population spiking activity.\n",
    "topics": "{'Hyperparameter Optimization': 0.9999894}",
    "score": 0.8026729781
  },
  {
    "id": "2005.07771",
    "title": "C3VQG: Category Consistent Cyclic Visual Question Generation",
    "abstract": "  Visual Question Generation (VQG) is the task of generating natural questions\nbased on an image. Popular methods in the past have explored image-to-sequence\narchitectures trained with maximum likelihood which have demonstrated\nmeaningful generated questions given an image and its associated ground-truth\nanswer. VQG becomes more challenging if the image contains rich context\ninformation describing its different semantic categories. In this paper, we try\nto exploit the different visual cues and concepts in an image to generate\nquestions using a variational autoencoder (VAE) without ground-truth answers.\nOur approach solves two major shortcomings of existing VQG systems: (i)\nminimize the level of supervision and (ii) replace generic questions with\ncategory relevant generations. Most importantly, through eliminating expensive\nanswer annotations, the required supervision is weakened. Using different\ncategories enables us to exploit different concepts as the inference requires\nonly the image and category. Mutual information is maximized between the image,\nquestion, and answer category in the latent space of our VAE. A novel category\nconsistent cyclic loss is proposed to enable the model to generate consistent\npredictions with respect to the answer category, reducing its redundancies and\nirregularities. Additionally, we also impose supplementary constraints on the\nlatent space of our generative model to provide structure based on categories\nand enhance generalization by encapsulating decorrelated features within each\ndimension. Through extensive experiments, the proposed C3VQG outperforms the\nstate-of-the-art visual question generation methods with weak supervision.\n",
    "topics": "{'Question Generation': 1.0, 'Visual Question Answering': 0.71237326}",
    "score": 0.8026597909
  },
  {
    "id": "1401.4128",
    "title": "Towards the selection of patients requiring ICD implantation by\n  automatic classification from Holter monitoring indices",
    "abstract": "  The purpose of this study is to optimize the selection of prophylactic\ncardioverter defibrillator implantation candidates. Currently, the main\ncriterion for implantation is a low Left Ventricular Ejection Fraction (LVEF)\nwhose specificity is relatively poor. We designed two classifiers aimed to\npredict, from long term ECG recordings (Holter), whether a low-LVEF patient is\nlikely or not to undergo ventricular arrhythmia in the next six months. One\nclassifier is a single hidden layer neural network whose variables are the most\nrelevant features extracted from Holter recordings, and the other classifier\nhas a structure that capitalizes on the physiological decomposition of the\narrhythmogenic factors into three disjoint groups: the myocardial substrate,\nthe triggers and the autonomic nervous system (ANS). In this ad hoc network,\nthe features were assigned to each group; one neural network classifier per\ngroup was designed and its complexity was optimized. The outputs of the\nclassifiers were fed to a single neuron that provided the required probability\nestimate. The latter was thresholded for final discrimination A dataset\ncomposed of 186 pre-implantation 30-mn Holter recordings of patients equipped\nwith an implantable cardioverter defibrillator (ICD) in primary prevention was\nused in order to design and test this classifier. 44 out of 186 patients\nunderwent at least one treated ventricular arrhythmia during the six-month\nfollow-up period. Performances of the designed classifier were evaluated using\na cross-test strategy that consists in splitting the database into several\ncombinations of a training set and a test set. The average arrhythmia\nprediction performances of the ad-hoc classifier are NPV = 77% $\\pm$ 13% and\nPPV = 31% $\\pm$ 19% (Negative Predictive Value $\\pm$ std, Positive Predictive\nValue $\\pm$ std). According to our study, improving prophylactic\nICD-implantation candidate selection by automatic classification from ECG\nfeatures may be possible, but the availability of a sizable dataset appears to\nbe essential to decrease the number of False Negatives.\n",
    "topics": "{}",
    "score": 0.8026379181
  },
  {
    "id": "2010.04511",
    "title": "Sickle-cell disease diagnosis support selecting the most appropriate\n  machinelearning method: Towards a general and interpretable approach for\n  cellmorphology analysis from microscopy images",
    "abstract": "  In this work we propose an approach to select the classification method and\nfeatures, based on the state-of-the-art, with best performance for diagnostic\nsupport through peripheral blood smear images of red blood cells. In our case\nwe used samples of patients with sickle-cell disease which can be generalized\nfor other study cases. To trust the behavior of the proposed system, we also\nanalyzed the interpretability.\n  We pre-processed and segmented microscopic images, to ensure high feature\nquality. We applied the methods used in the literature to extract the features\nfrom blood cells and the machine learning methods to classify their morphology.\nNext, we searched for their best parameters from the resulting data in the\nfeature extraction phase. Then, we found the best parameters for every\nclassifier using Randomized and Grid search.\n  For the sake of scientific progress, we published parameters for each\nclassifier, the implemented code library, the confusion matrices with the raw\ndata, and we used the public erythrocytesIDB dataset for validation. We also\ndefined how to select the most important features for classification to\ndecrease the complexity and the training time, and for interpretability purpose\nin opaque models. Finally, comparing the best performing classification methods\nwith the state-of-the-art, we obtained better results even with interpretable\nmodel classifiers.\n",
    "topics": "{}",
    "score": 0.8024857064
  },
  {
    "id": "1809.01701",
    "title": "Pack and Detect: Fast Object Detection in Videos Using\n  Region-of-Interest Packing",
    "abstract": "  Object detection in videos is an important task in computer vision for\nvarious applications such as object tracking, video summarization and video\nsearch. Although great progress has been made in improving the accuracy of\nobject detection in recent years due to the rise of deep neural networks, the\nstate-of-the-art algorithms are highly computationally intensive. In order to\naddress this challenge, we make two important observations in the context of\nvideos: (i) Objects often occupy only a small fraction of the area in each\nvideo frame, and (ii) There is a high likelihood of strong temporal correlation\nbetween consecutive frames. Based on these observations, we propose Pack and\nDetect (PaD), an approach to reduce the computational requirements of object\ndetection in videos. In PaD, only selected video frames called anchor frames\nare processed at full size. In the frames that lie between anchor frames\n(inter-anchor frames), regions of interest (ROIs) are identified based on the\ndetections in the previous frame. We propose an algorithm to pack the ROIs of\neach inter-anchor frame together into a reduced-size frame. The computational\nrequirements of the detector are reduced due to the lower size of the input. In\norder to maintain the accuracy of object detection, the proposed algorithm\nexpands the ROIs greedily to provide additional background around each object\nto the detector. PaD can use any underlying neural network architecture to\nprocess the full-size and reduced-size frames. Experiments using the ImageNet\nvideo object detection dataset indicate that PaD can potentially reduce the\nnumber of FLOPS required for a frame by $4\\times$. This leads to an overall\nincrease in throughput of $1.25\\times$ on a 2.1 GHz Intel Xeon server with a\nNVIDIA Titan X GPU at the cost of $1.1\\%$ drop in accuracy.\n",
    "topics": "{'Object Detection': 1.0, 'Video Summarization': 0.99953866, 'Object Tracking': 0.9976829, 'Video Object Detection': 0.48684233}",
    "score": 0.8023108
  },
  {
    "id": "1811.12506",
    "title": "3D Semi-Supervised Learning with Uncertainty-Aware Multi-View\n  Co-Training",
    "abstract": "  While making a tremendous impact in various fields, deep neural networks\nusually require large amounts of labeled data for training which are expensive\nto collect in many applications, especially in the medical domain. Unlabeled\ndata, on the other hand, is much more abundant. Semi-supervised learning\ntechniques, such as co-training, could provide a powerful tool to leverage\nunlabeled data. In this paper, we propose a novel framework, uncertainty-aware\nmulti-view co-training (UMCT), to address semi-supervised learning on 3D data,\nsuch as volumetric data from medical imaging. In our work, co-training is\nachieved by exploiting multi-viewpoint consistency of 3D data. We generate\ndifferent views by rotating or permuting the 3D data and utilize asymmetrical\n3D kernels to encourage diversified features in different sub-networks. In\naddition, we propose an uncertainty-weighted label fusion mechanism to estimate\nthe reliability of each view's prediction with Bayesian deep learning. As one\nview requires the supervision from other views in co-training, our\nself-adaptive approach computes a confidence score for the prediction of each\nunlabeled sample in order to assign a reliable pseudo label. Thus, our approach\ncan take advantage of unlabeled data during training. We show the effectiveness\nof our proposed semi-supervised method on several public datasets from medical\nimage segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile,\na fully-supervised method based on our approach achieved state-of-the-art\nperformances on both the LiTS liver tumor segmentation and the Medical\nSegmentation Decathlon (MSD) challenge, demonstrating the robustness and value\nof our framework, even when fully supervised training is feasible.\n",
    "topics": "{'Tumor Segmentation': 0.9999999, 'Medical Image Segmentation': 0.9983889, 'Semantic Segmentation': 0.71001446}",
    "score": 0.8022672201
  },
  {
    "id": "1802.02952",
    "title": "TSViz: Demystification of Deep Learning Models for Time-Series Analysis",
    "abstract": "  This paper presents a novel framework for demystification of convolutional\ndeep learning models for time-series analysis. This is a step towards making\ninformed/explainable decisions in the domain of time-series, powered by deep\nlearning. There have been numerous efforts to increase the interpretability of\nimage-centric deep neural network models, where the learned features are more\nintuitive to visualize. Visualization in time-series domain is much more\ncomplicated as there is no direct interpretation of the filters and inputs as\ncompared to the image modality. In addition, little or no concentration has\nbeen devoted for the development of such tools in the domain of time-series in\nthe past. TSViz provides possibilities to explore and analyze a network from\ndifferent dimensions at different levels of abstraction which includes\nidentification of parts of the input that were responsible for a prediction\n(including per filter saliency), importance of different filters present in the\nnetwork for a particular prediction, notion of diversity present in the network\nthrough filter clustering, understanding of the main sources of variation\nlearnt by the network through inverse optimization, and analysis of the\nnetwork's robustness against adversarial noise. As a sanity check for the\ncomputed influence values, we demonstrate results regarding pruning of neural\nnetworks based on the computed influence information. These representations\nallow to understand the network features so that the acceptability of deep\nnetworks for time-series data can be enhanced. This is extremely important in\ndomains like finance, industry 4.0, self-driving cars, health-care,\ncounter-terrorism etc., where reasons for reaching a particular prediction are\nequally important as the prediction itself. We assess the proposed framework\nfor interpretability with a set of desirable properties essential for any\nmethod.\n",
    "topics": "{'Time Series': 1.0, 'Self-Driving Cars': 1.0, 'Time Series Analysis': 0.99991906}",
    "score": 0.802264248
  },
  {
    "id": "1803.06589",
    "title": "Early hospital mortality prediction using vital signals",
    "abstract": "  Early hospital mortality prediction is critical as intensivists strive to\nmake efficient medical decisions about the severely ill patients staying in\nintensive care units. As a result, various methods have been developed to\naddress this problem based on clinical records. However, some of the laboratory\ntest results are time-consuming and need to be processed. In this paper, we\npropose a novel method to predict mortality using features extracted from the\nheart signals of patients within the first hour of ICU admission. In order to\npredict the risk, quantitative features have been computed based on the heart\nrate signals of ICU patients. Each signal is described in terms of 12\nstatistical and signal-based features. The extracted features are fed into\neight classifiers: decision tree, linear discriminant, logistic regression,\nsupport vector machine (SVM), random forest, boosted trees, Gaussian SVM, and\nK-nearest neighborhood (K-NN). To derive insight into the performance of the\nproposed method, several experiments have been conducted using the well-known\nclinical dataset named Medical Information Mart for Intensive Care III\n(MIMIC-III). The experimental results demonstrate the capability of the\nproposed method in terms of precision, recall, F1-score, and area under the\nreceiver operating characteristic curve (AUC). The decision tree classifier\nsatisfies both accuracy and interpretability better than the other classifiers,\nproducing an F1-score and AUC equal to 0.91 and 0.93, respectively. It\nindicates that heart rate signals can be used for predicting mortality in\npatients in the ICU, achieving a comparable performance with existing\npredictions that rely on high dimensional features from clinical records which\nneed to be processed and may contain missing information.\n",
    "topics": "{'Mortality Prediction': 1.0, 'Test results': 0.99999976}",
    "score": 0.8022088838
  },
  {
    "id": "1807.11205",
    "title": "Highly Scalable Deep Learning Training System with Mixed-Precision:\n  Training ImageNet in Four Minutes",
    "abstract": "  Synchronized stochastic gradient descent (SGD) optimizers with data\nparallelism are widely used in training large-scale deep neural networks.\nAlthough using larger mini-batch sizes can improve the system scalability by\nreducing the communication-to-computation ratio, it may hurt the generalization\nability of the models. To this end, we build a highly scalable deep learning\ntraining system for dense GPU clusters with three main contributions: (1) We\npropose a mixed-precision training method that significantly improves the\ntraining throughput of a single GPU without losing accuracy. (2) We propose an\noptimization approach for extremely large mini-batch size (up to 64k) that can\ntrain CNN models on the ImageNet dataset without losing accuracy. (3) We\npropose highly optimized all-reduce algorithms that achieve up to 3x and 11x\nspeedup on AlexNet and ResNet-50 respectively than NCCL-based training on a\ncluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the\nstate-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes\nand achieved 74.9\\% top-1 test accuracy, and another KNL-based system with 2048\nIntel KNLs spent 20 minutes and achieved 75.4\\% accuracy. Our training system\ncan achieve 75.8\\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40\nGPUs. When training AlexNet with 95 epochs, our system can achieve 58.7\\% top-1\ntest accuracy within 4 minutes, which also outperforms all other existing\nsystems.\n",
    "topics": "{'Semantic Segmentation': 0.49053353}",
    "score": 0.8021386544
  },
  {
    "id": "1706.02051",
    "title": "Automatic Emphysema Detection using Weakly Labeled HRCT Lung Images",
    "abstract": "  A method for automatically quantifying emphysema regions using\nHigh-Resolution Computed Tomography (HRCT) scans of patients with chronic\nobstructive pulmonary disease (COPD) that does not require manually annotated\nscans for training is presented. HRCT scans of controls and of COPD patients\nwith diverse disease severity are acquired at two different centers. Textural\nfeatures from co-occurrence matrices and Gaussian filter banks are used to\ncharacterize the lung parenchyma in the scans. Two robust versions of multiple\ninstance learning (MIL) classifiers, miSVM and MILES, are investigated. The\nclassifiers are trained with the weak labels extracted from the forced\nexpiratory volume in one minute (FEV$_1$) and diffusing capacity of the lungs\nfor carbon monoxide (DLCO). At test time, the classifiers output a patient\nlabel indicating overall COPD diagnosis and local labels indicating the\npresence of emphysema. The classifier performance is compared with manual\nannotations by two radiologists, a classical density based method, and\npulmonary function tests (PFTs). The miSVM classifier performed better than\nMILES on both patient and emphysema classification. The classifier has a\nstronger correlation with PFT than the density based method, the percentage of\nemphysema in the intersection of annotations from both radiologists, and the\npercentage of emphysema annotated by one of the radiologists. The correlation\nbetween the classifier and the PFT is only outperformed by the second\nradiologist. The method is therefore promising for facilitating assessment of\nemphysema and reducing inter-observer variability.\n",
    "topics": "{'Multiple Instance Learning': 1.0}",
    "score": 0.8021376463
  },
  {
    "id": "1806.02067",
    "title": "PieAPP: Perceptual Image-Error Assessment through Pairwise Preference",
    "abstract": "  The ability to estimate the perceptual error between images is an important\nproblem in computer vision with many applications. Although it has been studied\nextensively, however, no method currently exists that can robustly predict\nvisual differences like humans. Some previous approaches used hand-coded\nmodels, but they fail to model the complexity of the human visual system.\nOthers used machine learning to train models on human-labeled datasets, but\ncreating large, high-quality datasets is difficult because people are unable to\nassign consistent error labels to distorted images. In this paper, we present a\nnew learning-based method that is the first to predict perceptual image error\nlike human observers. Since it is much easier for people to compare two given\nimages and identify the one more similar to a reference than to assign quality\nscores to each, we propose a new, large-scale dataset labeled with the\nprobability that humans will prefer one image over another. We then train a\ndeep-learning model using a novel, pairwise-learning framework to predict the\npreference of one distorted image over the other. Our key observation is that\nour trained network can then be used separately with only one distorted image\nand a reference to predict its perceptual error, without ever being trained on\nexplicit human perceptual-error labels. The perceptual error estimated by our\nnew metric, PieAPP, is well-correlated with human opinion. Furthermore, it\nsignificantly outperforms existing algorithms, beating the state-of-the-art by\nalmost 3x on our test set in terms of binary error rate, while also\ngeneralizing to new kinds of distortions, unlike previous learning-based\nmethods.\n",
    "topics": "{}",
    "score": 0.8021254433
  },
  {
    "id": "1701.08968",
    "title": "Supervised Learning in Automatic Channel Selection for Epileptic Seizure\n  Detection",
    "abstract": "  Detecting seizure using brain neuroactivations recorded by intracranial\nelectroencephalogram (iEEG) has been widely used for monitoring, diagnosing,\nand closed-loop therapy of epileptic patients, however, computational\nefficiency gains are needed if state-of-the-art methods are to be implemented\nin implanted devices. We present a novel method for automatic seizure detection\nbased on iEEG data that outperforms current state-of-the-art seizure detection\nmethods in terms of computational efficiency while maintaining the accuracy.\nThe proposed algorithm incorporates an automatic channel selection (ACS) engine\nas a pre-processing stage to the seizure detection procedure. The ACS engine\nconsists of supervised classifiers which aim to find iEEGchannelswhich\ncontribute the most to a seizure. Seizure detection stage involves feature\nextraction and classification. Feature extraction is performed in both\nfrequency and time domains where spectral power and correlation between channel\npairs are calculated. Random Forest is used in classification of interictal,\nictal and early ictal periods of iEEG signals. Seizure detection in this paper\nis retrospective and patient-specific. iEEG data is accessed via Kaggle,\nprovided by International Epilepsy Electro-physiology Portal. The dataset\nincludes a training set of 6.5 hours of interictal data and 41 minin ictal data\nand a test set of 9.14 hours. Compared to the state-of-the-art on the same\ndataset, we achieve 49.4% increase in computational efficiency and 400 mins\nbetter in average for detection delay. The proposed model is able to detect a\nseizure onset at 91.95% sensitivity and 94.05% specificity with a mean\ndetection delay of 2.77 s. The area under the curve (AUC) is 96.44%, that is\ncomparable to the current state-of-the-art with AUC of 96.29%.\n",
    "topics": "{}",
    "score": 0.8020816083
  },
  {
    "id": "1905.08983",
    "title": "LapTool-Net: A Contextual Detector of Surgical Tools in Laparoscopic\n  Videos Based on Recurrent Convolutional Neural Networks",
    "abstract": "  We propose a new multilabel classifier, called LapTool-Net to detect the\npresence of surgical tools in each frame of a laparoscopic video. The novelty\nof LapTool-Net is the exploitation of the correlation among the usage of\ndifferent tools and, the tools and tasks - namely, the context of the tools'\nusage. Towards this goal, the pattern in the co-occurrence of the tools is\nutilized for designing a decision policy for a multilabel classifier based on a\nRecurrent Convolutional Neural Network (RCNN) architecture to simultaneously\nextract the spatio-temporal features. In contrast to the previous multilabel\nclassification methods, the RCNN and the decision model are trained in an\nend-to-end manner using a multitask learning scheme. To overcome the high\nimbalance and avoid overfitting caused by the lack of variety in the training\ndata, a high down-sampling rate is chosen based on the more frequent\ncombinations. Furthermore, at the post-processing step, the prediction for all\nthe frames of a video are corrected by designing a bi-directional RNN to model\nthe long-term task's order. LapTool-net was trained using a publicly available\ndataset of laparoscopic cholecystectomy. The results show LapTool-Net\noutperforms existing methods significantly, even while using fewer training\nsamples and a shallower architecture.\n",
    "topics": "{}",
    "score": 0.8020519537
  },
  {
    "id": "1704.03971",
    "title": "On the Effects of Batch and Weight Normalization in Generative\n  Adversarial Networks",
    "abstract": "  Generative adversarial networks (GANs) are highly effective unsupervised\nlearning frameworks that can generate very sharp data, even for data such as\nimages with complex, highly multimodal distributions. However GANs are known to\nbe very hard to train, suffering from problems such as mode collapse and\ndisturbing visual artifacts. Batch normalization (BN) techniques have been\nintroduced to address the training. Though BN accelerates the training in the\nbeginning, our experiments show that the use of BN can be unstable and\nnegatively impact the quality of the trained model. The evaluation of BN and\nnumerous other recent schemes for improving GAN training is hindered by the\nlack of an effective objective quality measure for GAN models. To address these\nissues, we first introduce a weight normalization (WN) approach for GAN\ntraining that significantly improves the stability, efficiency and the quality\nof the generated samples. To allow a methodical evaluation, we introduce\nsquared Euclidean reconstruction error on a test set as a new objective\nmeasure, to assess training performance in terms of speed, stability, and\nquality of generated samples. Our experiments with a standard DCGAN\narchitecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10)\nindicate that training using WN is generally superior to BN for GANs, achieving\n10% lower mean squared loss for reconstruction and significantly better\nqualitative results than BN. We further demonstrate the stability of WN on a\n21-layer ResNet trained with the CelebA data set. The code for this paper is\navailable at https://github.com/stormraiser/gan-weightnorm-resnet\n",
    "topics": "{'Image Generation': 0.95502704}",
    "score": 0.8020321646
  },
  {
    "id": "2010.04717",
    "title": "Unsupervised 3D Brain Anomaly Detection",
    "abstract": "  Anomaly detection (AD) is the identification of data samples that do not fit\na learned data distribution. As such, AD systems can help physicians to\ndetermine the presence, severity, and extension of a pathology. Deep generative\nmodels, such as Generative Adversarial Networks (GANs), can be exploited to\ncapture anatomical variability. Consequently, any outlier (i.e., sample falling\noutside of the learned distribution) can be detected as an abnormality in an\nunsupervised fashion. By using this method, we can not only detect expected or\nknown lesions, but we can even unveil previously unrecognized biomarkers. To\nthe best of our knowledge, this study exemplifies the first AD approach that\ncan efficiently handle volumetric data and detect 3D brain anomalies in one\nsingle model. Our proposal is a volumetric and high-detail extension of the 2D\nf-AnoGAN model obtained by combining a state-of-the-art 3D GAN with refinement\ntraining steps. In experiments using non-contrast computed tomography images\nfrom traumatic brain injury (TBI) patients, the model detects and localizes TBI\nabnormalities with an area under the ROC curve of ~75%. Moreover, we test the\npotential of the method for detecting other anomalies such as low quality\nimages, preprocessing inaccuracies, artifacts, and even the presence of\npost-operative signs (such as a craniectomy or a brain shunt). The method has\npotential for rapidly labeling abnormalities in massive imaging datasets, as\nwell as identifying new biomarkers.\n",
    "topics": "{'Anomaly Detection': 1.0}",
    "score": 0.8020259389
  },
  {
    "id": "1805.08400",
    "title": "Deep Learning with Cinematic Rendering: Fine-Tuning Deep Neural Networks\n  Using Photorealistic Medical Images",
    "abstract": "  Deep learning has emerged as a powerful artificial intelligence tool to\ninterpret medical images for a growing variety of applications. However, the\npaucity of medical imaging data with high-quality annotations that is necessary\nfor training such methods ultimately limits their performance. Medical data is\nchallenging to acquire due to privacy issues, shortage of experts available for\nannotation, limited representation of rare conditions and cost. This problem\nhas previously been addressed by using synthetically generated data. However,\nnetworks trained on synthetic data often fail to generalize to real data.\nCinematic rendering simulates the propagation and interaction of light passing\nthrough tissue models reconstructed from CT data, enabling the generation of\nphotorealistic images. In this paper, we present one of the first applications\nof cinematic rendering in deep learning, in which we propose to fine-tune\nsynthetic data-driven networks using cinematically rendered CT data for the\ntask of monocular depth estimation in endoscopy. Our experiments demonstrate\nthat: (a) Convolutional Neural Networks (CNNs) trained on synthetic data and\nfine-tuned on photorealistic cinematically rendered data adapt better to real\nmedical images and demonstrate more robust performance when compared to\nnetworks with no fine-tuning, (b) these fine-tuned networks require less\ntraining data to converge to an optimal solution, and (c) fine-tuning with data\nfrom a variety of photorealistic rendering conditions of the same scene\nprevents the network from learning patient-specific information and aids in\ngeneralizability of the model. Our empirical evaluation demonstrates that\nnetworks fine-tuned with cinematically rendered data predict depth with 56.87%\nless error for rendered endoscopy images and 27.49% less error for real porcine\ncolon endoscopy images.\n",
    "topics": "{'Monocular Depth Estimation': 0.99470633, 'Depth Estimation': 0.59370494}",
    "score": 0.8020233736
  },
  {
    "id": "2004.02587",
    "title": "A Two-Stage Reconstruction of Microstructures with Arbitrarily Shaped\n  Inclusions",
    "abstract": "  The main goal of our research is to develop an effective method with a wide\nrange of applications for the statistical reconstruction of heterogeneous\nmicrostructures with compact inclusions of any shape, such as highly irregular\ngrains. The devised approach uses multi-scale extended entropic descriptors\n(ED) that quantify the degree of spatial non-uniformity of configurations of\nfinite-sized objects. This technique is an innovative development of previously\nelaborated entropy methods for statistical reconstruction. Here, we discuss the\ntwo-dimensional case, but this method can be generalized into three dimensions.\nAt the first stage, the developed procedure creates a set of black synthetic\nclusters that serve as surrogate inclusions. The clusters have the same\nindividual areas and interfaces as their target counterparts, but random\nshapes. Then, from a given number of easy-to-generate synthetic cluster\nconfigurations, we choose the one with the lowest value of the cost function\ndefined by us using extended ED. At the second stage, we make a significant\nchange in the standard technique of simulated annealing (SA). Instead of\nswapping pixels of different phases, we randomly move each of the selected\nsynthetic clusters. To demonstrate the accuracy of the method, we reconstruct\nand analyze two-phase microstructures with irregular inclusions of silica in\nrubber matrix as well as stones in cement paste. The results show that the\ntwo-stage reconstruction (TSR) method provides convincing realizations for\nthese complex microstructures. The advantages of TSR include the ease of\nobtaining synthetic microstructures, very low computational costs, and\nsatisfactory mapping in the statistical context of inclusion shapes. Finally,\nits simplicity should greatly facilitate independent applications.\n",
    "topics": "{'3D Reconstruction': 0.58690727}",
    "score": 0.8020005602
  },
  {
    "id": "1905.01958",
    "title": "Text2Node: a Cross-Domain System for Mapping Arbitrary Phrases to a\n  Taxonomy",
    "abstract": "  Electronic health record (EHR) systems are used extensively throughout the\nhealthcare domain. However, data interchangeability between EHR systems is\nlimited due to the use of different coding standards across systems. Existing\nmethods of mapping coding standards based on manual human experts mapping,\ndictionary mapping, symbolic NLP and classification are unscalable and cannot\naccommodate large scale EHR datasets.\n  In this work, we present Text2Node, a cross-domain mapping system capable of\nmapping medical phrases to concepts in a large taxonomy (such as SNOMED CT).\nThe system is designed to generalize from a limited set of training samples and\nmap phrases to elements of the taxonomy that are not covered by training data.\nAs a result, our system is scalable, robust to wording variants between coding\nsystems and can output highly relevant concepts when no exact concept exists in\nthe target taxonomy. Text2Node operates in three main stages: first, the\nlexicon is mapped to word embeddings; second, the taxonomy is vectorized using\nnode embeddings; and finally, the mapping function is trained to connect the\ntwo embedding spaces. We compared multiple algorithms and architectures for\neach stage of the training, including GloVe and FastText word embeddings, CNN\nand Bi-LSTM mapping functions, and node2vec for node embeddings. We confirmed\nthe robustness and generalisation properties of Text2Node by mapping ICD-9-CM\nDiagnosis phrases to SNOMED CT and by zero-shot training at comparable\naccuracy.\n  This system is a novel methodological contribution to the task of normalizing\nand linking phrases to a taxonomy, advancing data interchangeability in\nhealthcare. When applied, the system can use electronic health records to\ngenerate an embedding that incorporates taxonomical medical knowledge to\nimprove clinical predictive models.\n",
    "topics": "{'Word Embeddings': 0.83976716}",
    "score": 0.8019690815
  },
  {
    "id": "1904.03603",
    "title": "Human Intracranial EEG Quantitative Analysis and Automatic Feature\n  Learning for Epileptic Seizure Prediction",
    "abstract": "  Objective: The aim of this study is to develop an efficient and reliable\nepileptic seizure prediction system using intracranial EEG (iEEG) data,\nespecially for people with drug-resistant epilepsy. The prediction procedure\nshould yield accurate results in a fast enough fashion to alert patients of\nimpending seizures. Methods: We quantitatively analyze the human iEEG data to\nobtain insights into how the human brain behaves before and between epileptic\nseizures. We then introduce an efficient pre-processing method for reducing the\ndata size and converting the time-series iEEG data into an image-like format\nthat can be used as inputs to convolutional neural networks (CNNs). Further, we\npropose a seizure prediction algorithm that uses cooperative multi-scale CNNs\nfor automatic feature learning of iEEG data. Results: 1) iEEG channels contain\ncomplementary information and excluding individual channels is not advisable to\nretain the spatial information needed for accurate prediction of epileptic\nseizures. 2) The traditional PCA is not a reliable method for iEEG data\nreduction in seizure prediction. 3) Hand-crafted iEEG features may not be\nsuitable for reliable seizure prediction performance as the iEEG data varies\nbetween patients and over time for the same patient. 4) Seizure prediction\nresults show that our algorithm outperforms existing methods by achieving an\naverage sensitivity of 87.85% and AUC score of 0.84. Conclusion: Understanding\nhow the human brain behaves before seizure attacks and far from them\nfacilitates better designs of epileptic seizure predictors. Significance:\nAccurate seizure prediction algorithms can warn patients about the next seizure\nattack so they could avoid dangerous activities. Medications could then be\nadministered to abort the impending seizure and minimize the risk of injury.\n",
    "topics": "{'EEG': 0.99986744, 'Time Series': 0.8999938}",
    "score": 0.801961934
  },
  {
    "id": "1807.07928",
    "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on\n  Mobile Devices",
    "abstract": "  A recent trend in DNN development is to extend the reach of deep learning\napplications to platforms that are more resource and energy constrained, e.g.,\nmobile devices. These endeavors aim to reduce the DNN model size and improve\nthe hardware processing efficiency, and have resulted in DNNs that are much\nmore compact in their structures and/or have high data sparsity. These compact\nor sparse models are different from the traditional large ones in that there is\nmuch more variation in their layer shapes and sizes, and often require\nspecialized hardware to exploit sparsity for performance improvement. Thus,\nmany DNN accelerators designed for large DNNs do not perform well on these\nmodels. In this work, we present Eyeriss v2, a DNN accelerator architecture\ndesigned for running compact and sparse DNNs. To deal with the widely varying\nlayer shapes and sizes, it introduces a highly flexible on-chip network, called\nhierarchical mesh, that can adapt to the different amounts of data reuse and\nbandwidth requirements of different data types, which improves the utilization\nof the computation resources. Furthermore, Eyeriss v2 can process sparse data\ndirectly in the compressed domain for both weights and activations, and\ntherefore is able to improve both processing speed and energy efficiency with\nsparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65nm CMOS\nprocess achieves a throughput of 1470.6 inferences/sec and 2560.3 inferences/J\nat a batch size of 1, which is 12.6x faster and 2.5x more energy efficient than\nthe original Eyeriss running MobileNet. We also present an analysis methodology\ncalled Eyexam that provides a systematic way of understanding the performance\nlimits for DNN processors as a function of specific characteristics of the DNN\nmodel and accelerator design; it applies these characteristics as sequential\nsteps to increasingly tighten the bound on the performance limits.\n",
    "topics": "{}",
    "score": 0.8019516253
  },
  {
    "id": "1811.09655",
    "title": "Unsupervised brain lesion segmentation from MRI using a convolutional\n  autoencoder",
    "abstract": "  Lesions that appear hyperintense in both Fluid Attenuated Inversion Recovery\n(FLAIR) and T2-weighted magnetic resonance images (MRIs) of the human brain are\ncommon in the brains of the elderly population and may be caused by ischemia or\ndemyelination. Lesions are biomarkers for various neurodegenerative diseases,\nmaking accurate quantification of them important for both disease diagnosis and\nprogression. Automatic lesion detection using supervised learning requires\nmanually annotated images, which can often be impractical to acquire.\nUnsupervised lesion detection, on the other hand, does not require any manual\ndelineation; however, these methods can be challenging to construct due to the\nvariability in lesion load, placement of lesions, and voxel intensities. Here\nwe present a novel approach to address this problem using a convolutional\nautoencoder, which learns to segment brain lesions as well as the white matter,\ngray matter, and cerebrospinal fluid by reconstructing FLAIR images as conical\ncombinations of softmax layer outputs generated from the corresponding T1, T2,\nand FLAIR images. Some of the advantages of this model are that it accurately\nlearns to segment lesions regardless of lesion load, and it can be used to\nquickly and robustly segment new images that were not in the training set.\nComparisons with state-of-the-art segmentation methods evaluated on ground\ntruth manual labels indicate that the proposed method works well for generating\naccurate lesion segmentations without the need for manual annotations.\n",
    "topics": "{'Lesion Segmentation': 0.99940836}",
    "score": 0.8018909469
  },
  {
    "id": "2003.10144",
    "title": "CF2-Net: Coarse-to-Fine Fusion Convolutional Network for Breast\n  Ultrasound Image Segmentation",
    "abstract": "  Breast ultrasound (BUS) image segmentation plays a crucial role in a\ncomputer-aided diagnosis system, which is regarded as a useful tool to help\nincrease the accuracy of breast cancer diagnosis. Recently, many deep learning\nmethods have been developed for segmentation of BUS image and show some\nadvantages compared with conventional region-, model-, and traditional\nlearning-based methods. However, previous deep learning methods typically use\nskip-connection to concatenate the encoder and decoder, which might not make\nfull fusion of coarse-to-fine features from encoder and decoder. Since the\nstructure and edge of lesion in BUS image are common blurred, these would make\nit difficult to learn the discriminant information of structure and edge, and\nreduce the performance. To this end, we propose and evaluate a coarse-to-fine\nfusion convolutional network (CF2-Net) based on a novel feature integration\nstrategy (forming an 'E'-like type) for BUS image segmentation. To enhance\ncontour and provide structural information, we concatenate a super-pixel image\nand the original image as the input of CF2-Net. Meanwhile, to highlight the\ndifferences in the lesion regions with variable sizes and relieve the imbalance\nissue, we further design a weighted-balanced loss function to train the CF2-Net\neffectively. The proposed CF2-Net was evaluated on an open dataset by using\nfour-fold cross validation. The results of the experiment demonstrate that the\nCF2-Net obtains state-of-the-art performance when compared with other deep\nlearning-based methods\n",
    "topics": "{'Semantic Segmentation': 0.9999622}",
    "score": 0.8018877779
  },
  {
    "id": "1801.07756",
    "title": "Deep Learning for Electromyographic Hand Gesture Signal Classification\n  Using Transfer Learning",
    "abstract": "  In recent years, deep learning algorithms have become increasingly more\nprominent for their unparalleled ability to automatically learn discriminant\nfeatures from large amounts of data. However, within the field of\nelectromyography-based gesture recognition, deep learning algorithms are seldom\nemployed as they require an unreasonable amount of effort from a single person,\nto generate tens of thousands of examples.\n  This work's hypothesis is that general, informative features can be learned\nfrom the large amounts of data generated by aggregating the signals of multiple\nusers, thus reducing the recording burden while enhancing gesture recognition.\nConsequently, this paper proposes applying transfer learning on aggregated data\nfrom multiple users, while leveraging the capacity of deep learning algorithms\nto learn discriminant features from large datasets. Two datasets comprised of\n19 and 17 able-bodied participants respectively (the first one is employed for\npre-training) were recorded for this work, using the Myo Armband. A third Myo\nArmband dataset was taken from the NinaPro database and is comprised of 10\nable-bodied participants. Three different deep learning networks employing\nthree different modalities as input (raw EMG, Spectrograms and Continuous\nWavelet Transform (CWT)) are tested on the second and third dataset. The\nproposed transfer learning scheme is shown to systematically and significantly\nenhance the performance for all three networks on the two datasets, achieving\nan offline accuracy of 98.31% for 7 gestures over 17 participants for the\nCWT-based ConvNet and 68.98% for 18 gestures over 10 participants for the raw\nEMG-based ConvNet. Finally, a use-case study employing eight able-bodied\nparticipants suggests that real-time feedback allows users to adapt their\nmuscle activation strategy which reduces the degradation in accuracy normally\nexperienced over time.\n",
    "topics": "{'Gesture Recognition': 1.0, 'Transfer Learning': 0.99997354}",
    "score": 0.8018853784
  },
  {
    "id": "2001.04066",
    "title": "Boosting Occluded Image Classification via Subspace Decomposition Based\n  Estimation of Deep Features",
    "abstract": "  Classification of partially occluded images is a highly challenging computer\nvision problem even for the cutting edge deep learning technologies. To achieve\na robust image classification for occluded images, this paper proposes a novel\nscheme using subspace decomposition based estimation (SDBE). The proposed\nSDBE-based classification scheme first employs a base convolutional neural\nnetwork to extract the deep feature vector (DFV) and then utilizes the SDBE to\ncompute the DFV of the original occlusion-free image for classification. The\nSDBE is performed by projecting the DFV of the occluded image onto the linear\nspan of a class dictionary (CD) along the linear span of an occlusion error\ndictionary (OED). The CD and OED are constructed respectively by concatenating\nthe DFVs of a training set and the occlusion error vectors of an extra set of\nimage pairs. Two implementations of the SDBE are studied in this paper: the\n$l_1$-norm and the squared $l_2$-norm regularized least-squares estimates. By\nemploying the ResNet-152, pre-trained on the ILSVRC2012 training set, as the\nbase network, the proposed SBDE-based classification scheme is extensively\nevaluated on the Caltech-101 and ILSVRC2012 datasets. Extensive experimental\nresults demonstrate that the proposed SDBE-based scheme dramatically boosts the\nclassification accuracy for occluded images, and achieves around $22.25\\%$\nincrease in classification accuracy under $20\\%$ occlusion on the ILSVRC2012\ndataset.\n",
    "topics": "{'Image Classification': 0.9999988}",
    "score": 0.8018517529
  },
  {
    "id": "1705.00938",
    "title": "Error Corrective Boosting for Learning Fully Convolutional Networks with\n  Limited Data",
    "abstract": "  Training deep fully convolutional neural networks (F-CNNs) for semantic image\nsegmentation requires access to abundant labeled data. While large datasets of\nunlabeled image data are available in medical applications, access to manually\nlabeled data is very limited. We propose to automatically create auxiliary\nlabels on initially unlabeled data with existing tools and to use them for\npre-training. For the subsequent fine-tuning of the network with manually\nlabeled data, we introduce error corrective boosting (ECB), which emphasizes\nparameter updates on classes with lower accuracy. Furthermore, we introduce\nSkipDeconv-Net (SD-Net), a new F-CNN architecture for brain segmentation that\ncombines skip connections with the unpooling strategy for upsampling. The\nSD-Net addresses challenges of severe class imbalance and errors along\nboundaries. With application to whole-brain MRI T1 scan segmentation, we\ngenerate auxiliary labels on a large dataset with FreeSurfer and fine-tune on\ntwo datasets with manual annotations. Our results show that the inclusion of\nauxiliary labels and ECB yields significant improvements. SD-Net segments a 3D\nscan in 7 secs in comparison to 30 hours for the closest multi-atlas\nsegmentation method, while reaching similar performance. It also outperforms\nthe latest state-of-the-art F-CNN models.\n",
    "topics": "{'Semantic Segmentation': 0.90221566}",
    "score": 0.8018310902
  },
  {
    "id": "2001.11773",
    "title": "Mixed-precision deep learning based on computational memory",
    "abstract": "  Deep neural networks (DNNs) have revolutionized the field of artificial\nintelligence and have achieved unprecedented success in cognitive tasks such as\nimage and speech recognition. Training of large DNNs, however, is\ncomputationally intensive and this has motivated the search for novel computing\narchitectures targeting this application. A computational memory unit with\nnanoscale resistive memory devices organized in crossbar arrays could store the\nsynaptic weights in their conductance states and perform the expensive weighted\nsummations in place in a non-von Neumann manner. However, updating the\nconductance states in a reliable manner during the weight update process is a\nfundamental challenge that limits the training accuracy of such an\nimplementation. Here, we propose a mixed-precision architecture that combines a\ncomputational memory unit performing the weighted summations and imprecise\nconductance updates with a digital processing unit that accumulates the weight\nupdates in high precision. A combined hardware/software training experiment of\na multilayer perceptron based on the proposed architecture using a phase-change\nmemory (PCM) array achieves 97.73% test accuracy on the task of classifying\nhandwritten digits (based on the MNIST dataset), within 0.6% of the software\nbaseline. The architecture is further evaluated using accurate behavioral\nmodels of PCM on a wide class of networks, namely convolutional neural\nnetworks, long-short-term-memory networks, and generative-adversarial networks.\nAccuracies comparable to those of floating-point implementations are achieved\nwithout being constrained by the non-idealities associated with the PCM\ndevices. A system-level study demonstrates 173x improvement in energy\nefficiency of the architecture when used for training a multilayer perceptron\ncompared with a dedicated fully digital 32-bit implementation.\n",
    "topics": "{'Speech Recognition': 0.544028}",
    "score": 0.8018048659
  },
  {
    "id": "1908.02422",
    "title": "Adversarial Seeded Sequence Growing for Weakly-Supervised Temporal\n  Action Localization",
    "abstract": "  Temporal action localization is an important yet challenging research topic\ndue to its various applications. Since the frame-level or segment-level\nannotations of untrimmed videos require amounts of labor expenditure, studies\non the weakly-supervised action detection have been springing up. However, most\nof existing frameworks rely on Class Activation Sequence (CAS) to localize\nactions by minimizing the video-level classification loss, which exploits the\nmost discriminative parts of actions but ignores the minor regions. In this\npaper, we propose a novel weakly-supervised framework by adversarial learning\nof two modules for eliminating such demerits. Specifically, the first module is\ndesigned as a well-designed Seeded Sequence Growing (SSG) Network for\nprogressively extending seed regions (namely the highly reliable regions\ninitialized by a CAS-based framework) to their expected boundaries. The second\nmodule is a specific classifier for mining trivial or incomplete action\nregions, which is trained on the shared features after erasing the seeded\nregions activated by SSG. In this way, a whole network composed of these two\nmodules can be trained in an adversarial manner. The goal of the adversary is\nto mine features that are difficult for the action classifier. That is, erasion\nfrom SSG will force the classifier to discover minor or even new action regions\non the input feature sequence, and the classifier will drive the seeds to grow,\nalternately. At last, we could obtain the action locations and categories from\nthe well-trained SSG and the classifier. Extensive experiments on two public\nbenchmarks THUMOS'14 and ActivityNet1.3 demonstrate the impressive performance\nof our proposed method compared with the state-of-the-arts.\n",
    "topics": "{'Action Localization': 1.0, 'Temporal Action Localization': 0.9999683, 'Action Detection': 0.99553156}",
    "score": 0.8017758377
  },
  {
    "id": "2008.10796",
    "title": "Variational Image Restoration Network",
    "abstract": "  Deep neural networks (DNNs) have achieved significant success in image\nrestoration tasks by directly learning a powerful non-linear mapping from\ncorrupted images to their latent clean ones. However, there still exist two\nmajor limitations for these deep learning (DL)-based methods. Firstly, the\nnoises contained in real corrupted images are very complex, usually neglected\nand largely under-estimated in most current methods. Secondly, existing DL\nmethods are mostly trained on one pre-assumed degradation process for all of\nthe training image pairs, such as the widely used bicubic downsampling\nassumption in the image super-resolution task, inevitably leading to poor\ngeneralization performance when the true degradation does not match with such\nassumed one. To address these issues, we propose a unified generative model for\nthe image restoration, which elaborately configures the degradation process\nfrom the latent clean image to the observed corrupted one. Specifically,\ndifferent from most of current methods, the pixel-wisely non-i.i.d. Gaussian\ndistribution, being with more flexibility, is adopted in our method to fit the\ncomplex real noises. Furthermore, the method is built on the general image\ndegradation process, making it capable of adapting diverse degradations under\none single model. Besides, we design a variational inference algorithm to learn\nall parameters involved in the proposed model with explicit form of objective\nloss. Specifically, beyond traditional variational methodology, two DNNs are\nemployed to parameterize the posteriori distributions, one to infer the\ndistribution of the latent clean image, and another to infer the distribution\nof the image noise. Extensive experiments demonstrate the superiority of the\nproposed method on three classical image restoration tasks, including image\ndenoising, image super-resolution and JPEG image deblocking.\n",
    "topics": "{'Image Super-Resolution': 1.0, 'Image Restoration': 1.0, 'Super-Resolution': 0.9993156, 'Super Resolution': 0.9992046, 'Image Denoising': 0.9991374, 'Denoising': 0.9860095, 'Variational Inference': 0.5739576}",
    "score": 0.8017267046
  },
  {
    "id": "1910.03177",
    "title": "Read, Highlight and Summarize: A Hierarchical Neural Semantic\n  Encoder-based Approach",
    "abstract": "  Traditional sequence-to-sequence (seq2seq) models and other variations of the\nattention-mechanism such as hierarchical attention have been applied to the\ntext summarization problem. Though there is a hierarchy in the way humans use\nlanguage by forming paragraphs from sentences and sentences from words,\nhierarchical models have usually not worked that much better than their\ntraditional seq2seq counterparts. This effect is mainly because either the\nhierarchical attention mechanisms are too sparse using hard attention or noisy\nusing soft attention. In this paper, we propose a method based on extracting\nthe highlights of a document; a key concept that is conveyed in a few\nsentences. In a typical text summarization dataset consisting of documents that\nare 800 tokens in length (average), capturing long-term dependencies is very\nimportant, e.g., the last sentence can be grouped with the first sentence of a\ndocument to form a summary. LSTMs (Long Short-Term Memory) proved useful for\nmachine translation. However, they often fail to capture long-term dependencies\nwhile modeling long sequences. To address these issues, we have adapted Neural\nSemantic Encoders (NSE) to text summarization, a class of memory-augmented\nneural networks by improving its functionalities and proposed a novel\nhierarchical NSE that outperforms similar previous models significantly. The\nquality of summarization was improved by augmenting linguistic factors, namely\nlemma, and Part-of-Speech (PoS) tags, to each word in the dataset for improved\nvocabulary coverage and generalization. The hierarchical NSE model on factored\ndataset outperformed the state-of-the-art by nearly 4 ROUGE points. We further\ndesigned and used the first GPU-based self-critical Reinforcement Learning\nmodel.\n",
    "topics": "{'Text Summarization': 1.0, 'Abstractive Text Summarization': 0.9990415, 'Machine Translation': 0.65022165}",
    "score": 0.8016394852
  },
  {
    "id": "2007.08461",
    "title": "How to trust unlabeled data? Instance Credibility Inference for Few-Shot\n  Learning",
    "abstract": "  Deep learning based models have excelled in many computer vision task and\nappear to surpass humans performance. However, these models require an\navalanche of expensive human labeled training data and many iterations to train\ntheir large number of parameters. This severely limits their scalability to the\nreal-world long-tail distributed categories. Learning from such extremely\nlimited labeled examples is known as Few-shot learning. Different to prior arts\nthat leverage meta-learning or data augmentation strategies to alleviate this\nextremely data-scarce problem, this paper presents a statistical approach,\ndubbed Instance Credibility Inference to exploit the support of unlabeled\ninstances for few-shot visual recognition. Typically, we repurpose the\nself-taught learning paradigm. To do so, we construct a (Generalized) Linear\nModel (LM/GLM) with incidental parameters to model the mapping from\n(un-)labeled features to their (pseudo-)labels, in which the sparsity of the\nincidental parameters indicates the credibility of corresponding pseudo-labeled\ninstance. We rank the credibility of pseudo-labels of unlabeled instances along\nthe regularization path of their corresponding incidental parameters, and the\nmost trustworthy pseudo-labeled examples are preserved as the augmented labeled\ninstances.This process is repeated until all the unlabeled samples are\niteratively included in the expanded training set. Theoretically, under mild\nconditions of restricted eigenvalue, irrepresentability, and large error, our\napproach is guaranteed to collect all the correctly-predicted pseudo-labeled\ninstances from the noisy pseudo-labeled set. Extensive experiments under two\nfew-shot settings show that our approach can establish new state of the art on\nfour widely used few-shot visual recognition benchmark datasets including\nminiImageNet, tieredImageNet, CIFAR-FS, and CUB.\n",
    "topics": "{'Few-Shot Learning': 1.0, 'Data Augmentation': 0.9969488, 'Meta-Learning': 0.98661554, 'Few-Shot Image Classification': 0.32349822}",
    "score": 0.8015877596
  },
  {
    "id": "2002.12455",
    "title": "Is the Meta-Learning Idea Able to Improve the Generalization of Deep\n  Neural Networks on the Standard Supervised Learning?",
    "abstract": "  Substantial efforts have been made on improving the generalization abilities\nof deep neural networks (DNNs) in order to obtain better performances without\nintroducing more parameters. On the other hand, meta-learning approaches\nexhibit powerful generalization on new tasks in few-shot learning. Intuitively,\nfew-shot learning is more challenging than the standard supervised learning as\neach target class only has a very few or no training samples. The natural\nquestion that arises is whether the meta-learning idea can be used for\nimproving the generalization of DNNs on the standard supervised learning. In\nthis paper, we propose a novel meta-learning based training procedure (MLTP)\nfor DNNs and demonstrate that the meta-learning idea can indeed improve the\ngeneralization abilities of DNNs. MLTP simulates the meta-training process by\nconsidering a batch of training samples as a task. The key idea is that the\ngradient descent step for improving the current task performance should also\nimprove a new task performance, which is ignored by the current standard\nprocedure for training neural networks. MLTP also benefits from all the\nexisting training techniques such as dropout, weight decay, and batch\nnormalization. We evaluate MLTP by training a variety of small and large neural\nnetworks on three benchmark datasets, i.e., CIFAR-10, CIFAR-100, and Tiny\nImageNet. The experimental results show a consistently improved generalization\nperformance on all the DNNs with different sizes, which verifies the promise of\nMLTP and demonstrates that the meta-learning idea is indeed able to improve the\ngeneralization of DNNs on the standard supervised learning.\n",
    "topics": "{'Meta-Learning': 1.0, 'Few-Shot Learning': 0.99997616}",
    "score": 0.8015453567
  },
  {
    "id": "1909.05587",
    "title": "Human-Machine Collaborative Design for Accelerated Design of Compact\n  Deep Neural Networks for Autonomous Driving",
    "abstract": "  An effective deep learning development process is critical for widespread\nindustrial adoption, particularly in the automotive sector. A typical\nindustrial deep learning development cycle involves customizing and\nre-designing an off-the-shelf network architecture to meet the operational\nrequirements of the target application, leading to considerable trial and error\nwork by a machine learning practitioner. This approach greatly impedes\ndevelopment with a long turnaround time and the unsatisfactory quality of the\ncreated models. As a result, a development platform that can aid engineers in\ngreatly accelerating the design and production of compact, optimized deep\nneural networks is highly desirable. In this joint industrial case study, we\nstudy the efficacy of the GenSynth AI-assisted AI design platform for\naccelerating the design of custom, optimized deep neural networks for\nautonomous driving through human-machine collaborative design. We perform a\nquantitative examination by evaluating 10 different compact deep neural\nnetworks produced by GenSynth for the purpose of object detection via a\nNASNet-based user network prototype design, targeted at a low-cost GPU-based\naccelerated embedded system. Furthermore, we quantitatively assess the talent\nhours and GPU processing hours used by the GenSynth process and three other\napproaches based on the typical industrial development process. In addition, we\nquantify the annual cloud cost savings for comprehensive testing using networks\nproduced by GenSynth. Finally, we assess the usability and merits of the\nGenSynth process through user feedback. The findings of this case study showed\nthat GenSynth is easy to use and can be effective at accelerating the design\nand production of compact, customized deep neural network.\n",
    "topics": "{'Autonomous Driving': 0.999982, 'Object Detection': 0.9753328}",
    "score": 0.8014289922
  },
  {
    "id": "1801.05889",
    "title": "Perceived Audiovisual Quality Modelling based on Decison Trees, Genetic\n  Programming and Neural Networks",
    "abstract": "  Our objective is to build machine learning based models that predict\naudiovisual quality directly from a set of correlated parameters that are\nextracted from a target quality dataset. We have used the bitstream version of\nthe INRS audiovisual quality dataset that reflects contemporary real-time\nconfigurations for video frame rate, video quantization, noise reduction\nparameters and network packet loss rate. We have utilized this dataset to build\nbitstream perceived quality estimation models based on the Random Forests,\nBagging, Deep Learning and Genetic Programming methods.\n  We have taken an empirical approach and have generated models varying from\nvery simple to the most complex depending on the number of features used from\nthe quality dataset. Random Forests and Bagging models have overall generated\nthe most accurate results in terms of RMSE and Pearson correlation coefficient\nvalues. Deep Learning and Genetic Programming based bitstream models have also\nachieved good results but that high performance was observed only with a\nlimited range of features. We have also obtained the epsilon-insensitive RMSE\nvalues for each model and have computed the significance of the difference\nbetween the correlation coefficients.\n  Overall we conclude that computing the bitstream information is worth the\neffort it takes to generate and helps to build more accurate models for\nreal-time communications. However, it is useful only for the deployment of the\nright algorithms with the carefully selected subset of the features. The\ndataset and tools that have been developed during this research are publicly\navailable for research and development purposes.\n",
    "topics": "{'Quantization': 0.8375059}",
    "score": 0.8014064415
  },
  {
    "id": "1806.01963",
    "title": "MILD-Net: Minimal Information Loss Dilated Network for Gland Instance\n  Segmentation in Colon Histology Images",
    "abstract": "  The analysis of glandular morphology within colon histopathology images is an\nimportant step in determining the grade of colon cancer. Despite the importance\nof this task, manual segmentation is laborious, time-consuming and can suffer\nfrom subjectivity among pathologists. The rise of computational pathology has\nled to the development of automated methods for gland segmentation that aim to\novercome the challenges of manual segmentation. However, this task is\nnon-trivial due to the large variability in glandular appearance and the\ndifficulty in differentiating between certain glandular and non-glandular\nhistological structures. Furthermore, a measure of uncertainty is essential for\ndiagnostic decision making. To address these challenges, we propose a fully\nconvolutional neural network that counters the loss of information caused by\nmax-pooling by re-introducing the original image at multiple points within the\nnetwork. We also use atrous spatial pyramid pooling with varying dilation rates\nfor preserving the resolution and multi-level aggregation. To incorporate\nuncertainty, we introduce random transformations during test time for an\nenhanced segmentation result that simultaneously generates an uncertainty map,\nhighlighting areas of ambiguity. We show that this map can be used to define a\nmetric for disregarding predictions with high uncertainty. The proposed network\nachieves state-of-the-art performance on the GlaS challenge dataset and on a\nsecond independent colorectal adenocarcinoma dataset. In addition, we perform\ngland instance segmentation on whole-slide images from two further datasets to\nhighlight the generalisability of our method. As an extension, we introduce\nMILD-Net+ for simultaneous gland and lumen segmentation, to increase the\ndiagnostic power of the network.\n",
    "topics": "{'Instance Segmentation': 1.0, 'whole slide images': 0.99990714, 'Decision Making': 0.99520797, 'Semantic Segmentation': 0.9836308}",
    "score": 0.8013390781
  },
  {
    "id": "1910.00565",
    "title": "Domain Expansion in DNN-based Acoustic Models for Robust Speech\n  Recognition",
    "abstract": "  Training acoustic models with sequentially incoming data -- while both\nleveraging new data and avoiding the forgetting effect-- is an essential\nobstacle to achieving human intelligence level in speech recognition. An\nobvious approach to leverage data from a new domain (e.g., new accented speech)\nis to first generate a comprehensive dataset of all domains, by combining all\navailable data, and then use this dataset to retrain the acoustic models.\nHowever, as the amount of training data grows, storing and retraining on such a\nlarge-scale dataset becomes practically impossible. To deal with this problem,\nin this study, we study several domain expansion techniques which exploit only\nthe data of the new domain to build a stronger model for all domains. These\ntechniques are aimed at learning the new domain with a minimal forgetting\neffect (i.e., they maintain original model performance). These techniques\nmodify the adaptation procedure by imposing new constraints including (1)\nweight constraint adaptation (WCA): keeping the model parameters close to the\noriginal model parameters; (2) elastic weight consolidation (EWC): slowing down\ntraining for parameters that are important for previously established domains;\n(3) soft KL-divergence (SKLD): restricting the KL-divergence between the\noriginal and the adapted model output distributions; and (4) hybrid SKLD-EWC:\nincorporating both SKLD and EWC constraints. We evaluate these techniques in an\naccent adaptation task in which we adapt a deep neural network (DNN) acoustic\nmodel trained with native English to three different English accents:\nAustralian, Hispanic, and Indian. The experimental results show that SKLD\nsignificantly outperforms EWC, and EWC works better than WCA. The hybrid\nSKLD-EWC technique results in the best overall performance.\n",
    "topics": "{'Speech Recognition': 0.99814963}",
    "score": 0.8012722725
  },
  {
    "id": "2008.08278",
    "title": "DONet: Dual Objective Networks for Skin Lesion Segmentation",
    "abstract": "  Skin lesion segmentation is a crucial step in the computer-aided diagnosis of\ndermoscopic images. In the last few years, deep learning based semantic\nsegmentation methods have significantly advanced the skin lesion segmentation\nresults. However, the current performance is still unsatisfactory due to some\nchallenging factors such as large variety of lesion scale and ambiguous\ndifference between lesion region and background. In this paper, we propose a\nsimple yet effective framework, named Dual Objective Networks (DONet), to\nimprove the skin lesion segmentation. Our DONet adopts two symmetric decoders\nto produce different predictions for approaching different objectives.\nConcretely, the two objectives are actually defined by different loss\nfunctions. In this way, the two decoders are encouraged to produce\ndifferentiated probability maps to match different optimization targets,\nresulting in complementary predictions accordingly. The complementary\ninformation learned by these two objectives are further aggregated together to\nmake the final prediction, by which the uncertainty existing in segmentation\nmaps can be significantly alleviated. Besides, to address the challenge of\nlarge variety of lesion scales and shapes in dermoscopic images, we\nadditionally propose a recurrent context encoding module (RCEM) to model the\ncomplex correlation among skin lesions, where the features with different scale\ncontexts are efficiently integrated to form a more robust representation.\nExtensive experiments on two popular benchmarks well demonstrate the\neffectiveness of the proposed DONet. In particular, our DONet achieves 0.881\nand 0.931 dice score on ISIC 2018 and $\\text{PH}^2$, respectively. Code will be\nmade public available.\n",
    "topics": "{'Lesion Segmentation': 1.0, 'Semantic Segmentation': 0.933092}",
    "score": 0.8012367011
  },
  {
    "id": "1807.11228",
    "title": "Predicting Conversion of Mild Cognitive Impairments to Alzheimer's\n  Disease and Exploring Impact of Neuroimaging",
    "abstract": "  Nowadays, a lot of scientific efforts are concentrated on the diagnosis of\nAlzheimer's Disease (AD) applying deep learning methods to neuroimaging data.\nEven for 2017, there were published more than a hundred papers dedicated to AD\ndiagnosis, whereas only a few works considered a problem of mild cognitive\nimpairments (MCI) conversion to the AD. However, the conversion prediction is\nan important problem since approximately 15% of patients with MCI converges to\nthe AD every year. In the current work, we are focusing on the conversion\nprediction using brain Magnetic Resonance Imaging and clinical data, such as\ndemographics, cognitive assessments, genetic, and biochemical markers. First of\nall, we applied state-of-the-art deep learning algorithms on the neuroimaging\ndata and compared these results with two machine learning algorithms that we\nfit using the clinical data. As a result, the models trained on the clinical\ndata outperform the deep learning algorithms applied to the MR images. To\nexplore the impact of neuroimaging further, we trained a deep feed-forward\nembedding using similarity learning with Histogram loss on all available MRIs\nand obtained 64-dimensional vector representation of neuroimaging data. The use\nof learned representation from the deep embedding allowed to increase the\nquality of prediction based on the neuroimaging. Finally, the current results\non this dataset show that the neuroimaging does affect conversion prediction,\nhowever, cannot noticeably increase the quality of the prediction. The best\nresults of predicting MCI-to-AD conversion are provided by XGBoost algorithm\ntrained on the clinical and embedding data. The resulting accuracy is 0.76 +-\n0.01 and the area under the ROC curve - 0.86 +- 0.01.\n",
    "topics": "{'Voice Conversion': 0.9806804}",
    "score": 0.8012134407
  },
  {
    "id": "1301.3530",
    "title": "The Neural Representation Benchmark and its Evaluation on Brain and\n  Machine",
    "abstract": "  A key requirement for the development of effective learning representations\nis their evaluation and comparison to representations we know to be effective.\nIn natural sensory domains, the community has viewed the brain as a source of\ninspiration and as an implicit benchmark for success. However, it has not been\npossible to directly test representational learning algorithms directly against\nthe representations contained in neural systems. Here, we propose a new\nbenchmark for visual representations on which we have directly tested the\nneural representation in multiple visual cortical areas in macaque (utilizing\ndata from [Majaj et al., 2012]), and on which any computer vision algorithm\nthat produces a feature space can be tested. The benchmark measures the\neffectiveness of the neural or machine representation by computing the\nclassification loss on the ordered eigendecomposition of a kernel matrix\n[Montavon et al., 2011]. In our analysis we find that the neural representation\nin visual area IT is superior to visual area V4. In our analysis of\nrepresentational learning algorithms, we find that three-layer models approach\nthe representational performance of V4 and the algorithm in [Le et al., 2012]\nsurpasses the performance of V4. Impressively, we find that a recent supervised\nalgorithm [Krizhevsky et al., 2012] achieves performance comparable to that of\nIT for an intermediate level of image variation difficulty, and surpasses IT at\na higher difficulty level. We believe this result represents a major milestone:\nit is the first learning algorithm we have found that exceeds our current\nestimate of IT representation performance. We hope that this benchmark will\nassist the community in matching the representational performance of visual\ncortex and will serve as an initial rallying point for further correspondence\nbetween representations derived in brains and machines.\n",
    "topics": "{}",
    "score": 0.8011931971
  },
  {
    "id": "2007.13952",
    "title": "EasierPath: An Open-source Tool for Human-in-the-loop Deep Learning of\n  Renal Pathology",
    "abstract": "  Considerable morphological phenotyping studies in nephrology have emerged in\nthe past few years, aiming to discover hidden regularities between clinical and\nimaging phenotypes. Such studies have been largely enabled by deep learning\nbased image analysis to extract sparsely located targeting objects (e.g.,\nglomeruli) on high-resolution whole slide images (WSI). However, such methods\nneed to be trained using labor-intensive high-quality annotations, ideally\nlabeled by pathologists. Inspired by the recent \"human-in-the-loop\" strategy,\nwe developed EasierPath, an open-source tool to integrate human physicians and\ndeep learning algorithms for efficient large-scale pathological image\nquantification as a loop. Using EasierPath, physicians are able to (1) optimize\nthe recall and precision of deep learning object detection outcomes adaptively,\n(2) seamlessly support deep learning outcomes refining using either our\nEasierPath or prevalent ImageScope software without changing physician's user\nhabit, and (3) manage and phenotype each object with user-defined classes. As a\nuser case of EasierPath, we present the procedure of curating large-scale\nglomeruli in an efficient human-in-the-loop fashion (with two loops). From the\nexperiments, the EasierPath saved 57 % of the annotation efforts to curate\n8,833 glomeruli during the second loop. Meanwhile, the average precision of\nglomerular detection was leveraged from 0.504 to 0.620. The EasierPath software\nhas been released as open-source to enable the large-scale glomerular\nprototyping. The code can be found in https://github.com/yuankaihuo/EasierPath\n",
    "topics": "{'whole slide images': 0.99998415, 'Object Detection': 0.99335986}",
    "score": 0.8011096156
  },
  {
    "id": "1912.05333",
    "title": "An Efficient Approach for Using Expectation Maximization Algorithm in\n  Capsule Networks",
    "abstract": "  Capsule Networks (CapsNets) are brand-new architectures that have shown\nground-breaking results in certain areas of Computer Vision (CV). In 2017,\nHinton and his team introduced CapsNets with routing-by-agreement in \"Sabour et\nal\" and in a more recent paper \"Matrix Capsules with EM Routing\" they proposed\na more complete architecture with Expectation-Maximization (EM) algorithm.\nUnlike the traditional convolutional neural networks (CNNs), this architecture\nis able to preserve the pose of the objects in the picture. Due to this\ncharacteristic, it has been able to beat the previous state-of-theart results\non the smallNORB dataset, which includes samples with various view points.\nAlso, this architecture is more robust to white box adversarial attacks.\nHowever, CapsNets have two major drawbacks. They can't perform as well as CNNs\non complex datasets and, they need a huge amount of time for training. We try\nto mitigate these shortcomings by finding optimum settings of EM routing\niterations for training CapsNets. Unlike the past studies, we use un-equal\nnumbers of EM routing iterations for different stages of the CapsNet. For our\nresearch, we use three datasets: Yale face dataset, Belgium Traffic Sign\ndataset, and Fashion-MNIST dataset.\n",
    "topics": "{}",
    "score": 0.8010954292
  },
  {
    "id": "2005.10516",
    "title": "An analysis on the use of autoencoders for representation learning:\n  fundamentals, learning task case studies, explainability and challenges",
    "abstract": "  In many machine learning tasks, learning a good representation of the data\ncan be the key to building a well-performant solution. This is because most\nlearning algorithms operate with the features in order to find models for the\ndata. For instance, classification performance can improve if the data is\nmapped to a space where classes are easily separated, and regression can be\nfacilitated by finding a manifold of data in the feature space. As a general\nrule, features are transformed by means of statistical methods such as\nprincipal component analysis, or manifold learning techniques such as Isomap or\nlocally linear embedding. From a plethora of representation learning methods,\none of the most versatile tools is the autoencoder. In this paper we aim to\ndemonstrate how to influence its learned representations to achieve the desired\nlearning behavior. To this end, we present a series of learning tasks: data\nembedding for visualization, image denoising, semantic hashing, detection of\nabnormal behaviors and instance generation. We model them from the\nrepresentation learning perspective, following the state of the art\nmethodologies in each field. A solution is proposed for each task employing\nautoencoders as the only learning method. The theoretical developments are put\ninto practice using a selection of datasets for the different problems and\nimplementing each solution, followed by a discussion of the results in each\ncase study and a brief explanation of other six learning applications. We also\nexplore the current challenges and approaches to explainability in the context\nof autoencoders. All of this helps conclude that, thanks to alterations in\ntheir structure as well as their objective function, autoencoders may be the\ncore of a possible solution to many problems which can be modeled as a\ntransformation of the feature space.\n",
    "topics": "{'Representation Learning': 0.9999902, 'Image Denoising': 0.99962604, 'Denoising': 0.9775442}",
    "score": 0.8010676862
  },
  {
    "id": "1811.09878",
    "title": "Hydra: A Peer to Peer Distributed Training & Data Collection Framework",
    "abstract": "  The world needs diverse and unbiased data to train deep learning models.\nCurrently data comes from a variety of sources that are unmoderated to a large\nextent. The outcomes of training neural networks with unverified data yields\nbiased models with various strains of homophobia, sexism and racism. Another\ntrend observed in the world of deep learning is the rise of distributed\ntraining. Although cloud companies provide high performance compute for\ntraining models in the form of GPU's connected with a low latency network,\nusing these services comes at a high cost. We propose Hydra, a system that\nseeks to solve both of these problems in a novel manner by proposing a\ndecentralized distributed framework which utilizes the substantial amount of\nidle compute of everyday electronic devices like smartphones and desktop\ncomputers for training and data collection purposes. Hydra couples a\nspecialized distributed training framework on a network of these low powered\ndevices with a reward scheme that incentivizes users to provide high quality\ndata to unleash the compute capability on this training framework. Such a\nsystem has the ability to capture data from a wide variety of diverse sources\nwhich has been an issue in the current scenario of deep learning. Hydra brings\nin several new innovations in training on low powered devices including a fault\ntolerant version of the All Reduce algorithm. Furthermore we introduce a\nreinforcement learning policy to decide the size of training jobs on different\nmachines on a heterogeneous cluster of devices with varying network latencies\nfor Synchronous SGD. The novel thing about such a network is the ability of\neach machine to shut down and resume training capabilities at any point of time\nwithout restarting the overall training. To enable such an asynchronous\nbehaviour we propose a communication framework inspired by the Bittorrent\nprotocol and the Kademlia DHT.\n",
    "topics": "{}",
    "score": 0.8010630185
  },
  {
    "id": "1804.04715",
    "title": "Sound Event Detection and Time-Frequency Segmentation from Weakly\n  Labelled Data",
    "abstract": "  Sound event detection (SED) aims to detect when and recognize what sound\nevents happen in an audio clip. Many supervised SED algorithms rely on strongly\nlabelled data which contains the onset and offset annotations of sound events.\nHowever, many audio tagging datasets are weakly labelled, that is, only the\npresence of the sound events is known, without knowing their onset and offset\nannotations. In this paper, we propose a time-frequency (T-F) segmentation\nframework trained on weakly labelled data to tackle the sound event detection\nand separation problem. In training, a segmentation mapping is applied on a T-F\nrepresentation, such as log mel spectrogram of an audio clip to obtain T-F\nsegmentation masks of sound events. The T-F segmentation masks can be used for\nseparating the sound events from the background scenes in the time-frequency\ndomain. Then a classification mapping is applied on the T-F segmentation masks\nto estimate the presence probabilities of the sound events. We model the\nsegmentation mapping using a convolutional neural network and the\nclassification mapping using a global weighted rank pooling (GWRP). In SED,\npredicted onset and offset times can be obtained from the T-F segmentation\nmasks. As a byproduct, separated waveforms of sound events can be obtained from\nthe T-F segmentation masks. We remixed the DCASE 2018 Task 1 acoustic scene\ndata with the DCASE 2018 Task 2 sound events data. When mixing under 0 dB, the\nproposed method achieved F1 scores of 0.534, 0.398 and 0.167 in audio tagging,\nframe-wise SED and event-wise SED, outperforming the fully connected deep\nneural network baseline of 0.331, 0.237 and 0.120, respectively. In T-F\nsegmentation, we achieved an F1 score of 0.218, where previous methods were not\nable to do T-F segmentation.\n",
    "topics": "{}",
    "score": 0.8010252006
  },
  {
    "id": "1910.12514",
    "title": "Multi-sequence Cardiac MR Segmentation with Adversarial Domain\n  Adaptation Network",
    "abstract": "  Automatic and accurate segmentation of the ventricles and myocardium from\nmulti-sequence cardiac MRI (CMR) is crucial for the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). However, due\nto the existence of domain shift among different modalities of datasets, the\nperformance of deep neural networks drops significantly when the training and\ntesting datasets are distinct. In this paper, we propose an unsupervised domain\nalignment method to explicitly alleviate the domain shifts among different\nmodalities of CMR sequences, \\emph{e.g.,} bSSFP, LGE, and T2-weighted. Our\nsegmentation network is attention U-Net with pyramid pooling module, where\nmulti-level feature space and output space adversarial learning are proposed to\ntransfer discriminative domain knowledge across different datasets. Moreover,\nwe further introduce a group-wise feature recalibration module to enforce the\nfine-grained semantic-level feature alignment that matching features from\ndifferent networks but with the same class label. We evaluate our method on the\nmulti-sequence cardiac MR Segmentation Challenge 2019 datasets, which contain\nthree different modalities of MRI sequences. Extensive experimental results\nshow that the proposed methods can obtain significant segmentation improvements\ncompared with the baseline models.\n",
    "topics": "{}",
    "score": 0.8009997062
  },
  {
    "id": "1811.05475",
    "title": "ML-Net: multi-label classification of biomedical texts with deep neural\n  networks",
    "abstract": "  In multi-label text classification, each textual document can be assigned\nwith one or more labels. Due to this nature, the multi-label text\nclassification task is often considered to be more challenging compared to the\nbinary or multi-class text classification problems. As an important task with\nbroad applications in biomedicine such as assigning diagnosis codes, a number\nof different computational methods (e.g. training and combining binary\nclassifiers for each label) have been proposed in recent years. However, many\nsuffered from modest accuracy and efficiency, with only limited success in\npractical use. We propose ML-Net, a novel deep learning framework, for\nmulti-label classification of biomedical texts. As an end-to-end system, ML-Net\ncombines a label prediction network with an automated label count prediction\nmechanism to output an optimal set of labels by leveraging both predicted\nconfidence score of each label and the contextual information in the target\ndocument. We evaluate ML-Net on three independent, publicly-available corpora\nin two kinds of text genres: biomedical literature and clinical notes. For\nevaluation, example-based measures such as precision, recall and f-measure are\nused. ML-Net is compared with several competitive machine learning baseline\nmodels. Our benchmarking results show that ML-Net compares favorably to the\nstate-of-the-art methods in multi-label classification of biomedical texts.\nML-NET is also shown to be robust when evaluated on different text genres in\nbiomedicine. Unlike traditional machine learning methods, ML-Net does not\nrequire human efforts in feature engineering and is highly efficient and\nscalable approach to tasks with a large set of labels (no need to build\nindividual classifiers for each separate label). Finally, ML-NET is able to\ndynamically estimate the label count based on the document context in a more\nsystematic and accurate manner.\n",
    "topics": "{'Multi-Label Classification': 1.0, 'Text Classification': 0.99999857, 'Feature Engineering': 0.99964917}",
    "score": 0.8009513083
  },
  {
    "id": "2008.13013",
    "title": "Lymph Node Gross Tumor Volume Detection in Oncology Imaging via\n  Relationship Learning Using Graph Neural Network",
    "abstract": "  Determining the spread of GTV$_{LN}$ is essential in defining the respective\nresection or irradiating regions for the downstream workflows of surgical\nresection and radiotherapy for many cancers. Different from the more common\nenlarged lymph node (LN), GTV$_{LN}$ also includes smaller ones if associated\nwith high positron emission tomography signals and/or any metastasis signs in\nCT. This is a daunting task. In this work, we propose a unified LN appearance\nand inter-LN relationship learning framework to detect the true GTV$_{LN}$.\nThis is motivated by the prior clinical knowledge that LNs form a connected\nlymphatic system, and the spread of cancer cells among LNs often follows\ncertain pathways. Specifically, we first utilize a 3D convolutional neural\nnetwork with ROI-pooling to extract the GTV$_{LN}$'s instance-wise appearance\nfeatures. Next, we introduce a graph neural network to further model the\ninter-LN relationships where the global LN-tumor spatial priors are included in\nthe learning process. This leads to an end-to-end trainable network to detect\nby classifying GTV$_{LN}$. We operate our model on a set of GTV$_{LN}$\ncandidates generated by a preliminary 1st-stage method, which has a sensitivity\nof $>85\\%$ at the cost of high false positive (FP) ($>15$ FPs per patient). We\nvalidate our approach on a radiotherapy dataset with 142 paired PET/RTCT scans\ncontaining the chest and upper abdominal body parts. The proposed method\nsignificantly improves over the state-of-the-art (SOTA) LN classification\nmethod by $5.5\\%$ and $13.1\\%$ in F1 score and the averaged sensitivity value\nat $2, 3, 4, 6$ FPs per patient, respectively.\n",
    "topics": "{}",
    "score": 0.8009028577
  },
  {
    "id": "2008.11586",
    "title": "Weakly Supervised Learning with Side Information for Noisy Labeled\n  Images",
    "abstract": "  In many real-world datasets, like WebVision, the performance of DNN based\nclassifier is often limited by the noisy labeled data. To tackle this problem,\nsome image related side information, such as captions and tags, often reveal\nunderlying relationships across images. In this paper, we present an efficient\nweakly supervised learning by using a Side Information Network (SINet), which\naims to effectively carry out a large scale classification with severely noisy\nlabels. The proposed SINet consists of a visual prototype module and a noise\nweighting module. The visual prototype module is designed to generate a compact\nrepresentation for each category by introducing the side information. The noise\nweighting module aims to estimate the correctness of each noisy image and\nproduce a confidence score for image ranking during the training procedure. The\npropsed SINet can largely alleviate the negative impact of noisy image labels,\nand is beneficial to train a high performance CNN based classifier. Besides, we\nreleased a fine-grained product dataset called AliProducts, which contains more\nthan 2.5 million noisy web images crawled from the internet by using queries\ngenerated from 50,000 fine-grained semantic classes. Extensive experiments on\nseveral popular benchmarks (i.e. Webvision, ImageNet and Clothing-1M) and our\nproposed AliProducts achieve state-of-the-art performance. The SINet has won\nthe first place in the classification task on WebVision Challenge 2019, and\noutperformed other competitors by a large margin.\n",
    "topics": "{}",
    "score": 0.8008778934
  },
  {
    "id": "2002.01368",
    "title": "End-to-end deep learning for big data analytics under a quasi-open set\n  assumption",
    "abstract": "  Neural network classifiers trained using end-to-end learning regimes are\nargued most viable for big data analytics due to their low system complexity,\nfast training and low computational cost. Generally, big data classification\nmodels are trained using a semi-supervised learning framework due to the\navailable unlabelled samples and the high cost to gather labelled samples. We\nassume that unlabelled training samples in big data are from both the same and\ndifferent classes to available labelled training samples which we call a\nquasi-open set. Under quasi-open set assumptions, end-to-end classifier models\nmust accurately classify samples from source classes represented by labelled\nand unlabelled training samples while also detecting samples from novel classes\nrepresented by only unlabelled training samples. To the best of our knowledge,\nno end-to-end work has trained under a quasi-open set assumption making our\nresults a first of its kind. Our proposed method extends the semi-supervised\nlearning using GANs framework to also explicitly train a certainty\nclassification measurement via end-to-end means. Different from other certainty\nmeasurements that aim to reduce misclassifications of source classes, ours aims\nto provide tractable means to separate source and novel classes. Experiments\nare conducted on a simulated quasi-open set using MNIST by selecting seven\nclasses as source classes and using the remaining three classes as possible\nnovel classes. For all experiments, we achieve near-perfect detection of\nsamples from novel classes. On the other hand, source class classification is\ndependant on the number of labelled training samples provided for the source\nclasses as per general end-to-end classification learning. End-to-end learning\nis held as the most tractable solution for big data analytics, but if only if\nmodels are trained to classify source classes and detect novel classes.\n",
    "topics": "{}",
    "score": 0.8008498962
  },
  {
    "id": "2001.04051",
    "title": "An Adversarial Approach for the Robust Classification of Pneumonia from\n  Chest Radiographs",
    "abstract": "  While deep learning has shown promise in the domain of disease classification\nfrom medical images, models based on state-of-the-art convolutional neural\nnetwork architectures often exhibit performance loss due to dataset shift.\nModels trained using data from one hospital system achieve high predictive\nperformance when tested on data from the same hospital, but perform\nsignificantly worse when they are tested in different hospital systems.\nFurthermore, even within a given hospital system, deep learning models have\nbeen shown to depend on hospital- and patient-level confounders rather than\nmeaningful pathology to make classifications. In order for these models to be\nsafely deployed, we would like to ensure that they do not use confounding\nvariables to make their classification, and that they will work well even when\ntested on images from hospitals that were not included in the training data. We\nattempt to address this problem in the context of pneumonia classification from\nchest radiographs. We propose an approach based on adversarial optimization,\nwhich allows us to learn more robust models that do not depend on confounders.\nSpecifically, we demonstrate improved out-of-hospital generalization\nperformance of a pneumonia classifier by training a model that is invariant to\nthe view position of chest radiographs (anterior-posterior vs.\nposterior-anterior). Our approach leads to better predictive performance on\nexternal hospital data than both a standard baseline and previously proposed\nmethods to handle confounding, and also suggests a method for identifying\nmodels that may rely on confounders. Code available at\nhttps://github.com/suinleelab/cxr_adv.\n",
    "topics": "{}",
    "score": 0.8007678302
  },
  {
    "id": "2001.03389",
    "title": "Real-Time RFI Mitigation for the Apertif Radio Transient System",
    "abstract": "  Current and upcoming radio telescopes are being designed with increasing\nsensitivity to detect new and mysterious radio sources of astrophysical origin.\nWhile this increased sensitivity improves the likelihood of discoveries, it\nalso makes these instruments more susceptible to the deleterious effects of\nRadio Frequency Interference (RFI). The challenge posed by RFI is exacerbated\nby the high data-rates achieved by modern radio telescopes, which require\nreal-time processing to keep up with the data. Furthermore, the high data-rates\ndo not allow for permanent storage of observations at high resolution. Offline\nRFI mitigation is therefore not possible anymore. The real-time requirement\nmakes RFI mitigation even more challenging because, on one side, the techniques\nused for mitigation need to be fast and simple, and on the other side they also\nneed to be robust enough to cope with just a partial view of the data.\n  The Apertif Radio Transient System (ARTS) is the real-time, time-domain,\ntransient detection instrument of the Westerbork Synthesis Radio Telescope\n(WSRT), processing 73 Gb of data per second. Even with a deep learning\nclassifier, the ARTS pipeline requires state-of-the-art real-time RFI\nmitigation to reduce the number of false-positive detections. Our solution to\nthis challenge is RFIm, a high-performance, open-source, tuned, and extensible\nRFI mitigation library. The goal of this library is to provide users with RFI\nmitigation routines that are designed to run in real-time on many-core\naccelerators, such as Graphics Processing Units, and that can be highly-tuned\nto achieve code and performance portability to different hardware platforms and\nscientific use-cases. Results on the ARTS show that we can achieve real-time\nRFI mitigation, with a minimal impact on the total execution time of the search\npipeline, and considerably reduce the number of false-positives.\n",
    "topics": "{}",
    "score": 0.8006248993
  },
  {
    "id": "2002.10179",
    "title": "HRank: Filter Pruning using High-Rank Feature Map",
    "abstract": "  Neural network pruning offers a promising prospect to facilitate deploying\ndeep neural networks on resource-limited devices. However, existing methods are\nstill challenged by the training inefficiency and labor cost in pruning\ndesigns, due to missing theoretical guidance of non-salient network components.\nIn this paper, we propose a novel filter pruning method by exploring the High\nRank of feature maps (HRank). Our HRank is inspired by the discovery that the\naverage rank of multiple feature maps generated by a single filter is always\nthe same, regardless of the number of image batches CNNs receive. Based on\nHRank, we develop a method that is mathematically formulated to prune filters\nwith low-rank feature maps. The principle behind our pruning is that low-rank\nfeature maps contain less information, and thus pruned results can be easily\nreproduced. Besides, we experimentally show that weights with high-rank feature\nmaps contain more important information, such that even when a portion is not\nupdated, very little damage would be done to the model performance. Without\nintroducing any additional constraints, HRank leads to significant improvements\nover the state-of-the-arts in terms of FLOPs and parameters reduction, with\nsimilar accuracies. For example, with ResNet-110, we achieve a 58.2%-FLOPs\nreduction by removing 59.2% of the parameters, with only a small loss of 0.14%\nin top-1 accuracy on CIFAR-10. With Res-50, we achieve a 43.8%-FLOPs reduction\nby removing 36.7% of the parameters, with only a loss of 1.17% in the top-1\naccuracy on ImageNet. The codes can be available at\nhttps://github.com/lmbxmu/HRank.\n",
    "topics": "{'Network Pruning': 0.9999956}",
    "score": 0.8005839491
  },
  {
    "id": "1609.02521",
    "title": "DiSMEC - Distributed Sparse Machines for Extreme Multi-label\n  Classification",
    "abstract": "  Extreme multi-label classification refers to supervised multi-label learning\ninvolving hundreds of thousands or even millions of labels. Datasets in extreme\nclassification exhibit fit to power-law distribution, i.e. a large fraction of\nlabels have very few positive instances in the data distribution. Most\nstate-of-the-art approaches for extreme multi-label classification attempt to\ncapture correlation among labels by embedding the label matrix to a\nlow-dimensional linear sub-space. However, in the presence of power-law\ndistributed extremely large and diverse label spaces, structural assumptions\nsuch as low rank can be easily violated.\n  In this work, we present DiSMEC, which is a large-scale distributed framework\nfor learning one-versus-rest linear classifiers coupled with explicit capacity\ncontrol to control model size. Unlike most state-of-the-art methods, DiSMEC\ndoes not make any low rank assumptions on the label matrix. Using double layer\nof parallelization, DiSMEC can learn classifiers for datasets consisting\nhundreds of thousands labels within few hours. The explicit capacity control\nmechanism filters out spurious parameters which keep the model compact in size,\nwithout losing prediction accuracy. We conduct extensive empirical evaluation\non publicly available real-world datasets consisting upto 670,000 labels. We\ncompare DiSMEC with recent state-of-the-art approaches, including - SLEEC which\nis a leading approach for learning sparse local embeddings, and FastXML which\nis a tree-based approach optimizing ranking based loss function. On some of the\ndatasets, DiSMEC can significantly boost prediction accuracies - 10% better\ncompared to SLECC and 15% better compared to FastXML, in absolute terms.\n",
    "topics": "{'Multi-Label Classification': 1.0, 'Multi-Label Learning': 0.9999269}",
    "score": 0.800566909
  },
  {
    "id": "1806.00749",
    "title": "TI-CNN: Convolutional Neural Networks for Fake News Detection",
    "abstract": "  With the development of social networks, fake news for various commercial and\npolitical purposes has been appearing in large numbers and gotten widespread in\nthe online world. With deceptive words, people can get infected by the fake\nnews very easily and will share them without any fact-checking. For instance,\nduring the 2016 US president election, various kinds of fake news about the\ncandidates widely spread through both official news media and the online social\nnetworks. These fake news is usually released to either smear the opponents or\nsupport the candidate on their side. The erroneous information in the fake news\nis usually written to motivate the voters' irrational emotion and enthusiasm.\nSuch kinds of fake news sometimes can bring about devastating effects, and an\nimportant goal in improving the credibility of online social networks is to\nidentify the fake news timely. In this paper, we propose to study the fake news\ndetection problem. Automatic fake news identification is extremely hard, since\npure model based fact-checking for news is still an open problem, and few\nexisting models can be applied to solve the problem. With a thorough\ninvestigation of a fake news data, lots of useful explicit features are\nidentified from both the text words and images used in the fake news. Besides\nthe explicit features, there also exist some hidden patterns in the words and\nimages used in fake news, which can be captured with a set of latent features\nextracted via the multiple convolutional layers in our model. A model named as\nTI-CNN (Text and Image information based Convolutinal Neural Network) is\nproposed in this paper. By projecting the explicit and latent features into a\nunified feature space, TI-CNN is trained with both the text and image\ninformation simultaneously. Extensive experiments carried on the real-world\nfake news datasets have demonstrate the effectiveness of TI-CNN.\n",
    "topics": "{'Fake News Detection': 1.0}",
    "score": 0.8005209114
  },
  {
    "id": "2009.00312",
    "title": "PIDNet: An Efficient Network for Dynamic Pedestrian Intrusion Detection",
    "abstract": "  Vision-based dynamic pedestrian intrusion detection (PID), judging whether\npedestrians intrude an area-of-interest (AoI) by a moving camera, is an\nimportant task in mobile surveillance. The dynamically changing AoIs and a\nnumber of pedestrians in video frames increase the difficulty and computational\ncomplexity of determining whether pedestrians intrude the AoI, which makes\nprevious algorithms incapable of this task. In this paper, we propose a novel\nand efficient multi-task deep neural network, PIDNet, to solve this problem.\nPIDNet is mainly designed by considering two factors: accurately segmenting the\ndynamically changing AoIs from a video frame captured by the moving camera and\nquickly detecting pedestrians from the generated AoI-contained areas. Three\nefficient network designs are proposed and incorporated into PIDNet to reduce\nthe computational complexity: 1) a special PID task backbone for feature\nsharing, 2) a feature cropping module for feature cropping, and 3) a lighter\ndetection branch network for feature compression. In addition, considering\nthere are no public datasets and benchmarks in this field, we establish a\nbenchmark dataset to evaluate the proposed network and give the corresponding\nevaluation metrics for the first time. Experimental results show that PIDNet\ncan achieve 67.1% PID accuracy and 9.6 fps inference speed on the proposed\ndataset, which serves as a good baseline for the future vision-based dynamic\nPID study.\n",
    "topics": "{'Intrusion Detection': 1.0}",
    "score": 0.800429598
  },
  {
    "id": "1906.00512",
    "title": "Stochastic Generalized Adversarial Label Learning",
    "abstract": "  The usage of machine learning models has grown substantially and is spreading\ninto several application domains. A common need in using machine learning\nmodels is collecting the data required to train these models. In some cases,\nlabeling a massive dataset can be a crippling bottleneck, so there is need to\ndevelop models that work when training labels for large amounts of data are not\neasily obtained. A possible solution is weak supervision, which uses noisy\nlabels that are easily obtained from multiple sources. The challenge is how\nbest to combine these noisy labels and train a model to perform well given a\ntask. In this paper, we propose stochastic generalized adversarial label\nlearning (Stoch-GALL), a framework for training machine learning models that\nperform well when noisy and possibly correlated labels are provided. Our\nframework allows users to provide different weak labels and multiple\nconstraints on these labels. Our model then attempts to learn parameters for\nthe data by solving a non-zero sum game optimization. The game is between an\nadversary that chooses labels for the data and a model that minimizes the error\nmade by the adversarial labels. We test our method on three datasets by\ntraining convolutional neural network models that learn to classify image\nobjects with limited access to training labels. Our approach is able to learn\neven in settings where the weak supervision confounds state-of-the-art weakly\nsupervised learning methods. The results of our experiments demonstrate the\napplicability of this approach to general classification tasks.\n",
    "topics": "{}",
    "score": 0.8004149571
  },
  {
    "id": "1912.07010",
    "title": "A Shape Transformation-based Dataset Augmentation Framework for\n  Pedestrian Detection",
    "abstract": "  Deep learning-based computer vision is usually data-hungry. Many researchers\nattempt to augment datasets with synthesized data to improve model robustness.\nHowever, the augmentation of popular pedestrian datasets, such as Caltech and\nCitypersons, can be extremely challenging because real pedestrians are commonly\nin low quality. Due to the factors like occlusions, blurs, and low-resolution,\nit is significantly difficult for existing augmentation approaches, which\ngenerally synthesize data using 3D engines or generative adversarial networks\n(GANs), to generate realistic-looking pedestrians. Alternatively, to access\nmuch more natural-looking pedestrians, we propose to augment pedestrian\ndetection datasets by transforming real pedestrians from the same dataset into\ndifferent shapes. Accordingly, we propose the Shape Transformation-based\nDataset Augmentation (STDA) framework. The proposed framework is composed of\ntwo subsequent modules, i.e. the shape-guided deformation and the environment\nadaptation. In the first module, we introduce a shape-guided warping field to\nhelp deform the shape of a real pedestrian into a different shape. Then, in the\nsecond stage, we propose an environment-aware blending map to better adapt the\ndeformed pedestrians into surrounding environments, obtaining more\nrealistic-looking pedestrians and more beneficial augmentation results for\npedestrian detection. Extensive empirical studies on different pedestrian\ndetection benchmarks show that the proposed STDA framework consistently\nproduces much better augmentation results than other pedestrian synthesis\napproaches using low-quality pedestrians. By augmenting the original datasets,\nour proposed framework also improves the baseline pedestrian detector by up to\n38% on the evaluated benchmarks, achieving state-of-the-art performance.\n",
    "topics": "{'Pedestrian Detection': 1.0}",
    "score": 0.8003155566
  },
  {
    "id": "1911.00996",
    "title": "A Study of Data Pre-processing Techniques for Imbalanced Biomedical Data\n  Classification",
    "abstract": "  Biomedical data are widely accepted in developing prediction models for\nidentifying a specific tumor, drug discovery and classification of human\ncancers. However, previous studies usually focused on different classifiers,\nand overlook the class imbalance problem in real-world biomedical datasets.\nThere are a lack of studies on evaluation of data pre-processing techniques,\nsuch as resampling and feature selection, on imbalanced biomedical data\nlearning. The relationship between data pre-processing techniques and the data\ndistributions has never been analysed in previous studies. This article mainly\nfocuses on reviewing and evaluating some popular and recently developed\nresampling and feature selection methods for class imbalance learning. We\nanalyse the effectiveness of each technique from data distribution perspective.\nExtensive experiments have been done based on five classifiers, four\nperformance measures, eight learning techniques across twenty real-world\ndatasets. Experimental results show that: (1) resampling and feature selection\ntechniques exhibit better performance using support vector machine (SVM)\nclassifier. However, resampling and Feature Selection techniques perform poorly\nwhen using C4.5 decision tree and Linear discriminant analysis classifiers; (2)\nfor datasets with different distributions, techniques such as Random\nundersampling and Feature Selection perform better than other data\npre-processing methods with T Location-Scale distribution when using SVM and\nKNN (K-nearest neighbours) classifiers. Random oversampling outperforms other\nmethods on Negative Binomial distribution using Random Forest classifier with\nlower level of imbalance ratio; (3) Feature Selection outperforms other data\npre-processing methods in most cases, thus, Feature Selection with SVM\nclassifier is the best choice for imbalanced biomedical data learning.\n",
    "topics": "{'Feature Selection': 1.0, 'Drug Discovery': 0.99998605}",
    "score": 0.8002515514
  },
  {
    "id": "1905.08352",
    "title": "Robust sound event detection in bioacoustic sensor networks",
    "abstract": "  Bioacoustic sensors, sometimes known as autonomous recording units (ARUs),\ncan record sounds of wildlife over long periods of time in scalable and\nminimally invasive ways. Deriving per-species abundance estimates from these\nsensors requires detection, classification, and quantification of animal\nvocalizations as individual acoustic events. Yet, variability in ambient noise,\nboth over time and across sensors, hinders the reliability of current automated\nsystems for sound event detection (SED), such as convolutional neural networks\n(CNN) in the time-frequency domain. In this article, we develop, benchmark, and\ncombine several machine listening techniques to improve the generalizability of\nSED models across heterogeneous acoustic environments. As a case study, we\nconsider the problem of detecting avian flight calls from a ten-hour recording\nof nocturnal bird migration, recorded by a network of six ARUs in the presence\nof heterogeneous background noise. Starting from a CNN yielding\nstate-of-the-art accuracy on this task, we introduce two noise adaptation\ntechniques, respectively integrating short-term (60 milliseconds) and long-term\n(30 minutes) context. First, we apply per-channel energy normalization (PCEN)\nin the time-frequency domain, which applies short-term automatic gain control\nto every subband in the mel-frequency spectrogram. Secondly, we replace the\nlast dense layer in the network by a context-adaptive neural network (CA-NN)\nlayer. Combining them yields state-of-the-art results that are unmatched by\nartificial data augmentation alone. We release a pre-trained version of our\nbest performing system under the name of BirdVoxDetect, a ready-to-use detector\nof avian flight calls in field recordings.\n",
    "topics": "{'Data Augmentation': 0.99839836}",
    "score": 0.800035105
  },
  {
    "id": "1801.05525",
    "title": "Identification of Seed Cells in Multispectral Images for GrowCut\n  Segmentation",
    "abstract": "  The segmentation of satellite images is a necessary step to perform\nobject-oriented image classification, which has become relevant due to its\napplicability on images with a high spatial resolution. To perform\nobject-oriented image classification, the studied image must first be segmented\nin uniform regions. This segmentation requires manual work by an expert user,\nwho must exhaustively explore the image to establish thresholds that generate\nuseful and representative segments without oversegmenting and without\ndiscarding representative segments. We propose a technique that automatically\nsegments the multispectral image while facing these issues. We identify in the\nimage homogenous zones according to their spectral signatures through the use\nof morphological filters. These homogenous zones are representatives of\ndifferent types of land coverings in the image and are used as seeds for the\nGrowCut multispectral segmentation algorithm. GrowCut is a cellular automaton\nwith competitive region growth, its cells are linked to every pixel in the\nimage through three parameters: the pixel's spectral signature, a label, and a\nstrength factor that represents the strength with which a cell defends its\nlabel. The seed cells possess maximum strength and maintain their state\nthroughout the automaton's evolution. Starting from seed cells, each cell in\nthe image is iteratively attacked by its neighboring cells. When the automaton\nstops updating its states, we obtain a segmented image where each pixel has\ntaken the label of one of its cells. In this paper the algorithm was applied in\nan image acquired by Landsat8 on agricultural land of Calabozo, Guarico,\nVenezuela where there are different types of land coverings: agriculture, urban\nregions, water bodies, and savannas with different degrees of human\nintervention. The segmentation obtained is presented as irregular polygons\nenclosing geographical objects.\n",
    "topics": "{'Image Classification': 0.9999893}",
    "score": 0.8000006197
  },
  {
    "id": "1810.11921",
    "title": "AutoInt: Automatic Feature Interaction Learning via Self-Attentive\n  Neural Networks",
    "abstract": "  Click-through rate (CTR) prediction, which aims to predict the probability of\na user clicking on an ad or an item, is critical to many online applications\nsuch as online advertising and recommender systems. The problem is very\nchallenging since (1) the input features (e.g., the user id, user age, item id,\nitem category) are usually sparse and high-dimensional, and (2) an effective\nprediction relies on high-order combinatorial features (\\textit{a.k.a.} cross\nfeatures), which are very time-consuming to hand-craft by domain experts and\nare impossible to be enumerated. Therefore, there have been efforts in finding\nlow-dimensional representations of the sparse and high-dimensional raw features\nand their meaningful combinations. In this paper, we propose an effective and\nefficient method called the \\emph{AutoInt} to automatically learn the\nhigh-order feature interactions of input features. Our proposed algorithm is\nvery general, which can be applied to both numerical and categorical input\nfeatures. Specifically, we map both the numerical and categorical features into\nthe same low-dimensional space. Afterwards, a multi-head self-attentive neural\nnetwork with residual connections is proposed to explicitly model the feature\ninteractions in the low-dimensional space. With different layers of the\nmulti-head self-attentive neural networks, different orders of feature\ncombinations of input features can be modeled. The whole model can be\nefficiently fit on large-scale raw data in an end-to-end fashion. Experimental\nresults on four real-world datasets show that our proposed approach not only\noutperforms existing state-of-the-art approaches for prediction but also offers\ngood explainability. Code is available at:\n\\url{https://github.com/DeepGraphLearning/RecommenderSystems}.\n",
    "topics": "{'Click-Through Rate Prediction': 0.99999154, 'Recommendation Systems': 0.9904265}",
    "score": 0.7999786678
  },
  {
    "id": "2003.01223",
    "title": "A Deep learning Approach to Generate Contrast-Enhanced Computerised\n  Tomography Angiography without the Use of Intravenous Contrast Agents",
    "abstract": "  Contrast-enhanced computed tomography angiograms (CTAs) are widely used in\ncardiovascular imaging to obtain a non-invasive view of arterial structures.\nHowever, contrast agents are associated with complications at the injection\nsite as well as renal toxicity leading to contrast-induced nephropathy (CIN)\nand renal failure. We hypothesised that the raw data acquired from a\nnon-contrast CT contains sufficient information to differentiate blood and\nother soft tissue components. We utilised deep learning methods to define the\nsubtleties between soft tissue components in order to simulate contrast\nenhanced CTAs without contrast agents. Twenty-six patients with paired\nnon-contrast and CTA images were randomly selected from an approved clinical\nstudy. Non-contrast axial slices within the AAA from 10 patients (n = 100) were\nsampled for the underlying Hounsfield unit (HU) distribution at the lumen,\nintra-luminal thrombus and interface locations. Sampling of HUs in these\nregions revealed significant differences between all regions (p<0.001 for all\ncomparisons), confirming the intrinsic differences in the radiomic signatures\nbetween these regions. To generate a large training dataset, paired axial\nslices from the training set (n=13) were augmented to produce a total of 23,551\n2-D images. We trained a 2-D Cycle Generative Adversarial Network (cycleGAN)\nfor this non-contrast to contrast (NC2C) transformation task. The accuracy of\nthe cycleGAN output was assessed by comparison to the contrast image. This\npipeline is able to differentiate between visually incoherent soft tissue\nregions in non-contrast CT images. The CTAs generated from the non-contrast\nimages bear strong resemblance to the ground truth. Here we describe a novel\napplication of Generative Adversarial Network for CT image processing. This is\npoised to disrupt clinical pathways requiring contrast enhanced CT imaging.\n",
    "topics": "{'Computed Tomography (CT)': 0.9930101}",
    "score": 0.7998941714
  },
  {
    "id": "1710.01559",
    "title": "Monitoring tool usage in surgery videos using boosted convolutional and\n  recurrent neural networks",
    "abstract": "  This paper investigates the automatic monitoring of tool usage during a\nsurgery, with potential applications in report generation, surgical training\nand real-time decision support. Two surgeries are considered: cataract surgery,\nthe most common surgical procedure, and cholecystectomy, one of the most common\ndigestive surgeries. Tool usage is monitored in videos recorded either through\na microscope (cataract surgery) or an endoscope (cholecystectomy). Following\nstate-of-the-art video analysis solutions, each frame of the video is analyzed\nby convolutional neural networks (CNNs) whose outputs are fed to recurrent\nneural networks (RNNs) in order to take temporal relationships between events\ninto account. Novelty lies in the way those CNNs and RNNs are trained.\nComputational complexity prevents the end-to-end training of \"CNN+RNN\" systems.\nTherefore, CNNs are usually trained first, independently from the RNNs. This\napproach is clearly suboptimal for surgical tool analysis: many tools are very\nsimilar to one another, but they can generally be differentiated based on past\nevents. CNNs should be trained to extract the most useful visual features in\ncombination with the temporal context. A novel boosting strategy is proposed to\nachieve this goal: the CNN and RNN parts of the system are simultaneously\nenriched by progressively adding weak classifiers (either CNNs or RNNs) trained\nto improve the overall classification accuracy. Experiments were performed in a\ndataset of 50 cataract surgery videos and a dataset of 80 cholecystectomy\nvideos. Very good classification performance are achieved in both datasets:\ntool usage could be labeled with an average area under the ROC curve of $A_z =\n0.9961$ and $A_z = 0.9939$, respectively, in offline mode (using past, present\nand future information), and $A_z = 0.9957$ and $A_z = 0.9936$, respectively,\nin online mode (using past and present information only).\n",
    "topics": "{}",
    "score": 0.7998403146
  },
  {
    "id": "1810.00740",
    "title": "Improving the Generalization of Adversarial Training with Domain\n  Adaptation",
    "abstract": "  By injecting adversarial examples into training data, adversarial training is\npromising for improving the robustness of deep learning models. However, most\nexisting adversarial training approaches are based on a specific type of\nadversarial attack. It may not provide sufficiently representative samples from\nthe adversarial domain, leading to a weak generalization ability on adversarial\nexamples from other attacks. Moreover, during the adversarial training,\nadversarial perturbations on inputs are usually crafted by fast single-step\nadversaries so as to scale to large datasets. This work is mainly focused on\nthe adversarial training yet efficient FGSM adversary. In this scenario, it is\ndifficult to train a model with great generalization due to the lack of\nrepresentative adversarial samples, aka the samples are unable to accurately\nreflect the adversarial domain. To alleviate this problem, we propose a novel\nAdversarial Training with Domain Adaptation (ATDA) method. Our intuition is to\nregard the adversarial training on FGSM adversary as a domain adaption task\nwith limited number of target domain samples. The main idea is to learn a\nrepresentation that is semantically meaningful and domain invariant on the\nclean domain as well as the adversarial domain. Empirical evaluations on\nFashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly\nimprove the generalization of adversarial training and the smoothness of the\nlearned models, and outperforms state-of-the-art methods on standard benchmark\ndatasets. To show the transfer ability of our method, we also extend ATDA to\nthe adversarial training on iterative attacks such as PGD-Adversial Training\n(PAT) and the defense performance is improved considerably.\n",
    "topics": "{'Adversarial Attack': 0.999335, 'Domain Adaptation': 0.6628593}",
    "score": 0.7997775635
  },
  {
    "id": "1805.10546",
    "title": "Transductive Label Augmentation for Improved Deep Network Learning",
    "abstract": "  A major impediment to the application of deep learning to real-world problems\nis the scarcity of labeled data. Small training sets are in fact of no use to\ndeep networks as, due to the large number of trainable parameters, they will\nvery likely be subject to overfitting phenomena. On the other hand, the\nincrement of the training set size through further manual or semi-automatic\nlabellings can be costly, if not possible at times. Thus, the standard\ntechniques to address this issue are transfer learning and data augmentation,\nwhich consists of applying some sort of \"transformation\" to existing labeled\ninstances to let the training set grow in size. Although this approach works\nwell in applications such as image classification, where it is relatively\nsimple to design suitable transformation operators, it is not obvious how to\napply it in more structured scenarios. Motivated by the observation that in\nvirtually all application domains it is easy to obtain unlabeled data, in this\npaper we take a different perspective and propose a \\emph{label augmentation}\napproach. We start from a small, curated labeled dataset and let the labels\npropagate through a larger set of unlabeled data using graph transduction\ntechniques. This allows us to naturally use (second-order) similarity\ninformation which resides in the data, a source of information which is\ntypically neglected by standard augmentation techniques. In particular, we show\nthat by using known game theoretic transductive processes we can create larger\nand accurate enough labeled datasets which use results in better trained neural\nnetworks. Preliminary experiments are reported which demonstrate a consistent\nimprovement over standard image classification datasets.\n",
    "topics": "{'Image Classification': 0.9999881, 'Data Augmentation': 0.9633088}",
    "score": 0.7997416067
  },
  {
    "id": "1810.05801",
    "title": "Deep learning based cloud detection for medium and high resolution\n  remote sensing images of different sensors",
    "abstract": "  Cloud detection is an important preprocessing step for the precise\napplication of optical satellite imagery. In this paper, we propose a deep\nlearning based cloud detection method named multi-scale convolutional feature\nfusion (MSCFF) for remote sensing images of different sensors. In the network\narchitecture of MSCFF, the symmetric encoder-decoder module, which provides\nboth local and global context by densifying feature maps with trainable\nconvolutional filter banks, is utilized to extract multi-scale and high-level\nspatial features. The feature maps of multiple scales are then up-sampled and\nconcatenated, and a novel multi-scale feature fusion module is designed to fuse\nthe features of different scales for the output. The two output feature maps of\nthe network are cloud and cloud shadow maps, which are in turn fed to binary\nclassifiers outside the model to obtain the final cloud and cloud shadow mask.\nThe MSCFF method was validated on hundreds of globally distributed optical\nsatellite images, with spatial resolutions ranging from 0.5 to 50 m, including\nLandsat-5/7/8, Gaofen-1/2/4, Sentinel-2, Ziyuan-3, CBERS-04, Huanjing-1, and\ncollected high-resolution images exported from Google Earth. The experimental\nresults show that MSCFF achieves a higher accuracy than the traditional\nrule-based cloud detection methods and the state-of-the-art deep learning\nmodels, especially in bright surface covered areas. The effectiveness of MSCFF\nmeans that it has great promise for the practical application of cloud\ndetection for multiple types of medium and high-resolution remote sensing\nimages. Our established global high-resolution cloud detection validation\ndataset has been made available online.\n",
    "topics": "{}",
    "score": 0.799737468
  },
  {
    "id": "1812.05705",
    "title": "Exploring Embedding Methods in Binary Hyperdimensional Computing: A Case\n  Study for Motor-Imagery based Brain-Computer Interfaces",
    "abstract": "  Key properties of brain-inspired hyperdimensional (HD) computing make it a\nprime candidate for energy-efficient and fast learning in biosignal processing.\nThe main challenge is however to formulate embedding methods that map biosignal\nmeasures to a binary HD space. In this paper, we explore variety of such\nembedding methods and examine them with a challenging application of motor\nimagery brain-computer interface (MI-BCI) from electroencephalography (EEG)\nrecordings. We explore embedding methods including random projections,\nquantization based thermometer and Gray coding, and learning HD representations\nusing end-to-end training. All these methods, differing in complexity, aim to\nrepresent EEG signals in binary HD space, e.g. with 10,000 bits. This leads to\ndevelopment of a set of HD learning and classification methods that can be\nselectively chosen (or configured) based on accuracy and/or computational\ncomplexity requirements of a given task. We compare them with state-of-the-art\nlinear support vector machine (SVM) on an NVIDIA TX2 board using the 4-class\nBCI competition IV-2a dataset as well as a new 3-class dataset. Compared to\nSVM, results on 3-class dataset show that simple thermometer embedding achieves\nmoderate average accuracy (79.56% vs. 82.67%) with 26.8$\\times$ faster training\ntime and 22.3$\\times$ lower energy; on the other hand, switching to end-to-end\ntraining with learned HD representations wipes out these training benefits\nwhile boosting the accuracy to 84.22% (1.55% higher than SVM). Similar trend is\nobserved on the 4-class dataset where SVM achieves on average 74.29%: the\nthermometer embedding achieves 89.9$\\times$ faster training time and\n58.7$\\times$ lower energy, but a lower accuracy (67.09%) than the learned\nrepresentation of 72.54%.\n",
    "topics": "{'EEG': 0.9998592, 'Quantization': 0.75899774}",
    "score": 0.7992693558
  },
  {
    "id": "1808.02355",
    "title": "Capturing global spatial context for accurate cell classification in\n  skin cancer histology",
    "abstract": "  The spectacular response observed in clinical trials of immunotherapy in\npatients with previously uncurable Melanoma, a highly aggressive form of skin\ncancer, calls for a better understanding of the cancer-immune interface.\nComputational pathology provides a unique opportunity to spatially dissect such\ninterface on digitised pathological slides. Accurate cellular classification is\na key to ensure meaningful results, but is often challenging even with\nstate-of-art machine learning and deep learning methods.\n  We propose a hierarchical framework, which mirrors the way pathologists\nperceive tumour architecture and define tumour heterogeneity to improve cell\nclassification methods that rely solely on cell nuclei morphology. The SLIC\nsuperpixel algorithm was used to segment and classify tumour regions in low\nresolution H&E-stained histological images of melanoma skin cancer to provide a\nglobal context. Classification of superpixels into tumour, stroma, epidermis\nand lumen/white space, yielded a 97.7% training set accuracy and 95.7% testing\nset accuracy in 58 whole-tumour images of the TCGA melanoma dataset. The\nsuperpixel classification was projected down to high resolution images to\nenhance the performance of a single cell classifier, based on cell nuclear\nmorphological features, and resulted in increasing its accuracy from 86.4% to\n91.6%. Furthermore, a voting scheme was proposed to use global context as\nbiological a priori knowledge, pushing the accuracy further to 92.8%.\n  This study demonstrates how using the global spatial context can accurately\ncharacterise the tumour microenvironment and allow us to extend significantly\nbeyond single-cell morphological classification.\n",
    "topics": "{}",
    "score": 0.7991795345
  },
  {
    "id": "1807.11697",
    "title": "Multimodal Deep Domain Adaptation",
    "abstract": "  Typically a classifier trained on a given dataset (source domain) does not\nperforms well if it is tested on data acquired in a different setting (target\ndomain). This is the problem that domain adaptation (DA) tries to overcome and,\nwhile it is a well explored topic in computer vision, it is largely ignored in\nrobotic vision where usually visual classification methods are trained and\ntested in the same domain. Robots should be able to deal with unknown\nenvironments, recognize objects and use them in the correct way, so it is\nimportant to explore the domain adaptation scenario also in this context. The\ngoal of the project is to define a benchmark and a protocol for multi-modal\ndomain adaptation that is valuable for the robot vision community. With this\npurpose some of the state-of-the-art DA methods are selected: Deep Adaptation\nNetwork (DAN), Domain Adversarial Training of Neural Network (DANN), Automatic\nDomain Alignment Layers (AutoDIAL) and Adversarial Discriminative Domain\nAdaptation (ADDA). Evaluations have been done using different data types: RGB\nonly, depth only and RGB-D over the following datasets, designed for the\nrobotic community: RGB-D Object Dataset (ROD), Web Object Dataset (WOD),\nAutonomous Robot Indoor Dataset (ARID), Big Berkeley Instance Recognition\nDataset (BigBIRD) and Active Vision Dataset. Although progresses have been made\non the formulation of effective adaptation algorithms and more realistic object\ndatasets are available, the results obtained show that, training a sufficiently\ngood object classifier, especially in the domain adaptation scenario, is still\nan unsolved problem. Also the best way to combine depth with RGB informations\nto improve the performance is a point that needs to be investigated more.\n",
    "topics": "{'Domain Adaptation': 0.9999999}",
    "score": 0.7989463755
  },
  {
    "id": "2002.12641",
    "title": "AdarGCN: Adaptive Aggregation GCN for Few-Shot Learning",
    "abstract": "  Existing few-shot learning (FSL) methods assume that there exist sufficient\ntraining samples from source classes for knowledge transfer to target classes\nwith few training samples. However, this assumption is often invalid,\nespecially when it comes to fine-grained recognition. In this work, we define a\nnew FSL setting termed few-shot fewshot learning (FSFSL), under which both the\nsource and target classes have limited training samples. To overcome the source\nclass data scarcity problem, a natural option is to crawl images from the web\nwith class names as search keywords. However, the crawled images are inevitably\ncorrupted by large amount of noise (irrelevant images) and thus may harm the\nperformance. To address this problem, we propose a graph convolutional network\n(GCN)-based label denoising (LDN) method to remove the irrelevant images.\nFurther, with the cleaned web images as well as the original clean training\nimages, we propose a GCN-based FSL method. For both the LDN and FSL tasks, a\nnovel adaptive aggregation GCN (AdarGCN) model is proposed, which differs from\nexisting GCN models in that adaptive aggregation is performed based on a\nmulti-head multi-level aggregation module. With AdarGCN, how much and how far\ninformation carried by each graph node is propagated in the graph structure can\nbe determined automatically, therefore alleviating the effects of both noisy\nand outlying training samples. Extensive experiments show the superior\nperformance of our AdarGCN under both the new FSFSL and the conventional FSL\nsettings.\n",
    "topics": "{'Few-Shot Learning': 1.0, 'Transfer Learning': 0.9875832, 'Denoising': 0.959159}",
    "score": 0.7988831412
  },
  {
    "id": "1812.02183",
    "title": "Deep Learning at Scale for the Construction of Galaxy Catalogs in the\n  Dark Energy Survey",
    "abstract": "  The scale of ongoing and future electromagnetic surveys pose formidable\nchallenges to classify astronomical objects. Pioneering efforts on this front\ninclude citizen science campaigns adopted by the Sloan Digital Sky Survey\n(SDSS). SDSS datasets have been recently used to train neural network models to\nclassify galaxies in the Dark Energy Survey (DES) that overlap the footprint of\nboth surveys. Herein, we demonstrate that knowledge from deep learning\nalgorithms, pre-trained with real-object images, can be transferred to classify\ngalaxies that overlap both SDSS and DES surveys, achieving state-of-the-art\naccuracy $\\gtrsim99.6\\%$. We demonstrate that this process can be completed\nwithin just eight minutes using distributed training. While this represents a\nsignificant step towards the classification of DES galaxies that overlap\nprevious surveys, we need to initiate the characterization of unlabelled DES\ngalaxies in new regions of parameter space. To accelerate this program, we use\nour neural network classifier to label over ten thousand unlabelled DES\ngalaxies, which do not overlap previous surveys. Furthermore, we use our neural\nnetwork model as a feature extractor for unsupervised clustering and find that\nunlabeled DES images can be grouped together in two distinct galaxy classes\nbased on their morphology, which provides a heuristic check that the learning\nis successfully transferred to the classification of unlabelled DES images. We\nconclude by showing that these newly labeled datasets can be combined with\nunsupervised recursive training to create large-scale DES galaxy catalogs in\npreparation for the Large Synoptic Survey Telescope era.\n",
    "topics": "{}",
    "score": 0.7987796764
  },
  {
    "id": "2006.05594",
    "title": "Adversarial Attacks on Brain-Inspired Hyperdimensional Computing-Based\n  Classifiers",
    "abstract": "  Being an emerging class of in-memory computing architecture, brain-inspired\nhyperdimensional computing (HDC) mimics brain cognition and leverages random\nhypervectors (i.e., vectors with a dimensionality of thousands or even more) to\nrepresent features and to perform classification tasks. The unique hypervector\nrepresentation enables HDC classifiers to exhibit high energy efficiency, low\ninference latency and strong robustness against hardware-induced bit errors.\nConsequently, they have been increasingly recognized as an appealing\nalternative to or even replacement of traditional deep neural networks (DNNs)\nfor local on device classification, especially on low-power Internet of Things\ndevices. Nonetheless, unlike their DNN counterparts, state-of-the-art designs\nfor HDC classifiers are mostly security-oblivious, casting doubt on their\nsafety and immunity to adversarial inputs. In this paper, we study for the\nfirst time adversarial attacks on HDC classifiers and highlight that HDC\nclassifiers can be vulnerable to even minimally-perturbed adversarial samples.\nConcretely, using handwritten digit classification as an example, we construct\na HDC classifier and formulate a grey-box attack problem, where an attacker's\ngoal is to mislead the target HDC classifier to produce erroneous prediction\nlabels while keeping the amount of added perturbation noise as little as\npossible. Then, we propose a modified genetic algorithm to generate adversarial\nsamples within a reasonably small number of queries. Our results show that\nadversarial images generated by our algorithm can successfully mislead the HDC\nclassifier to produce wrong prediction labels with a high probability (i.e.,\n78% when the HDC classifier uses a fixed majority rule for decision). Finally,\nwe also present two defense strategies -- adversarial training and retraining--\nto strengthen the security of HDC classifiers.\n",
    "topics": "{}",
    "score": 0.7987231285
  },
  {
    "id": "2010.00882",
    "title": "Remote Sensing Image Scene Classification with Self-Supervised Paradigm\n  under Limited Labeled Samples",
    "abstract": "  With the development of deep learning, supervised learning methods perform\nwell in remote sensing images (RSIs) scene classification. However, supervised\nlearning requires a huge number of annotated data for training. When labeled\nsamples are not sufficient, the most common solution is to fine-tune the\npre-training models using a large natural image dataset (e.g. ImageNet).\nHowever, this learning paradigm is not a panacea, especially when the target\nremote sensing images (e.g. multispectral and hyperspectral data) have\ndifferent imaging mechanisms from RGB natural images. To solve this problem, we\nintroduce new self-supervised learning (SSL) mechanism to obtain the\nhigh-performance pre-training model for RSIs scene classification from large\nunlabeled data. Experiments on three commonly used RSIs scene classification\ndatasets demonstrated that this new learning paradigm outperforms the\ntraditional dominant ImageNet pre-trained model. Moreover, we analyze the\nimpacts of several factors in SSL on RSIs scene classification tasks, including\nthe choice of self-supervised signals, the domain difference between the source\nand target dataset, and the amount of pre-training data. The insights distilled\nfrom our studies can help to foster the development of SSL in the remote\nsensing community. Since SSL could learn from unlabeled massive RSIs which are\nextremely easy to obtain, it will be a potentially promising way to alleviate\ndependence on labeled samples and thus efficiently solve many problems, such as\nglobal mapping.\n",
    "topics": "{'Scene Classification': 1.0, 'Self-Supervised Learning': 0.99992704}",
    "score": 0.7987073195
  },
  {
    "id": "2009.04632",
    "title": "Assignment Flow for Order-Constrained OCT Segmentation",
    "abstract": "  At the present time Optical Coherence Tomography (OCT) is among the most\ncommonly used non-invasive imaging methods for the acquisition of large\nvolumetric scans of human retinal tissues and vasculature. To resolve decisive\ninformation from extracted OCT volumes and to make it applicable for further\ndiagnostic analysis, the exact identification of retinal layer thicknesses\nserves as an essential task be done for each patient separately. However, the\nmanual examination of multiple OCT scans in a row is a demanding and time\nconsuming task, which results in a lengthy qualification process and is\nfrequently confounded in the presence of tissue-dependent speckle noise.\nTherefore, the elaboration of automated segmentation models has become an\nimportant task in the field of medical image processing. We propose a novel,\npurely data driven \\textit{geometric approach to order-constrained 3D OCT\nretinal cell layer segmentation} which takes as input data in any metric space\nand comes along with basic operations that can be effectively computed in\nparallel. As opposed to many established retina detection methods, our\npresented formulation avoids the use of any shape prior and accomplishes the\nnatural order of the retina in a purely geometric way. This makes the approach\nunbiased and hence suited for the detection of local anatomical changes of\nretinal tissue structure. To demonstrate robustness of the proposed approach,\nwe compare two different choices of features on a data set of manually\nannotated 3D OCT volumes of healthy human retina. The quality of computed\nsegmentations is compared to the state of the art in terms of mean absolute\nerror and the Dice similarity coefficient. The results indicate a great\npotential for applying our method to the classification of diseased retina and\nopens a new research direction regarding the joint segmentation of retinal cell\nlayers and blood vessel structures.\n",
    "topics": "{}",
    "score": 0.7986992086
  },
  {
    "id": "2003.04949",
    "title": "LC-GAN: Image-to-image Translation Based on Generative Adversarial\n  Network for Endoscopic Images",
    "abstract": "  Intelligent vision is appealing in computer-assisted and robotic surgeries.\nVision-based analysis with deep learning usually requires large labeled\ndatasets, but manual data labeling is expensive and time-consuming in medical\nproblems. We investigate a novel cross-domain strategy to reduce the need for\nmanual data labeling by proposing an image-to-image translation model\nlive-cadaver GAN (LC-GAN) based on generative adversarial networks (GANs). We\nconsider a situation when a labeled cadaveric surgery dataset is available\nwhile the task is instrument segmentation on an unlabeled live surgery dataset.\nWe train LC-GAN to learn the mappings between the cadaveric and live images.\nFor live image segmentation, we first translate the live images to\nfake-cadaveric images with LC-GAN and then perform segmentation on the\nfake-cadaveric images with models trained on the real cadaveric dataset. The\nproposed method fully makes use of the labeled cadaveric dataset for live image\nsegmentation without the need to label the live dataset. LC-GAN has two\ngenerators with different architectures that leverage the deep feature\nrepresentation learned from the cadaveric image based segmentation task.\nMoreover, we propose the structural similarity loss and segmentation\nconsistency loss to improve the semantic consistency during translation. Our\nmodel achieves better image-to-image translation and leads to improved\nsegmentation performance in the proposed cross-domain segmentation task.\n",
    "topics": "{'Image-to-Image Translation': 1.0, 'Semantic Segmentation': 0.9892457, 'Unsupervised Image-To-Image Translation': 0.33275288}",
    "score": 0.7985443225
  },
  {
    "id": "1706.08658",
    "title": "Fast and accurate classification of echocardiograms using deep learning",
    "abstract": "  Echocardiography is essential to modern cardiology. However, human\ninterpretation limits high throughput analysis, limiting echocardiography from\nreaching its full clinical and research potential for precision medicine. Deep\nlearning is a cutting-edge machine-learning technique that has been useful in\nanalyzing medical images but has not yet been widely applied to\nechocardiography, partly due to the complexity of echocardiograms' multi view,\nmulti modality format. The essential first step toward comprehensive computer\nassisted echocardiographic interpretation is determining whether computers can\nlearn to recognize standard views. To this end, we anonymized 834,267\ntransthoracic echocardiogram (TTE) images from 267 patients (20 to 96 years, 51\npercent female, 26 percent obese) seen between 2000 and 2017 and labeled them\naccording to standard views. Images covered a range of real world clinical\nvariation. We built a multilayer convolutional neural network and used\nsupervised learning to simultaneously classify 15 standard views. Eighty\npercent of data used was randomly chosen for training and 20 percent reserved\nfor validation and testing on never seen echocardiograms. Using multiple images\nfrom each clip, the model classified among 12 video views with 97.8 percent\noverall test accuracy without overfitting. Even on single low resolution\nimages, test accuracy among 15 views was 91.7 percent versus 70.2 to 83.5\npercent for board-certified echocardiographers. Confusional matrices, occlusion\nexperiments, and saliency mapping showed that the model finds recognizable\nsimilarities among related views and classifies using clinically relevant image\nfeatures. In conclusion, deep neural networks can classify essential\nechocardiographic views simultaneously and with high accuracy. Our results\nprovide a foundation for more complex deep learning assisted echocardiographic\ninterpretation.\n",
    "topics": "{}",
    "score": 0.7984413724
  },
  {
    "id": "2009.10283",
    "title": "End-to-End Learning of Speech 2D Feature-Trajectory for Prosthetic Hands",
    "abstract": "  Speech is one of the most common forms of communication in humans. Speech\ncommands are essential parts of multimodal controlling of prosthetic hands. In\nthe past decades, researchers used automatic speech recognition systems for\ncontrolling prosthetic hands by using speech commands. Automatic speech\nrecognition systems learn how to map human speech to text. Then, they used\nnatural language processing or a look-up table to map the estimated text to a\ntrajectory. However, the performance of conventional speech-controlled\nprosthetic hands is still unsatisfactory. Recent advancements in\ngeneral-purpose graphics processing units (GPGPUs) enable intelligent devices\nto run deep neural networks in real-time. Thus, architectures of intelligent\nsystems have rapidly transformed from the paradigm of composite subsystems\noptimization to the paradigm of end-to-end optimization. In this paper, we\npropose an end-to-end convolutional neural network (CNN) that maps speech 2D\nfeatures directly to trajectories for prosthetic hands. The proposed\nconvolutional neural network is lightweight, and thus it runs in real-time in\nan embedded GPGPU. The proposed method can use any type of speech 2D feature\nthat has local correlations in each dimension such as spectrogram, MFCC, or\nPNCC. We omit the speech to text step in controlling the prosthetic hand in\nthis paper. The network is written in Python with Keras library that has a\nTensorFlow backend. We optimized the CNN for NVIDIA Jetson TX2 developer kit.\nOur experiment on this CNN demonstrates a root-mean-square error of 0.119 and\n20ms running time to produce trajectory outputs corresponding to the voice\ninput data. To achieve a lower error in real-time, we can optimize a similar\nCNN for a more powerful embedded GPGPU such as NVIDIA AGX Xavier.\n",
    "topics": "{}",
    "score": 0.7984092979
  },
  {
    "id": "2008.07712",
    "title": "Contact Area Detector using Cross View Projection Consistency for\n  COVID-19 Projects",
    "abstract": "  The ability to determine what parts of objects and surfaces people touch as\nthey go about their daily lives would be useful in understanding how the\nCOVID-19 virus spreads. To determine whether a person has touched an object or\nsurface using visual data, images, or videos, is a hard problem. Computer\nvision 3D reconstruction approaches project objects and the human body from the\n2D image domain to 3D and perform 3D space intersection directly. However, this\nsolution would not meet the accuracy requirement in applications due to\nprojection error. Another standard approach is to train a neural network to\ninfer touch actions from the collected visual data. This strategy would require\nsignificant amounts of training data to generalize over scale and viewpoint\nvariations. A different approach to this problem is to identify whether a\nperson has touched a defined object. In this work, we show that the solution to\nthis problem can be straightforward. Specifically, we show that the contact\nbetween an object and a static surface can be identified by projecting the\nobject onto the static surface through two different viewpoints and analyzing\ntheir 2D intersection. The object contacts the surface when the projected\npoints are close to each other; we call this cross view projection consistency.\nInstead of doing 3D scene reconstruction or transfer learning from deep\nnetworks, a mapping from the surface in the two camera views to the surface\nspace is the only requirement. For planar space, this mapping is the Homography\ntransformation. This simple method can be easily adapted to real-life\napplications. In this paper, we apply our method to do office occupancy\ndetection for studying the COVID-19 transmission pattern from an office desk in\na meeting room using the contact information.\n",
    "topics": "{'3D Reconstruction': 0.9984102, 'Transfer Learning': 0.9382888}",
    "score": 0.798360465
  },
  {
    "id": "1910.06761",
    "title": "Causal Mechanism Transfer Network for Time Series Domain Adaptation in\n  Mechanical Systems",
    "abstract": "  Data-driven models are becoming essential parts in modern mechanical systems,\ncommonly used to capture the behavior of various equipment and varying\nenvironmental characteristics. Despite the advantages of these data-driven\nmodels on excellent adaptivity to high dynamics and aging equipment, they are\nusually hungry to massive labels over historical data, mostly contributed by\nhuman engineers at an extremely high cost. The label demand is now the major\nlimiting factor to modeling accuracy, hindering the fulfillment of visions for\napplications. Fortunately, domain adaptation enhances the model generalization\nby utilizing the labelled source data as well as the unlabelled target data and\nthen we can reuse the model on different domains. However, the mainstream\ndomain adaptation methods cannot achieve ideal performance on time series data,\nbecause most of them focus on static samples and even the existing time series\ndomain adaptation methods ignore the properties of time series data, such as\ntemporal causal mechanism. In this paper, we assume that causal mechanism is\ninvariant and present our Causal Mechanism Transfer Network(CMTN) for time\nseries domain adaptation. By capturing and transferring the dynamic and\ntemporal causal mechanism of multivariate time series data and alleviating the\ntime lags and different value ranges among different machines, CMTN allows the\ndata-driven models to exploit existing data and labels from similar systems,\nsuch that the resulting model on a new system is highly reliable even with very\nlimited data. We report our empirical results and lessons learned from two\nreal-world case studies, on chiller plant energy optimization and boiler fault\ndetection, which outperforms the existing state-of-the-art method.\n",
    "topics": "{'Time Series': 0.9999976, 'Domain Adaptation': 0.99902785, 'Fault Detection': 0.9688854}",
    "score": 0.7982792758
  },
  {
    "id": "1811.07859",
    "title": "OrthoSeg: A Deep Multimodal Convolutional Neural Network for Semantic\n  Segmentation of Orthoimagery",
    "abstract": "  This paper addresses the task of semantic segmentation of orthoimagery using\nmultimodal data e.g. optical RGB, infrared and digital surface model. We\npropose a deep convolutional neural network architecture termed OrthoSeg for\nsemantic segmentation using multimodal, orthorectified and coregistered data.\nWe also propose a training procedure for supervised training of OrthoSeg. The\ntraining procedure complements the inherent architectural characteristics of\nOrthoSeg for preventing complex co-adaptations of learned features, which may\narise due to probable high dimensionality and spatial correlation in multimodal\nand/or multispectral coregistered data. OrthoSeg consists of parallel encoding\nnetworks for independent encoding of multimodal feature maps and a decoder\ndesigned for efficiently fusing independently encoded multimodal feature maps.\nA softmax layer at the end of the network uses the features generated by the\ndecoder for pixel-wise classification. The decoder fuses feature maps from the\nparallel encoders locally as well as contextually at multiple scales to\ngenerate per-pixel feature maps for final pixel-wise classification resulting\nin segmented output. We experimentally show the merits of OrthoSeg by\ndemonstrating state-of-the-art accuracy on the ISPRS Potsdam 2D Semantic\nSegmentation dataset. Adaptability is one of the key motivations behind\nOrthoSeg so that it serves as a useful architectural option for a wide range of\nproblems involving the task of semantic segmentation of coregistered multimodal\nand/or multispectral imagery. Hence, OrthoSeg is designed to enable independent\nscaling of parallel encoder networks and decoder network to better match\napplication requirements, such as the number of input channels, the effective\nfield-of-view, and model capacity.\n",
    "topics": "{'Semantic Segmentation': 0.9999999}",
    "score": 0.798273928
  },
  {
    "id": "1911.04052",
    "title": "Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic\n  Manipulation Dataset through Human Reasoning and Dexterity",
    "abstract": "  Large, richly annotated datasets have accelerated progress in fields such as\ncomputer vision and natural language processing, but replicating these\nsuccesses in robotics has been challenging. While prior data collection\nmethodologies such as self-supervision have resulted in large datasets, the\ndata can have poor signal-to-noise ratio. By contrast, previous efforts to\ncollect task demonstrations with humans provide better quality data, but they\ncannot reach the same data magnitude. Furthermore, neither approach places\nguarantees on the diversity of the data collected, in terms of solution\nstrategies. In this work, we leverage and extend the RoboTurk platform to scale\nup data collection for robotic manipulation using remote teleoperation. The\nprimary motivation for our platform is two-fold: (1) to address the\nshortcomings of prior work and increase the total quantity of manipulation data\ncollected through human supervision by an order of magnitude without\nsacrificing the quality of the data and (2) to collect data on challenging\nmanipulation tasks across several operators and observe a diverse set of\nemergent behaviors and solutions. We collected over 111 hours of robot\nmanipulation data across 54 users and 3 challenging manipulation tasks in 1\nweek, resulting in the largest robot dataset collected via remote\nteleoperation. We evaluate the quality of our platform, the diversity of\ndemonstrations in our dataset, and the utility of our dataset via quantitative\nand qualitative analysis. For additional results, supplementary videos, and to\ndownload our dataset, visit http://roboturk.stanford.edu/realrobotdataset .\n",
    "topics": "{}",
    "score": 0.7982375219
  },
  {
    "id": "1810.00314",
    "title": "CODIT: Code Editing with Tree-Based Neural Models",
    "abstract": "  The way developers edit day-to-day code tends to be repetitive, often using\nexisting code elements. Many researchers have tried to automate repetitive code\nchanges by learning from specific change templates which are applied to limited\nscope. The advancement of deep neural networks and the availability of vast\nopen-source evolutionary data opens up the possibility of automatically\nlearning those templates from the wild. However, deep neural network based\nmodeling for code changes and code in general introduces some specific problems\nthat needs specific attention from research community. For instance, compared\nto natural language, source code vocabulary can be significantly larger.\nFurther, good changes in code do not break its syntactic structure. Thus,\ndeploying state-of-the-art neural network models without adapting the methods\nto the source code domain yields sub-optimal results. To this end, we propose a\nnovel tree-based neural network system to model source code changes and learn\ncode change patterns from the wild. Specifically, we propose a tree-based\nneural machine translation model to learn the probability distribution of\nchanges in code. We realize our model with a change suggestion engine, CODIT,\nand train the model with more than 24k real-world changes and evaluate it on 5k\npatches. Our evaluation shows the effectiveness of CODITin learning and\nsuggesting patches. CODIT can also learn specific bug fix pattern from bug\nfixing patches and can fix 25 bugs out of 80 bugs in Defects4J.\n",
    "topics": "{'Code Generation': 0.8784386}",
    "score": 0.7982295522
  },
  {
    "id": "2002.03736",
    "title": "Universal Semantic Segmentation for Fisheye Urban Driving Images",
    "abstract": "  Semantic segmentation is a critical method in the field of autonomous\ndriving. When performing semantic image segmentation, a wider field of view\n(FoV) helps to obtain more information about the surrounding environment,\nmaking automatic driving safer and more reliable, which could be offered by\nfisheye cameras. However, large public fisheye datasets are not available, and\nthe fisheye images captured by the fisheye camera with large FoV comes with\nlarge distortion, so commonly-used semantic segmentation model cannot be\ndirectly utilized. In this paper, a seven degrees of freedom (DoF) augmentation\nmethod is proposed to transform rectilinear image to fisheye image in a more\ncomprehensive way. In the training process, rectilinear images are transformed\ninto fisheye images in seven DoF, which simulates the fisheye images taken by\ncameras of different positions, orientations and focal lengths. The result\nshows that training with the seven-DoF augmentation can improve the model's\naccuracy and robustness against different distorted fisheye data. This\nseven-DoF augmentation provides a universal semantic segmentation solution for\nfisheye cameras in different autonomous driving applications. Also, we provide\nspecific parameter settings of the augmentation for autonomous driving. At\nlast, we tested our universal semantic segmentation model on real fisheye\nimages and obtained satisfactory results. The code and configurations are\nreleased at https://github.com/Yaozhuwa/FisheyeSeg.\n",
    "topics": "{'Semantic Segmentation': 1.0, 'Autonomous Driving': 1.0}",
    "score": 0.7981815182
  },
  {
    "id": "1403.4777",
    "title": "MFCC based Enlargement of the Training Set for Emotion Recognition in\n  Speech",
    "abstract": "  Emotional state recognition through speech is being a very interesting\nresearch topic nowadays. Using subliminal information of speech, denominated as\nprosody, it is possible to recognize the emotional state of the person. One of\nthe main problems in the design of automatic emotion recognition systems is the\nsmall number of available patterns. This fact makes the learning process more\ndifficult, due to the generalization problems that arise under these\nconditions. In this work we propose a solution to this problem consisting in\nenlarging the training set through the creation the new virtual patterns. In\nthe case of emotional speech, most of the emotional information is included in\nspeed and pitch variations. So, a change in the average pitch that does not\nmodify neither the speed nor the pitch variations does not affect the expressed\nemotion. Thus, we use this prior information in order to create new patterns\napplying a gender dependent pitch shift modification in the feature extraction\nprocess of the classification system. For this purpose, we propose a frequency\nscaling modification of the Mel Frequency Cepstral Coefficients, used to\nclassify the emotion. For this purpose, we propose a gender dependent frequency\nscaling modification. This proposed process allows us to synthetically increase\nthe number of available patterns in the training set, thus increasing the\ngeneralization capability of the system and reducing the test error. Results\ncarried out with two different classifiers with different degree of\ngeneralization capability demonstrate the suitability of the proposal.\n",
    "topics": "{'Emotion Recognition': 0.9997379}",
    "score": 0.7980872696
  },
  {
    "id": "1809.04212",
    "title": "Hyperspectral Image Classification in the Presence of Noisy Labels",
    "abstract": "  Label information plays an important role in supervised hyperspectral image\nclassification problem. However, current classification methods all ignore an\nimportant and inevitable problem---labels may be corrupted and collecting clean\nlabels for training samples is difficult, and often impractical. Therefore, how\nto learn from the database with noisy labels is a problem of great practical\nimportance. In this paper, we study the influence of label noise on\nhyperspectral image classification, and develop a random label propagation\nalgorithm (RLPA) to cleanse the label noise. The key idea of RLPA is to exploit\nknowledge (e.g., the superpixel based spectral-spatial constraints) from the\nobserved hyperspectral images and apply it to the process of label propagation.\nSpecifically, RLPA first constructs a spectral-spatial probability transfer\nmatrix (SSPTM) that simultaneously considers the spectral similarity and\nsuperpixel based spatial information. It then randomly chooses some training\nsamples as \"clean\" samples and sets the rest as unlabeled samples, and\npropagates the label information from the \"clean\" samples to the rest unlabeled\nsamples with the SSPTM. By repeating the random assignment (of \"clean\" labeled\nsamples and unlabeled samples) and propagation, we can obtain multiple labels\nfor each training sample. Therefore, the final propagated label can be\ncalculated by a majority vote algorithm. Experimental studies show that RLPA\ncan reduce the level of noisy label and demonstrates the advantages of our\nproposed method over four major classifiers with a significant margin---the\ngains in terms of the average OA, AA, Kappa are impressive, e.g., 9.18%, 9.58%,\nand 0.1043. The Matlab source code is available at\nhttps://github.com/junjun-jiang/RLPA\n",
    "topics": "{'Hyperspectral Image Classification': 0.9999995, 'Image Classification': 0.9999987}",
    "score": 0.7980442843
  },
  {
    "id": "1807.07769",
    "title": "Physical Adversarial Examples for Object Detectors",
    "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial\nexamples-maliciously crafted inputs that cause DNNs to make incorrect\npredictions. Recent work has shown that these attacks generalize to the\nphysical domain, to create perturbations on physical objects that fool image\nclassifiers under a variety of real-world conditions. Such attacks pose a risk\nto deep learning models used in safety-critical cyber-physical systems. In this\nwork, we extend physical attacks to more challenging object detection models, a\nbroader class of deep learning algorithms widely used to detect and label\nmultiple objects within a scene. Improving upon a previous physical attack on\nimage classifiers, we create perturbed physical objects that are either ignored\nor mislabeled by object detection models. We implement a Disappearance Attack,\nin which we cause a Stop sign to \"disappear\" according to the detector-either\nby covering thesign with an adversarial Stop sign poster, or by adding\nadversarial stickers onto the sign. In a video recorded in a controlled lab\nenvironment, the state-of-the-art YOLOv2 detector failed to recognize these\nadversarial Stop signs in over 85% of the video frames. In an outdoor\nexperiment, YOLO was fooled by the poster and sticker attacks in 72.5% and\n63.5% of the video frames respectively. We also use Faster R-CNN, a different\nobject detection model, to demonstrate the transferability of our adversarial\nperturbations. The created poster perturbation is able to fool Faster R-CNN in\n85.9% of the video frames in a controlled lab environment, and 40.2% of the\nvideo frames in an outdoor environment. Finally, we present preliminary results\nwith a new Creation Attack, where in innocuous physical stickers fool a model\ninto detecting nonexistent objects.\n",
    "topics": "{'Object Detection': 0.9995388}",
    "score": 0.7979692545
  },
  {
    "id": "1906.05675",
    "title": "Privacy-Preserving Deep Action Recognition: An Adversarial Learning\n  Framework and A New Dataset",
    "abstract": "  We investigate privacy-preserving, video-based action recognition in deep\nlearning, a problem with growing importance in smart camera applications. A\nnovel adversarial training framework is formulated to learn an anonymization\ntransform for input videos such that the trade-off between target utility task\nperformance and the associated privacy budgets is explicitly optimized on the\nanonymized videos. Notably, the privacy budget, often defined and measured in\ntask-driven contexts, cannot be reliably indicated using any single model\nperformance because strong protection of privacy should sustain against any\nmalicious model that tries to steal private information. To tackle this\nproblem, we propose two new optimization strategies of model restarting and\nmodel ensemble to achieve stronger universal privacy protection against any\nattacker models. Extensive experiments have been carried out and analyzed. On\nthe other hand, given few public datasets available with both utility and\nprivacy labels, the data-driven (supervised) learning cannot exert its full\npower on this task. We first discuss an innovative heuristic of cross-dataset\ntraining and evaluation, enabling the use of multiple single-task datasets (one\nwith target task labels and the other with privacy labels) in our problem. To\nfurther address this dataset challenge, we have constructed a new dataset,\ntermed PA-HMDB51, with both target task labels (action) and selected privacy\nattributes (skin color, face, gender, nudity, and relationship) annotated on a\nper-frame basis. This first-of-its-kind video dataset and evaluation protocol\ncan greatly facilitate visual privacy research and open up other opportunities.\nOur codes, models, and the PA-HMDB51 dataset are available at\nhttps://github.com/VITA-Group/PA-HMDB51.\n",
    "topics": "{'Action Recognition': 0.9999629}",
    "score": 0.7979646378
  },
  {
    "id": "1804.10462",
    "title": "A generalizable approach for multi-view 3D human pose regression",
    "abstract": "  Despite the significant improvement in the performance of monocular pose\nestimation approaches and their ability to generalize to unseen environments,\nmulti-view (MV) approaches are often lagging behind in terms of accuracy and\nare specific to certain datasets. This is mainly due to the fact that (1)\ncontrary to real world single-view (SV) datasets, MV datasets are often\ncaptured in controlled environments to collect precise 3D annotations, which do\nnot cover all real world challenges, and (2) the model parameters are learned\nfor specific camera setups. To alleviate these problems, we propose a two-stage\napproach to detect and estimate 3D human poses, which separates SV pose\ndetection from MV 3D pose estimation. This separation enables us to utilize\neach dataset for the right task, i.e. SV datasets for constructing robust pose\ndetection models and MV datasets for constructing precise MV 3D regression\nmodels. In addition, our 3D regression approach only requires 3D pose data and\nits projections to the views for building the model, hence removing the need\nfor collecting annotated data from the test setup. Our approach can therefore\nbe easily generalized to a new environment by simply projecting 3D poses into\n2D during training according to the camera setup used at test time. As 2D poses\nare collected at test time using a SV pose detector, which might generate\ninaccurate detections, we model its characteristics and incorporate this\ninformation during training. We demonstrate that incorporating the detector's\ncharacteristics is important to build a robust 3D regression model and that the\nresulting regression model generalizes well to new MV environments. Our\nevaluation results show that our approach achieves competitive results on the\nHuman3.6M dataset and significantly improves results on a MV clinical dataset\nthat is the first MV dataset generated from live surgery recordings.\n",
    "topics": "{'Pose Estimation': 0.9995914, '3D Pose Estimation': 0.7373606}",
    "score": 0.7979296722
  },
  {
    "id": "1812.01737",
    "title": "Decompose to manipulate: Manipulable Object Synthesis in 3D Medical\n  Images with Structured Image Decomposition",
    "abstract": "  The performance of medical image analysis systems is constrained by the\nquantity of high-quality image annotations. Such systems require data to be\nannotated by experts with years of training, especially when diagnostic\ndecisions are involved. Such datasets are thus hard to scale up. In this\ncontext, it is hard for supervised learning systems to generalize to the cases\nthat are rare in the training set but would be present in real-world clinical\npractices. We believe that the synthetic image samples generated by a system\ntrained on the real data can be useful for improving the supervised learning\ntasks in the medical image analysis applications. Allowing the image synthesis\nto be manipulable could help synthetic images provide complementary information\nto the training data rather than simply duplicating the real-data manifold. In\nthis paper, we propose a framework for synthesizing 3D objects, such as\npulmonary nodules, in 3D medical images with manipulable properties. The\nmanipulation is enabled by decomposing of the object of interests into its\nsegmentation mask and a 1D vector containing the residual information. The\nsynthetic object is refined and blended into the image context with two\nadversarial discriminators. We evaluate the proposed framework on lung nodules\nin 3D chest CT images and show that the proposed framework could generate\nrealistic nodules with manipulable shapes, textures and locations, etc. By\nsampling from both the synthetic nodules and the real nodules from 2800 3D CT\nvolumes during the classifier training, we show the synthetic patches could\nimprove the overall nodule detection performance by average 8.44% competition\nperformance metric (CPM) score.\n",
    "topics": "{'Image Generation': 0.9897424}",
    "score": 0.7977908931
  },
  {
    "id": "2009.14416",
    "title": "Efficient Kernel Transfer in Knowledge Distillation",
    "abstract": "  Knowledge distillation is an effective way for model compression in deep\nlearning. Given a large model (i.e., teacher model), it aims to improve the\nperformance of a compact model (i.e., student model) by transferring the\ninformation from the teacher. An essential challenge in knowledge distillation\nis to identify the appropriate information to transfer. In early works, only\nthe final output of the teacher model is used as the soft label to help the\ntraining of student models. Recently, the information from intermediate layers\nis also adopted for better distillation. In this work, we aim to optimize the\nprocess of knowledge distillation from the perspective of kernel matrix. The\noutput of each layer in a neural network can be considered as a new feature\nspace generated by applying a kernel function on original images. Hence, we\npropose to transfer the corresponding kernel matrix (i.e., Gram matrix) from\nteacher models to student models for distillation. However, the size of the\nwhole kernel matrix is quadratic to the number of examples. To improve the\nefficiency, we decompose the original kernel matrix with Nystr{\\\"{o}}m method\nand then transfer the partial matrix obtained with landmark points, whose size\nis linear in the number of examples. More importantly, our theoretical analysis\nshows that the difference between the original kernel matrices of teacher and\nstudent can be well bounded by that of their corresponding partial matrices.\nFinally, a new strategy of generating appropriate landmark points is proposed\nfor better distillation. The empirical study on benchmark data sets\ndemonstrates the effectiveness of the proposed algorithm. Code will be\nreleased.\n",
    "topics": "{'Model Compression': 0.9999703}",
    "score": 0.7976652511
  },
  {
    "id": "1805.07621",
    "title": "CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule\n  Subspaces",
    "abstract": "  In this paper, we formalize the idea behind capsule nets of using a capsule\nvector rather than a neuron activation to predict the label of samples. To this\nend, we propose to learn a group of capsule subspaces onto which an input\nfeature vector is projected. Then the lengths of resultant capsules are used to\nscore the probability of belonging to different classes. We train such a\nCapsule Projection Network (CapProNet) by learning an orthogonal projection\nmatrix for each capsule subspace, and show that each capsule subspace is\nupdated until it contains input feature vectors corresponding to the associated\nclass. We will also show that the capsule projection can be viewed as\nnormalizing the multiple columns of the weight matrix simultaneously to form an\northogonal basis, which makes it more effective in incorporating novel\ncomponents of input features to update capsule representations. In other words,\nthe capsule projection can be viewed as a multi-dimensional weight\nnormalization in capsule subspaces, where the conventional weight normalization\nis simply a special case of the capsule projection onto 1D lines. Only a small\nnegligible computing overhead is incurred to train the network in\nlow-dimensional capsule subspaces or through an alternative hyper-power\niteration to estimate the normalization matrix. Experiment results on image\ndatasets show the presented model can greatly improve the performance of the\nstate-of-the-art ResNet backbones by $10-20\\%$ and that of the Densenet by\n$5-7\\%$ respectively at the same level of computing and memory expenses. The\nCapProNet establishes the competitive state-of-the-art performance for the\nfamily of capsule nets by significantly reducing test errors on the benchmark\ndatasets.\n",
    "topics": "{'Image Classification': 0.447492}",
    "score": 0.7975631879
  },
  {
    "id": "2004.14753",
    "title": "Real-World Textured Things: a Repository of Textured Models Generated\n  with Modern Photo-Reconstruction Tools",
    "abstract": "  We are witnessing a proliferation of textured 3D models captured from the\nreal world with automatic photo-reconstruction tools. Digital 3D models of this\nclass come with a unique set of characteristics and defects -- especially\nconcerning their parametrization -- setting them starkly apart from 3D models\noriginating from other, more traditional, sources. We study this class of 3D\nmodels by collecting a significant number of representatives and quantitatively\nevaluating their quality according to several metrics. These include a new\ninvariant metric we design to assess the fragmentation of the UV map, one of\nthe main weaknesses hindering the usability of these models. Our results back\nthe widely shared notion that such models are not fit for direct use in\ndownstream applications (such as videogames), and require challenging\nprocessing steps. Regrettably, existing automatic geometry processing tools are\nnot always up to the task: for example, we verify that available tools for UV\noptimization often fail due mesh inconsistencies, geometric and topological\nnoise, excessive resolution, or other factors; moreover, even when an output is\nproduced, it is rarely a significant improvement over the input (according to\nthe aforementioned measures). Therefore, we argue that further advancements are\nrequired specifically targeted at this class of models. Towards this goal, we\nshare the models we collected in the form of a new public repository,\nReal-World Textured Things (RWTT), a benchmark to systematic field-test and\ncompare algorithms. RWTT consists of 568 carefully selected textured 3D models\nrepresentative of all the main modern off-the-shelf photo-reconstruction tools.\nThe repository is available at http://texturedmesh.isti.cnr.it/ and is\nbrowsable by metadata collected during experiments, and comes with a tool,\nTexMetro, providing the same set of measures for generic UV mapped datasets.\n",
    "topics": "{}",
    "score": 0.7975297187
  },
  {
    "id": "2007.08566",
    "title": "SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses\n  for Mobile Platforms",
    "abstract": "  Virtual applications through mobile platforms are one of the most critical\nand ever-growing fields in AI, where ubiquitous and real-time person\nauthentication has become critical after the breakthrough of all services\nprovided via mobile devices. In this context, face verification technologies\ncan provide reliable and robust user authentication, given the availability of\ncameras in these devices, as well as their widespread use in everyday\napplications. The rapid development of deep Convolutional Neural Networks has\nresulted in many accurate face verification architectures. However, their\ntypical size (hundreds of megabytes) makes them infeasible to be incorporated\nin downloadable mobile applications where the entire file typically may not\nexceed 100 Mb. Accordingly, we address the challenge of developing a\nlightweight face recognition network of just a few megabytes that can operate\nwith sufficient accuracy in comparison to much larger models. The network also\nshould be able to operate under different poses, given the variability\nnaturally observed in uncontrolled environments where mobile devices are\ntypically used. In this paper, we adapt the lightweight SqueezeNet model, of\njust 4.4MB, to effectively provide cross-pose face recognition. After trained\non the MS-Celeb-1M and VGGFace2 databases, our model achieves an EER of 1.23%\non the difficult frontal vs. profile comparison, and0.54% on profile vs.\nprofile images. Under less extreme variations involving frontal images in any\nof the enrolment/query images pair, EER is pushed down to<0.3%, and the FRR at\nFAR=0.1%to less than 1%. This makes our light model suitable for face\nrecognition where at least acquisition of the enrolment image can be\ncontrolled. At the cost of a slight degradation in performance, we also test an\neven lighter model (of just 2.5MB) where regular convolutions are replaced with\ndepth-wise separable convolutions.\n",
    "topics": "{'Face Verification': 1.0, 'Face Recognition': 1.0}",
    "score": 0.7974936825
  },
  {
    "id": "1908.01308",
    "title": "Theme Aware Aesthetic Distribution Prediction with Full Resolution\n  Photos",
    "abstract": "  Aesthetic quality assessment (AQA) of photos is a challenging task due to the\nsubjective and diverse factors in human assessment process. Nowadays, it is\ncommon to tackle AQA with deep neural networks (DNNs) for their superior\nperformance on modeling such complex relations. However, traditional DNNs\nrequire fix-sized inputs, and resizing various inputs to a uniform size may\nsignificantly change their aesthetic features. Such transformations lead to the\nmismatches between photos and their aesthetic evaluations. Existing methods\nusually adopt two solutions for it. Some methods directly crop fix-sized\npatches from the inputs. The others alternately capture the aesthetic features\nfrom pre-defined multi-size inputs by inserting adaptive pooling or removing\nfully connected layers. However, the former destroys the global structures and\nlayout information, which are crucial in most situations. The latter has to\nresize images into several pre-defined sizes, which is not enough to reflect\nthe diversity of image sizes, and the aesthetic features are still destroyed.\nTo address this issue, we propose a simple and effective method that can handle\nthe arbitrary sizes of batch inputs to achieve AQA on the full resolution\nimages by combining image padding with ROI (region of interest) pooling.\nPadding keeps inputs of the same size, while ROI pooling cuts off the forward\npropagation of features on padding regions, thus eliminates the side effects of\npadding. Besides, we observe that the same image may receive different scores\nunder different themes, which we call the theme criterion bias. However,\nprevious works only focus on the aesthetic features of the images and ignore\nthe criterion bias brought by their themes. In this paper, we introduce the\ntheme information and propose a theme aware model. Extensive experiments prove\nthe effectiveness of the proposed method over the state-of-the-arts.\n",
    "topics": "{}",
    "score": 0.7973623333
  },
  {
    "id": "2008.06837",
    "title": "Open source tools for management and archiving of digital microscopy\n  data to allow integration with patient pathology and treatment information",
    "abstract": "  Virtual microscopy includes digitisation of histology slides and the use of\ncomputer technologies for complex investigation of diseases such as cancer.\nHowever, automated image analysis, or website publishing of such digital\nimages, is hampered by their large file sizes. We have developed two Java based\nopen source tools: Snapshot Creator and NDPI-Splitter. Snapshot Creator\nconverts a portion of a large digital slide into a desired quality JPEG image.\nThe image is linked to the patients clinical and treatment information in a\ncustomised open source cancer data management software (Caisis) in use at the\nAustralian Breast Cancer Tissue Bank (ABCTB) and then published on the ABCTB\nwebsite www.abctb.org.au using Deep Zoom open source technology. Using the\nABCTB online search engine, digital images can be searched by defining various\ncriteria such as cancer type, or biomarkers expressed. NDPI-Splitter splits a\nlarge image file into smaller sections of TIFF images so that they can be\neasily analysed by image analysis software such as Metamorph or Matlab.\nNDPI-Splitter also has the capacity to filter out empty images. Snapshot\nCreator and NDPI-Splitter are novel open source Java tools. They convert\ndigital slides into files of smaller size for further processing. In\nconjunction with other open source tools such as Deep Zoom and Caisis, this\nsuite of tools is used for the management and archiving of digital microscopy\nimages, enabling digitised images to be explored and zoomed online. Our online\nimage repository also has the capacity to be used as a teaching resource. These\ntools also enable large files to be sectioned for image analysis.\n",
    "topics": "{'whole slide images': 0.57322276}",
    "score": 0.7973103481
  },
  {
    "id": "2006.06325",
    "title": "CoMIR: Contrastive Multimodal Image Representation for Registration",
    "abstract": "  We propose contrastive coding to learn shared, dense image representations,\nreferred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs\nenable the registration of multimodal images where existing registration\nmethods often fail due to a lack of sufficiently similar image structures.\nCoMIRs reduce the multimodal registration problem to a monomodal one in which\ngeneral intensity-based, as well as feature-based, registration algorithms can\nbe applied. The method involves training one neural network per modality on\naligned images, using a contrastive loss based on noise-contrastive estimation\n(InfoNCE). Unlike other contrastive coding methods, used for e.g.\nclassification, our approach generates image-like representations that contain\nthe information shared between modalities. We introduce a novel,\nhyperparameter-free modification to InfoNCE, to enforce rotational equivariance\nof the learnt representations, a property essential to the registration task.\nWe assess the extent of achieved rotational equivariance and the stability of\nthe representations with respect to weight initialization, training set, and\nhyperparameter settings, on a remote sensing dataset of RGB and near-infrared\nimages. We evaluate the learnt representations through registration of a\nbiomedical dataset of bright-field and second-harmonic generation microscopy\nimages; two modalities with very little apparent correlation. The proposed\napproach based on CoMIRs significantly outperforms registration of\nrepresentations created by GAN-based image-to-image translation, as well as a\nstate-of-the-art, application-specific method which takes additional knowledge\nabout the data into account. Code is available at:\nhttps://github.com/dqiamsdoayehccdvulyy/CoMIR.\n",
    "topics": "{'Point Cloud Registration': 0.76871777, 'Image-to-Image Translation': 0.47380063}",
    "score": 0.7972734924
  },
  {
    "id": "1703.08705",
    "title": "Comparing Rule-Based and Deep Learning Models for Patient Phenotyping",
    "abstract": "  Objective: We investigate whether deep learning techniques for natural\nlanguage processing (NLP) can be used efficiently for patient phenotyping.\nPatient phenotyping is a classification task for determining whether a patient\nhas a medical condition, and is a crucial part of secondary analysis of\nhealthcare data. We assess the performance of deep learning algorithms and\ncompare them with classical NLP approaches.\n  Materials and Methods: We compare convolutional neural networks (CNNs),\nn-gram models, and approaches based on cTAKES that extract pre-defined medical\nconcepts from clinical notes and use them to predict patient phenotypes. The\nperformance is tested on 10 different phenotyping tasks using 1,610 discharge\nsummaries extracted from the MIMIC-III database.\n  Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The\naverage F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our\nmodel having an F1-score up to 37 points higher than alternative approaches. We\nadditionally assess the interpretability of our model by presenting a method\nthat extracts the most salient phrases for a particular prediction.\n  Conclusion: We show that NLP methods based on deep learning improve the\nperformance of patient phenotyping. Our CNN-based algorithm automatically\nlearns the phrases associated with each patient phenotype. As such, it reduces\nthe annotation complexity for clinical domain experts, who are normally\nrequired to develop task-specific annotation rules and identify relevant\nphrases. Our method performs well in terms of both performance and\ninterpretability, which indicates that deep learning is an effective approach\nto patient phenotyping based on clinicians' notes.\n",
    "topics": "{}",
    "score": 0.7972394507
  },
  {
    "id": "1904.06969",
    "title": "Segmenting Potentially Cancerous Areas in Prostate Biopsies using\n  Semi-Automatically Annotated Data",
    "abstract": "  Gleason grading specified in ISUP 2014 is the clinical standard in staging\nprostate cancer and the most important part of the treatment decision. However,\nthe grading is subjective and suffers from high intra and inter-user\nvariability. To improve the consistency and objectivity in the grading, we\nintroduced glandular tissue WithOut Basal cells (WOB) as the ground truth. The\npresence of basal cells is the most accepted biomarker for benign glandular\ntissue and the absence of basal cells is a strong indicator of acinar prostatic\nadenocarcinoma, the most common form of prostate cancer. Glandular tissue can\nobjectively be assessed as WOB or not WOB by using specific immunostaining for\nglandular tissue (Cytokeratin 8/18) and for basal cells (Cytokeratin 5/6 +\np63). Even more, WOB allowed us to develop a semi-automated data generation\npipeline to speed up the tremendously time consuming and expensive process of\nannotating whole slide images by pathologists. We generated 295 prostatectomy\nimages exhaustively annotated with WOB. Then we used our Deep Learning\nFramework, which achieved the $2^{nd}$ best reported score in Camelyon17\nChallenge, to train networks for segmenting WOB in needle biopsies. Evaluation\nof the model on 63 needle biopsies showed promising results which were improved\nfurther by finetuning the model on 118 biopsies annotated with WOB, achieving\nF1-score of 0.80 and Precision-Recall AUC of 0.89 at the pixel-level. Then we\ncompared the performance of the model against 17 biopsies annotated\nindependently by 3 pathologists using only H\\&E staining. The comparison\ndemonstrated that the model performed on a par with the pathologists. Finally,\nthe model detected and accurately outlined existing WOB areas in two biopsies\nincorrectly annotated as totally WOB-free biopsies by three pathologists and in\none biopsy by two pathologists.\n",
    "topics": "{'whole slide images': 0.9999093}",
    "score": 0.797213964
  },
  {
    "id": "2010.04889",
    "title": "Deep Active Learning for Joint Classification & Segmentation with Weak\n  Annotator",
    "abstract": "  CNN visualization and interpretation methods, like class activation maps\n(CAMs), are typically used to highlight the image regions linked to the class\npredictions. These models allow to simultaneously classify images and yield\npixel-wise localization scores, without the need for costly pixel-level\nannotations. However, they are prone to high false positive localization, and\nthus poor visualisations when processing challenging images, such as histology\nimages for cancer grading and localization. In this paper, an active learning\n(AL) framework is proposed to alleviate this issue by progressively integrating\npixel-wise annotation during training. Given training data with global\nclass-level labels, our deep weakly-supervised learning (WSL) model\nsimultaneously allows for supervised learning for classification, and active\nlearning for segmentation of images selected for pixel-level annotation by an\noracle. Unlike traditional AL methods that focus on acquisition method, we also\npropose leveraging the unlabeled images to improve model accuracy with less\noracle-annotation. To this end, self-learning is considered where the model is\nused to pseudo-annotate a large number of relevant unlabeled samples, which are\nthen integrated during the learning process with oracle-annotated samples. Our\nextensive experiments are conducted on complex high resolution medical and\nnatural images from two benchmark datasets -- GlaS for colon cancer, and\nCUB-200-2011 for bird species. Results indicate that by using simply random\nacquisition, our approach can significantly outperform segmentation obtained\nwith state-of the-art CAMs and AL methods, using an identical\noracle-supervision budget. Our method provides an efficient solution to improve\nthe regions of interest (ROI) segmentation accuracy for real-world visual\nrecognition applications.\n",
    "topics": "{'Active Learning': 0.99999523}",
    "score": 0.7971768816
  },
  {
    "id": "1910.09359",
    "title": "Separable Convolutional Eigen-Filters (SCEF): Building Efficient CNNs\n  Using Redundancy Analysis",
    "abstract": "  Deep Convolutional Neural Networks (CNNs) have been widely used in computer\nvision due to its effectiveness. While the high model complexity of CNN enables\nremarkable learning capacity, the large number of trainable parameters comes\nwith a high cost. In addition to the demand of a large amount of resources, the\nhigh complexity of the network can result in a high variance in its\ngeneralization performance from a statistical learning theory perspective. One\nway to reduce the complexity of a network without sacrificing its accuracy is\nto define and identify redundancies in order to remove them. In this work, we\npropose a method to observe and analyze redundancies in the weights of 2D\nconvolutional (Conv2D) filters. From our experiments, we observe that 1) the\nvectorized Conv2D filters exhibit low rank behaviors; 2) the effective ranks of\nthese filters typically decrease when the network goes deeper, and 3) these\neffective ranks are converging over training steps. Inspired by these\nobservations, we propose a new layer called Separable Convolutional\nEigen-Filters (SCEF) as an alternative parameterization to Conv2D filters. A\nSCEF layer can be easily implemented using the depthwise separable convolutions\ntrained with our proposed training strategy. In addition to the decreased\nnumber of trainable parameters by using SCEF, depthwise separable convolutions\nare known to be more computationally efficient compared to Conv2D operations,\nwhich reduces the runtime FLOPs as well. Experiments are conducted on the\nCIFAR-10 and ImageNet datasets by replacing the Conv2D layers with SCEF. The\nresults have shown an increased accuracy using about 2/3 of the original\nparameters and reduce the number of FLOPs to 2/3 of the base net.\n",
    "topics": "{}",
    "score": 0.7971053206
  },
  {
    "id": "1909.06993",
    "title": "Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal\n  Representations",
    "abstract": "  Machines are a long way from robustly solving open-world perception-control\ntasks, such as first-person view (FPV) aerial navigation. While recent advances\nin end-to-end Machine Learning, especially Imitation and Reinforcement Learning\nappear promising, they are constrained by the need of large amounts of\ndifficult-to-collect labeled real-world data. Simulated data, on the other\nhand, is easy to generate, but generally does not render safe behaviors in\ndiverse real-life scenarios. In this work we propose a novel method for\nlearning robust visuomotor policies for real-world deployment which can be\ntrained purely with simulated data. We develop rich state representations that\ncombine supervised and unsupervised environment data. Our approach takes a\ncross-modal perspective, where separate modalities correspond to the raw camera\ndata and the system states relevant to the task, such as the relative pose of\ngates to the drone in the case of drone racing. We feed both data modalities\ninto a novel factored architecture, which learns a joint low-dimensional\nembedding via Variational Auto Encoders. This compact representation is then\nfed into a control policy, which we trained using imitation learning with\nexpert trajectories in a simulator. We analyze the rich latent spaces learned\nwith our proposed representations, and show that the use of our cross-modal\narchitecture significantly improves control policy performance as compared to\nend-to-end learning or purely unsupervised feature extractors. We also present\nreal-world results for drone navigation through gates in different track\nconfigurations and environmental conditions. Our proposed method, which runs\nfully onboard, can successfully generalize the learned representations and\npolicies across simulation and reality, significantly outperforming baseline\napproaches.\n  Supplementary video: https://youtu.be/VKc3A5HlUU8\n",
    "topics": "{'Imitation Learning': 0.97582686}",
    "score": 0.7969029214
  },
  {
    "id": "1706.01805",
    "title": "SegAN: Adversarial Network with Multi-scale $L_1$ Loss for Medical Image\n  Segmentation",
    "abstract": "  Inspired by classic generative adversarial networks (GAN), we propose a novel\nend-to-end adversarial neural network, called SegAN, for the task of medical\nimage segmentation. Since image segmentation requires dense, pixel-level\nlabeling, the single scalar real/fake output of a classic GAN's discriminator\nmay be ineffective in producing stable and sufficient gradient feedback to the\nnetworks. Instead, we use a fully convolutional neural network as the segmentor\nto generate segmentation label maps, and propose a novel adversarial critic\nnetwork with a multi-scale $L_1$ loss function to force the critic and\nsegmentor to learn both global and local features that capture long- and\nshort-range spatial relationships between pixels. In our SegAN framework, the\nsegmentor and critic networks are trained in an alternating fashion in a\nmin-max game: The critic takes as input a pair of images, (original_image $*$\npredicted_label_map, original_image $*$ ground_truth_label_map), and then is\ntrained by maximizing a multi-scale loss function; The segmentor is trained\nwith only gradients passed along by the critic, with the aim to minimize the\nmulti-scale loss function. We show that such a SegAN framework is more\neffective and stable for the segmentation task, and it leads to better\nperformance than the state-of-the-art U-net segmentation method. We tested our\nSegAN method using datasets from the MICCAI BRATS brain tumor segmentation\nchallenge. Extensive experimental results demonstrate the effectiveness of the\nproposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance\ncomparable to the state-of-the-art for whole tumor and tumor core segmentation\nwhile achieves better precision and sensitivity for Gd-enhance tumor core\nsegmentation; on BRATS 2015 SegAN achieves better performance than the\nstate-of-the-art in both dice score and precision.\n",
    "topics": "{'Tumor Segmentation': 1.0, 'Medical Image Segmentation': 1.0, 'Brain Tumor Segmentation': 0.99999917, 'Semantic Segmentation': 0.9999379}",
    "score": 0.7968681432
  },
  {
    "id": "1705.02743",
    "title": "ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition",
    "abstract": "  In this paper, we introduce a new and challenging large-scale food image\ndataset called \"ChineseFoodNet\", which aims to automatically recognizing\npictured Chinese dishes. Most of the existing food image datasets collected\nfood images either from recipe pictures or selfie. In our dataset, images of\neach food category of our dataset consists of not only web recipe and menu\npictures but photos taken from real dishes, recipe and menu as well.\nChineseFoodNet contains over 180,000 food photos of 208 categories, with each\ncategory covering a large variations in presentations of same Chinese food. We\npresent our efforts to build this large-scale image dataset, including food\ncategory selection, data collection, and data clean and label, in particular\nhow to use machine learning methods to reduce manual labeling work that is an\nexpensive process. We share a detailed benchmark of several state-of-the-art\ndeep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose\na novel two-step data fusion approach referred as \"TastyNet\", which combines\nprediction results from different CNNs with voting method. Our proposed\napproach achieves top-1 accuracies of 81.43% on the validation set and 81.55%\non the test set, respectively. The latest dataset is public available for\nresearch and can be achieved at https://sites.google.com/view/chinesefoodnet.\n",
    "topics": "{}",
    "score": 0.7968313166
  },
  {
    "id": "1810.05952",
    "title": "Comparison-Based Convolutional Neural Networks for Cervical Cell/Clumps\n  Detection in the Limited Data Scenario",
    "abstract": "  Automated detection of cervical cancer cells or cell clumps has the potential\nto significantly reduce error rate and increase productivity in cervical cancer\nscreening. However, most traditional methods rely on the success of accurate\ncell segmentation and discriminative hand-crafted features extraction. Recently\nthere are emerging deep learning-based methods which train convolutional neural\nnetworks (CNN) to classify image patches, but they are computationally\nexpensive. In this paper we propose an efficient CNN-based object detection\nmethods for cervical cancer cells/clumps detection. Specifically, we utilize\nthe state-of-the-art two-stage object detection method, the Faster-RCNN with\nFeature Pyramid Network (FPN) as the baseline and propose a novel comparison\ndetector to deal with the limited data problem. The key idea is that classify\nthe proposals by comparing with the reference samples of each category in\nobject detection. In addition, we propose to learn the reference samples of the\nbackground from data instead of manually choosing them by some heuristic rules.\nExperimental results show that the proposed Comparison Detector yields\nsignificant improvement on the small dataset, achieving a mean Average\nPrecision (mAP) of 26.3% and an Average Recall (AR) of 35.7%, both improving\nabout 20 points compared to the baseline. Moreover, Comparison Detector\nimproved AR by 4.6 points and achieved marginally better performance in terms\nof mAP compared with baseline model when training on the medium dataset. Our\nmethod is promising for the development of automation-assisted cervical cancer\nscreening systems. Code is available at\nhttps://github.com/kuku-sichuan/ComparisonDetector.\n",
    "topics": "{'Object Detection': 0.99988234, 'Cell Segmentation': 0.99982774, 'Few-Shot Learning': 0.81156236}",
    "score": 0.7967112362
  },
  {
    "id": "2002.04098",
    "title": "Outlier Guided Optimization of Abdominal Segmentation",
    "abstract": "  Abdominal multi-organ segmentation of computed tomography (CT) images has\nbeen the subject of extensive research interest. It presents a substantial\nchallenge in medical image processing, as the shape and distribution of\nabdominal organs can vary greatly among the population and within an individual\nover time. While continuous integration of novel datasets into the training set\nprovides potential for better segmentation performance, collection of data at\nscale is not only costly, but also impractical in some contexts. Moreover, it\nremains unclear what marginal value additional data have to offer. Herein, we\npropose a single-pass active learning method through human quality assurance\n(QA). We built on a pre-trained 3D U-Net model for abdominal multi-organ\nsegmentation and augmented the dataset either with outlier data (e.g.,\nexemplars for which the baseline algorithm failed) or inliers (e.g., exemplars\nfor which the baseline algorithm worked). The new models were trained using the\naugmented datasets with 5-fold cross-validation (for outlier data) and withheld\noutlier samples (for inlier data). Manual labeling of outliers increased Dice\nscores with outliers by 0.130, compared to an increase of 0.067 with inliers\n(p<0.001, two-tailed paired t-test). By adding 5 to 37 inliers or outliers to\ntraining, we find that the marginal value of adding outliers is higher than\nthat of adding inliers. In summary, improvement on single-organ performance was\nobtained without diminishing multi-organ performance or significantly\nincreasing training time. Hence, identification and correction of baseline\nfailure cases present an effective and efficient method of selecting training\ndata to improve algorithm performance.\n",
    "topics": "{'Computed Tomography (CT)': 0.99972147, 'Active Learning': 0.9832267}",
    "score": 0.7966946716
  },
  {
    "id": "2009.14712",
    "title": "Deep Learning-based Pipeline for Module Power Prediction from EL\n  Measurements",
    "abstract": "  Automated inspection plays an important role in monitoring large-scale\nphotovoltaic power plants. Commonly, electroluminescense measurements are used\nto identify various types of defects on solar modules but have not been used to\ndetermine the power of a module. However, knowledge of the power at maximum\npower point is important as well, since drops in the power of a single module\ncan affect the performance of an entire string. By now, this is commonly\ndetermined by measurements that require to discontact or even dismount the\nmodule, rendering a regular inspection of individual modules infeasible. In\nthis work, we bridge the gap between electroluminescense measurements and the\npower determination of a module. We compile a large dataset of 719\nelectroluminescense measurementsof modules at various stages of degradation,\nespecially cell cracks and fractures, and the corresponding power at maximum\npower point. Here,we focus on inactive regions and cracks as the predominant\ntype of defect. We set up a baseline regression model to predict the power from\nelectroluminescense measurements with a mean absolute error of 9.0+/-3.7W\n(4.0+/-8.4%). Then, we show that deep-learning can be used to train a model\nthat performs significantly better (7.3+/-2.7W or 3.2+/-6.5%). With this work,\nwe aim to open a new research topic. Therefore, we publicly release the\ndataset, the code and trained models to empower other researchers to compare\nagainst our results. Finally, we present a thorough evaluation of certain\nboundary conditions like the dataset size and an automated preprocessing\npipeline for on-site measurements showing multiple modules at once.\n",
    "topics": "{}",
    "score": 0.7966555184
  },
  {
    "id": "1907.02882",
    "title": "Generating large labeled data sets for laparoscopic image processing\n  tasks using unpaired image-to-image translation",
    "abstract": "  In the medical domain, the lack of large training data sets and benchmarks is\noften a limiting factor for training deep neural networks. In contrast to\nexpensive manual labeling, computer simulations can generate large and fully\nlabeled data sets with a minimum of manual effort. However, models that are\ntrained on simulated data usually do not translate well to real scenarios. To\nbridge the domain gap between simulated and real laparoscopic images, we\nexploit recent advances in unpaired image-to-image translation. We extent an\nimage-to-image translation method to generate a diverse multitude of\nrealistically looking synthetic images based on images from a simple\nlaparoscopy simulation. By incorporating means to ensure that the image content\nis preserved during the translation process, we ensure that the labels given\nfor the simulated images remain valid for their realistically looking\ntranslations. This way, we are able to generate a large, fully labeled\nsynthetic data set of laparoscopic images with realistic appearance. We show\nthat this data set can be used to train models for the task of liver\nsegmentation of laparoscopic images. We achieve average dice scores of up to\n0.89 in some patients without manually labeling a single laparoscopic image and\nshow that using our synthetic data to pre-train models can greatly improve\ntheir performance. The synthetic data set will be made publicly available,\nfully labeled with segmentation maps, depth maps, normal maps, and positions of\ntools and camera (http://opencas.dkfz.de/image2image).\n",
    "topics": "{'Image-to-Image Translation': 1.0}",
    "score": 0.7965897916
  },
  {
    "id": "1305.4077",
    "title": "Indexing Medical Images based on Collaborative Experts Reports",
    "abstract": "  A patient is often willing to quickly get, from his physician, reliable\nanalysis and concise explanation according to provided linked medical images.\nThe fact of making choices individually by the patient's physician may lead to\nmalpractices and consequently generates unforeseeable damages. The Institute of\nMedicine of the National Sciences Academy(IMNAS) in USA published a study\nestimating that up to 98,000 hospital deathseach year can be attributed to\nmedical malpractice [1]. Moreover, physician, in charge of medical image\nanalysis, might be unavailable at the right time, which may complicate the\npatient's state. The goal of this paper is to provide to physicians and\npatients, a social network that permits to foster cooperation and to overcome\nthe problem of unavailability of doctors on site any time. Therefore, patients\ncan submit their medical images to be diagnosed and commented by several\nexperts instantly. Consequently, the need to process opinions and to extract\ninformation automatically from the proposed social network became a necessity\ndue to the huge number of comments expressing specialist's reviews. For this\nreason, we propose a kind of comments' summary keywords-based method which\nextracts the major current terms and relevant words existing on physicians'\nannotations. The extracted keywords will present a new and robust method for\nimage indexation. In fact, significant extracted terms will be used later to\nindex images in order to facilitate their discovery for any appropriate use. To\novercome this challenge, we propose our Terminology Extraction of Annotation\n(TEA) mixed approach which focuses on algorithms mainly based on statistical\nmethods and on external semantic resources.\n",
    "topics": "{}",
    "score": 0.7965014875
  },
  {
    "id": "2010.06693",
    "title": "Handwriting Quality Analysis using Online-Offline Models",
    "abstract": "  This work is part of an innovative e-learning project allowing the\ndevelopment of an advanced digital educational tool that provides feedback\nduring the process of learning handwriting for young school children (three to\neight years old). In this paper, we describe a new method for children\nhandwriting quality analysis. It automatically detects mistakes, gives\nreal-time on-line feedback for children's writing, and helps teachers\ncomprehend and evaluate children's writing skills. The proposed method adjudges\nfive main criteria shape, direction, stroke order, position respect to the\nreference lines, and kinematics of the trace. It analyzes the handwriting\nquality and automatically gives feedback based on the combination of three\nextracted models: Beta-Elliptic Model (BEM) using similarity detection (SD) and\ndissimilarity distance (DD) measure, Fourier Descriptor Model (FDM), and\nperceptive Convolutional Neural Network (CNN) with Support Vector Machine (SVM)\ncomparison engine. The originality of our work lies partly in the system\narchitecture which apprehends complementary dynamic, geometric, and visual\nrepresentation of the examined handwritten scripts and in the efficient\nselected features adapted to various handwriting styles and multiple script\nlanguages such as Arabic, Latin, digits, and symbol drawing. The application\noffers two interactive interfaces respectively dedicated to learners,\neducators, experts or teachers and allows them to adapt it easily to the\nspecificity of their disciples. The evaluation of our framework is enhanced by\na database collected in Tunisia primary school with 400 children. Experimental\nresults show the efficiency and robustness of our suggested framework that\nhelps teachers and children by offering positive feedback throughout the\nhandwriting learning process using tactile digital devices.\n",
    "topics": "{'Handwriting Recognition': 1.0}",
    "score": 0.7963814452
  },
  {
    "id": "1909.13834",
    "title": "Geometric Brain Surface Network For Brain Cortical Parcellation",
    "abstract": "  A large number of surface-based analyses on brain imaging data adopt some\nspecific brain atlases to better assess structural and functional changes in\none or more brain regions. In these analyses, it is necessary to obtain an\nanatomically correct surface parcellation scheme in an individual brain by\nreferring to the given atlas. Traditional ways to accomplish this goal are\nthrough a designed surface-based registration or hand-crafted surface features,\nalthough both of them are time-consuming. A recent deep learning approach\ndepends on a regular spherical parameterization of the mesh, which is\ncomputationally prohibitive in some cases and may also demand further\npost-processing to refine the network output. Therefore, an accurate and\nfully-automatic cortical surface parcellation scheme directly working on the\noriginal brain surfaces would be highly advantageous. In this study, we propose\nan end-to-end deep brain cortical parcellation network, called \\textbf{DBPN}.\nThrough intrinsic and extrinsic graph convolution kernels, DBPN dynamically\ndeciphers neighborhood graph topology around each vertex and encodes the\ndeciphered knowledge into node features. Eventually, a non-linear mapping\nbetween the node features and parcellation labels is constructed. Our model is\na two-stage deep network which contains a coarse parcellation network with a\nU-shape structure and a refinement network to fine-tune the coarse results. We\nevaluate our model in a large public dataset and our work achieves superior\nperformance than state-of-the-art baseline methods in both accuracy and\nefficiency\n",
    "topics": "{'Brain Segmentation': 0.37448442}",
    "score": 0.7963747953
  },
  {
    "id": "1904.08653",
    "title": "Fooling automated surveillance cameras: adversarial patches to attack\n  person detection",
    "abstract": "  Adversarial attacks on machine learning models have seen increasing interest\nin the past years. By making only subtle changes to the input of a\nconvolutional neural network, the output of the network can be swayed to output\na completely different result. The first attacks did this by changing pixel\nvalues of an input image slightly to fool a classifier to output the wrong\nclass. Other approaches have tried to learn \"patches\" that can be applied to an\nobject to fool detectors and classifiers. Some of these approaches have also\nshown that these attacks are feasible in the real-world, i.e. by modifying an\nobject and filming it with a video camera. However, all of these approaches\ntarget classes that contain almost no intra-class variety (e.g. stop signs).\nThe known structure of the object is then used to generate an adversarial patch\non top of it.\n  In this paper, we present an approach to generate adversarial patches to\ntargets with lots of intra-class variety, namely persons. The goal is to\ngenerate a patch that is able successfully hide a person from a person\ndetector. An attack that could for instance be used maliciously to circumvent\nsurveillance systems, intruders can sneak around undetected by holding a small\ncardboard plate in front of their body aimed towards the surveillance camera.\n  From our results we can see that our system is able significantly lower the\naccuracy of a person detector. Our approach also functions well in real-life\nscenarios where the patch is filmed by a camera. To the best of our knowledge\nwe are the first to attempt this kind of attack on targets with a high level of\nintra-class variety like persons.\n",
    "topics": "{'Human Detection': 0.99998796}",
    "score": 0.7963208634
  },
  {
    "id": "1709.07368",
    "title": "Multi-label Pixelwise Classification for Reconstruction of Large-scale\n  Urban Areas",
    "abstract": "  Object classification is one of the many holy grails in computer vision and\nas such has resulted in a very large number of algorithms being proposed\nalready. Specifically in recent years there has been considerable progress in\nthis area primarily due to the increased efficiency and accessibility of deep\nlearning techniques. In fact, for single-label object classification [i.e. only\none object present in the image] the state-of-the-art techniques employ deep\nneural networks and are reporting very close to human-like performance. There\nare specialized applications in which single-label object-level classification\nwill not suffice; for example in cases where the image contains multiple\nintertwined objects of different labels.\n  In this paper, we address the complex problem of multi-label pixelwise\nclassification. We present our distinct solution based on a convolutional\nneural network (CNN) for performing multi-label pixelwise classification and\nits application to large-scale urban reconstruction. A supervised learning\napproach is followed for training a 13-layer CNN using both LiDAR and satellite\nimages. An empirical study has been conducted to determine the hyperparameters\nwhich result in the optimal performance of the CNN. Scale invariance is\nintroduced by training the network on five different scales of the input and\nlabeled data. This results in six pixelwise classifications for each different\nscale. An SVM is then trained to map the six pixelwise classifications into a\nsingle-label. Lastly, we refine boundary pixel labels using graph-cuts for\nmaximum a-posteriori (MAP) estimation with Markov Random Field (MRF) priors.\nThe resulting pixelwise classification is then used to accurately extract and\nreconstruct the buildings in large-scale urban areas. The proposed approach has\nbeen extensively tested and the results are reported.\n",
    "topics": "{'Object Classification': 1.0}",
    "score": 0.7962157164
  },
  {
    "id": "1910.13141",
    "title": "Decomposable-Net: Scalable Low-Rank Compression for Neural Networks",
    "abstract": "  Compressing deep neural networks (DNNs) is important for real-world\napplications operating on resource-constrained devices. However, it is not\nstraightforward to change the model size (i.e., computational complexity) once\ntraining and compression are completed, calling for retraining to construct\nmodels suitable for different devices. In this paper, we propose a novel\nmethod, Decomposable-Net (the network decomposable in any size), which allows\nflexible changes to model size without retraining. We decompose weight matrices\nin the DNNs via singular value decomposition and adjust ranks according to the\ntarget model size. Unlike the existing methods, (1) we propose a learning\nmethod that explicitly minimizes losses for both of full-rank and low-rank\nnetworks, which is designed not only to maintain the performance of a full-rank\nnetwork but also to improve multiple low-rank networks in a single model. (2)\nWe also provide a mathematical analysis for the scalability of the\napproximation error with respect to the rank in each layer. Moreover, on the\nbasis of the analysis, (3) we introduce a simple criterion for rank selection\nthat effectively suppresses approximation error. In experiments on\nimage-classification tasks on CIFAR-10/100 and ImageNet datasets,\nDecomposable-Net yields favorable performance in a broader range of compressed\nmodels. In particular, Decomposable-Net achieves the top-1 accuracy of $73.2\\%$\nwith $0.27\\times$MACs on the ImageNet classification task with ResNet-50,\ncompared to low-rank tensor (Tucker) decomposition ($67.4\\% / 0.30\\times$) and\nuniversally slimmable networks ($70.6\\% / 0.26\\times$).\n",
    "topics": "{}",
    "score": 0.7961847123
  },
  {
    "id": "1803.04873",
    "title": "Using Convolutional Neural Networks for Determining Reticulocyte\n  Percentage in Cats",
    "abstract": "  Recent advances in artificial intelligence (AI), specifically in computer\nvision (CV) and deep learning (DL), have created opportunities for novel\nsystems in many fields. In the last few years, deep learning applications have\ndemonstrated impressive results not only in fields such as autonomous driving\nand robotics, but also in the field of medicine, where they have, in some\ncases, even exceeded human-level performance. However, despite the huge\npotential, adoption of deep learning-based methods is still slow in many areas,\nespecially in veterinary medicine, where we haven't been able to find any\nresearch papers using modern convolutional neural networks (CNNs) in medical\nimage processing. We believe that using deep learning-based medical imaging can\nenable more accurate, faster and less expensive diagnoses in veterinary\nmedicine. In order to do so, however, these methods have to be accessible to\neveryone in this field, not just to computer scientists. To show the potential\nof this technology, we present results on a real-world task in veterinary\nmedicine that is usually done manually: feline reticulocyte percentage. Using\nan open source Keras implementation of the Single-Shot MultiBox Detector (SSD)\nmodel architecture and training it on only 800 labeled images, we achieve an\naccuracy of 98.7% at predicting the correct number of aggregate reticulocytes\nin microscope images of cat blood smears. The main motivation behind this paper\nis to show not only that deep learning can approach or even exceed human-level\nperformance on a task like this, but also that anyone in the field can\nimplement it, even without a background in computer science.\n",
    "topics": "{'Autonomous Driving': 0.99843603}",
    "score": 0.7961718042
  },
  {
    "id": "2006.06624",
    "title": "SLIC-UAV: A Method for monitoring recovery in tropical restoration\n  projects through identification of signature species using UAVs",
    "abstract": "  Logged forests cover four million square kilometres of the tropics and\nrestoring these forests is essential if we are to avoid the worst impacts of\nclimate change, yet monitoring recovery is challenging. Tracking the abundance\nof visually identifiable, early-successional species enables successional\nstatus and thereby restoration progress to be evaluated. Here we present a new\npipeline, SLIC-UAV, for processing Unmanned Aerial Vehicle (UAV) imagery to map\nearly-successional species in tropical forests. The pipeline is novel because\nit comprises: (a) a time-efficient approach for labelling crowns from UAV\nimagery; (b) machine learning of species based on spectral and textural\nfeatures within individual tree crowns, and (c) automatic segmentation of\northomosaiced UAV imagery into 'superpixels', using Simple Linear Iterative\nClustering (SLIC). Creating superpixels reduces the dataset's dimensionality\nand focuses prediction onto clusters of pixels, greatly improving accuracy. To\ndemonstrate SLIC-UAV, support vector machines and random forests were used to\npredict the species of hand-labelled crowns in a restoration concession in\nIndonesia. Random forests were most accurate at discriminating species for\nwhole crowns, with accuracy ranging from 79.3% when mapping five common\nspecies, to 90.5% when mapping the three most visually-distinctive species. In\ncontrast, support vector machines proved better for labelling automatically\nsegmented superpixels, with accuracy ranging from 74.3% to 91.7% for the same\nspecies. Models were extended to map species across 100 hectares of forest. The\nstudy demonstrates the power of SLIC-UAV for mapping characteristic\nearly-successional tree species as an indicator of successional stage within\ntropical forest restoration areas. Continued effort is needed to develop\neasy-to-implement and low-cost technology to improve the affordability of\nproject management.\n",
    "topics": "{}",
    "score": 0.7960252059
  },
  {
    "id": "2008.01466",
    "title": "Taking Notes on the Fly Helps BERT Pre-training",
    "abstract": "  How to make unsupervised language pre-training more efficient and less\nresource-intensive is an important research direction in NLP. In this paper, we\nfocus on improving the efficiency of language pre-training methods through\nproviding better data utilization. It is well-known that in language data\ncorpus, words follow a heavy-tail distribution. A large proportion of words\nappear only very few times and the embeddings of rare words are usually poorly\noptimized. We argue that such embeddings carry inadequate semantic signals.\nThey could make the data utilization inefficient and slow down the pre-training\nof the entire model. To solve this problem, we propose Taking Notes on the Fly\n(TNF). TNF takes notes for rare words on the fly during pre-training to help\nthe model understand them when they occur next time. Specifically, TNF\nmaintains a note dictionary and saves a rare word's context information in it\nas notes when the rare word occurs in a sentence. When the same rare word\noccurs again in training, TNF employs the note information saved beforehand to\nenhance the semantics of the current sentence. By doing so, TNF provides a\nbetter data utilization since cross-sentence information is employed to cover\nthe inadequate semantics caused by rare words in the sentences. Experimental\nresults show that TNF significantly expedite the BERT pre-training and improve\nthe model's performance on downstream tasks. TNF's training time is $60\\%$ less\nthan BERT when reaching the same performance. When trained with same number of\niterations, TNF significantly outperforms BERT on most of downstream tasks and\nthe average GLUE score.\n",
    "topics": "{'Unsupervised Pre-training': 0.36774063}",
    "score": 0.79599555
  },
  {
    "id": "2004.10404",
    "title": "Logical Natural Language Generation from Open-Domain Tables",
    "abstract": "  Neural natural language generation (NLG) models have recently shown\nremarkable progress in fluency and coherence. However, existing studies on\nneural NLG are primarily focused on surface-level realizations with limited\nemphasis on logical inference, an important aspect of human thinking and\nlanguage. In this paper, we suggest a new NLG task where a model is tasked with\ngenerating natural language statements that can be \\emph{logically entailed} by\nthe facts in an open-domain semi-structured table. To facilitate the study of\nthe proposed logical NLG problem, we use the existing TabFact dataset\n\\cite{chen2019tabfact} featured with a wide range of logical/symbolic\ninferences as our testbed, and propose new automatic metrics to evaluate the\nfidelity of generation models w.r.t.\\ logical inference. The new task poses\nchallenges to the existing monotonic generation frameworks due to the mismatch\nbetween sequence order and logical order. In our experiments, we\ncomprehensively survey different generation architectures (LSTM, Transformer,\nPre-Trained LM) trained with different algorithms (RL, Adversarial Training,\nCoarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained\nLM can significantly boost both the fluency and logical fidelity metrics, 2) RL\nand Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine\ngeneration can help partially alleviate the fidelity issue while maintaining\nhigh language fluency. The code and data are available at\n\\url{https://github.com/wenhuchen/LogicNLG}.\n",
    "topics": "{'Text Generation': 0.9999502}",
    "score": 0.7959908082
  },
  {
    "id": "1703.02589",
    "title": "Texture Classification of MR Images of the Brain in ALS using CoHOG",
    "abstract": "  Texture analysis is a well-known research topic in computer vision and image\nprocessing and has many applications. Gradient-based texture methods have\nbecome popular in classification problems. For the first time we extend a\nwell-known gradient-based method, Co-occurrence Histograms of Oriented\nGradients (CoHOG) to extract texture features from 2D Magnetic Resonance Images\n(MRI). Unlike the original CoHOG method, we use the whole image instead of\nsub-regions for feature calculation. Also, we use a larger neighborhood size.\nGradient orientations of the image pixels are calculated using Sobel, Gaussian\nDerivative (GD) and Local Frequency Descriptor Gradient (LFDG) operators. The\nextracted feature vector size is very large and classification using a large\nnumber of similar features does not provide the best results. In our proposed\nmethod, for the first time to our best knowledge, only a minimum number of\nsignificant features are selected using area under the receiver operator\ncharacteristic (ROC) curve (AUC) thresholds with <= 0.01. In this paper, we\napply the proposed method to classify Amyotrophic Lateral Sclerosis (ALS)\npatients from the controls. It is observed that selected texture features from\ndownsampled images are significantly different between patients and controls.\nThese features are used in a linear support vector machine (SVM) classifier to\ndetermine the classification accuracy. Optimal sensitivity and specificity are\nalso calculated. Three different cohort datasets are used in the experiments.\nThe performance of the proposed method using three gradient operators and two\ndifferent neighborhood sizes is analyzed. Region based analysis is performed to\ndemonstrate that significant changes between patients and controls are limited\nto the motor cortex.\n",
    "topics": "{'Texture Classification': 0.9999993}",
    "score": 0.7959482129
  },
  {
    "id": "2004.07864",
    "title": "A Neural Architecture Search based Framework for Liquid State Machine\n  Design",
    "abstract": "  Liquid State Machine (LSM), also known as the recurrent version of Spiking\nNeural Networks (SNN), has attracted great research interests thanks to its\nhigh computational power, biological plausibility from the brain, simple\nstructure and low training complexity. By exploring the design space in network\narchitectures and parameters, recent works have demonstrated great potential\nfor improving the accuracy of LSM model with low complexity. However, these\nworks are based on manually-defined network architectures or predefined\nparameters. Considering the diversity and uniqueness of brain structure, the\ndesign of LSM model should be explored in the largest search space possible. In\nthis paper, we propose a Neural Architecture Search (NAS) based framework to\nexplore both architecture and parameter design space for automatic\ndataset-oriented LSM model. To handle the exponentially-increased design space,\nwe adopt a three-step search for LSM, including multi-liquid architecture\nsearch, variation on the number of neurons and parameters search such as\npercentage connectivity and excitatory neuron ratio within each liquid.\nBesides, we propose to use Simulated Annealing (SA) algorithm to implement the\nthree-step heuristic search. Three datasets, including image dataset of MNIST\nand NMNIST and speech dataset of FSDD, are used to test the effectiveness of\nour proposed framework. Simulation results show that our proposed framework can\nproduce the dataset-oriented optimal LSM models with high accuracy and low\ncomplexity. The best classification accuracy on the three datasets is 93.2%,\n92.5% and 84% respectively with only 1000 spiking neurons, and the network\nconnections can be averagely reduced by 61.4% compared with a single LSM.\nMoreover, we find that the total quantity of neurons in optimal LSM models on\nthree datasets can be further reduced by 20% with only about 0.5% accuracy\nloss.\n",
    "topics": "{'Neural Architecture Search': 1.0}",
    "score": 0.7957032747
  },
  {
    "id": "1803.06905",
    "title": "TBD: Benchmarking and Analyzing Deep Neural Network Training",
    "abstract": "  The recent popularity of deep neural networks (DNNs) has generated a lot of\nresearch interest in performing DNN-related computation efficiently. However,\nthe primary focus is usually very narrow and limited to (i) inference -- i.e.\nhow to efficiently execute already trained models and (ii) image classification\nnetworks as the primary benchmark for evaluation.\n  Our primary goal in this work is to break this myopic view by (i) proposing a\nnew benchmark for DNN training, called TBD (TBD is short for Training Benchmark\nfor DNNs), that uses a representative set of DNN models that cover a wide range\nof machine learning applications: image classification, machine translation,\nspeech recognition, object detection, adversarial networks, reinforcement\nlearning, and (ii) by performing an extensive performance analysis of training\nthese different applications on three major deep learning frameworks\n(TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU,\nmulti-GPU, and multi-machine). TBD currently covers six major application\ndomains and eight different state-of-the-art models.\n  We present a new toolchain for performance analysis for these models that\ncombines the targeted usage of existing performance analysis tools, careful\nselection of new and existing metrics and methodologies to analyze the results,\nand utilization of domain specific characteristics of DNN training. We also\nbuild a new set of tools for memory profiling in all three major frameworks;\nmuch needed tools that can finally shed some light on precisely how much memory\nis consumed by different data structures (weights, activations, gradients,\nworkspace) in DNN training. By using our tools and methodologies, we make\nseveral important observations and recommendations on where the future research\nand optimization of DNN training should be focused.\n",
    "topics": "{'Image Classification': 0.9968305, 'Object Detection': 0.9857917, 'Speech Recognition': 0.9149569, 'Machine Translation': 0.6809474}",
    "score": 0.7956570667
  },
  {
    "id": "1902.08906",
    "title": "On the Use of Emojis to Train Emotion Classifiers",
    "abstract": "  Nowadays, the automatic detection of emotions is employed by many\napplications in different fields like security informatics, e-learning, humor\ndetection, targeted advertising, etc. Many of these applications focus on\nsocial media and treat this problem as a classification problem, which requires\npreparing training data. The typical method for annotating the training data by\nhuman experts is considered time consuming, labor intensive and sometimes prone\nto error. Moreover, such an approach is not easily extensible to new\ndomains/languages since such extensions require annotating new training data.\nIn this study, we propose a distant supervised learning approach where the\ntraining sentences are automatically annotated based on the emojis they have.\nSuch training data would be very cheap to produce compared with the manually\ncreated training data, thus, much larger training data can be easily obtained.\nOn the other hand, this training data would naturally have lower quality as it\nmay contain some errors in the annotation. Nonetheless, we experimentally show\nthat training classifiers on cheap, large and possibly erroneous data annotated\nusing this approach leads to more accurate results compared with training the\nsame classifiers on the more expensive, much smaller and error-free manually\nannotated training data. Our experiments are conducted on an in-house dataset\nof emotional Arabic tweets and the classifiers we consider are: Support Vector\nMachine (SVM), Multinomial Naive Bayes (MNB) and Random Forest (RF). In\naddition to experimenting with single classifiers, we also consider using an\nensemble of classifiers. The results show that using an automatically annotated\ntraining data (that is only one order of magnitude larger than the manually\nannotated one) gives better results in almost all settings considered.\n",
    "topics": "{}",
    "score": 0.7956546813
  },
  {
    "id": "1903.04207",
    "title": "Distributed deep learning for robust multi-site segmentation of CT\n  imaging after traumatic brain injury",
    "abstract": "  Machine learning models are becoming commonplace in the domain of medical\nimaging, and with these methods comes an ever-increasing need for more data.\nHowever, to preserve patient anonymity it is frequently impractical or\nprohibited to transfer protected health information (PHI) between institutions.\nAdditionally, due to the nature of some studies, there may not be a large\npublic dataset available on which to train models. To address this conundrum,\nwe analyze the efficacy of transferring the model itself in lieu of data\nbetween different sites. By doing so we accomplish two goals: 1) the model\ngains access to training on a larger dataset that it could not normally obtain\nand 2) the model better generalizes, having trained on data from separate\nlocations. In this paper, we implement multi-site learning with disparate\ndatasets from the National Institutes of Health (NIH) and Vanderbilt University\nMedical Center (VUMC) without compromising PHI. Three neural networks are\ntrained to convergence on a computed tomography (CT) brain hematoma\nsegmentation task: one only with NIH data,one only with VUMC data, and one\nmulti-site model alternating between NIH and VUMC data. Resultant lesion masks\nwith the multi-site model attain an average Dice similarity coefficient of 0.64\nand the automatically segmented hematoma volumes correlate to those done\nmanually with a Pearson correlation coefficient of 0.87,corresponding to an 8%\nand 5% improvement, respectively, over the single-site model counterparts.\n",
    "topics": "{'Computed Tomography (CT)': 0.9999492}",
    "score": 0.7955955438
  },
  {
    "id": "2004.04394",
    "title": "Hierarchical Group Sparse Regularization for Deep Convolutional Neural\n  Networks",
    "abstract": "  In a deep neural network (DNN), the number of the parameters is usually huge\nto get high learning performances. For that reason, it costs a lot of memory\nand substantial computational resources, and also causes overfitting. It is\nknown that some parameters are redundant and can be removed from the network\nwithout decreasing performance. Many sparse regularization criteria have been\nproposed to solve this problem. In a convolutional neural network (CNN), group\nsparse regularizations are often used to remove unnecessary subsets of the\nweights, such as filters or channels. When we apply a group sparse\nregularization for the weights connected to a neuron as a group, each\nconvolution filter is not treated as a target group in the regularization. In\nthis paper, we introduce the concept of hierarchical grouping to solve this\nproblem, and we propose several hierarchical group sparse regularization\ncriteria for CNNs. Our proposed the hierarchical group sparse regularization\ncan treat the weight for the input-neuron or the output-neuron as a group and\nconvolutional filter as a group in the same group to prune the unnecessary\nsubsets of weights. As a result, we can prune the weights more adequately\ndepending on the structure of the network and the number of channels keeping\nhigh performance. In the experiment, we investigate the effectiveness of the\nproposed sparse regularizations through intensive comparison experiments on\npublic datasets with several network architectures. Code is available on\nGitHub: \"https://github.com/K-Mitsuno/hierarchical-group-sparse-regularization\"\n",
    "topics": "{}",
    "score": 0.7955506391
  },
  {
    "id": "2005.11930",
    "title": "A Bayesian-inspired, deep learning, semi-supervised domain adaptation\n  technique for land cover mapping",
    "abstract": "  Land cover maps are a vital input variable to many types of environmental\nresearch and management. While they can be produced automatically by machine\nlearning techniques, these techniques require substantial training data to\nachieve high levels of accuracy, which are not always available. One technique\nresearchers use when labelled training data are scarce is domain adaptation\n(DA) -- where data from an alternate region, known as the source domain, are\nused to train a classifier and this model is adapted to map the study region,\nor target domain. The scenario we address in this paper is known as\nsemi-supervised DA, where some labelled samples are available in the target\ndomain. In this paper we present Sourcerer, a Bayesian-inspired, deep\nlearning-based, semi-supervised DA technique for producing land cover maps from\nSITS data. The technique takes a convolutional neural network trained on a\nsource domain and then trains further on the available target domain with a\nnovel regularizer applied to the model weights. The regularizer adjusts the\ndegree to which the model is modified to fit the target data, limiting the\ndegree of change when the target data are few in number and increasing it as\ntarget data quantity increases. Our experiments on Sentinel-2 time series\nimages compare Sourcerer with two state-of-the-art semi-supervised domain\nadaptation techniques and four baseline models. We show that on two different\nsource-target domain pairings Sourcerer outperforms all other methods for any\nquantity of labelled target data available. In fact, the results on the more\ndifficult target domain show that the starting accuracy of Sourcerer (when no\nlabelled target data are available), 74.2%, is greater than the next-best\nstate-of-the-art method trained on 20,000 labelled target instances.\n",
    "topics": "{'Domain Adaptation': 0.99968493, 'Time Series': 0.59368056}",
    "score": 0.7955285283
  },
  {
    "id": "2006.04388",
    "title": "Generalized Focal Loss: Learning Qualified and Distributed Bounding\n  Boxes for Dense Object Detection",
    "abstract": "  One-stage detector basically formulates object detection as dense\nclassification and localization. The classification is usually optimized by\nFocal Loss and the box location is commonly learned under Dirac delta\ndistribution. A recent trend for one-stage detectors is to introduce an\nindividual prediction branch to estimate the quality of localization, where the\npredicted quality facilitates the classification to improve detection\nperformance. This paper delves into the representations of the above three\nfundamental elements: quality estimation, classification and localization. Two\nproblems are discovered in existing practices, including (1) the inconsistent\nusage of the quality estimation and classification between training and\ninference and (2) the inflexible Dirac delta distribution for localization when\nthere is ambiguity and uncertainty in complex scenes. To address the problems,\nwe design new representations for these elements. Specifically, we merge the\nquality estimation into the class prediction vector to form a joint\nrepresentation of localization quality and classification, and use a vector to\nrepresent arbitrary distribution of box locations. The improved representations\neliminate the inconsistency risk and accurately depict the flexible\ndistribution in real data, but contain continuous labels, which is beyond the\nscope of Focal Loss. We then propose Generalized Focal Loss (GFL) that\ngeneralizes Focal Loss from its discrete form to the continuous version for\nsuccessful optimization. On COCO test-dev, GFL achieves 45.0\\% AP using\nResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\\%) and ATSS\n(43.6\\%) with higher or comparable inference speed, under the same backbone and\ntraining settings. Notably, our best model can achieve a single-model\nsingle-scale AP of 48.2\\%, at 10 FPS on a single 2080Ti GPU. Code and models\nare available at https://github.com/implus/GFocal.\n",
    "topics": "{'Object Detection': 0.9991678}",
    "score": 0.7954463175
  },
  {
    "id": "1808.10564",
    "title": "Multi-Cell Multi-Task Convolutional Neural Networks for Diabetic\n  Retinopathy Grading",
    "abstract": "  Diabetic Retinopathy (DR) is a non-negligible eye disease among patients with\nDiabetes Mellitus, and automatic retinal image analysis algorithm for the DR\nscreening is in high demand. Considering the resolution of retinal image is\nvery high, where small pathological tissues can be detected only with large\nresolution image and large local receptive field are required to identify those\nlate stage disease, but directly training a neural network with very deep\narchitecture and high resolution image is both time computational expensive and\ndifficult because of gradient vanishing/exploding problem, we propose a\n\\textbf{Multi-Cell} architecture which gradually increases the depth of deep\nneural network and the resolution of input image, which both boosts the\ntraining time but also improves the classification accuracy. Further,\nconsidering the different stages of DR actually progress gradually, which means\nthe labels of different stages are related. To considering the relationships of\nimages with different stages, we propose a \\textbf{Multi-Task} learning\nstrategy which predicts the label with both classification and regression.\nExperimental results on the Kaggle dataset show that our method achieves a\nKappa of 0.841 on test set which is the 4-th rank of all state-of-the-arts\nmethods. Further, our Multi-Cell Multi-Task Convolutional Neural Networks\n(M$^2$CNN) solution is a general framework, which can be readily integrated\nwith many other deep neural network architectures.\n",
    "topics": "{'Multi-Task Learning': 0.9336581}",
    "score": 0.7954178313
  },
  {
    "id": "1809.07491",
    "title": "OxIOD: The Dataset for Deep Inertial Odometry",
    "abstract": "  Advances in micro-electro-mechanical (MEMS) techniques enable inertial\nmeasurements units (IMUs) to be small, cheap, energy efficient, and widely used\nin smartphones, robots, and drones. Exploiting inertial data for accurate and\nreliable navigation and localization has attracted significant research and\nindustrial interest, as IMU measurements are completely ego-centric and\ngenerally environment agnostic. Recent studies have shown that the notorious\nissue of drift can be significantly alleviated by using deep neural networks\n(DNNs), e.g. IONet. However, the lack of sufficient labelled data for training\nand testing various architectures limits the proliferation of adopting DNNs in\nIMU-based tasks. In this paper, we propose and release the Oxford Inertial\nOdometry Dataset (OxIOD), a first-of-its-kind data collection for\ninertial-odometry research, with all sequences having ground-truth labels. Our\ndataset contains 158 sequences totalling more than 42 km in total distance,\nmuch larger than previous inertial datasets. Another notable feature of this\ndataset lies in its diversity, which can reflect the complex motions of\nphone-based IMUs in various everyday usage. The measurements were collected\nwith four different attachments (handheld, in the pocket, in the handbag and on\nthe trolley), four motion modes (halting, walking slowly, walking normally, and\nrunning), five different users, four types of off-the-shelf consumer phones,\nand large-scale localization from office buildings. Deep inertial tracking\nexperiments were conducted to show the effectiveness of our dataset in training\ndeep neural network models and evaluate learning-based and model-based\nalgorithms. The OxIOD Dataset is available at: http://deepio.cs.ox.ac.uk\n",
    "topics": "{}",
    "score": 0.7954176217
  },
  {
    "id": "1901.03107",
    "title": "Cricket stroke extraction: Towards creation of a large-scale cricket\n  actions dataset",
    "abstract": "  In this paper, we deal with the problem of temporal action localization for a\nlarge-scale untrimmed cricket videos dataset. Our action of interest for\ncricket videos is a cricket stroke played by a batsman, which is, usually,\ncovered by cameras placed at the stands of the cricket ground at both ends of\nthe cricket pitch. After applying a sequence of preprocessing steps, we have\n~73 million frames for 1110 videos in the dataset at constant frame rate and\nresolution. The method of localization is a generalized one which applies a\ntrained random forest model for CUTs detection(using summed up grayscale\nhistogram difference features) and two linear SVM camera models(CAM1 and CAM2)\nfor first frame detection, trained on HOG features of CAM1 and CAM2 video\nshots. CAM1 and CAM2 are assumed to be part of the cricket stroke. At the\npredicted boundary positions, the HOG features of the first frames are computed\nand a simple algorithm was used to combine the positively predicted camera\nshots. In order to make the process as generic as possible, we did not consider\nany domain specific knowledge, such as tracking or specific shape and motion\nfeatures.\n  The detailed analysis of our methodology is provided along with the metrics\nused for evaluation of individual models, and the final predicted segments. We\nachieved a weighted mean TIoU of 0.5097 over a small sample of the test set.\n",
    "topics": "{'Action Localization': 0.99999976, 'Temporal Action Localization': 0.59867746}",
    "score": 0.7953981759
  },
  {
    "id": "1912.03817",
    "title": "Machine Unlearning",
    "abstract": "  Once users have shared their data online, it is generally difficult for them\nto revoke access and ask for the data to be deleted. Machine learning (ML)\nexacerbates this problem because any model trained with said data may have\nmemorized it, putting users at risk of a successful privacy attack exposing\ntheir information. Yet, having models unlearn is notoriously difficult. We\nintroduce SISA training, a framework that expedites the unlearning process by\nstrategically limiting the influence of a data point in the training procedure.\nWhile our framework is applicable to any learning algorithm, it is designed to\nachieve the largest improvements for stateful algorithms like stochastic\ngradient descent for deep neural networks. SISA training reduces the\ncomputational overhead associated with unlearning, even in the worst-case\nsetting where unlearning requests are made uniformly across the training set.\nIn some cases, the service provider may have a prior on the distribution of\nunlearning requests that will be issued by users. We may take this prior into\naccount to partition and order data accordingly, and further decrease overhead\nfrom unlearning. Our evaluation spans several datasets from different domains,\nwith corresponding motivations for unlearning. Under no distributional\nassumptions, for simple learning tasks, we observe that SISA training improves\ntime to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the\nSVHN dataset, over retraining from scratch. SISA training also provides a\nspeed-up of 1.36x in retraining for complex learning tasks such as ImageNet\nclassification; aided by transfer learning, this results in a small degradation\nin accuracy. Our work contributes to practical data governance in machine\nunlearning.\n",
    "topics": "{'Transfer Learning': 0.97288835}",
    "score": 0.7953883991
  }
]
